

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="发言人   00:00Well, good afternoon, everybody. Welcome, good to see you. So this week, we’re going to study how to incorporate concurrency into programs.下午好，大家。欢迎，很高兴见到你。所以这周，我们将学习如何将并发整合到程序中。 发言人   00:1">
<meta property="og:type" content="article">
<meta property="og:title" content="深入理解计算机系统 024-Concurrent Programming">
<meta property="og:url" content="http://example.com/2025/10/12/15213-024/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="发言人   00:00Well, good afternoon, everybody. Welcome, good to see you. So this week, we’re going to study how to incorporate concurrency into programs.下午好，大家。欢迎，很高兴见到你。所以这周，我们将学习如何将并发整合到程序中。 发言人   00:1">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-12T02:00:23.000Z">
<meta property="article:modified_time" content="2025-10-19T11:16:51.139Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>深入理解计算机系统 024-Concurrent Programming - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="深入理解计算机系统 024-Concurrent Programming"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-12 10:00" pubdate>
          2025年10月12日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          15k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          122 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">深入理解计算机系统 024-Concurrent Programming</h1>
            
            
              <div class="markdown-body">
                
                <p>发言人   00:00<br>Well, good afternoon, everybody. Welcome, good to see you. So this week, we’re going to study how to incorporate concurrency into programs.<br>下午好，大家。欢迎，很高兴见到你。所以这周，我们将学习如何将并发整合到程序中。</p>
<p>发言人   00:14<br>We’ve seen concurrency before in the form of processes, exception handlers. And in the case of processes, we used processes as a mechanism to run multiple independent application programs. Now, but you could concurrency also exists in application programs.<br>我们之前已经看到并发以进程、异常处理程序的形式出现。在进程的情况下，我们使用进程作为运行多个独立应用程序的机制。现在，并发也存在于应用程序中。</p>
<p>发言人   00:40<br>Now, we’ve seen a little bit of this when we study signal handlers. So a signal handler is a concurrent flow that runs concurrently with your main application program. And we’ve we’ve seen how some of the difficulties that can arise, when we introduce concurrency in our programs. So even with something like a signal handler, which isn’t doing very much, it’s very hard for us to reason about this kind of thing where we have two concurrent flows running at the same time. Like our brains just tend to be kind of sequential. We want to think about things happening one after the other, and it’s much easier for us to reason about that reasoning about multiple things happening at the same time really causes problems, and that the fundamental reason is that to really reason about, say, 2, 2 concurrent flows, we have to. Account for all of the possible interleavings of those flows, and that grows exponentially with the number of flows. So you saw this with your signal handlers.<br>现在，当我们研究信号处理程序时，我们已经看到了一些这种情况。因此，信号处理程序是与主应用程序同时运行的并发流。我们已经看到了在我们的程序中引入并发时可能出现的一些困难。因此，即使使用像信号处理程序这样的东西，它也没有做太多事情，我们也很难推断出这种同时运行两个并发流的情况。就像我们的大脑往往是有顺序的。我们想要思考一个接一个发生的事情，对我们来说，推理同时发生多个事情确实会导致问题要容易得多，而根本原因是要真正推理，比如2，2个并发流，我们必须这样做。考虑到这些流所有可能的交织，并且随着流数量呈指数增长。所以你在你的信号处理程序中看到了这一点。</p>
<p>发言人   02:00<br>When you did shell La, you had two concurrent flows, the main program in, and you signal handler, both accessing a shared resource in the form of a jobs list. And you have to be very careful to prevent an interleaving where that data structure was being referenced in an inconsistent state. So what we’re going to do this week and into next week is we’re going to look at that kind of application level concurrency, but in a principled in a more principled way than we encountered with signal handlers.<br>当你使用shell La时，你有两个并发流，一个是主程序，一个是信号处理程序，都以作业列表的形式访问共享资源。而且你必须非常小心地防止数据结构在不一致状态下被引用的交错。所以我们本周和下周要做的是，我们将以比我们遇到的信号处理程序更原则性的方式来研究这种应用程序级别的并发性。</p>
<p>发言人   02:45<br>So as soon as you have multiple flows, accessing shared resources, all kinds of bad things can happen in your program. And these have been. These bad, these problems that occur have been objects of study and computer science for decades. But the kinds of things that can happen are races, which we’ve seen when we did the shell lab, where the outcome, whether a good or bad outcome, depends on some arbitrary scheduling decision. One of the races we saw in the case of a shell was the case where that a child, just because of a scheduling decision by the colonel, runs and finishes before the parent has a chance to add that child to the job list. That’s a classic example of a race. Similarly, if you have two people that are accessing a reservation system on a for an airline who get if they both access at the same time, who actually gets a seat? Just depends on various scheduling decisions that are going on in the reservation system.<br>因此，一旦您有多个流程访问共享资源，您的程序中就可能发生各种不良事件。这些已经存在了。几十年来，这些不好的问题一直是计算机科学的研究对象。但是可能发生的事情是种族竞赛，我们在做shell实验室时已经看到过这种情况，无论结果是好是坏，都取决于一些任意的调度决定。我们在shell的情况下看到的一种比赛是，一个孩子仅仅因为上校的安排决定，在父母有机会将该孩子添加到作业列表之前奔跑并完成的情况。这是种族的典型例子。类似地，如果有两个人正在访问一家航空公司的预订系统，如果他们同时访问，谁会得到一个座位？这取决于预订系统中进行的各种调度决策。</p>
<p>发言人   04:02<br>Another kind of a problem that occurs is deadlock. So a deadlock is a condition that exists where you have multiple flows waiting for an event that will never occur. So using printf in a signal handler is an example of this kind of problem or introduces the potential for that kind of problem.<br>另一种出现的问题是死锁。因此，死锁是一种存在的条件，其中有多个流等待永远不会发生的事件。因此，在信号处理程序中使用printf是这种问题的一个例子，或者引入了这种问题的潜力。</p>
<p>发言人   04:26<br>So in your main routine, you execute a print f, and that printf acquires a lock on some system resource. I think it’s a terminal lock. And then after that main print F acquires that lock, it gets interrupted by a signal handler. And now the signal handler, if it does a print f, that printf will try to acquire that lock, but it won’t be able to get it because the printf and main routine has it. So now your signal hand the printf and the signal handler is waiting for an event that will never occur. It’s waiting for that lock to be released. And it will never occur because the printf and the main routine can’t release the lock until the signal handler returns, And the signal handler can’t acquire the lock until the printf in the main routine term. So that’s a classic example of deadlock.<br>因此，在您的主例程中，您执行一个print f，该printf将获取对某些系统资源的锁定。我想这是终端锁。然后在主打印F获取该锁之后，它被信号处理程序中断。现在信号处理程序，如果它执行了print f，则printf将尝试获取该锁，但无法获取它，因为printf和main例程都有它。现在您的信号传递printf，信号处理程序正在等待一个永远不会发生的事件。它正在等待锁被释放。并且它永远不会发生，因为printf和main例程在信号处理程序返回之前无法释放锁，并且信号处理程序在main例程项中的printf之前无法获取锁。这是死锁的典型例子。</p>
<p>发言人   05:28<br>Another more from real life that imagine that all drivers follow the rules very precisely. And the rule for a four way stop is that whoever gets there first gets to go first. If four cars arrive at the intersection exactly the same time, then you have a deadlock. You have none of the drivers was first, so none of the drivers goes. And so they’re all waiting for a condition that’ll never occur.<br>另一个来自现实生活的想象是所有司机都非常精确地遵守规则。四路停车的规则是，谁先到，谁就先走。如果四辆车完全同时到达十字路口，那么你就会陷入僵局。你没有一个司机是第一个，所以没有一个司机去。所以他们都在等待一个永远不会发生的条件。</p>
<p>发言人   06:04<br>And then other classical things that can go wrong are things like liveness, starvation, fairness. So this starvation occurs when you’re trying to do something, but you fail to make progress because somebody else keeps getting all the work. So if you had two processes and the kernel always scheduled, process A instead of process B, process B, that would be an example of process B being starved out because of an improper scheduling decision. And then we would say that that scheduling policy to always schedule B is unfair. So it doesn’t have this, this property of fairness where every entity in the system gets, it’s sort of a reasonable chunk of the processor. So like I said, I mean, this concurrency has been studied for years. It’s a very deep, difficult topic because of this, because of this sort of exponential explosion in the number of interleavings. So we’re not going to cover all of them, but we will cover some so you get a reasonable idea.<br>然后其他经典的事情可能出错，比如活力、饥饿、公平。因此，当你试图做某事，但由于其他人一直在做所有的工作而未能取得进展时，这种饥饿就会发生。因此，如果您有两个进程，并且内核始终调度，则进程A而不是进程B，进程B，这将是由于不正确的调度决策而导致进程B被耗尽的一个例子。然后我们会说总是调度B的调度策略是不公平的。所以它没有这个，系统中的每个实体都得到的公平性，它是处理器的一个合理的块。就像我说的，我的意思是，这种并发性已经被研究多年了。这是一个非常深刻、困难的话题，因为交织的数量呈指数级增长。所以我们不会涵盖所有这些，但我们会涵盖一些，以便您得到一个合理的想法。</p>
<p>发言人   07:16<br>How to incorporate concurrency in your programs? For our study of application level concurrency, we’re going to use servers as a motivating example. And the reason is that you cannot write a correct server without using concurrency. So it’s a really good. Motivation, and here’s the reason.<br>如何在你的程序中包含并发性？对于我们对应用级并发的研究，我们将使用服务器作为激励示例。原因是不使用并发性就无法编写正确的服务器。所以这真的很好。动机，这就是原因。</p>
<p>发言人   07:42<br>So, so far, we’ve looked at servers that are iterative. So they only process request from one client at a time. And once they finish processing a request from a client, then they go on to the next client so. So like with our iterative echo server, you can see each of these clients makes a connection request. Then it writes a line of text to the server, and it waits for the server to echo that back. And in this case, this simple case, then it just closes and the server waits for a connection request and accept, and then waits, and then. Once it accepts that connection request, it reads and waits for that client to write something to the connection, and then it echoes it back. And then it waits for the next line until.<br>到目前为止，我们已经研究了迭代的服务器。所以他们一次只处理来自一个客户端的请求。一旦他们完成处理来自客户端的请求，然后他们就会转到下一个客户端。就像我们的迭代回声服务器一样，您可以看到每个客户端都发出连接请求。然后，它向服务器写入一行文本，并等待服务器将其回显。在这种简单情况下，它只是关闭，服务器等待连接请求并接受，然后等待，然后等待。一旦它接受了该连接请求，它就会读取并等待客户端向连接中写入内容，然后将其回显。然后它等待下一行，直到。</p>
<p>发言人   08:52<br>The client closes that connection, and then the server closes its connection. And then only then does it do another step to wait for the next connection request. So in this example, client two is also making a connection request, but it never runs. It has to wait until the server actually echoes back the response.<br>客户端关闭该连接，然后服务器关闭其连接。然后才执行另一个步骤以等待下一个连接请求。因此，在这个例子中，客户端2也在发出连接请求，但它从不运行。它必须等待服务器实际回显响应。</p>
<p>发言人   09:20<br>Now, there’s a little subtlety here that and where exactly this client waits. So the semantics of kine, you would think that k would block until the connection was established. But actually, if you try this out, it turns out that connect actually initiates the connection process inside the kernel, but then it returns before the connection has been established, and then it does it right. And that right also returns immediately. So it doesn’t wait until the server reads that the string that was written, and it doesn’t block until it calls the read function waiting for the echoed response from the server. So it actually, it doesn’t block until it hits this read, and then it spends all it weights, weights, weights, weights, and finally the server accepts the connection request and then write, writes the echoes, the string back to the client.<br>现在，这里有一点微妙之处，以及这个客户等待的确切位置。所以kine的语义，你可能会认为k会阻塞直到连接建立。但实际上，如果你尝试这个，结果发现connect实际上在内核中启动了连接过程，但它会在连接建立之前返回，然后它会正确地完成。这项权利也会立即返还。所以它不会等待服务器读取写入的字符串，并且不会阻塞，直到它调用读取函数等待服务器的回显响应。因此，它实际上不会阻塞，直到它命中此读取，然后花费所有的权重，最后服务器接受连接请求，然后将回送字符串写入客户端。</p>
<p>发言人   10:41<br>So the call to connect actually returns immediately. And it exploits this feature in the kernel that can queue up these connection requests. So the kernel, now it is going through all the process of setting up the connection, but the application program continues. And then the right, the right inside the client doesn’t block because the kernel can also queue up the data that’s written. So it’ll queue it up. Remember that it was written when the connection actually gets created, then it’ll send that data along. But there’s no way to avoid the read from blocking. A read can’t return until it gets some data, so Reed has to block.<br>因此，连接调用实际上会立即返回。它利用了内核中的这个功能，可以将这些连接请求排队。所以内核现在正在经历建立连接的所有过程，但应用程序仍在继续。然后是右边，客户端内部的右边不会阻塞，因为内核也可以将写入的数据排队。所以它会把它排队。请记住，它是在连接实际创建时写入的，然后它将发送这些数据。但是没有办法避免读取阻塞。读取无法返回，直到它获得一些数据，因此Reed必须阻塞。</p>
<p>发言人   11:30<br>Now, here’s the fundamental flaw of an iterative server. And the reason why we have to write them using with the concurrency.<br>现在，这是迭代服务器的根本缺陷。以及为什么我们必须使用并发编写它们的原因。</p>
<p>发言人   11:40<br>So let’s say in our Echo server example, we have a client that. Creates a connection? It requests a connection. It’s accepted in the server, does the right, the server echoes back 1 one string, and then the client blocks again instead of doing the next right or closing the connection, the person. The user goes out to lunch and never types in a string to the Echo client. So at this point, the server calls read, and then it blocks waiting for this user to type in something so that the client can send it to the server to be echoed. But the user’s gone, gets hit by a truck, who knows? Anyway, this never. This read then blocks for an indeterminate amount of time, right?<br>在我们的Echo服务器示例中，我们有一个客户端。创建一个连接？它请求连接。它在服务器中被接受，做正确的事情，服务器回显一个字符串，然后客户端再次阻塞，而不是做下一个正确的事情或关闭连接。用户出去吃午饭时，从不在Echo客户端输入字符串。因此此时，服务器调用read，然后阻塞等待此用户输入内容，以便客户端可以将其发送给服务器进行回显。但是用户不见了，被卡车撞了，谁知道呢？无论如何，这从来没有。这个读取会阻塞不确定的时间，对吗？</p>
<p>发言人   12:50<br>And while it’s blocking client two, which also wants service, it has to block. So now you’re in an untenable situation where one client has sort of totally affected all of the other clients in the system, and none of the other clients can get service. So if this were a web server, if one client for some reason blocked, no other users would be able to use that web service or look at pages on that site. So obviously, we can’t have this.<br>当它阻止客户端二 (也需要服务) 时，它必须阻止。所以现在你处于一个站不住脚的情况下，一个客户端完全影响了系统中的所有其他客户端，而其他客户端都无法获得服务。因此，如果这是一个web服务器，如果一个客户端由于某种原因被阻止，其他用户将无法使用该web服务或查看该站点上的页面。很明显，我们不能拥有这个。</p>
<p>发言人   13:30<br>So the solution is to write a concurrent server instead of an iterative server, where we’ll have a separate concurrent flow handle each clients request and interact with each client. So now if one client for some reason is slow or misbehaves or blocks the system, other clients won’t be affected because those clients will be handled by concurrent flows. So there are several ways, a number of ways to create these, yes.<br>因此，解决方案是编写一个并发服务器而不是迭代服务器，在那里我们将有一个单独的并发流处理每个客户端的请求并与每个客户端交互。所以现在，如果一个客户端由于某种原因运行缓慢、行为不当或阻塞了系统，其他客户端不会受到影响，因为这些客户端将由并发流处理。所以有几种方法，多种方法来创建这些，是的。</p>
<p>发言人   14:09<br>C it reads from multiple clients from like, say, four or 5. If there are other clients trying to take it and put send their rights over to the queue of all those it actually could. And in fact, that’s a form of concurrency. So the question is, could the server queue up requests from clients? It could, but no, I guess actually it would have to queue up. It would somehow have to accept those connections, right? No, so that wouldn’t work.<br>C它从多个客户端读取，比如说，四个或五个。如果有其他客户试图接受它并将他们的权利发送到所有实际可以接收的队列中。实际上，这是一种并发形式。所以问题是，服务器可以将客户端的请求排队吗？可以，但不行，我想实际上它必须排队。它会以某种方式接受这些连接，对吗？不，那不起作用。</p>
<p>发言人   14:50<br>Since since the accept calls are iterative, sequential, there’s no way to get data from those other clients. But actually what you’re suggesting is very similar to something called an event based server that will, which is one of the ways we can create concurrent flows. So there’s three ways to create these concurrent flows.<br>由于accept调用是迭代、顺序的，因此无法从其他客户端获取数据。但实际上你所建议的非常类似于基于事件的服务器，这是我们创建并发流的一种方式。因此，有三种方法可以创建这些并发流。</p>
<p>发言人   15:19<br>One is to use processes like we’ve already seen. So in this case, the kernel. Handles all the scheduling for and interleaves it interleaves the process execution automatically for us. And then as we saw before, each flow has its own private address space. So each flow is independent and scheduled by the kernel.<br>一种是使用我们已经看到的流程。所以在这种情况下，就是内核。处理所有调度并为我们自动交错执行流程。然后，正如我们之前所看到的，每个流都有自己的私有地址空间。因此每个流都是独立的，并由内核调度。</p>
<p>发言人   15:46<br>Now there’s another approach called event based where the programmer manually interleaves the flows. So instead of relying on the kernel to interleave these different flows, the user, the programmer, creates these flows and then manually interleaves them, okay? And since it’s one program, all of the flows share the same address space. So they have access to all the same global data structures. And they do this interleaving using a technique called IO multiplexing. And I’ll talk briefly about that. It’s addressed in much more detail in your book.<br>现在有另一种称为基于事件的方法，程序员手动交错流。因此，用户，程序员，不是依靠内核来交错这些不同的流程，而是创建这些流程，然后手动交错它们，好吗？由于它是一个程序，所有流共享相同的地址空间。因此，他们可以访问所有相同的全局数据结构。他们使用一种称为IO多路复用的技术进行交错。我将简要谈谈这个问题。在你的书中对此进行了更详细的讨论。</p>
<p>发言人   16:35<br>The third approach, which is kind of a hybrid of process based and event based, is thread based, so each. Of these, flows is implemented using something called a thread. The kernel like processes, the kernel automatically interleaves these different threads, but unlike a process, each thread shares the same address space. So each thread has access to all the global variables declared in the program. So it’s like process based in that the kernel automatically schedules it for us, but it’s like event based in the sense that every flow shares the same address space. So let’s look at all three of these approaches in more detail.<br>第三种方法是基于进程和基于事件的混合，是基于线程的，因此每个方法都是这样。其中，流是使用称为线程的东西实现的。内核就像进程一样，内核自动交错这些不同的线程，但与进程不同的是，每个线程共享相同的地址空间。所以每个线程都可以访问程序中声明的所有全局变量。所以这就像基于进程，内核会自动为我们调度它，但这就像基于事件的感知，每个流共享相同的地址空间。让我们更详细地看看这三种方法。</p>
<p>发言人   17:34<br>So the first approach is to create these flows using processes. So in this case, this is our echo server example. The client requests a connection and then calls F, gets to wait for the user to type something in at the keyboard, but the user’s gone. And so F, this client just blocks in the call to F gets so the server when it. Accepts the connection request and returns from the accept call. And after it returns from the accept call, it forks a child, and then that child interacts.<br>因此，第一种方法是使用流程创建这些流程。在这种情况下，这是我们的echo服务器示例。客户端请求一个连接，然后调用F，等待用户在键盘上输入内容，但用户已经离开。因此，这个客户端只是在对F的调用中阻塞，以便服务器在它运行时获取。接受连接请求并从accept调用返回。在它从接受呼叫返回后，它分叉一个子级，然后该子级进行交互。</p>
<p>发言人   18:26<br>That child process now will be responsible for interacting with client number one. So the child blocks waiting for data from client one, which is never going to show up. The user left, but that’s okay because it doesn’t stop the server.<br>这个子流程现在将负责与第一个客户进行交互。所以子块等待来自客户端1的数据，这永远不会出现。用户离开了，但没关系，因为它不会停止服务器。</p>
<p>发言人   18:47<br>The server, after it forks, a child goes right back and calls accept, and now can accept the connection request from client two and fork Goth. Another different child that can interact with client two. So that child will read, waits for data to show up from the client, and then it echoes it back and at some point then closes this connection.<br>服务器在分叉后，一个孩子立即返回并调用accept，现在可以接受来自客户端2和分叉Goth的连接请求。另一个可以与客户端二互动的不同的孩子。这样孩子就会读取数据，等待数据从客户端显示出来，然后将其回显，并在某个时刻关闭此连接。</p>
<p>发言人   19:18<br>So you see that this misbehaving client, number one, now, because we have concurrent flows interacting with all the clients, this misbehaving client can’t aversely affect other clients. So now, and this idea of creating concurrent flows to interact with clients is fundamental. You have to do this in order to have sort of a working server implementation, right?<br>所以你可以看到这个行为不端的客户端，排名第一，因为我们有并发流与所有客户端交互，这个行为不端的客户端不会影响其他客户端。所以现在，创建并发流与客户端交互的想法是基本的。你必须这样做才能有一个有效的服务器实现，对吗？</p>
<p>发言人   19:48<br>So how would we actually program this process based concurrent server? It’s actually surprisingly compact.<br>那么我们如何实际编程这个基于并发服务器的过程？它实际上非常紧凑。</p>
<p>发言人   20:00<br>So we’re going to pass in argv. We’re going to pass in a port number that we want this server to listen on. We’ve got a listening descriptor and a connected descriptor we’ve got, and then we’ve got a length and an address field, and the address is declared in a protocol independent way using this sock adder storage type, which is guaranteed to be big enough. As you saw last time, it’s guaranteed to be big enough to handle any type of address, either Ipv 4 or Ipv 6. Okay, so we install a sigchi al handler, and then we use the open listen FD, a call from your textbook, to create a listening descriptor on the port that we pass in as the argument to this program. And then the server goes into a loop. And in each iteration, it.<br>所以我们要传入argv。我们将传入一个端口号，希望该服务器监听该端口号。我们已经得到了一个监听描述符和一个连接描述符，然后我们得到了一个长度和一个地址域，并且使用这种袜子加法器存储类型以协议独立的方式声明地址，该地址保证足够大。正如你上次看到的，它保证足够大，可以处理任何类型的地址，无论是Ipv 4还是Ipv 6。好的，所以我们安装了一个sigchi处理程序，然后使用开放的listen FD，从您的教科书调用，在我们作为此程序的参数传递的端口上创建一个监听描述符。然后服务器进入循环。在每次迭代中，它。</p>
<p>发言人   21:11<br>Gets the size of the socket or storage type and puts it into client. Client lend and then it calls accept with pointers to the client’s address and client line.<br>获取套接字或存储类型的大小并将其放入客户端。客户端借出，然后它使用指向客户端地址和客户端行的指针调用accept。</p>
<p>发言人   21:33<br>Using the listening descriptor that was returned by open listen FD, the accept call, after it gets a connection request, it returns with the address of the client that made the that at the. Other end of the connection, along with the true length of that address. So the case of Ipv 4 b 4, 4 B. And then the acceptor turns this connected file descriptor. Then that.<br>使用由open listen FD返回的监听描述符，accept调用在获取连接请求后，会返回在进行连接请求的客户端的地址。连接的另一端，以及该地址的实际长度。因此，Ipv 4 b 4，4 B的情况。然后接收方将此连接的文件描述符转换。然后那个。</p>
<p>发言人   22:08<br>The child that? It can use to read and write and interact with that client. So it creates a, it forks the child, and then the child closes its listening descriptor, and then it calls the echo routine to interact with the.<br>那个孩子？它可以用来读写并与该客户端进行交互。因此它创建了一个，它分叉了子级，然后子级关闭了它的监听描述符，然后它调用echo例程与进行交互。</p>
<p>发言人   22:31<br>Client and when the echo routine returns, the client closes this connector descriptor and then exits. And so this close. Isn’t absolutely necessary, but we just did it to be careful. Now the parent, and this is important, closes the connected descriptor because it’s not going to use that connected descriptor. Only the child will use that connected descriptor. So in order to avoid this memory leak, it’s very important for the child to close this descriptor. Because remember, this server running in an infinite loop, in theory, it would never terminate. And then avoid.<br>当echo例程返回时，客户端关闭此连接器描述符，然后退出。所以这就结束了。不是绝对必要的，但我们这样做只是为了小心。现在，父代关闭连接的描述符，这很重要，因为它不会使用连接的描述符。只有孩子会使用连接的描述符。因此，为了避免这种内存泄漏，孩子关闭这个描述符非常重要。因为记住，这个服务器在无限循环中运行，理论上它永远不会终止。然后避免。</p>
<p>发言人   23:22<br>A memory leak in our handler, we have to have a Sig child handler that we’ll reap all of the children that have terminated.<br>如果我们的处理程序出现内存泄漏，我们必须有一个Sig子处理程序，以便获取所有已终止的子处理程序。</p>
<p>发言人   23:36<br>So let’s look at a little more detail how this accept works. So you have a client with a client file descriptor, and then you have a server that creates a listening descriptor. So let’s say that descriptors are indexed by small integers. So let’s say that that index is 3, the descriptor number is 3, so the server blocks and accept waiting for the connection request.<br>让我们更详细地看一下这个接受是如何工作的。因此，您有一个带有客户端文件描述符的客户端，然后您有一个创建监听描述符的服务器。所以我们假设描述符被小整数索引。因此，假设索引是3，描述符编号是3，因此服务器阻止并接受等待连接请求。</p>
<p>发言人   24:06<br>The client makes a connection request using the connect call. The server accepts the connect call, and then it creates a child. And then the child interacts with the client using the connected file descriptor that was returned from the accept. So that would be, say, descriptor number 4, just be some different number.<br>客户端使用连接调用发出连接请求。服务器接受连接呼叫，然后创建一个子级。然后子级使用从accept返回的连接的文件描述符与客户端交互。这将是，比如说，描述符4，只是一些不同的数字。</p>
<p>发言人   24:35<br>So the execution model we have for these process based on servers is that we have the A server processor listening for connection requests one after the other from clients. And then we have clients interacting concurrently with multiple children, interacting concurrently with multiple clients. Since each of each of these children are processes, there’s no shared state between them, and both parent and child inherit the. They inherit the descriptor table. So they both have copies of listen FD and the listening descriptor and the connected descriptor. And as we saw before, the parent must close its copy of the connected file descriptor. The child should close the listening descriptor. But it’s just to be just because it’s not needed.<br>因此，我们基于服务器的这些进程的执行模型是，我们有一个服务器处理器，一个接一个地监听客户端的连接请求。然后我们让客户同时与多个孩子交互，同时与多个客户交互。由于每个子项都是进程，因此它们之间没有共享状态，父项和子项都继承。它们继承描述符表。因此它们都有listen FD、监听描述符和连接描述符的副本。正如我们之前所看到的，父级必须关闭其连接的文件描述符的副本。孩子应该关闭听音描述符。但这只是因为它不是需要的。</p>
<p>发言人   25:42<br>When you so these are actually pretty simple to create. And there’s just a couple of things you have to keep in mind when you build a process based on server. So first, as with any process that creates children, it has to reap these children that have terminated to avoid this memory leak, the parent process has to close its copy of a connected file descriptor. And there’s a couple reasons. In fact, if it doesn’t, it’ll not only create a memory leak, but that the state associated with that descriptor will actually stay around forever because the kernel won’t close that connection.<br>当你这样做时，这些实际上非常容易创建。在基于服务器构建流程时，有几件事情需要记住。因此，首先，与任何创建子项的进程一样，它必须收获这些已终止的子项以避免此内存泄漏，父进程必须关闭其连接的文件描述符的副本。有几个原因。事实上，如果不这样做，它不仅会造成内存泄漏，而且与该描述符相关的状态实际上将永远存在，因为内核不会关闭该连接。</p>
<p>发言人   26:30<br>So as we saw when we looked at file IO, this is the same kind of Fio we looked at before. So the kernel keeps a reference count for each, each socket that’s open. So after the fork, now there’s 2, there’s a parent and the child, which you’re accessing the file table associated with the connected file descriptor. So, and the connection won’t be closed until the reference count for that connected file descript is 0, so that file table entry won’t be removed from the kernel until there’s zero references to it. So both the parent and the child have to close that descriptor.<br>因此，正如我们在查看文件IO时所看到的，这与我们之前看过的Fio是同一种类的。因此内核为每个打开的套接字保留引用计数。因此，在分叉之后，现在有2个，有一个父节点和子节点，您正在访问与连接的文件描述符关联的文件表。因此，在连接的文件描述的引用计数为0之前，连接不会被关闭，因此在没有对文件表项的零引用之前，不会从内核中删除该文件表项。因此，父母和孩子都必须关闭该描述符。</p>
<p>发言人   27:23<br>Now, the good thing about process based servers is that they do the job for us that we asked them to do. We wanted them to handle, to interact with multiple clients concurrently or have that ability. There’s a very clean sharing model. So there’s private address spaces between all of the children and the parent. They have separate descriptors, but they share, they have separate copies of the descriptor table, but they share the same open file table. And there’s so in some sense, this is the simplest possible way to create a concurrent servers. And if you can get by with not sharing any global variables or sharing address spaces, then this is the way to go.<br>现在，基于流程的服务器的好处是，它们为我们完成了我们要求它们完成的工作。我们希望他们能够处理，同时与多个客户端交互或具有这种能力。有一种非常干净的共享模式。因此，所有子节点和父节点之间都有私有地址空间。它们有单独的描述符，但它们共享，它们有描述符表的单独副本，但它们共享相同的打开文件表。在某些感知，这是创建并发服务器的最简单方法。如果你可以不共享任何全局变量或共享地址空间，那么这就是你的选择。</p>
<p>发言人   28:16<br>Now, the disadvantage is that there’s additional overhead for processes, even with this copy on write trick that we saw, sharing the address space between the parent and the child, still, it’s still non-striving all overhead. And you have to actually do a lot of work if you want to share data between processes. So like, let’s say you want to have some kind of a shared cache between multiple processes. You either have to use files on disk, or if you want to share memory, you have to do, you have to do some kind of memory mapping, or you have to use these interprocess communication mechanisms, which we haven’t talked about. There’s ways pipes are probably the ones you’re most familiar with. So a pipe allows one process to send data to another process. And there’s ways to share memory between processes, but they’re cumbersome and require have to be implemented with care.<br>现在，缺点是进程有额外的开销，即使使用我们看到的这种复制写入技巧，在父进程和子进程之间共享地址空间，但仍然是非努力的所有开销。如果你想在进程之间共享数据，你实际上必须做很多工作。比如说，假设你想在多个进程之间拥有某种共享缓存。你要么必须使用磁盘上的文件，或者如果你想共享内存，你必须做一些内存映射，或者你必须使用这些我们还没有讨论过的进程间通信机制。管道可能是你最熟悉的方式。因此，管道允许一个进程向另一个进程发送数据。并且有在进程之间共享内存的方法，但它们很麻烦，需要小心实施。</p>
<p>发言人   29:30<br>The second approach is we call an event based server. So the idea here is that the server maintains a set of active connections. It has an array of connected file descriptors from different clients, and then it determines which of those. And it also has a listening descriptor. And then it determines which of those descriptive have pending input. And it determines this using a system call called Select or Epo.<br>第二种方法是我们称之为基于事件的服务器。所以这里的想法是服务器维护一组活动连接。它具有来自不同客户端的连接文件描述符的数组，然后它确定其中的一个。它还有一个监听描述符。然后它确定哪些描述性输入有挂起的输入。它使用称为Select或Epo的系统调用来确定这一点。</p>
<p>发言人   30:08<br>There’s several ways to determine this, but basically using Select or Epo, you can determine which of a set of descriptors has pending input. And then, and so the arrival of input at a descriptor is called an event because it changes the state of the descriptor. So an event is event in general, is always some kind of state change. So in this case, when data arrives on a socket, that’s a change in the state. There was no data before the event, after the event.<br>有几种方法可以确定这一点，但基本上使用Select或Epo，您可以确定一组描述符中有哪些等待输入。然后输入到达描述符被称为事件，因为它改变了描述符的状态。因此，事件通常是事件，总是某种状态的变化。在这种情况下，当数据到达套接字时，这是状态的变化。在事件之前、之后都没有数据。</p>
<p>发言人   30:51<br>Now there’s data that the server can read. So if the listening descriptor has input, then the server calls accept to accept the connection. And then all connected descriptors that have pending inputs, it services those. It reads from those in some order.<br>现在有服务器可以读取的数据。因此，如果监听描述符有输入，那么服务器调用accept来接受连接。然后所有连接的描述符都有待处理的输入，它为这些输入提供服务。它以某种顺序从那些人那里读出来。</p>
<p>发言人   31:17<br>Now, the details for how to do this are described in the book. But basically. I mean, conceptually, it’s pretty simple. It’s actually tricky to implement. But the idea is that there’s some set of active descriptors. There’s some set of descriptors, connected descriptors that you’re using that are being used right now to interact with a client. There’s some that are inactive. So if a descriptor is closed, then it’s no longer active. And then there’s other descriptors that have never been used. So we just have this array of descriptors, and then we record there the deor number connected for each of. Those descriptors?<br>现在，如何做到这一点的细节在书中描述。但基本上。我的意思是，从概念上讲，它非常简单。这实际上很难实现。但是这个想法是有一些活动描述符的集合。有一些描述符，您正在使用的连接描述符，现在正在用于与客户端交互。有一些是不活跃的。因此，如果描述符已关闭，则它不再处于活动状态。还有其他从未被使用过的描述符。所以我们只需要这个描述符数组，然后记录每个描述符连接的deor数。这些描述符？</p>
<p>发言人   32:19<br>And then using select or epol or some other mechanism, we somehow determine which of those active descriptors have input. And then we service each of those in the case of listen FD by calling accept. In the case of these connected descriptors actually are this should be con FD, not client FD, But in the case of these connected descriptors, we read the data from them.<br>然后使用select、epol或其他机制，我们以某种方式确定哪些活动描述符有输入。然后我们通过调用accept为listen FD中的每一个提供服务。在这些连接的描述符的情况下，实际上应该是con FD，而不是客户端FD，但在这些连接的描述符的情况下，我们从它们读取数据。</p>
<p>发言人   32:51<br>And when we read the data from each descriptor, we do some work. So data arrives at a descriptor, and then we read that data. And then we do some kind of work. Maybe in the case of an echo server, we echo it right back. In the case of a web server, we, if that data was Http request, we might go and fetch a file from disk and return it. But in any case, we notice that the descriptor has some data. We read that data, and then we respond to it in some way. So that response, those multiple responses, are concurrent flows.<br>当我们从每个描述符读取数据时，我们会做一些工作。所以数据到达描述符，然后我们读取该数据。然后我们做一些工作。也许在echo服务器的情况下，我们会将其回显。在web服务器的情况下，如果该数据是Http请求，我们可能会从磁盘中提取文件并返回它。但在任何情况下，我们注意到描述符有一些数据。我们阅读这些数据，然后以某种方式对其做出回应。因此，多个响应是并发流。</p>
<p>发言人   33:45<br>When we’re interacting with that client, we’re interacting, we’re creating conrector flows, a concurrent flow for each client, and we’re servicing those clients concurrently. So even though it’s a sequential program, right, we’re not using fork or anything. It’s just AC program, straightforward C program. We’re writing in such a way that we’re creating our own concurrent flows.<br>当我们与该客户端交互时，我们正在交互，我们正在为每个客户端创建并发流，并同时为这些客户端提供服务。所以即使它是一个顺序程序，我们也没有使用fork或其他任何东西。这只是交流程序，简单明了的C程序。我们正在以这样一种方式编写，即我们正在创建自己的并发流。</p>
<p>发言人   34:18<br>So as with any approach, there’s advantages and disadvantages. So a big advantages of event based servers is it’s one process with one address space. So it’s very easy. You can use conventional debugger Gdb to step through. You can see everything, you have access to everything. So in that sense, they’re much simpler to debug, understand? And then there’s no process or thread control overhead. When we serve as a particular descriptor, there’s very little overhead. The only overhead is sort of determining that that descriptor has input available. Because of that?<br>因此，任何方法都有优点和缺点。因此，基于事件的服务器的一个巨大优势是它是一个具有一个地址空间的进程。所以很容易。你可以使用传统的调试器Gdb来逐步调试。你可以看到一切，你可以接触到一切。所以在这种感知下，它们的调试要简单得多，明白吗？然后没有进程或线程控制开销。当我们作为一个特定的描述符时，开销非常小。唯一的开销就是确定该描述符是否有可用的输入。因为这个？</p>
<p>发言人   35:08<br>High performance web servers like node JS, ngx tornado, they all use this event based approach. If you want to get over 10000 requests per second, you have to go with something like this. The disadvantage is it’s much harder, more complex to code up than the other processor thread based designs, and it’s very difficult.<br>像node JS、ngx tornado这样的高性能web服务器，它们都使用这种基于事件的方法。如果你想要每秒获得超过10000个请求，你必须采用这样的方式。缺点是它比其他基于处理器线程的设计更难、更复杂地编写代码，并且非常困难。</p>
<p>发言人   35:41<br>So one of the hardest aspects of writing an event based on server is that you have to figure out how much work you’re going to do in response to an event. So let me give you, let’s say this server is a web server and you get input on A on one of your connected file descriptor. The simplest thing to do would be to then assume to read the entire Http request and not return until you’ve read the entire request. So in that case, the amount of work that you do in response to an event, it’s very coarse grained. There’s a lot of instructions because we’re going to read every single, every single line in that Http request header. But so that’s an example of coarse grain.<br>因此，编写基于服务器的事件最困难的方面之一是，您必须弄清楚要执行多少工作来响应事件。所以让我给你，假设这个服务器是一个web服务器，你可以在一个连接的文件描述符上获得输入。最简单的做法是假设读取整个Http请求，直到您阅读了整个请求才返回。因此，在这种情况下，您响应事件所做的工作量非常粗糙。有很多说明，因为我们将阅读Http请求标头中的每一行。但这是粗颗粒的一个例子。</p>
<p>发言人   36:35<br>It’s very simple because every time you get a request on a connected descriptor, you just read the whole Http request and then send a response. So on the other hand, it’s vulnerable because what if a client misbehaves and doesn’t send the entire Http request? What if it sends half of the request? So if you were doing a design and event based web server, you probably wouldn’t want to do that because a single client, we’d be back in the situation we were before where a single misbehaving client could sort of shut down the whole server.<br>这非常简单，因为每次您在连接的描述符上收到请求时，您只需读取整个Http请求，然后发送响应。所以另一方面，它是脆弱的，因为如果客户端行为不当并且没有发送整个Http请求怎么办？如果它发送了一半的请求呢？所以，如果你正在设计一个基于事件的web服务器，你可能不想这样做，因为一个客户端，我们会回到以前的情况，一个行为不端的客户端可能会关闭整个服务器。</p>
<p>发言人   37:16<br>So you might say, well, to I’m going to. My unit of work that I do in response to a request will be to read a single line from a request. So I’ll read a single line, and then I’ll return. So every so now we’re in reading, reading single lines. And once I’ve read the entire request, then I’ll send the response. So that’s better.<br>所以你可能会说，好吧，我要去。我为响应请求而做的工作单元将是从请求中读取一行。所以我会读一行，然后我会回来。所以现在我们每天都在阅读，阅读单行。一旦我阅读了整个请求，我就会发送响应。这样更好。</p>
<p>发言人   37:49<br>A misbehaving client, if it’s sending like whole text lines at a time, even if it stops halfway through, we’ll still be able to make progress and service other clients. So that’s a finer grained approach. It’s better, it’s probably more robust than waiting for the whole, the whole request, but it’s still vulnerable because the client could just send a partial line. So now we’re back. So really, the only way to write a robust event based web server is to be able to handle partial lines.<br>一个行为不端的客户端，如果它一次发送整个文本行，即使它停止了一半，我们仍然可以取得进展并为其他客户端提供服务。所以这是一种更细粒度的方法。这更好，它可能比等待整个请求更强大，但它仍然很容易受到攻击，因为客户端可能只发送部分行。所以现在我们回来了。所以实际上，编写基于事件的web服务器的唯一方法是能够处理部分行。</p>
<p>发言人   38:27<br>Just read when there’s data available on a descriptor. You just read whatever data is available. You remember how much you read. If it’s not a whole line, you somehow have to remember that you have to buffer it, remember it. So now it’s getting really complicated, right? But that’s the price you pay for this low overhead, sort of easier to debug kind of kind of model.<br>当描述符上有可用数据时，只需读取。你只需要读取任何可用的数据。你记得你读了多少书。如果它不是一整行，你必须记住你必须缓冲它，记住它。所以现在变得非常复杂了，对吧？但这就是你为这种低开销、更容易调试的模型所付出的代价。</p>
<p>发言人   38:52<br>And then another disadvantage is that you can’t, since it’s really a sequential program, it’s 1 C program, you can’t take advantage of multiple cores. So the only way to get sort of more performance out of something, an event based server, is just to replicate copies of that server. But you can’t make an individual server go faster by using multiple cores.<br>然后另一个缺点是你不能，因为它实际上是一个顺序程序，它是一个C程序，你不能利用多核优势。因此，从基于事件的服务器中获得更多性能的唯一方法就是复制该服务器的副本。但您无法通过使用多个核心使单个服务器运行得更快。</p>
<p>发言人   39:18<br>The third approach is to use threads to create these concurrent flows. It’s very similar to processes, but there are some important differences. So let’s look first at what we mean by a thread. So let’s go back.<br>第三种方法是使用线程来创建这些并发流。它与过程非常相似，但有一些重要的区别。让我们先看看我们所说的线程是什么意思。那我们回去吧。</p>
<p>发言人   39:38<br>I’m going to, let’s go back and look at the traditional view of a process. So we think of a process as some context. That’s data structures in the kernel, data that the kernel keeps about that process, as well as this private address space, which contains a stack code and data, and then the stack, and as part of. The process context consists of context that’s associated with the program, like the registers, condition codes, program counter, stack pointer. And then it contains kernel context, which is information in the kernel that the kernel needs to implement this idea of a process. So all of this data is actually stored in the kernel, but some of the data is is directly associated with the program, and other other data is sort of support data that the kernel needs to implement processes.<br>我打算，让我们回看一下流程的传统观点。因此，我们将过程视为某种背景。这是内核中的数据结构，内核保存的关于该进程的数据，以及这个私有地址空间，其中包含堆栈代码和数据，然后是堆栈，作为其中的一部分。进程上下文由与程序相关的上下文组成，如寄存器、条件代码、程序计数器、堆栈指针。然后它包含内核上下文，这是内核中实现这个进程概念所需的信息。所有这些数据实际上都存储在内核中，但其中一些数据直接与程序相关联，而其他数据则是内核实现进程所需的支持数据。</p>
<p>发言人   40:46<br>Okay, so let’s just take this picture and we’re just going to just move things around a little bit. So what I’ve done is I’ve taken the stack off of this virtual address space and sort of pulled it out along with a stack pointer and the context that’s associated with the program. So the data registers the condition codes the stack pointer in the program counter, okay? And I’ve just renamed a thread context instead of program context. But it’s the same. It’s the same thing. And then I’m going to call this, this whole thing, the combination of a stack in this thread context. I’m going to call that a thread. And then everything else remains the same.<br>好的，那么让我们拍这张照片，我们只是稍微移动一下东西。所以我所做的是将堆栈从这个虚拟地址空间中提取出来，并将其与堆栈指针和与程序关联的上下文一起拉出。所以数据会在程序计数器中注册条件代码和堆栈指针，好吗？我刚刚重命名了线程上下文而不是程序上下文。但都是一样的。这是同样的事情。然后我将把这个称为整个东西，在这个线程上下文中堆栈的组合。我将称之为一个线程。然后其他一切都保持不变。</p>
<p>发言人   41:47<br>We have, we still have our code and data, and we have the kernel context. Now, by doing this sort of refactoring and just moving things around, I can now imagine. So this stack isn’t very much, right? So there’s this stack space, and then there’s a little bit of data here in the form of some registers and some things to be stored. This is a fairly small amount of data.<br>我们有，我们仍然有我们的代码和数据，我们有内核上下文。现在，通过进行这种重构并只是移动事物，我现在可以想象了。所以这个堆栈并不是很多，对吗？所以这里有堆栈空间，还有一些数据以一些寄存器和一些要存储的东西的形式在这里。这只是相当少量的数据。</p>
<p>发言人   42:20<br>Now, when I refactor like this, now I can think about there being multiple threads associated with the same process. If I just keep different set of data thread context for each thread, and then a private, a separate portion of the stack that’s associated with that thread. So now each thread shares the same code and data, the same virtual address space, has the same kernel context, the same IO structures. But now it has its own separate individual stacks. So local variables, things that you would store on a stack now would be private, sort of independent. And it has its its own program counter, its own stack pointer, its own set of registers and condition codes.<br>现在，当我像这样重构时，我可以考虑同一个进程有多个线程关联。如果我只是为每个线程保留一组不同的数据线程上下文，然后保留一个私有线程，即与该线程关联的堆栈的一个单独部分。所以现在每个线程共享相同的代码和数据，相同的虚拟地址空间，具有相同的内核上下文，相同的IO结构。但现在它有自己独立的独立堆栈。所以局部变量，你现在存储在堆栈上的东西将是私有的，有点独立。并且它有自己的程序计数器、自己的堆栈指针、自己的一组寄存器和条件代码。</p>
<p>发言人   43:25<br>And then the one difference is now is that each thread, there’s still a process ID for this process that’s part of the kernel context. But each thread now has its own thread ID as part of its thread context. So now?<br>然后唯一的区别是现在每个线程仍然有一个为这个进程账号的进程，这是内核上下文的一部分。但是每个线程现在都有自己的线程账号，作为其线程上下文的一部分。那么现在？</p>
<p>发言人   43:46<br>The kernel can just treat each of these as separate flows. There’s a separate, so it’s just like a process. Now the kernel can remember where each thread has its own program counter, but they’re running code out of the same code section in the virtual address space, so the sharing code, sharing data, but they have their own program counters. So the kernel can provide, can create each of these threads as a separate flow of control that it then schedules, just like it does a process or a similar way it does is a process, but the difference is the reason, the reason that four threads in the first place is that when the kernel wants to context switch from 1 thread to another, there’s not that much information that has to be saved and restored. It’s just a small amount of data. So the kernel has to save the data thread, one context in some data structure some way and then restore the context for thread two, but we’re talking about a very low overhead kind of operation, it doesn’t have to mess around with page tables, virtual address space, or any other other process context.<br>内核可以将每个流程视为独立的流程。有一个单独的过程，所以它就像一个过程。现在，内核可以记住每个线程都有自己的程序计数器的位置，但是它们运行的代码来自虚拟地址空间中的相同代码部分，因此共享代码，共享数据，但它们有自己的程序计数器。因此内核可以提供，可以将每个线程创建为单独的控制流，然后调度，就像它执行一个进程或类似的方式是一个进程，但区别在于原因，首先需要四个线程的原因是，当内核想要从一个线程切换到另一个线程时，不需要保存和恢复太多信息。这只是一小部分数据。因此内核必须以某种方式保存数据线程，一个数据结构中的上下文，然后恢复线程二的上下文，但我们谈论的是一种非常低开销的操作，它不必处理页表，虚拟地址空间，或任何其他流程上下文。</p>
<p>发言人   45:09<br>So threads are kind of like processes, but they’re different in the sense that they share the same virtual address space. And unlike processes which are created by fork, which creates a strict process hierarchy, threads are just pools. You can think of threads as pools of concurrent flows, then access the same code and data. And then the kernel is responsible for scheduling those flows in a way that so each flow gets time on the processor. So much like processes, much like concurrent processes.<br>所以线程有点像进程，但它们的不同之处在于它们共享相同的虚拟地址空间的感知。不像fork创建的进程，fork创建严格的进程层次结构，线程只是池。你可以将线程看作是并发流的池，然后访问相同的代码和数据。然后，内核负责以某种方式调度这些流，以便每个流在处理器上获得时间。非常像进程，非常像并发进程。</p>
<p>发言人   45:55<br>We say that two threads are concurrent if their flows overlap in. Otherwise they’re sequential. So this is the exact same example that I showed you when we looked at processes. So here, instead of three processes, we have three threads running it within the same process. Thread A runs for a little bit, and then the kernel decides to swap it out and run thread B, so then thread B runs for a little bit, and then the kernel decides to give thread C some time. It saves thread B’s context, restores thread C’s context, sets the PC to the PC value in thread CS context, and so C runs, and then the kernel decides to give a some time again. So then A runs some more. So because thread A and B overlap in time, they’re running concurrently.<br>如果两个线程的流重叠，我们说它们是并发的。否则它们是连续的。所以这是我在研究过程时向您展示的完全相同的例子。所以在这里，我们有三个线程在同一个进程内运行它，而不是三个进程。线程A运行一段时间，然后内核决定将其交换并运行线程B，然后线程B运行一段时间，然后内核决定给线程C一些时间。它保存线程B的上下文，恢复线程C的上下文，将PC设置为线程CS上下文中的PC值，因此C运行，然后内核决定再次给出一些时间。然后A运行更多。因此，由于线程A和B在时间上重叠，它们同时运行。</p>
<p>发言人   46:56<br>B and C don’t overlap in time, so they’re not running concurrently. But A and C are concurrent because they overlap in time. And so you could, you also have the option if there’s multiple cores, then a thread can run on each core. So then you can have true parallelism.<br>B和C在时间上不重叠，因此它们不会同时运行。但是A和C是并发的，因为它们在时间上重叠。所以你也可以选择，如果有多个核心，那么一个线程可以在每个核心上运行。这样你就可以拥有真正的并行。</p>
<p>发言人   47:24<br>Okay, so like I said, threads and processes are similar ideas, but in the sense that they each. Each thread and process corresponds to some kind of logical flow, and they can run concurrently with other flows, and each is scheduled and context switched by the kernel. But they’re different because threads share all code and data except their local stacks. And in fact, those local stacks, although each thread has its own, it’s really just sharing the same stack. And so it’s really the same portion of the virtual address space. It’s just that each, each thread is assigned its own part of that stack. So even though threads have their own individual stacks, since it’s all part of the same virtual address space, a thread can access the stack of any other thread. And that’s not a good thing to do, but it’s possible.<br>好的，就像我说的，线程和进程是相似的想法，但感知它们各自不同。每个线程和进程都对应于某种逻辑流，它们可以与其他流并发运行，并且每个线程和进程都由内核进行调度和上下文切换。但它们是不同的，因为线程共享除本地堆栈外的所有代码和数据。实际上，这些本地堆栈虽然每个线程都有自己的，但实际上只是共享同一个堆栈。因此，它实际上是虚拟地址空间的相同部分。只是每个线程都被分配了自己堆栈的一部分。因此，即使线程有自己的单独堆栈，由于它们都是同一虚拟地址空间的一部分，线程也可以访问任何其他线程的堆栈。这不是一件好事，但这是可能的。</p>
<p>发言人   48:36<br>So processes don’t share any of this state, right? They have their own private address spaces and threads are less expensive than processes. It’s cheaper to create them. And the main reason is that there’s just less context associated with the thread than there is as a process.<br>所以进程不共享任何状态，对吗？它们有自己的私有地址空间，线程比进程便宜。创建它们会更便宜。主要原因是与线程关联的上下文比作为进程关联的上下文要少。</p>
<p>发言人   48:56<br>And so in our system, when we measure A of, we just create and wait for a bunch, create a process, wait for a process over and over again, and measure the time turns out to be about 20000 cycles to create and reap a process about 10000 cycles, create and reap a.<br>因此在我们的系统中，当我们测量一个时，我们只需要创建并等待一个堆，创建一个进程，一遍又一遍地等待一个进程，然后测量大约20000个周期来创建和收获一个进程，大约10000个周期，创建和收获一个进程。</p>
<p>发言人   49:20<br>So that the kernel provides threads to us using an interface called p threads, Posix threads. So this is a fairly recent standard that all Linux Unix systems and Windows Macintosh. So this is a sort of standard Posix standard.<br>因此内核使用称为p线程 (Posix threads) 的接口向我们提供线程。所以这是一个相当新的标准，适用于所有的Linux Unix系统和Windows Macintosh。所以这是一种标准的Posix标准。</p>
<p>发言人   49:46<br>For manipulating threads? So you can do things like create and reap threads. This is sort of like fork and this is sort of like weight, but not not quite because it doesn’t create. There’s no hierarchy associated with these.<br>用于操作线程？所以你可以做一些像创建和收获线程这样的事情。这有点像叉子，这有点像重量，但并不完全是因为它不会产生。这些没有关联的层次结构。</p>
<p>发言人   50:08<br>Just like get PID, there’s a function to get your thread ID you can kill thread. So one thread can kill another thread A, there’s a function to exit a thread. The normal exit system call terminates all the threads and return is similar to p-three in the sense that it just terminates the current thread, the thread that calls it. And then there’s ways to access, synchronize access to shared variables, which we’ll look at on Thursday in more detail.<br>就像获取PID一样，有一个函数可以获取你的线程，账号你可以杀死线程。所以一个线程可以杀死另一个线程，有一个退出线程的函数。正常的退出系统调用终止所有线程，并在感知上类似于p3，它只是终止当前线程，即调用它的线程。然后还有一些方法可以访问、同步访问共享变量，我们将在周四更详细地了解。</p>
<p>发言人   50:50<br>Okay, so here’s the P threads Hello world program. You, your K and R book, the C reference manual. Very, the very first thing that it does is it shows you how to write the simplest possible C program. So the famous Hello World program. So that goes all the way back to like 1978 when K and R book was first written, But that’s caught on. And whenever we learn a new language or we learn a new concept, we always write the Hello World program for that concept. So the Hello World program for threads, this is the simplest Threads program that I can think of. So I call it the Hello World program for threads, so this program.<br>好的，这是P线程Hello world程序。你，你的K和R书，C参考手册。非常，它做的第一件事就是向您展示如何编写最简单的C程序。所以是著名的 “你好世界” 节目。所以这可以追溯到1978年，当时K和R的书刚开始写，但这已经流行起来了。每当我们学习一种新语言或一个新概念时，我们总是为这个概念编写Hello World程序。所以用于线程的Hello World程序，这是我能想到的最简单的线程程序。所以我称之为线程的Hello World程序，所以这个程序。</p>
<p>发言人   51:39<br>Defines a function? So in Posix, a thread is executed by executing the code in a function called the thread routine, and Posix imposes. This thread routine takes a generic pointer as an argument, optional generic pointer, and it returns a generic pointer.<br>定义一个函数？因此，在Posix中，线程通过执行称为线程例程的函数中的代码来执行，并且Posix强制执行。这个线程例程将一个泛型指针作为参数，一个可选的泛型指针，并返回一个泛型指针。</p>
<p>发言人   52:06<br>So anytime, if you want to pass anything to a thread, you’ve somehow got to pack up all that data into a single object that then you can take an address of. But you can see that this is an extremely general purpose. So our Hello World program creates a thread by calling P thread create. And we say instead of a child, we call this a peer thread so that there’s no parent child in relationship. The threads can reap other threads, whether they created those threads. Or not so you don’t have this strict parent child hierarchy PFA create.<br>因此，无论何时，如果您想将任何内容传递给线程，您都必须以某种方式将所有数据打包到一个对象中，然后您可以获取其地址。但你可以看到这是一个非常普遍的目的。所以我们的Hello World程序通过调用P线程创建来创建一个线程。我们称之为同伴线程，而不是孩子，这样在关系中就没有父子关系。线程可以收获其他线程，无论它们是否创建了这些线程。或者不是，所以你没有这个严格的父子层次结构，PFA创建。</p>
<p>发言人   52:57<br>Creates a thread that executes the thread routine in this third argument and returns the thread idea of that thread address in the integer pointed at by the first argument. The second argument is. There’s ways to set attributes of threads are beyond the scope of this course. And we’ll always just have those nu. And then the fourth argument is the optional argument that you want to pass to your thread routine. So in this case, we’re saying call the routine is that’s called thread with no arguments. And then our thread routine just prints out hello world, and then it returns.<br>创建一个线程，该线程执行此第三个参数中的线程例程，并在第一个参数指向的整数中返回该线程地址的线程思路。第二个论点是。有一些方法可以设置线程的属性，这些方法超出了本课程的范围。我们将永远只拥有那些nu。然后第四个参数是您想要传递给线程例程的可选参数。所以在这种情况下，我们说调用例程是没有参数的线程。然后我们的线程例程只打印出 “hello world”，然后它返回。</p>
<p>发言人   53:52<br>In this case, it doesn’t return anything, so it returns no. But if we wanted to returns something to the calling program, we could have returned a pointer to some generic object.<br>在这种情况下，它不返回任何东西，因此它返回no。但如果我们想向调用程序返回一些东西，我们本可以返回一个指向某个通用对象的指针。</p>
<p>发言人   54:14<br>So the thread ID the thread attributes are null, the thread routine, the thread arguments are avoid star p, and the return value is a void double. So it’s a pointer to the pointer that you want to return. All right, so let’s look at what happens when we execute hello world. So the main thread runs for a while, then it calls P thread create. Which creates the pure thread, which now is a concurrent flow that’s running? Once the P thread create returns, then we’re running two concurrent flows. We’re running the main thread and we’re running the pure thread.<br>因此线程账号线程属性为空，线程例程和线程参数避免使用星号p，并且返回值是双精度空。所以它是一个指向你想要返回的指针的指针。好的，那么让我们看看当我们执行hello world时会发生什么。主线程运行一段时间，然后调用P线程创建。这会创建纯线程，而纯线程现在是一个正在运行的并发流？一旦P线程创建了返回，那么我们将运行两个并发流。我们正在运行主线程，同时我们正在运行纯线程。</p>
<p>发言人   55:05<br>And so in this case, our hello world waits for the pure thread to finish by calling p thread join. The pure thread, after it calls its print out, it returns null, which terminates the thread. At that point, the P thread join returns and the main thread continues. So using using these create this create function, how would we write a thread-based concurrent echo server? And again, it’s very similar to the way we did it with the process based design.<br>因此，在这种情况下，我们的hello world通过调用p线程连接来等待纯线程完成。纯线程在调用其打印输出后返回null，这将终止线程。此时，P线程连接返回，主线程继续。所以使用这些创建这个创建函数，我们如何编写一个基于线程的并发echo服务器？再次强调，这与我们基于流程的设计非常相似。</p>
<p>发言人   55:53<br>So we acquire a listening descriptor by calling our open listen FD function just as before, and then inside this infinite server loop. We get the size of the. Client adder struct, which is going to be large enough to fit any address. And then we space for the connected file descriptor. So we’re Maliki, one integer sized portion of dynamic storage for this connected descriptor that we’re going to get back from accept. And we’re going to this is actually really important to avoid a nasty race condition that I’ll show you in a second. So now we call accept.<br>因此，我们通过像以前一样调用开放的listen FD函数来获取监听描述符，然后在这个无限服务器循环中。我们得到的大小。客户端加法器结构，它将足够大以适合任何地址。然后我们为连接的文件描述符留出空间。所以我们是Maliki，一个整数大小的动态存储部分，用于这个连接的描述符，我们将从接受中获取。我们要讨论的是，这实际上非常重要，以避免我稍后会向您展示的恶劣比赛条件。所以现在我们称之为accept。</p>
<p>发言人   56:52<br>With our listening descriptor and client address and client length, just like before and accept returns the connected descriptor. And then we dereference this connected descriptor pointer and store the value returned by accept in this location in the heap. And then we call p-three by giving it the name of our thread routine, which in this case is just simply a function we’ve defined in our program called thread. And the pointer to the connected file descriptor. Now our client, which our client, which our thread routine then will use to interact with the client.<br>使用我们的监听描述符、客户端地址和客户端长度，就像之前一样，并accept返回连接的描述符。然后我们取消引用这个连接的描述符指针，并将accept返回的值存储在堆的这个位置。然后我们通过给线程例程的名称来调用p-3，在这种情况下，它只是我们在程序中定义的名为线程的函数。和指向连接的文件描述符的指针。现在是我们的客户端，我们的线程例程将使用该客户端与客户端进行交互。</p>
<p>发言人   57:40<br>Okay, so the thread routine dereferences. The argument, remember, is a pointer to a connected file descriptor. So it D references that pointer to get the actual integer connected descriptor. And then it detaches the thread. So by default, threads, threads run in independent and attached mode. So they can be, they can be joined by other threads, and they can be killed by other threads, but.<br>好的，所以线程例程取消引用。请记住，参数是指向连接的文件描述符的指针。因此它引用该指针以获取实际的整数连接描述符。然后它会分离线程。因此，默认情况下，线程以独立和附加模式运行。所以它们可以被其他线程加入，也可以被其他线程杀死，但是。</p>
<p>发言人   58:18<br>By default, it’s similar when threads are running in sort of unattached mode. Undetached mode When they die, they have to be reaped by a join function too, to acquire those resources. But if we detach a thread, then it can’t be joined by any threads. But when it dies, the kernel will automatically restore the resources associated with that thread. So in this case, we’re going to just detach this thread so we don’t have to worry about reaping it when it finishes.<br>默认情况下，当线程以未连接模式运行时，它是类似的。当它们死亡时，它们也必须通过连接函数来获取那些资源。但是如果我们分离一个线程，那么它就不能被任何线程连接。但当它死亡时，内核将自动恢复与该线程关联的资源。所以在这种情况下，我们将只是分离这个线程，这样我们就不必担心在它完成时收获它。</p>
<p>发言人   59:05<br>And then we’re going to free this, this memory that was malit. So this is important. We have to free this memory. That was Malik by the main thread in order to avoid a memory leak. And then we call our echo function. So we interact with the Echo client until the Echo client is finished. And then we close this descriptor again to avoid a potentially fatal memory leak.<br>然后我们将释放这个被恶意破坏的内存。所以这很重要。我们必须释放这些记忆。这是主线程Malik的，以避免内存泄漏。然后我们调用我们的echo函数。所以我们与Echo客户端进行交互，直到Echo客户端完成。然后我们再次关闭这个描述符，以避免潜在的致命内存泄漏。</p>
<p>发言人   59:39<br>So this thread based execution model is very similar to the execution model that we saw with processes. So we have a main thread that’s listening for connection requests or waiting for connection requests via except, and then we have for each client, we have a peer thread that interacts with that client using the connected descriptor that was passed in when we created the thread. And then each thread, since it has its own stack, it has. Separate space for its local variables. And this is really the powerful thing about threads. Now with these things, by declaring these local variables, we can, we can create threads that won’t interact with each other and won’t and can run independently, yes?<br>因此，这种基于线程的执行模型与我们在进程中看到的执行模型非常相似。因此，我们有一个主线程正在监听连接请求或通过except等待连接请求，然后对于每个客户端，我们有一个对等线程，该线程使用在创建线程时传入的连接描述符与该客户端进行交互。然后每个线程，因为它有自己的堆栈，所以它有。为其局部变量分隔空间。这就是线程的强大之处。现在有了这些东西，通过声明这些局部变量，我们可以创建不会相互交互并且可以独立运行的线程，是吗？</p>
<p>发言人   01:00:43<br>All. I guess, yeah. The question is, is there any time you wouldn’t want to run detached? So when you run detached, you give up the power to kill other threads? So I don’t know, it’s hard to come up with good example, but if you wanted the ability to be able to terminate other threads maybe had, maybe if you were running a pool of worker threads and at some point.<br>所有。我想是的。问题是，是否有任何时候你不想运行分离？所以当你运行分离时，你放弃了杀死其他线程的能力？所以我不知道，很难想出一个好的例子，但如果你想能够终止其他线程，也许你正在运行一个工作线程池，在某个时候。</p>
<p>发言人   01:01:19<br>At some point, if I mean, I guess you can imagine a scenario where suppose you’re running a pool of worker threads, you give them all jobs to do. The first one that finishes, you take the results, and you don’t need the results from the other threads. So you might just want to kill those threads. But yeah, it’s hard to come up with a really compelling reason.<br>在某个时候，我的意思是，我猜你可以想象这样一个场景，假设你正在运行一个工作线程池，你给它们所有的工作来做。第一个完成的线程，您获取结果，并且您不需要其他线程的结果。所以你可能只想杀死那些线程。但是，很难想出一个真正令人信服的理由。</p>
<p>发言人   01:01:51<br>So there’s a few things to think about when you’re writing threads, basest servers. So the first is that you need to run detached to avoid potential memory leaks. I’m sorry, I forgot this word. So the opposite of detached is joinable. And so joinable threads, like I mentioned, can be reaped and killed by other threads. Detached threads can not, and their resources are automatically claimed on termination. So the default state’s joinable. And you have to use this detach function call to make the thread detached.<br>因此，当您编写线程和最基本的服务器时，有一些事情需要考虑。所以第一个是你需要运行分离以避免潜在的内存泄漏。对不起，我忘了这个词。因此，分离的反面是可接合的。这样可接合的线程，就像我提到的，可以被其他线程收割和杀死。分离的线程不能，它们的资源在终止时自动声明。因此，默认状态是可连接的。你必须使用这个分离函数调用来使线程分离。</p>
<p>发言人   01:02:27<br>The biggest issue with threads though, like the beautiful thing about threads, is that you’re sharing the same global address space. So it’s very easy to share data structures if you had multiple threads, if you had a web server, that concurrent web server that was built with multiple threads, be very easy to implement a cache that all the threads could use because they’re all sharing that same virtual address space.<br>不过，线程最大的问题，就像线程的美妙之处一样，就是你共享同一个全局地址空间。因此，如果您有多个线程，共享数据结构非常容易。如果您有一个web服务器，该服务器是由多个线程构建的并发web服务器，那么实现所有线程都可以使用的缓存非常容易，因为它们都共享相同的虚拟地址空间。</p>
<p>发言人   01:02:52<br>But the thing that makes thread so nice, the ease with which you can share resources is also the thing that makes them very tricky to deal with. So as soon as just like we saw with our shell lab handlers, as soon as you have multiple flows accessing shared resources, you have to be very careful. It’s very easy to make mistakes and it’s very easy, or it’s possible to share resources in unexpected and unintended ways. For example, if one thread passes the address of a local variable on its stack to another thread, then now the called thread now has access to the caller’s thread and there’s nothing to prevent. That called thread from manipulating local variables on the caller stack. Now that would be a very bad thing to do, but it’s possible you might forget. You might forget that the variable you’re passing is a local variable and not a global.<br>但是，使线程如此美好的原因，即您可以轻松共享资源，也是使它们非常难以处理的原因。因此，就像我们在shell实验室处理程序中看到的那样，一旦您有多个流访问共享资源，您就必须非常小心。很容易犯错误，很容易，或者有可能以意想不到的和意想不到的方式共享资源。例如，如果一个线程将其堆栈上局部变量的地址传递给另一个线程，那么现在被调用的线程现在可以访问调用方的线程，没有什么可以阻止的。它通过操作调用堆栈上的局部变量来调用线程。现在那将是一件非常糟糕的事情，但你可能会忘记。你可能会忘记你传递的变量是一个局部变量而不是全局变量。</p>
<p>发言人   01:04:11<br>And really bad mistake In our hello in our Echo server example, you know, we were very careful to Mali space for this. For this connected file descriptor that we passed into, the pure thread that we were creating would have been much easier just to pass the address of the connected file descriptor into our peer thread, be much easier. But it would also be wrong.<br>在我们的Echo服务器示例中，我们的hello出现了非常严重的错误，你知道，我们非常小心地为此占用了很多空间。对于我们传入的连接的文件描述符，我们所创建的纯线程将更容易，只需将连接的文件描述符的地址传递给我们的对等线程，就会容易得多。但这也将是错误的。</p>
<p>发言人   01:04:54<br>Can you see why?<br>你明白为什么吗？</p>
<p>发言人   01:05:02<br>Let’s say right here, when we call P 3 create, instead of passing a pointer to a separately allocated region of the heap, instead of doing that, we just passed the address of the connected file descriptor. Same thing, right?<br>让我们在这里说，当我们调用p3create时，我们不是传递指向单独分配的堆区域的指针，而是传递连接的文件描述符的地址。同样的事情，对吧？</p>
<p>发言人   01:05:22<br>And then in our thread routine, we dereference that pointer to get the connected file descriptor. Okay, if we just pass the address of the connected file descriptor, this is, it’s really bad. Can you see why? Points to a variable? It does, that’s true. And why is that bad?<br>然后在我们的线程例程中，取消引用该指针以获取连接的文件描述符。好的，如果我们只是传递连接的文件描述符的地址，这真的很糟糕。你明白为什么吗？指向一个变量？确实如此。为什么这么糟糕？</p>
<p>发言人   01:06:03<br>Yeah that’s right. Is assuming by passing the address of this connected file descriptor, we’re introducing a race in the race what we’re assuming? That the pure thread will be able to dereference that pointer before the main thread goes back up and gets a new connected file descriptor? So what happens?<br>是的，没错。假设通过传递此连接的文件描述符的地址，我们将在比赛中引入一场比赛，我们假设什么？纯线程能够在主线程重新启动并获取新的连接文件描述符之前取消引用该指针吗？所以发生了什么？</p>
<p>发言人   01:06:37<br>What happens? We can’t a concurrent system. We can’t make any assumptions about how the kernel is going to schedule things. We saw the same thing with processes. So what happens if after pthread? The main thread runs instead of the pure thread. So have passed, We’ve passed the address of the connected file descriptor for this client that we accepted the connection request from. And then before the pure thread can dereference that argument, we get a new connected file descriptor. Okay? And now the child runs and it dereferences that that descriptor. But what it gets now is the descriptor that’s corresponding to the second child, not the first child. So now we have two threads talking to the same client using the same descriptor.<br>发生了什么？我们无法建立并发系统。我们不能对内核将如何安排事情做出任何假设。我们在流程方面也看到了同样的情况。那么，如果在pthread之后会发生什么？主线程运行而不是纯线程。所以已经过去了，我们已经传递了接受连接请求的这个客户端的连接文件描述符的地址。然后在纯线程可以取消引用该参数之前，我们会得到一个新的连接文件描述符。好吗？现在孩子运行并取消引用该描述符。但现在它得到的是对应于第二个孩子的描述符，而不是第一个孩子。所以现在我们有两个线程使用相同的描述符与同一客户端对话。</p>
<p>发言人   01:07:54<br>So do you see that? So it’s very tricky. This is like a real subtle. This is an example of sort of subtle errors that you can introduce because of this unintended sharing. And it, the root cause is, as you’ve correctly pointed out, is that they’re both sharing the same memory on the caller stack. Yes, saying the. In this case, what you could do, there’s another thing you could do. You could just pass the descriptor itself. So you could just cast the descriptor to a generic pointer and just pass that. And that’s just kind of yucky though that would work because instead of dereferencing it, the child would would just use it directly. Okay, so good, that’s good.<br>你看到了吗？所以这非常棘手。这就像一个真正的微妙之处。这是一个例子，说明由于这种无意的分享，你可能会引入一些微妙的错误。而且，正如您正确指出的那样，根本原因是它们在调用堆栈上共享相同的内存。是的，说这个。在这种情况下，你能做的还有另一件事。你可以只传递描述符本身。所以你可以将描述符转换为通用指针，然后传递它。这只是有点令人讨厌，尽管这可能会起作用，因为孩子不会取消引用，而是直接使用它。好的，太好了，那很好。</p>
<p>发言人   01:09:05<br>Okay? So the really good things with threads is this ease of sharing, but that sharing also introduces. Introduces complications. In fact, that’s what we’re going to look at, ways to sort of control the sharing so that we don’t, so that we don’t get intens unintended sharing.<br>好吗？因此，线程的真正好处在于共享的便利性，但这种共享也引入了。引入了并发症。事实上，这就是我们要研究的，控制共享的方法，这样我们就不会，这样我们就不会得到意想不到的共享。</p>
<p>发言人   01:09:41<br>Okay, so to summarize the approaches to concurrency that we’ve looked at, we have process based concurrency. So it’s hard to share resources, but it’s easy to avoid unintended sharing. So in some ways, it’s safer and easier to program event based. So it’s very low level ISS, very tedious. You have to be very careful about how you, the granularity of the work that you do in response to events.<br>好的，总结一下我们看过的并发方法，我们有基于流程的并发。所以很难共享资源，但很容易避免意外共享。因此，在某些方面，基于事件编程更安全、更容易。所以这是非常低的水平，非常乏味。你必须非常小心你如何应对事件，你所做的工作的粒度。</p>
<p>发言人   01:10:12<br>You have total control over scheduling, so you can decide which descriptors you’re going to service and in which order. Since there’s a single flow of control, you can debug it with a debugger, but it doesn’t make use of multi-core. So there’s a handful of trade offs there. With threads, basest systems, it’s very easy to share resources, but that sharing can create problems of its own. It’s fairly efficient compared to processors. You don’t have much control over the scheduling, just like we saw, you can’t really control which threads get executed in which order, and it can be difficult to debug because there can be races that occur very rarely, very infrequently. So the probability of sort of creating one of those race conditions is difficult.<br>您可以完全控制调度，因此您可以决定要服务哪些描述符以及以什么顺序服务。由于只有一个控制流，您可以使用调试器对其进行调试，但它不使用多核。所以这里有一些权衡。使用线程和基本系统，共享资源非常容易，但这种共享也会带来问题。与处理器相比，它相当高效。你对调度没有太多的控制，就像我们看到的那样，你无法真正控制哪些线程以什么顺序执行，而且调试可能很困难，因为可能会出现很少发生的竞争。因此，产生其中一个竞争条件的概率是困难的。</p>
<p>发言人   01:11:14<br>Okay, so that’s it for today. Tomorrow we’ll look at thread based servers in more detail and how to write thread based systems efficiently and correctly.<br>好的，今天就到这里。明天我们将更详细地介绍基于线程的服务器，以及如何高效正确地编写基于线程的系统。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>深入理解计算机系统 024-Concurrent Programming</div>
      <div>http://example.com/2025/10/12/15213-024/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月12日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/12/15213-025/" title="深入理解计算机系统 025-Synchronization, Basic">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">深入理解计算机系统 025-Synchronization, Basic</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/12/15213-023/" title="深入理解计算机系统 023-Network Programming, Part II">
                        <span class="hidden-mobile">深入理解计算机系统 023-Network Programming, Part II</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
