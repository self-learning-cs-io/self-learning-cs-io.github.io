

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="发言人   00:00Well, good afternoon, everybody. Welcome, good to see you and welcome to our viewers on video as well. So today we’re going to look at some additional issues around the problem of synchroni">
<meta property="og:type" content="article">
<meta property="og:title" content="深入理解计算机系统 026-Synchronization, Advanced">
<meta property="og:url" content="http://example.com/2025/10/12/15213-026/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="发言人   00:00Well, good afternoon, everybody. Welcome, good to see you and welcome to our viewers on video as well. So today we’re going to look at some additional issues around the problem of synchroni">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-12T02:00:25.000Z">
<meta property="article:modified_time" content="2025-10-19T11:16:51.139Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>深入理解计算机系统 026-Synchronization, Advanced - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="深入理解计算机系统 026-Synchronization, Advanced"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-12 10:00" pubdate>
          2025年10月12日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          17k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          142 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">深入理解计算机系统 026-Synchronization, Advanced</h1>
            
            
              <div class="markdown-body">
                
                <p>发言人   00:00<br>Well, good afternoon, everybody. Welcome, good to see you and welcome to our viewers on video as well. So today we’re going to look at some additional issues around the problem of synchronizing threaded programs. First, though, let’s review from last time a few of the concepts.<br>下午好，大家。欢迎，很高兴见到你，也欢迎我们的视频观众。所以今天我们将围绕同步线程程序的问题来看一些其他问题。首先，让我们回顾一下上一次的一些概念。</p>
<p>发言人   00:22<br>So recall that a semaphore is a non negative of global synchronization variables manipulated by P and V operations. And the P operation takes as an argument a semaphore. If the semaphore values nonzero, it decrements the semaphore and then continues. If the semaphore value is 0, then it blocks waiting for that semaphore value to be incremented by a V operation after the V operation increments. After some V operation increments the semaphore, the V operation continues by decrementing S and then returning control to the collar.<br>回想一下信号量是由P和V操作操纵的全局同步变量的非负数。而P操作接受一个信号量作为参数。如果信号量值非零，则它递减信号量，然后继续。如果信号量值为0，则在V操作增加后，它会阻塞等待该信号量值通过V操作增加。在一些V操作增加信号量之后，V操作继续通过减少S然后将控制权返回给领。</p>
<p>发言人   01:10<br>The V operation never blocks First, it increments the semaphore value S, and then it looks in the queue to see if there’s any processes that are blocked, waiting for that semaphore to be nonzero. If there are, then it selects one of those using some unspecified criteria, and then it restarts that. Thread or that P operation that’s waiting on the semaphore? And then the semantics of the P and V ensure that semaphore values are always greater than or equal to 0.<br>V操作从不首先阻塞，它会增加信号量值S，然后在队列中查看是否有任何进程被阻塞，等待该信号量为非零值。如果有，那么它会使用一些未指定的条件选择其中之一，然后重新启动它。线程或正在等待信号量的P操作？然后P和V的语义确保信号量值始终大于或等于0。</p>
<p>发言人   01:57<br>Now, the first thing we saw, how to protect shared variables by using a semaphore called the mutex that guarantees mutually exclusive access to the critical sections that are updating that Those variables are data structures. And the way that we do this is very simple. We initialize the mutex to one and then surround the critical section with AP and a v, now there’s other ways that we can. So here’s an example where we’re using semaphores to provide mutual exclusion, but we can also use semaphores to coordinate access to share data structures in different ways, And so the idea here, before we were using the semaphore just to protect the access to a shared variable. But we can also coordinate access in different ways. In these kind of scenarios, we’re using the semaphore to keep track of state, to count, to count things, to keep track of state, and to notify other threads of changes in state. So it’s a very different usage model.<br>现在，我们看到的第一件事是，如何通过使用称为互斥锁的信号量来保护共享变量，该信号量保证对更新这些变量是数据结构的关键部分的互斥访问。我们这样做的方式非常简单。我们初始化互斥锁为一，然后用AP和v包围临界区，现在我们还有其他方法可以使用。这里有一个例子，我们使用信号量来提供互斥，但我们也可以使用信号量来协调访问，以不同的方式共享数据结构，所以在我们使用信号量之前，这里的想法只是为了保护对共享变量的访问。但我们也可以以不同的方式协调访问。在这种情况下，我们使用信号量来跟踪状态，计数，计数，跟踪状态，并通知其他线程状态的变化。所以这是一个非常不同的使用模式。</p>
<p>发言人   03:11<br>And two classic examples that we’re going to look at are the producer consumer problem and the reader’s writer’s problem.<br>我们将要探讨的两个经典例子是生产者消费者问题和读者作者问题。</p>
<p>发言人   03:18<br>So let’s look at producer consumer first. So the idea in the producer consumer problem is that you have a shared resource in the form of a buffer. The buffer has a bounded size, so it consists of n slots, and each slot can hold an item. There’s a producer thread which produces items and then inserts them into the buffer. And there’s a consumer thread that retrieves, removes items from the buffer, and then consumes them by acting on them in some way, processing on them in some way.<br>让我们先看看生产者消费者。因此，生产者消费者问题的想法是以缓冲区的形式共享资源。缓冲区有一个有界的大小，因此它由n个插槽组成，每个插槽可以容纳一个项目。有一个生产者线程，它产生项目，然后将它们插入到缓冲区中。并且有一个消费者线程，它从缓冲区中检索和删除项目，然后通过以某种方式对它们进行操作和处理来消耗它们。</p>
<p>发言人   03:59<br>So the synchronization variable, the synchronization pattern is that the producer waits for an empty slot. If the buffer is full, the producer can’t insert an item into the buffer. So it waits for an empty slot. And then when it finds an empty slot, when an empty slot becomes available, it inserts the item into the buffer, and then it notifies the consumer that there’s now a new item in the. In the buffer, the consumer, of course, has to wait for an item to show up in the buffer. Otherwise, I mean, it can’t remove an item from an empty buffer. So it has to wait for an item to be available in the buffer. And when an item becomes available, it removes it from the buffer and then notifies the producer that there’s now an available slot.<br>因此，同步变量，同步模式是创建者等待一个空槽。如果缓冲区已满，则生产者无法将项目插入缓冲区。所以它等待一个空插槽。然后，当它发现一个空槽时，当一个空槽变得可用时，它将该项插入到缓冲区中，然后通知使用者现在有一个新的项。在缓冲区中，消费者当然必须等待项目出现在缓冲区中。否则，我的意思是，它不能从空缓冲区中删除一个项目。所以它必须等待缓冲区中的项目可用。当一个项目变得可用时，它会将其从缓冲区中删除，然后通知生产者现在有一个可用的槽。</p>
<p>发言人   04:59<br>So this actually, this very simple pattern is actually really, really useful and shows up in a lot of applications. So for example, a multimedia application. In a multimedia application, the producer might be producing, say, big Mpeg frames in a video, and a consumer would be consuming those Mpeg frames and then painting the screen appropriately.<br>因此，这个非常简单的模式实际上非常非常有用，并在许多应用程序中出现。例如，多媒体应用程序。在多媒体应用中，制作者可能会在视频中产生大的Mpeg帧，消费者将消费这些Mpeg帧，然后适当地绘制屏幕。</p>
<p>发言人   05:25<br>Another important application is in graphical user interfaces. So graphical user interfaces are typically implemented using this producer consumer model, where mouse clicks motions in the of the mouse, keyboard clicks, those are all recorded as events. They’re detected by the system, recorded as events, and placed into a some kind of a queue, and then various other parts of the system, retrieve items from the queue and react to them. The graphics system will retrieve events like mouse events and mouse movements, mouse clicks, and it’ll paint the screen accordingly. So it’ll reflect, it’ll repaint the screen. So to show you that the cursor is moving, or if you change the focus, it’ll repaint that. So it’s a very common model. And as we’ll see later, multi thread AED.<br>另一个重要的应用是图形用户界面。因此，图形用户界面通常使用这种生产者消费者模型实现，其中鼠标点击动作，键盘点击，这些都被记录为事件。它们被系统检测到，记录为事件，并被放入某种队列中，然后系统的其他部分从队列中检索项目并做出反应。图形系统将检索诸如鼠标事件、鼠标移动、鼠标点击等事件，并相应地绘制屏幕。所以它会反射，它会重绘屏幕。因此，为了向您显示光标正在移动，或者如果您更改焦点，它将重新绘制光标。所以这是一个非常常见的模型。正如我们稍后将看到的，多线程AED。</p>
<p>发言人   06:28<br>We can build multi thread AED concurrent servers using this model. So let’s see how we would implement the producer consumer on an end element buffer. The implementation requires a mutex to guarantee mutually exclusive access to the resource, of course. And then it requires then two other semaphores.<br>我们可以使用这个模型构建多线程并发服务器。那么让我们看看如何在结束元素缓冲区上实现生产者消费者。当然，实现需要一个互斥锁来保证对资源的互斥访问。然后它需要另外两个信号量。</p>
<p>发言人   06:51<br>Counting semaphores. Slots is a semaphore that counts the available slots in the buffer and items counts the available items. And we can implement it with this package called S buff. So S Buff The S buff package defines a type called S buff underscore t that packages up all of the data structures that are needed to implement the shared buffer.<br>计算信号量。插槽是一个信号量，用于计算缓冲区中的可用插槽，而项则计算可用项。我们可以用这个叫做S buff的包来实现它。So S Buff包定义了一个名为S buff下划线t的类型，该类型打包了实现共享缓冲区所需的所有数据结构。</p>
<p>发言人   07:25<br>So there’s a pointer to the buffer, which we were going to implement is an array. And we’ll implement a circular buffer on this array, the maximum number of slots n, so the size of the buffer, and then two pointers, front and rear, to keep track of the front and rear of the buffer. So to keep track of the first and last items in the buffer. And then the three semaphores, the mutexes, and the two counting semaphores. And then the package consists of these four public functions, SBF init, which creates is called ones to create the buffer and initialize everything, allocate the space and initialize things, and then DNI, which frees up the space, and then a function to insert an item into the queue and a function to remove an item from a queue and return that item.<br>所以有一个指向缓冲区的指针，我们要实现的是一个数组。我们将在此数组上实现一个循环缓冲区，最大插槽数n，即缓冲区的大小，然后是前后两个指针，以跟踪缓冲区的前后。这样可以跟踪缓冲区中的第一个和最后一个项目。然后是三个信号量、互斥锁和两个计数信号量。然后这个包由这四个公共函数组成，SBF init创建被称为ones创建缓冲区并初始化所有内容，分配空间并初始化事物，然后DNI释放空间，然后是将项目插入队列的函数，以及从队列中删除项目并返回该项目的函数。</p>
<p>发言人   08:25<br>So in this case, items are just ints.<br>所以在这种情况下，项目只是整数。</p>
<p>发言人   08:31<br>So to create, to initialize this buffer with n slots, we first allocate the space for the buffer n ins. We set the size to be the value n that was passed in. We indicate the empty buffer by setting front and rear to be 0. And so whenever front and rear are 0, we have an empty buffer. And then we initialize the three semaphores. So the mutex, like all mutexes, is initialized to one, the slot semaphore, which keeps track of the number of available slots, is initialized to n, and the item semaphore is initialized to 0. And DN, it is really simple. It just frees up the heap space that we allocated.<br>因此，为了创建，为了用n个插槽初始化这个缓冲区，我们首先为缓冲区分配空间。我们将大小设置为传入的值n。我们通过将前面和后面设置为0来指示空缓冲区。因此，每当前后为0时，我们就有一个空的缓冲区。然后我们初始化这三个信号量。因此，像所有的互斥锁一样，互斥锁被初始化为1，跟踪可用插槽数量的插槽信号量被初始化为n，项目信号量被初始化为0。和DN，这真的很简单。它只是释放了我们分配的堆空间。</p>
<p>发言人   09:33<br>Okay, so now let’s look at how we insert an item into the buffer. So we want to insert this integer item into this buffer pointed at by Sp. First, the thread waits for an available slot by doing AP on the slot semaphore, so P will block until slots is greater than or equal to one. Once there’s an available slot, then it, then it’s going to update the rear of the buffer. So we’re going to put this item onto the rear of the buffer, and so it needs to protect that access to that shared buffer with a mutex. Then it does by doing AP on the mutex, updating our rear pointer. So we pre increment it. So we increment the rear pointer and then take the mod of that end.<br>好的，现在让我们看看如何将项目插入到缓冲区中。所以我们想将这个整数项插入到Sp指向的缓冲区中。首先，线程通过对插槽信号量执行AP等待可用插槽，因此P将一直阻塞，直到插槽大于或等于1。一旦有一个可用的插槽，它就会更新缓冲区的后部。所以我们将把这个项目放在缓冲区的后面，因此它需要使用互斥锁来保护对共享缓冲区的访问。然后它通过在互斥上执行AP，更新我们的后方指针来实现。所以我们预先递增它。所以我们递增后方指针，然后取该端的mod。</p>
<p>发言人   10:40<br>To compute the index that we’re going to insert the item into. Do a V on the mutex so that other threads can update that that shared data structure. And then we do a V on the number of items to notify consumers that there’s now an item in the buffer. So this V is kind of interesting. You can think of it as kind of like a signal. So you’re sort of signaling consumers that now some event has occurred in the system.<br>计算我们要插入项目的索引。对互斥锁执行V，以便其他线程可以更新该共享数据结构。然后我们对项目数量执行V，以通知消费者缓冲区中现在有一个项目。所以这个V有点有趣。你可以把它想象成一种信号。所以你有点像在向消费者发出信号，现在系统中发生了一些事件。</p>
<p>发言人   11:18<br>Now, to remove an item, it’s symmetric, but instead of operating on the slot semaphore, it operates on the item semaphore. So to remove an item, a consumer first does AP on the items semaphore.<br>现在，要删除一个项目，它是对称的，但不是在槽信号量上操作，而是在项目信号量上操作。因此，要删除一项，消费者首先对该项信号量执行AP。</p>
<p>发言人   11:34<br>So this now is waiting for an available item. It’s waiting for the semaphore to be greater than or equal to one. When that happens, then the p, the p returns. And then we do, we do the update protected by a mutex by pre incrementing front, taking the mod n, and then reading that value and returning it. And placing it into this local variable item, then we release the mutex. And then we do a v on the number of slots, which is an announcement to any producers that there’s now a new available slot, okay? Is there any questions about that?<br>所以现在正在等待可用的项目。它正在等待信号量大于或等于1。当这种情况发生时，p，p返回。然后我们这样做，我们通过预先增加前面，获取mod n，然后读取该值并返回它来进行互斥保护的更新。并将其放入此局部变量项中，然后我们释放互斥锁。然后我们对插槽数量进行v，这对任何制作人来说都是一个公告，现在有一个新的可用插槽，好吗？对此有什么问题吗？</p>
<p>发言人   12:25<br>So it’s a little more subtle. This looks really simple, but like all concurrency problems, it’s actually very subtle. So you can have potentially, you can have many producers and many consumers all operating on the same, the same shared buffer.<br>所以它更加微妙。这看起来非常简单，但就像所有并发问题一样，它实际上非常微妙。因此，您可以有许多生产者和消费者都在同一共享缓冲区上运行。</p>
<p>发言人   12:46<br>So what would happen if we had two consumers did a AP on this item semaphore at the exact same time they were running on 2 cores and we have two threads, and they each execute that P at the same time on that same, that same item semaphore. So even though they access it at the same time, the kernel will make sure that one of them will execute first. So the kernel will serialize those P operations, and there’s no telling which one gets it first. But whoever, whoever runs their P first will decrement this item semaphore. And then when the next thread executes, its p, items will either be 0 or nonzero as a result. So somebody wins. It’s kind of like a controlled race. And somebody wins the race always wins the race because the colonel is serializing these P operations, and it’s executing them atomically.<br>那么如果我们有两个使用者在两个内核上运行这个项目信号量的同时执行一个AP，并且我们有两个线程，它们每个都在同一时间执行同一个项目信号量的那个P，会发生什么呢？因此，即使它们同时访问它，内核也会确保其中一个将首先执行。所以内核将序列化这些P个操作，并且无法知道哪一个先得到它。但是，无论谁首先运行他们的P，都会减少此项目信号量。然后，当下一个线程执行时，其p，结果项将为0或非零。有人赢了。这有点像一场受控的比赛。有人赢得比赛总是赢得比赛，因为上校正在序列化这些P操作，并以原子方式执行它们。</p>
<p>发言人   13:59<br>Now the there’s a generalization of the mutual exclusion problem called the reader writer problem. So with the mutual exclusion problem, we were guaranteeing each thread mutually exclusive access to its critical section that’s updating a particular resource or accessing a particular resource. And we did this the exact same way, whether that access was a read or a write. But that’s overly conservative in this case because could, if all we were doing, if we had multiple threads that were just reading the resource, there would be no reason to do P’s and V’s on that resource. If we’re not changing. If we’re not changing the resource, we’re just reading it and there’s nobody else writing it, then there’s no need to protect for readers. We can have as many readers as we want all at the same time.<br>现在有一个相互排斥问题的推广，称为读者作者问题。因此，对于互斥问题，我们保证每个线程对其临界区的互斥访问，该临界区正在更新特定资源或访问特定资源。无论访问是读还是写，我们都是以完全相同的方式完成的。但是在这种情况下，这过于保守了，因为如果我们所做的一切，如果我们有多个线程只是读取资源，就没有理由对该资源进行P和V的操作。如果我们不改变。如果我们不改变资源，我们只是在阅读它，没有其他人在写它，那么就没有必要保护读者。我们可以同时拥有尽可能多的读者。</p>
<p>发言人   14:59<br>Reading, reading the resource, the variable or set of variables, and there’s no need for any kind of synchronization at all. So the producer consumer is sort of a generalization of that mutual exclusion problem, which enforces mutual exclusion only when it’s absolutely necessary. So there can be as. Many readers reading the resource. But when a writer wants to write then it it has to have mutual exclusive access to the resource.<br>读取，读取资源，变量或变量集，根本不需要任何类型的同步。所以，生产者消费者是互斥问题的一种推广，只有在绝对必要时才实施互斥。所以可以有。许多读者正在阅读该资源。但是当一个作家想要写作时，他必须对资源具有相互排斥的访问权限。</p>
<p>发言人   15:35<br>So this is the kind of thing. This is also a very useful pattern you have in an online airline reservation system. You have multiple clients accessing a shared database. So as long as those clients are reading the database, you can have, they can all be reading at the same time. But as soon as somebody wants to make a reservation and update the database, then that update has to happen a mutually exclusive way. If you had also, if you had any kind of shared data structure, like a cache in a concurrent proxy that you’re going to be writing soon, or if you haven’t already started, that cache is being shared by multiple threads, and multiple threads may be reading that cache. But when you get a new page and you want to cache it, then that constitutes a right. And that right needs to happen in a mutually exclusive way.<br>这就是那种事情。这也是在线航空公司预订系统中非常有用的模式。您有多个客户端访问一个共享数据库。因此，只要那些客户端正在读取数据库，你就可以拥有，它们都可以同时读取。但是，一旦有人想要进行预订并更新数据库，那么更新必须以相互排斥的方式进行。如果您还有，如果您有任何类型的共享数据结构，例如您即将编写的并发代理中的缓存，或者如果您还没有开始，则该缓存由多个线程共享，并且多个线程可能正在读取该缓存。但是当你得到一个新页面并且想要缓存它时，那就构成了一个权利。而这种权利需要以一种相互排斥的方式发生。</p>
<p>发言人   16:34<br>Now, the initial research, the researchers that. That pose to this reader’s writers problems? Define sort of several classes of of reader writer problems.<br>现在，初步研究，研究人员。这对读者的作家造成了什么问题？定义读者写作问题的几类。</p>
<p>发言人   16:50<br>The first writer reader’s writer’s problems is an implementation that favors readers. So the idea is that no reader should be kept waiting. Unless a writer already has AP, has sort of acquired the lock that or the mutex on that resource. So in this case, say there’s a writer waiting to acquire the mutex, and another reader comes in. Then in this implementation, that reader would get priority over the writer, and it would be able to read the to do its read, and the writer would have to wait. And so of course, now if multiple readers keep coming in, then this could starve out the writer that. So a writer could be starve sort of indefinitely waiting for all these readers to finish.<br>第一个作家读者的问题是一个有利于读者的实现。所以这个想法是没有读者应该等待。除非写作者已经拥有AP，已经获取了该资源的锁或互斥锁。在这种情况下，假设有一个编写器正在等待获取互斥锁，然后另一个读取器进来了。然后在这个实现中，该读取器将获得比编写器更多的优先级，并且它将能够读取以完成其读取，并且编写器将不得不等待。当然，现在如果多个读者不断进来，那么这可能会让作者挨饿。因此，一个作家可能会无限期地等待所有这些读者完成，从而挨饿。</p>
<p>发言人   17:51<br>It’s just based on. How the operating system decides to schedule these various reader threads? The writer could be starved out. So that’s what we mean by when we say that it favors readers. Now, the second reader’s various problem is sort of the opposite of that. So it favors writers. So once a writer is ready to write, then it gets priority over any weighting readers. So in this case have, if we have multiple writers that want to write, they could starve out readers.<br>它只是基于。操作系统如何决定调度这些不同的读者线程？这个作家可能饿了。这就是我们所说的有利于读者的意思。现在，第二个读者的各种问题有点相反。所以它有利于作家。所以一旦一个作家准备写作，那么它就会比任何加权读者都优先。因此，在这种情况下，如果我们有多个作家想要写作，他们可能会饿死读者。</p>
<p>发言人   18:33<br>And now there’s other variants that sort of deal with this starvation issue, this potential starvation issue. We won’t look at them here.<br>现在还有其他的变种可以处理这种饥饿问题，这种潜在的饥饿问题。我们不会在这里看它们。</p>
<p>发言人   18:44<br>So let’s look at the solution to the first reader’s writer’s problem. So right, initially, a thread is either a reader in this sort of simplification, or it’s a writer. If we look at the writers, this is pretty simple. It’s just the writers are just. Doing AP on this semaphore. So this semaphore w is the of serves as like a mutex that protects the resource for writer. So it ensures that there’s at most one writer at any time executing its critical section.<br>那么，让我们来看看解决第一个读者的作者问题的方法。所以，最初，一个线程要么是这种简化的读者，要么是一个作家。如果我们看一下作者，这是非常简单的。这只是作者是公正的。在这个信号灯上使用AP。所以这个信号量w就像一个互斥锁一样，为写入器保护资源。因此，它确保在任何时候执行其关键部分时，最多有一个编写器。</p>
<p>发言人   19:27<br>And it does that by just this familiar p followed by a V, you know, readers are a little more interesting. So with the reader, we have this global variable called read count, which is going to keep track of the number of readers that are waiting to read the to read the resource. And there’s a mutex semaphore Tex that protects the accesses, the updates to read count, the reads, and the rights to up to read count. And then there’s this W semaphore, which, as we saw here, is used to protect the critical section in the writer. And initially, both of these are set to one. So the reader in this infinite loop, in each iteration of this infinite loop, is going to increment read count. So it protects that, that access by doing AP on the mutex, which is associated with read count. So only only one reader can can be so this region between the p and the v constitutes the critical section corresponding to read count.<br>它只是通过这个熟悉的p后面跟着一个V来做到这一点，你知道，读者会更有趣一些。因此，对于读取器，我们有这个名为读取计数的全局变量，它将跟踪等待读取资源的读取器数量。并且有一个互斥信号量Tex，用于保护访问、读取计数最新进展、读取以及读取计数权限。然后还有这个W信号量，正如我们在这里看到的，它用于保护作者中的关键部分。最初，这两个都被设置为1。因此，在这个无限循环中的读取器，在这个无限循环的每次迭代中，将增加读取计数。因此，它通过在互斥上执行AP来保护访问，互斥与读取计数相关联。因此只能有一个读取器，因此p和v之间的这个区域构成了对应于读取计数的临界区。</p>
<p>发言人   20:46<br>So after the p completes, then we increment the read count. So now there’s an additional reader. So we can have potentially arbitrary number of these reader threads.<br>所以在p完成后，我们增加读取计数。所以现在又多了一位读者。因此，我们可以有可能任意数量的这些读者线程。</p>
<p>发言人   21:00<br>So we’re indicating that there’s a new reader now, and if read count is one, that means we’re the first reader, so this is sort of the first reader into the implicit cue of waiting readers. So if read count is one, if we’re the first reader, then we do AP on w, which now will lock out any future writers. Now, if there’s already a writer that’s done, it’s p of w, then this will block waiting for that writer to finish, but if there’s no writer inside of its critical section, then this p will just decrement the semaphore w from one to 0 and then lock out any subsequent writers. So now after it’s done this increment and read of the read count variable, then it releases the mutex so that other readers can can access read count.<br>所以我们表示现在有一个新读者，如果阅读计数是1，那意味着我们是第一个读者，所以这是第一个进入等待读者的隐含提示的读者。因此，如果read count是1，如果我们是第一个读者，那么我们会在w上执行AP，这将锁定任何未来的作者。现在，如果已经有一个编写器完成了，它是w中的p，那么这将阻止等待该编写器完成，但是如果其关键部分内没有编写器，然后，这个p将把信号量w从一减为0，然后锁定任何后续的编写器。所以现在在读取计数变量的增量和读取完成后，它会释放互斥锁，以便其他读取器可以访问读取计数。</p>
<p>发言人   22:11<br>Came out, but it’s still holding. It’s holding. So this is kind of interesting, right? It’s holding this semaphore that locks out the writers, but it’s not holding any mutex. So the reader now can just, it can just read, it can do its read, and other readers that are in the same section of the code can also do their reads. So we’re. Allowing multiple readers now inside this critical section, but no writers. So everything, so everything works. Looks like it works good.<br>出来了，但它仍然保持着。它保持着。这有点有趣，对吧？它持有这个信号量来锁定编写器，但它没有持有任何互斥锁。所以读者现在可以，它可以阅读，它可以阅读，在代码的同一部分中的其他读者也可以阅读。所以我们是。现在允许此关键部分中的多个读者，但不允许作者。所以一切，一切都正常。看起来效果不错。</p>
<p>发言人   22:57<br>Now after we read now the number of readers now is going to be we want to decrement the number of readers. So we acquire the mutex on read count, we decrement read count, and then we check to see if we’re the last reader. So if there’s no more readers, in other words, if read count is 0, then now we can release the mutex for the writer so that now writers can access that resource. And after we release that, the writer’s. Mutex then we release the mutex for read count. So any questions about this?<br>现在阅读之后，读者数量将会减少。因此，我们获取了读取计数的互斥，减少了读取计数，然后检查我们是否是最后一个读取者。因此，如果没有更多的读取器，换句话说，如果读取计数为0，那么现在我们可以释放写入程序的互斥锁，以便现在写入程序可以访问该资源。在我们发布之后，作者的。然后我们释放互斥锁以进行读取计数。对此有什么问题吗？</p>
<p>发言人   23:44<br>So an interesting thing for you to think about, just if you have any spare time, is how you might write a version of this readers writers problem that favors writers instead of readers. Yes?<br>所以你需要考虑的一件有趣的事情是，如果你有空闲时间，你可能会写一个有利于作家而不是读者的读者作家问题的版本。是吗？</p>
<p>发言人   24:07<br>Oh no, okay. So the question is, a mutex allows for multiple readers. The fact that we, no, the fact that we’re releasing this mutex here, this mutex is only only protecting access to read count. So we acquire it here, and we release it here after we’ve accessed recount, but we’re not protecting the critical section of the reader with any mutex, except for we’re keeping writers out by, if we’re the first one in. If we’re the first reader, you can, there’s like this implicit cue of readers that we’re keeping track of with read count, okay? So if we’re the first ones in meaning we’re the first reader, then we acquire the p on the writer’s mutex. Now, of course, if there’s a writer inside this critical section, then this P will wait until the writer releases. But once we’ve acquired this mutex w, then we’re locking out any writers, because any writer that arrives will do AP, and it’ll block right here waiting for that w to be nonzero.<br>哦，不，好吧。所以问题是，互斥锁允许多个读取器。我们在这里释放这个互斥锁的事实，这个互斥锁只是保护对读取计数的访问。所以我们在这里获得它，并在访问重新计数后在这里释放它，但我们没有使用任何互斥锁保护读取器的关键部分，除了如果我们是第一个，我们会将作者排除在外面。如果我们是第一个读者，你可以这样做，读者的隐含提示我们正在跟踪阅读计数，好吗？因此，如果我们是第一个读者，那么我们就获得了写入器互斥上的p。当然，如果这个关键部分中有一个作者，那么这个P会等到作者发布。但是一旦我们获得了这个互斥锁w，那么我们就会锁定所有编写器，因为任何到达的编写器都会执行AP，并且它会在这里阻塞，等待w为非零值。</p>
<p>发言人   25:31<br>Okay, so we’re just, we’re blocking out any writers, but then we’re allowing any readers to just to access the resource. Okay, good. Yes, question?<br>好的，所以我们只是，我们阻止任何作者，但是我们允许任何读者访问资源。好的，好的。是的，有问题吗？</p>
<p>发言人   25:51<br>Question is blocking slow? It can be depends. Well, first of all, you’re making a call into the kernel, so it’s a system call. So you’re crossing that boundary, and there’s always overhead associated with that, and then are blocking, you’re blocked until it can be sort of an arbitrary amount of time, right?<br>问题阻碍了吗？这可能取决于。首先，你正在对内核进行调用，所以这是一个系统调用。所以你正在跨越这个界限，总是有与此相关的开销，然后阻塞，你被阻塞，直到它可以是任意的时间，对吗？</p>
<p>发言人   26:08<br>Until someone does some thread, does a V, so yeah, it can be slow. It just depends. You can’t, it’s really hard to bound the time that you’re going to be blocked. Now, if you write the program correctly, eventually you’ll be unblocked. Eventually some thread will execute a V? But when we’re sort of assuming that the kernel does some kind of, when it implements its V, it does some kind of fair scheduling that so a blocked P won’t be in its queue forever, that the kernel does some kind of something that’s fair. So it guarantees that AP won’t remain blocked indefinitely.<br>直到有人执行一些线程，才会执行V，所以是的，它可能很慢。这取决于情况。你不能，很难限制你将被阻塞的时间。现在，如果您正确编写程序，最终您将被解除阻塞。最终，一些线程将执行一个V？但是当我们假设内核做了某种公平的事情，当它实现了它的V时，它会做某种公平的调度，这样被阻塞的P就不会永远在它的队列中，内核会做某种公平的事情。因此，它保证了AP不会无限期地被阻止。</p>
<p>发言人   26:56<br>Any other questions?<br>还有什么问题吗？</p>
<p>发言人   27:02<br>So we can kind of put all of this together and use the use our producer consumer model to implement a pre threshed concurrent echo server.<br>所以我们可以把所有这些结合在一起，使用我们的生产者消费者模型来实现一个预阈值的并发echo服务器。</p>
<p>发言人   27:15<br>Now, so far when we’ve used threads in all our examples of using threads and processes for servers, we created a new thread or process whenever a new connection request arrived and then. That threat interacted with the client. And then whenever that interaction was finished, it closed the connection and exited and killed the thread or process. Now, that’s okay, but it’s inefficient because we’re this. Creating and killing threads introduces overhead. So another way to do this is to create the threads or processes ahead of time, Create a pool of threads where each thread, so we create a pool of these worker threads, where each of these worker threads interact with, can interact with a client. So instead of sort of creating processes and threads on demand, we create what we call a set of pre, pre threshed threads or pre forked processes that do the work.<br>到目前为止，我们在所有使用服务器线程和进程的示例中都使用了线程，每当新的连接请求到达时，我们都会创建一个新的线程或进程。该威胁与客户互动。然后，每当交互完成时，它都会关闭连接并退出并终止线程或进程。现在没关系，但是效率很低，因为我们就是这样。创建和杀死线程会引入开销。因此，另一种方法是提前创建线程或进程，创建一个线程池，每个线程都在其中，因此我们创建一个这些工作线程池，其中每个工作线程都可以与客户端交互。因此，我们不是按需创建进程和线程，而是创建所谓的一组预先、预先阈值的线程或预分叉的进程来完成工作。</p>
<p>发言人   28:27<br>So the idea is that we have this master thread in our server that’s waiting for connection requests from clients by repeated calls to accept. And then when this thread receives a connection request from the client, the accept call, the accept function returns a connected file descriptor associated with the connection to the client. And then it inserts that descriptor into AB. Now remember, descriptors are just small integers that index the descriptor table. So they can be passed around from thread to thread because all the threads are sharing the same, the same descriptor table. So the master threads. Accepts connection request and inserts the corresponding connected file descriptor into the buffer.<br>所以这个想法是，我们的服务器中有一个主线程，它通过重复的调用来等待客户端的连接请求接受。然后，当此线程收到来自客户端的连接请求 (accept调用) 时，accept函数会返回与客户端连接关联的已连接文件描述符。然后它将该描述符插入到AB中。现在记住，描述符只是索引描述符表的小整数。所以它们可以在线程之间传递，因为所有线程都共享相同的描述符表。所以主线程。接受连接请求并将相应的已连接文件描述符插入缓冲区。</p>
<p>发言人   29:31<br>Now each worker threads. So in this case, the items are descriptors. So the worker threads, they’re all waiting on for items to appear in this buffer. And when an item appears, one of the threads will remove that item. And then use that descriptor to interact with the client over the connected file descriptor associated with the connection that exists between the client and the server.<br>现在每个工作线程。所以在这种情况下，项目是描述符。所以工作线程都在等待项目出现在这个缓冲区中。当一个项目出现时，其中一个线程将删除该项目。然后使用该描述符通过与客户端和服务器之间存在的连接相关联的连接文件描述符与客户端进行交互。</p>
<p>发言人   30:06<br>Okay, so now we have the concurrency comes in the form of these multiple worker threads interacting with multiple clients. And then when a worker thread finishes servicing a particular client, then it just goes and it checks for the next file descriptor in the buffer. So this is much more efficient than our previous model, where for each new client, we had to create a thread or a process and then destroy that threader process once we are finished. So we’re sort of re amortizing all the work that we had to go through to create these worker threads by leaving them running and then replacing the destruction or the killing of that thread, replacing it with just a simple, a simple and very fast operation of removing an item from the buffer.<br>好的，现在我们以多个工作线程与多个客户端交互的形式实现并发。然后，当工作线程完成为特定客户端提供服务时，它就会开始检查缓冲区中的下一个文件描述符。因此，这比我们以前的模型要高效得多，在以前的模型中，对于每个新客户端，我们必须创建一个线程或进程，然后在完成后销毁该线程进程。因此，我们通过让它们运行，然后替换线程的销毁或杀死，并用一个简单、简单且非常快速的从缓冲区中删除项的操作来替换它，从而在某种程度上重新摊销了我们创建这些工作线程所必须完成的所有工作。</p>
<p>发言人   31:04<br>So let’s see how we would implement this. And like all of these server examples, it’s surprisingly simple. This is a fully functioning, a real server, but we can do it in one page, 1 page of code.<br>那么让我们看看我们将如何实现这一点。就像所有这些服务器示例一样，它非常简单。这是一个功能齐全的真正服务器，但我们可以用one page一页的代码来完成。</p>
<p>发言人   31:22<br>So for this pre, we’re going to use threads for our concurrent server and we’re going to use the S buff package. So we’re going to create this shared buffer global variable called SBF. And we have listening descriptor and connected descriptor. And we have the client length and client adder that will be used in the accept call. And we have the thread ID that’ll be used in when we create this thread.<br>因此，在这个预先计划中，我们将在并发服务器中使用线程，并且我们将使用S buff包。所以我们将创建一个名为SBF的共享缓冲区全局变量。我们有监听描述符和连接描述符。我们有将在接受呼叫中使用的客户端长度和客户端加法器。并且我们有在创建此线程时将使用的线程账号。</p>
<p>发言人   31:55<br>So now we start by, we’re going in this program, we’re going to pass in the port number. So the server is going to be listening on some port. So we pass that port number in as the first argument. So we call open listen FD on RV of 1. And open listen FD creates a listening descriptor and returns. It returns the value of that descriptor in listen FD.<br>所以现在我们开始，我们将进入这个程序，我们将传入端口号。所以服务器将监听某个端口。所以我们将该端口号作为第一个参数传递进来。所以我们在1的RV上调用开放监听FD。并打开listen FD创建一个监听描述符并返回。它返回该描述符在listen FD中的值。</p>
<p>发言人   32:24<br>And then we call s buff init to initialize our shared buffer. With S buff size file descriptors. And then we create a collection of n threads, worker threads, each of which will execute the thread routine, which we’ve called thread and no argument. So once we create all of these threads, then we go in this infinite loop where we.<br>然后我们调用s buff init来初始化我们的共享缓冲区。带有S buff大小的文件描述符。然后我们创建了一个由n个线程组成的集合，每个线程都将执行我们称之为线程且没有参数的线程例程。所以一旦我们创建了所有这些线程，我们就会进入这个无限循环。</p>
<p>发言人   32:57<br>Call accept. On this listening descriptor, So then acceptable block until a connection request arrives. And when it does, the accept returns with a connected file descriptor that can be used to interact with the client. And once we get that connected file descriptor, then we just simply insert it. We insert that that connected file descriptor into our shared buffer, and then wait for the connection request, the next connection request. So our servers, very efficient, right?<br>呼叫接受。在这个监听描述符上，因此在连接请求到达之前，可接受的阻塞。当它这样做时，accept返回一个连接的文件描述符，可用于与客户端交互。一旦我们获得了连接的文件描述符，那么我们只需简单地插入它。我们将连接的文件描述符插入到共享缓冲区中，然后等待连接请求，下一个连接请求。所以我们的服务器非常高效，对吧？</p>
<p>发言人   33:32<br>We’re just doing an accept and then a very fast insert into the buffer. And then we’re going to let the worker threads do all the work associated with that. That queue of descriptors?<br>我们只是在进行接受，然后非常快速地插入到缓冲区中。然后我们将让工作线程完成与此相关的所有工作。那串描述符？</p>
<p>发言人   33:49<br>Now the thread routine first detaches. So this is a case where we don’t want to run joinable because to join, we’re never going to. Wait for these threads or have any reason to kill them from the main thread. So, so now this worker thread.<br>现在线程例程首先分离。所以这是一个我们不想运行可连接的情况，因为要加入，我们永远不会。等待这些线程或有任何理由从主线程中杀死它们。所以，现在这个工作线程。</p>
<p>发言人   34:10<br>And in this infinite loop, each iteration, it removes an item from the buffer. So it blocks until there’s an item that it can remove from the buffer and it sets it to this local variable, con FD. And then it calls a helper, sort of. This is like the helper function that implements the logic for this particular server. And in this case, it’s an echo server. So this echo count routine will interact with the client, echoing whatever the client sends us until the client closes the connection. And so whenever the client is finished, then we close. Our end of the connection. And go back and get the next item out of the buffer.<br>在这个无限循环中，每次迭代都会从缓冲区中删除一个项目。因此它会一直阻塞，直到有一个项目可以从缓冲区中删除，并将其设置为此局部变量con FD。然后它会调用助手。这就像实现此特定服务器逻辑的帮助函数。在这种情况下，它是一个echo服务器。所以这个echo计数例程将与客户端交互，回响客户端发送给我们的任何内容，直到客户端关闭连接。因此，每当客户完成时，我们就会关闭。我们的连接结束。然后返回并从缓冲区中取出下一个项目。</p>
<p>发言人   35:04<br>And I should point out that echo count is just a placeholder. Anything. This could be the logic for a web server or any kind of web service or any kind of service.<br>我应该指出，回声计数只是一个占位符。任何事情。这可以是web服务器或任何类型的web服务或任何类型的服务的逻辑。</p>
<p>发言人   35:17<br>Now? To initialize this echo count function, we need to. Initialize the mutex. So so this echo count function is going to have it defines a global variable called byte count.<br>现在？初始化这个回声计数函数，我们需要。初始化互斥锁。所以这个echo计数函数将定义一个名为字节计数的全局变量。</p>
<p>发言人   35:41<br>So in this echo server, we’re going to keep track of the number of bytes that we’ve received from all the clients that we’re interacting with. So there’s a global variable called byte count, which is shared by all the threads. And we’re going to update this byte count variable every time we receive something, every time we receive data from the client. And we’re going to use mutex to protect the accesses to byte count. So we’re going to initially have to initialize, we’re going to have to call a function that initializes. This by first initializing the mutex and then setting byte count to 0. And then within echo count itself.<br>因此，在这个echo服务器中，我们将跟踪我们从与我们交互的所有客户端收到的字节数。因此，有一个称为字节计数的全局变量，由所有线程共享。并且我们将在每次收到某些内容时更新这个字节计数变量，每次从客户端接收数据时。我们将使用互斥锁来保护对字节数的访问。所以我们最初必须初始化，我们将不得不调用一个初始化的函数。首先初始化互斥锁，然后将字节计数设置为0。然后在回声计数本身内。</p>
<p>发言人   36:39<br>We’ve already seen a way to initialize a package if we have some kind of package of library functions that are going to be used by multiple threads. There are several ways to actually initialize this package. So one way is to explicitly call have the main thread called the initialization function once. So we’ve seen that with like the S buff package, the main thread has to call. The main thread calls S Buffin at once, before any of the pure threads execute. Any of the worker threads execute. But there’s another way we can do this too.<br>我们已经看到了一种初始化包的方法，如果我们有某种类型的库函数包将被多个线程使用。有几种方法可以实际初始化这个包。所以一种方法是显式调用主线程调用初始化函数一次。所以我们已经看到，使用像S buff包，主线程必须调用。主线程在任何纯线程执行之前立即调用S Buffin。任何工作线程都会执行。但我们也有另一种方法可以做到这一点。</p>
<p>发言人   37:22<br>We can have the worker threads actually called the initialization function, and we can use this technique provided by P threads, where we define a static variable. So this is a static local variable. But you recall that this is actually treated like a global variable. Every thread has access to this variable, but its scope is limited to the echo count function, So no other function can access this variable. But each thread that executes this thread routine has access to it. And in this context, it’s treated like a global. So if one thread updates the value, other every thread sees that same value. So we can use this mechanism from p threads.<br>我们可以让工作线程实际调用初始化函数，并且我们可以使用P线程提供的这种技术，在那里我们定义一个静态变量。所以这是一个静态局部变量。但你记得这实际上被视为一个全局变量。每个线程都可以访问此变量，但其范围仅限于echo计数函数，因此没有其他函数可以访问此变量。但每个执行此线程例程的线程都可以访问它。在这种情况下，它被视为全球性的。因此，如果一个线程最新进展该值，其他每个线程都会看到相同的值。所以我们可以在p个线程中使用这种机制。</p>
<p>发言人   38:25<br>So we can create this, this variable of type P thread once t, and initialize it to this special P threads value, which is sort of like the P threads uninitialized value. So this is a value that P threads knows about that indicates that that this variable once hasn’t been initialized. And then we call the p-three function passing at this, that the address of this variable created, that we created, and the address of the function to call to initialize whatever it is we want to initialize, in this case, the echo count. The echo count variable. And so what this does, every thread we’ll call p thread once, but only one thread will actually call the initialization function.<br>所以我们可以创建这个，类型为P线程一次t的变量，并将其初始化为这个特殊的P线程值，这有点像P线程未初始化的值。所以这是P个线程知道的值，表明这个变量曾经没有被初始化。然后我们调用p-三个函数传递给这个变量，这个变量的地址是我们创建的，以及用来初始化我们想要初始化的任何东西的函数的地址，在这种情况下是echo计数。echo计数变量。这样做的目的是，每个线程我们都会调用p线程一次，但只有一个线程会实际调用初始化函数。</p>
<p>发言人   39:30<br>Only the very first thread that executes pthread once, we’ll call it the other threads. This pthread call will be like a Noah, yes. Well, that’s the other option. So the advantage of this is that. You can. I guess the advantage is, I don’t know. It’s just another way you can do it. I guess it avoids having to do it in the master thread that you can make your, you can make this package sort of self contained, right? That you’re not really counting on the master doing anything. But yeah, that’s the other way we could have done it.<br>只有第一个执行pthread一次的线程，我们称之为其他线程。这个pthread调用将像一个诺亚，是的。好吧，那是另一个选择。这样做的好处是。你可以。我想优势是，我不知道。这只是你可以做的另一种方式。我猜它避免了在你可以制作的主线程中完成它，你可以使这个包自我包含，对吧？你并不真的指望主人做任何事情。但是，是的，这是我们本可以做到的另一种方式。</p>
<p>发言人   40:17<br>So I just wanted to show you this other technique.<br>所以我只是想向你展示另一种技术。</p>
<p>发言人   40:22<br>So once we initialize. Some thread calls a net echo count. Then we initialize the Rio package for all of our accesses on this connected descriptor. And then we repeatedly read a line of text from the client. And then in a protected way, we increment byte count with the number of bytes that we received from the client, which is returned by this Rio readline B function. And then we print a little message just to sort of keep track so we can see, keep track of our running total. And then we release the mutex on the byte count, the global byte count variable. And then we echo that line back to the client.<br>所以一旦我们初始化。一些线程调用了净回声计数。然后我们为这个连接的描述符上的所有访问初始化Rio包。然后我们反复从客户端读取一行文本。然后以一种受保护的方式，我们根据从客户端收到的字节数递增字节计数，这是由这个Rio readline B函数返回的。然后我们打印一条小信息，只是为了跟踪，这样我们就可以看到，跟踪我们的累计总数。然后我们释放字节数 (全局字节数变量) 上的互斥锁。然后我们将这句话反馈给客户。</p>
<p>发言人   41:18<br>So any questions about that? Yes, questions. So the line that declares it will only get executed for the first time that time.<br>有什么问题吗？是的，有问题。所以声明它的行只会在第一次执行时执行。</p>
<p>发言人   41:30<br>Okay? The question is, the line that declares the static variable once, will it only be executed the first time a thread executes that statement? So the answer is no. Every thread will define this variable and assign it to this P thread. Once value. What will only happen once is the call to a net echo count. So the first thread that executes this p thread once call, we’ll actually call a n echo count. Every other thread, every subsequent thread will not call it, it’ll be like a no-op. Yeah, uses the ones. It’s just an opaque.<br>好吗？问题是，声明一次静态变量的行，是否仅在线程第一次执行该语句时才执行？所以答案是否定的。每个线程都将定义此变量并将其分配给此P线程。一旦有了价值。只会发生一次的是对网络回声计数的调用。因此，执行这个p线程的第一个线程一旦调用，我们将实际调用一个n echo计数。在每个其他线程中，每个后续线程都不会调用它，它将像一个无操作。是的，使用那些。它只是一个不透明的。</p>
<p>发言人   42:25<br>When the other function calls, the declaration line wanted be resetting the flag by resetting its speed once in it. Somehow it will, but somehow the P thread once is keeping track that it’s executed. So I’m really not sure how it’s implemented. So somehow pthread want, I guess, every know, I mean, this is just AC declaration. So there pthread has no control over declarations, so every thread will get. Every thread will sort of update this static variable. And you’re right, the first thread. The second thread would overwrite this value again, but somehow pthread once keeps track of that in some way that I’m not sure about how that works, but this is the way you get that behavior.<br>当另一个函数调用时，声明行想要通过在其中重置其速度来重置标志。不知何故它会，但不知何故，P线程曾经一直在跟踪它的执行情况。所以我真的不确定它是如何实现的。所以不知何故，pthread想要，我想每个人都知道，我的意思是，这只是交流声明。因此，pthread无法控制声明，因此每个线程都将获取。每个线程都会更新这个静态变量。你说得对，第一条线索。第二个线程会再次覆盖此值，但不知何故，pthread曾经以某种方式跟踪过这一点，我不确定它是如何工作的，但这就是您获得这种行为的方式。</p>
<p>发言人   43:39<br>Now, there’s some other issues around synchronizing threads, sort of correctness issues that we have to be aware of. So I hope you’re sort of getting a sense that this threaded programming is kind of a tricky business, right? And so one issue that we always have to be aware of is this idea called threadsa. So in general, a thread routine can only, should only call functions that are thread safe that have this property called thread safety. And a function is thread-safe if and only if that function can be invoked by multiple threads at the same time. So if we have a function f, it’s thread safe if and only if it’s execution can be interleaved by multiple threads.<br>现在，在同步线程方面还有一些其他问题，我们必须注意一些正确性问题。所以我希望你能感知这种线程编程是一件棘手的事情，对吧？因此，我们必须始终注意的一个问题是这个被称为线程的想法。因此，通常情况下，线程例程只能调用具有称为线程安全属性的线程安全函数。一个函数是线程安全的，当且仅当该函数可以同时被多个线程调用。因此，如果我们有一个函数f，当且仅当它的执行可以被多线程交错时，它是线程安全的。</p>
<p>发言人   44:37<br>And so we can identify four different classes of thread unsafe functions. One class is the functions that fail to protect shared variables with mutexes. So we’ve already seen that with bad count. That was an example of a threat, unsafe. That main routine was an example of an unsafe thread function. Or no? The thread function was an example of an unsafe thread function because it didn’t protect the update of the count variable. Another class of functions that are thread unsafe are those functions that keep track of state across multiple invocations. So if they’re storing state in some global variable, private or public global variable, the net, that’s thread unsafe because multiple threads will be accessing that state.<br>因此，我们可以识别四类不同的线程不安全函数。一类是未能保护具有互斥锁的共享变量的函数。所以我们已经看到了错误的计数。这是一个威胁的例子，不安全。那个主例程是一个不安全的线程函数的例子。还是没有？thread函数是不安全线程函数的一个例子，因为它没有保护count变量的更新。线程不安全的另一类函数是那些跟踪多个调用状态的函数。因此，如果它们将状态存储在某个全局变量，私有或公共全局变量中，那么这是线程不安全的，因为多个线程将访问该状态。</p>
<p>发言人   45:36<br>Another kind of thread unsafe function or functions that return a pointer to a static variable. So there’s a number of functions in the standard C library that were written before threads were even on anybody’s radar. And so an example is the C time function, which takes as an argument a time struct, a binary time struct, and returns a pointer to a string, a date and time string. But the address in that pointer is always the same. So this function is defining some kind of static variable, and it’s always returned. And then it’s converting the binary time stretched into a string that’s always at the same location, and it’s returning the address of that string. So every invocation returns the exact same address, but with different content at that address. And they didn’t realize that this was a bad thing to do for threaded programs, because nobody was writing threaded programs at the time.<br>另一种线程不安全的函数或返回指向静态变量的指针的函数。因此，标准C库中有许多函数是在线程出现在任何人的雷达之前编写的。一个例子是C时间函数，它接受一个时间结构、一个二进制时间结构作为参数，并返回一个指向字符串、一个日期和时间字符串的指针。但是该指针中的地址始终是相同的。所以这个函数正在定义某种静态变量，并且它总是返回。然后它将二进制时间拉伸转换为始终在同一位置的字符串，并返回该字符串的地址。因此，每次调用都会返回完全相同的地址，但在该地址具有不同的内容。他们没有意识到这对于线程程序来说是一件坏事，因为当时没有人编写线程程序。</p>
<p>发言人   46:45<br>And then obviously, any function that calls a thread unsafe function is thread unsafe.<br>然后显然，任何调用线程不安全函数的函数都是线程不安全的。</p>
<p>发言人   46:50<br>So let’s look at these different classes of functions. So the class 1 functions fail to protect shared variables. And so the fix, as we’ve seen, is to use P and V to guarantee mutually exclusive access and thereby protect the accesses to the variable. So we saw this with that good count program. And then the problem also, as we saw with good count, is that the synchronization operations can be slow. So if they’re in a tight inner loop, it can really slow your program down.<br>那么让我们来看看这些不同类的函数。所以1类函数无法保护共享变量。因此，正如我们所看到的，解决方法是使用P和V来保证互斥访问，从而保护对变量的访问。所以我们在那个好的计数程序中看到了这一点。然后问题也是，正如我们在良好的计数中看到的那样，同步操作可能很慢。因此，如果它们处于紧密的内部循环中，它可能会真正减慢您的程序速度。</p>
<p>发言人   47:25<br>The Class Ii threat unsafe functions rely on some kind of persistent state across invocations of that function. So the classic example is the libc rand function. And this an implementation of which is I took from the K and R book.<br>Ii类威胁不安全函数依赖于该函数调用之间的某种持久状态。所以经典的例子是libc rand函数。这个实现是我从K和R书中获得的。</p>
<p>发言人   47:48<br>So this rant, this is a pseudorandom number generator, a pseudorandom. In the sense that if you give it the same key, it’ll return the same sequence of values. So this is kind of nice because it allows, when you’re testing, it allows repeatability. So every time you call it, if you call it with the same seed, you’re guaranteed you’ll get the same results.<br>所以这个rant，这是一个伪随机数生成器，一个伪随机数。感知是，如果您为其指定相同的键，它将返回相同的值序列。所以这有点不错，因为它允许在测试时可重复性。因此，每次您调用它时，如果您使用相同的种子调用它，则可以保证获得相同的结果。</p>
<p>发言人   48:16<br>And the way this is implemented. Is that there’s that the seed? There’s a. Seed variable called next, which is used in each iteration of the random number generator. It’s defined as a global private. So static makes it private, so it’s not accessible to programs. They’re calling the rand function, but it’s used by the rand function. And so this variable is initialized to one. There’s a function called srand, which allows the user to set the seed value. The default seed value is one, but if the user calls sran, they can pass in a seed, which which will be which is assigned to this next variable.<br>以及它的实施方式。那是种子吗？有一个。称为next的种子变量，用于随机数生成器的每次迭代。它被定义为全球私有。因此静态使其成为私有的，因此程序无法访问它。他们正在调用ranand函数，但它被ranand函数使用。所以这个变量被初始化为一。有一个名为srand的函数，它允许用户设置种子值。默认种子值为1，但如果用户调用sran，则可以传入一个种子，该种子将分配给下一个变量。</p>
<p>发言人   49:09<br>And then each iteration of rand does an operation on the seed. So it takes the next value that’s going to be used is the property of the previous value, the function of the previous value. And then it returns a pseudorandom number that’s a function of that next value.<br>然后ranand的每次迭代都对种子进行操作。因此，它将使用下一个值，即前一个值的属性，前一个值的函数。然后它返回一个伪随机数，它是下一个值的函数。</p>
<p>发言人   49:31<br>So it’s relying on this each iteration, each time you call Rand, you’re relying on the this next value that was computed by the previous time that you called rand. Now, this is perfectly fine, and there’s no problem with this in a non threaded situation, but what happens if multiple threads now? So suppose you have multiple threads and they’re each calling this rand function, sort of interleaving calls to rand. The fact that Rand is relying on this previous state. If multiple threads are calling ran, it’s going to break this pseudorandom property. So each thread, the random numbers each thread gets back are not only a function of the previous, the seed from the previous time that thread called a function, but also also a function of the other threads that are calling it.<br>所以它依赖于每次迭代，每次你调用ranand时，你都依赖于之前调用ranand时计算出的下一个值。现在，这很好，在非线程情况下也没有问题，但是如果现在有多个线程会发生什么？因此，假设您有多个线程，每个线程都在调用这个ranand函数，类似于对ranand的交错调用。兰特依赖这个先前的状态的事实。如果多个线程正在调用运行，它将破坏这个伪随机属性。因此，每个线程所得到的随机数不仅是前一个线程调用函数时的种子函数，也是其他调用该函数的线程的函数。</p>
<p>发言人   50:34<br>So. So if a particular thread calls this random number generator multiple times, it potentially won’t see the same sequence of pseudorandom numbers because other threads will be jumping in.<br>所以。因此，如果特定线程多次调用此随机数生成器，它可能不会看到相同的伪随机数序列，因为其他线程将会跳进来。</p>
<p>发言人   50:54<br>It’s not incorrect in that the program will fail. But if the program is counting on the pseudorandom property, then it creates a problem. So the solution to this is to rewrite rand and require it, require the caller to. Keep track of this next variable so each caller will keep its own local copy of Next, and it’ll pass in a pointer to Rand. Rand will compute that value. So now this will be updating state in the calling thread. This is local state on that thread stack, so every thread will have its own copy of Next.<br>这不是错误的，因为程序会失败。但是如果程序依赖于伪随机属性，那么就会产生问题。所以解决这个问题的方法是重写ran过并要求它，要求调用者这样做。跟踪这个下一个变量，以便每个调用者都保留自己的下一个本地副本，并传递一个指向rant的指针。Rand将计算该值。所以现在这将在调用线程中更新状态。这是线程堆栈上的本地状态，因此每个线程都有自己的下一个副本。</p>
<p>发言人   51:43<br>But we have to create a new function, and we’ll call it underscore r, This stands for reentrant, which is a property we’ll look at in just a second. But it’s more work for the programmer, because now the programmer has to maintain this sort of this next value, okay?<br>但我们必须创建一个新的函数，我们将其称为下划线r，这代表可重入，这是我们将在一秒钟内查看的一个属性。但这对程序员来说是更多的工作，因为现在程序员必须维护这种下一个值，好吗？</p>
<p>发言人   52:05<br>Another way that threads that functions are unsafe is are these functions that always return a pointer to the same global variable? Typically, it’s a static variable, but they always return the same value each time, the same address. So you can see this is similar to that race that we encountered before where we were passing the address of a connected file descriptor to a worker thread. So now we’re creating a race.<br>线程不安全的另一种方式是这些函数是否始终返回指向同一全局变量的指针？通常，它是一个静态变量，但它们每次都返回相同的值，相同的地址。因此，您可以看到这类似于我们之前遇到的竞争，我们正在将连接的文件描述符的地址传递给工作线程。所以现在我们正在创造一场比赛。</p>
<p>发言人   52:39<br>So let’s say one, let’s say one thread calls this function, for example, c time, takes this time struct as an argument, which can correspond to an arbitrary time. It could be the current time, or just some arbitrary time that the caller constructed, and it returns a pointer to a char star. So it just returns a pointer to a string that represents the date and the time. So an Ascii Ascii string that represents the date and time. But it’s always returning a pointer to the same, the same location in memory. So you can see the problem.<br>那么让我们假设一个线程调用这个函数，例如c time，将这个时间结构作为参数，它可以对应于任意时间。它可以是当前时间，也可以是调用者构造的任意时间，它返回一个指向char star的指针。所以它只是返回一个指向代表日期和时间的字符串的指针。因此，一个代表日期和时间的Ascii字符串。但它总是返回指向内存中相同位置的指针。这样你就可以看到问题所在。</p>
<p>发言人   53:26<br>If thread A calls this C time function with with one time struct, it gets back a pointer to the character string corresponding to that time struct. But now let’s say before thread A can use that, read that string, another thread calls z time. In that instance of c time will overwrite the copy of the time string for that thread A computed. So when thread A finally gets a chance to access that time string, it’s accessing thread B is time string and not thread A’s time string. And it just depends if thread A can get to that and read that variable before thread B overwrites it, then everything’s fine. Otherwise thread A is accessing the wrong time string. So there’s a couple of ways to fix this.<br>如果线程A使用一个时间结构调用此C时间函数，则它将返回指向与该时间结构对应的字符串的指针。但是现在让我们说在线程A可以使用之前，读取该字符串，另一个线程调用z时间。在c的实例中，时间将覆盖该线程计算的时间字符串的副本。因此，当线程A终于有机会访问该时间字符串时，它访问的是线程B的时间字符串，而不是线程a的时间字符串。这取决于线程A是否能够在线程B覆盖它之前读取该变量，那么一切都很好。否则线程A正在访问错误的时间字符串。所以有几种方法可以解决这个问题。</p>
<p>发言人   54:25<br>Like we could rewrite the function, the C time function to take add another argument that passes in the. The location, the address of the time string. So we could require the caller to allocate space for the time string and pass in the address to the C time function. But this? Would require us to change all the instances where we call C time, but we’d also have to change the implementation of C time in the Li C in the library, right? And so we can’t, we don’t have access to lise’s source on our system. So that’s just not a feasible thing. Plus it would break every other program that called C time. So we just can’t do that.<br>就像我们可以重写函数一样，C时间函数加入另一个传入的参数。时间字符串的位置、地址。所以我们可以要求调用者为时间字符串分配空间并将地址传递给C time函数。但是这个？这将需要我们更改所有调用C时间的实例，但我们还必须更改库中Li C中C时间的实现，对吗？所以我们不能，我们无法在我们的系统上访问lise的源代码。所以这不是一个可行的事情。再加上它会中断所有其他调用C时间的程序。所以我们就是不能这样做。</p>
<p>发言人   55:20<br>Another, a better option is to create a new function of our own called c time underscore TS for thread-safe So we’ll create our own sort of wrapper function for the c time, and we’ll use a technique called lock and copy to provide thread safe access to C time. So the way it works is we’ll write this new function c time underscore TS, which just like c time, takes this pointer to this time struct, but then it adds a second argument, which is a pointer to the thread’s private copy of the time string, so the collar allocates the space and passes the pointer to this to that string, and then within c time, we have a local variable called shared the shared pointer. So this is going to point to that shared global data structure that ctime is accessing, and so first we do the lock, that’s the lock part of lock and copy by acquiring a mutex, and then we call c time. So only one thread at a time will have this mutex. So whatever thread. So once we return from p, we know that we’re the only thread in this critical section.<br>另一个更好的选择是面向消费者我们自己的名为c时间下划线TS的新函数来实现线程安全，因此我们将为c时间创建我们自己的包装函数，并且我们将使用称为锁定和复制的技术来提供线程安全访问面向消费者。所以它的工作方式是我们将编写这个新函数c时间下划线TS，就像c时间一样，将此指针指向此时间结构，但然后它添加了第二个参数，这是指向线程的时间字符串的私有副本的指针。因此，项圈分配空间并将指向此的指针传递给该字符串，然后在c时间内，我们有一个名为共享共享指针的局部变量。所以这将指向ctime正在访问的共享全局数据结构，因此我们首先进行锁定，即通过获取互斥锁来锁定和复制的锁定部分，然后我们调用c time。所以一次只有一个线程会有这个互斥锁。所以无论线程。因此，一旦我们从p返回，我们就知道我们是这个关键部分中的唯一线程。</p>
<p>发言人   56:56<br>So we call c time the normal Li CZ time function, which returns a pointer to the same location. And then we do the copy part. We copy that string to the private string that was passed in just into our function. Once we’ve done the copy, then we can release the mutex, and then we return a pointer.<br>所以我们调用正常的Li CZ时间函数c时间，它返回指向同一位置的指针。然后我们进行复制部分。我们将该字符串复制到刚刚传递给我们的函数的私有字符串中。一旦我们完成了复制，我们就可以释放互斥锁，然后返回一个指针。</p>
<p>发言人   57:25<br>We return private p back to the caller. So, and this is just more of a convenience to the caller, programs that are using C time are expecting to get that pointer back. So by using lock and copy, we have to make changes, we have to write this new function. But it’s fairly simple. We have to make changes every place in our program where we call C time, we have to update those to calls to ctime underscore TS and create this local string array. Yes, used by y, it has to be a pointer, but like, why not any type of variable that would cause this kind of.<br>我们将私人p返回给呼叫者。因此，这只是为了方便调用者，使用C时间的程序期望将该指针返回。因此，通过使用锁定和复制，我们必须进行更改，我们必须编写这个新函数。但这相当简单。我们必须在程序中调用C时间的每个地方进行更改，我们必须更新这些更改以调用ctime下划线并创建此本地字符串数组。是的，被y使用，它必须是一个指针，但是，为什么不是任何类型的变量都会导致这种情况。</p>
<p>发言人   58:24<br>Well, typically, these functions are returning pointers to some data structure. And so they’re sort of updating the data structure and then returning a pointer to it. It wouldn’t make, I don’t know how, I guess it could return a struct and it would always return no, no. I can’t think of. I can’t think of any reason why they would turn anything but a pointer because they’re typically updating some data structure and then returning a pointer to it.<br>通常情况下，这些函数会返回指向某些数据结构的指针。因此，他们在某种程度上更新数据结构，然后返回指向它的指针。它不会做，我不知道怎么做，我猜它可能会返回一个结构，并且它总是返回no，no。我想不起来了。我想不出任何原因为什么他们会把指针转到其他地方，因为他们通常会更新一些数据结构，然后返回一个指向它的指针。</p>
<p>发言人   59:05<br>If they were returning. If they were returning scalars, those scalars would always be returned in EA or Rax. So actually, that would be okay. It’s the pointer that causes a problem because it’s always returning that value in eix, but it’s always returning the same value in E, always pointing to the same, the same data structure. Okay, good. Okay, now one.<br>如果他们回来了。如果他们是返回的标量，这些标量将始终以EA或Rax返回。所以实际上，那没关系。指针会导致问题，因为它总是在eix中返回该值，但它总是在E中返回相同的值，总是指向相同的数据结构。好的，好的。好的，现在一个。</p>
<p>发言人   59:41<br>Potentially significant disadvantage of Locke and copy is that this copy might not always be as simple as just doing like a stir copy. If the function that you’re calling is computing some complex data structure, like a Nest, a struct which contains structs and pointers to arrays, then this copy can get quite complicated. It would require what we call a deep copy. So that can be very difficult too. But in this case, it’s simple. We’re just copying one string to another.<br>锁和复制的潜在重大缺点是，这种复制可能并不总是像搅拌复制那样简单。如果你正在调用的函数正在计算一些复杂的数据结构，比如嵌套，一个包含结构和数组指针的结构，那么这个副本可能会变得相当复杂。这将需要我们所谓的深度复制。所以这也可能非常困难。但在这种情况下，它很简单。我们只是将一个字符串复制到另一个字符串。</p>
<p>发言人   01:00:21<br>And then finally, the fourth class of thread on, say, functions are functions that call unsafe functions. So it’s kind of obvious. And then the obvious fix is to not call threat unsafe functions from within your function. And then you can make it thread safe. Now, there’s a very interesting, an important subclass of thread safe functions called re-entering functions.<br>最后，函数上的第四类线程是调用不安全函数的函数。所以这是显而易见的。然后显而易见的解决办法是不要从您的函数内部调用威胁不安全的函数。然后你可以使它线程安全。现在，有一个非常有趣的线程安全函数的重要子类，称为重新进入函数。</p>
<p>发言人   01:00:49<br>So a function is reentrant if it contains no accesses to shared variables. So if every variable that it accesses is contained is declared as a local variable and stored on the stack for that function, that’s called a reentrant function. And because there’s no accesses of any kind to shared variables, there’s no synchronization required because every function is operating, accessing its own local copy of all the variables. And if multiple threads execute two instances of a re-entering function, it’s okay. Each thread has its own separate stack, so you don’t need to worry about any kind of synchronization that can run independently. So the reason reentrant functions are so important is because it’s expensive to do synchronization. And so you can avoid it completely with these reentrant functions. So they’re efficient.<br>因此，如果一个函数不包含对共享变量的访问，它就是可重入的。因此，如果它访问的每个变量都被声明为本地变量并存储在该函数的堆栈上，则称为可重入函数。由于没有任何类型的共享变量访问，因此不需要同步，因为每个函数都在操作，访问所有变量的本地副本。如果多个线程执行两个重新进入函数的实例，也没关系。每个线程都有自己独立的堆栈，因此您无需担心任何可以独立运行的同步。所以可重入函数如此重要的原因是因为同步代价很高。因此，您可以使用这些可重入函数完全避免它。所以他们很有效率。</p>
<p>发言人   01:02:01<br>So as the diagram shows, every re-entering function is thread-safe, but not every thread safe function is reentrant. So we saw that before, right? When we have a function that accesses a shared variable, we can make it thread safe by protecting it with a mutex. But it’s not reentrant because it’s accessing shared variables.<br>因此，如图所示，每个重新进入的函数都是线程安全的，但并不是每个线程安全的函数都是可重入的。所以我们之前就看到了，对吧？当我们有一个访问共享变量的函数时，我们可以通过用互斥锁保护它来使其线程安全。但它不是可重入的，因为它正在访问共享变量。</p>
<p>发言人   01:02:32<br>Now all the functions in the standard C library, which are enumerated in the back of your K and R text are thread-safe, but not necessarily reentering. And most syscalls are thread safe, just with a few exceptions that I’ve listed here. I don’t think this is complete, but these are just sort of examples of some notable ones. And so for each of these thread unsafe functions Linux provides a. Reentrant version, which is denoted by underscore r, and then that reentrant version has a different set of parameters. Typically, the only exception that I know about is Inet N 2 A, which is sort of an obsolete network protocol for converting sort of binary network addresses to human readable Ascii addresses. But this has been obsoleted by other calls. So it’s, I guess they just never, never bothered to create a reentrant version for it because there’s other options, alternatives to using that.<br>现在标准C库中的所有函数 (在你的K和R文本后面枚举) 都是线程安全的，但不一定要重新进入。大多数系统调用都是线程安全的，只有我在此处列出的少数例外。我不认为这是完整的，但这些只是一些值得注意的例子。因此，对于每个线程不安全的函数，Linux都提供了一个。可重入版本，用下划线r表示，然后可重入版本有一组不同的参数。通常情况下，我所知道的唯一例外是Inet N 2 A，它是一种过时的网络协议，用于将二进制网络地址转换为人类可读的Ascii地址。但这已经被其他电话淘汰了。所以我想他们从来没有，从来没有费心为它创建一个可重入版本，因为还有其他选择，可以使用它的替代方案。</p>
<p>发言人   01:03:45<br>Okay, so another thing we have to worry about, as we’ve seen, is races. This is the real bugaboo in threaded programs, and it typically involves some kind of unexpected sharing. So in this case I’m going to revisit this. Incorrect threaded program that where we introduce a race by passing. When we create the thread, we pass the argument to the thread, which is like the local thread ID. We pass an address of a variable that we have stored on the stack, in this case, the eye, the loop iterator, and so. We’ve seen that this causes a race.<br>好的，我们必须担心的另一件事，正如我们所看到的，就是种族。这是线程程序中的真正问题，通常涉及某种意外的共享。所以在这种情况下，我将重新审视这个。不正确的线程程序，我们通过传递引入比赛。当我们创建线程时，我们将参数传递给线程，这就像本地线程账号一样。我们传递一个存储在堆栈上的变量的地址，在本例中是眼睛、循环迭代器等等。我们已经看到这会导致一场比赛。</p>
<p>发言人   01:04:40<br>We set initially I as zero. Then we create a new thread, which is pure thread 0. And then this thread dereferences the pointer to get its local copy of the of this sort of local threat ID. But now we’ve introduced the race between the I and the dereferencing incrementing of I in the main thread and the dereferencing of I in the pure thread. So if this dereferencing happens before I is incremented, then we’re good. But if this dereferencing happens after we increment I, so in other words, that when I equal one, then we get the wrong value in the pure thread for myo. So you might wonder, and I think there was a question before about this, seems the odds of this happening seems so low, why? Why are we even worrying about it?<br>我们最初将I设置为零。然后我们创建一个新线程，它是纯线程0。然后此线程取消引用指针以获取此类本地威胁账号的本地副本。但是现在我们引入了主线程中的I和取消引用增量之间的竞争，以及纯线程中的I取消引用。所以，如果这个取消引用发生在I递增之前，那么我们很好。但是如果这种取消引用在我们递增I之后发生，换句话说，当I等于一时，那么我们在myo的纯线程中得到错误的值。所以你可能会想，我想之前有一个问题，似乎这种情况发生的几率如此之低，为什么？为什么我们还要担心呢？</p>
<p>发言人   01:05:44<br>So we actually, just to sort of test this out, we wrote a program to see if we could see if we could actually see this race in practice. And that’s one of the great things about 213 is that we can just try stuff out. So we just tried it out.<br>所以我们实际上，只是为了测试一下，我们写了一个程序，看看我们是否能在实践中看到这场比赛。这就是213的伟大之处之一，我们可以尝试一些东西。所以我们刚刚尝试了一下。</p>
<p>发言人   01:06:05<br>So we wrote a simple main thread that creates 100 different threads each. We as the argument, we pass the address of this local variable I. And then in each pure thread, we detach the thread dereference. And then we have a function that saves the value.<br>所以我们编写了一个简单的主线程，每个线程创建100个不同的线程。我们作为参数，我们传递这个局部变量I的地址。然后在每个纯线程中，我们分离线程取消引用。然后我们有一个保存值的函数。</p>
<p>发言人   01:06:28<br>So we’re storing, we’re storing that value of I for future reference. So now if there’s no race, each of the 100 threads would get a separate distinct thread ID. So each value zero through 99, if we made a histogram of it, there would be exactly one instance of each value of I, but if there was a race be for some values of I, there would be multiple instances that were encountered in multiple threads. So you can see if we go back here if. The pure thread loses the race and I gets incremented before it can dereference.<br>所以我们正在存储，我们正在存储I的值以供将来参考。所以现在如果没有竞争，100个线程中的每个线程都将得到一个单独的不同线程账号。因此，每个值从零到99，如果我们对其制作直方图，则I的每个值都会有一个实例，但如果某些I值存在竞争，则会在多个线程中遇到多个实例。所以你可以看看我们是否回到这里。纯线程在比赛中输了，我在取消引用之前被增加了。</p>
<p>发言人   01:07:19<br>Now we’ve got pure thread. 0 actually gets an ID of 1. And then pure thread one, if there’s no race, it’ll get the correct value of one. So now we’ve got two instances of one.<br>现在我们有了纯净的线程。0实际上得到的账号为1。然后是纯线程1，如果没有比赛，它将获得正确的值1。所以现在我们有两个实例。</p>
<p>发言人   01:07:37<br>Okay, so let’s look, so this is the case. So we’ve plotted the results for a case where there’s no race. So along the x axis, sorry, this is too small, the x axis gives us all the 100 values of I 0 through 99. And then the y axis is the count. So this is we’re doing a histogram of for all the values zero through 99. So in this case, every value has exactly one instance, so no race, there was no race, no races involved in all 99 or all 100 instances.<br>好的，让我们来看看，就是这种情况。所以我们绘制了一个没有种族的案例的结果。所以沿着x轴，抱歉，这太小了，x轴给我们所有的i0到99的100个值。然后y轴是计数。所以我们正在为从0到99的所有值做一个直方图。所以在这种情况下，每个值都只有一个实例，所以没有竞争，没有竞争，在所有99个或所有100个实例中都没有竞争。</p>
<p>发言人   01:08:20<br>If we run it on a single core laptop. So now each thread is sort of taking its turn on a single core, It happens a few times. So there’s a few times where the one thread gets preempted and the other thread begins to run. Before it can. So one thread gets preemptive before it can dereference the variable, so it gets the wrong value, but it’s not very common. It happened 1, 3, 4, 5, 6, 7 times. But now if we run this program on a multi course server, you can see it happens a lot. In fact, it happens most of the time. So it almost never, we almost never get the correct, the correct value for my ID.<br>如果我们在单核笔记本电脑上运行它。所以现在每个线程都在一个核心上轮流运行，这种情况会发生几次。所以有几次，一个线程被抢占，另一个线程开始运行。在它可以之前。因此，一个线程在可以取消引用变量之前被抢占，因此它得到了错误的值，但这不是很常见。它发生了1、3、4、5、6、7次。但是现在如果我们在一个多门课程服务器上运行这个程序，你可以看到它经常发生。事实上，这种情况大部分时间都在发生。所以它几乎从来没有，我们几乎从来没有为我的账号得到正确的、正确的值。</p>
<p>发言人   01:09:10<br>Okay? So this is just another example of some of the things that can just drive you crazy if you’re not careful when you program in threads with threads. And so as we saw, the way to eliminate these kind of erases is to avoid the sharing of state. And in this case, by allocating for each thread, allocating a separate block in the heap that’ll hold the, that’ll hold the local ID for that thread, and then passing a pointer to that, that unique block of storage to the thread.<br>好吗？所以这只是另一个例子，说明如果您在使用线程编程时不小心，可能会让您发疯。因此，正如我们所看到的，消除这些类型的擦除的方法是避免状态共享。在这种情况下，通过为每个线程分配，在堆中分配一个单独的块来保存该线程的本地账号，然后将指向该唯一存储块的指针传递给该线程。</p>
<p>发言人   01:09:55<br>Okay, so if all of that isn’t enough to worry about, and by now you should be losing sleep at the very thought of writing a threaded program, another thing to worry about is deadlock.<br>好的，所以如果所有这些还不足以担心，而且现在你应该在编写线程程序的想法上失去睡眠，另一件需要担心的事情是死锁。</p>
<p>发言人   01:10:11<br>Here, a program is deadlocked. If it’s waiting for some condition to occur that will never occur. Okay, so let’s say a typical scenario, right? P, the P operation is a potential problem because it blocks and it’s waiting for that semaphore that it’s blocking on to become nonzero. Well, it’s not too hard to imagine scenarios where there’s some combination of p’s of p operations that sort of block each other out and make it impossible for the condition they’re waiting on to occur. So for example, let’s say you’ve got two threads that need two threads, 1 and 2 that need two different resources, a and b in order to proceed. So they have to acquire, they have to do AP tex on the mutex. That’s associated with the mutexes that are associated with these two resources.<br>在这里，一个程序陷入了僵局。如果它正在等待某些永远不会发生的情况发生。好的，那么让我们举一个典型的场景，对吧？P，P操作是一个潜在的问题，因为它阻塞并且它正在等待阻塞的信号量变为非零。嗯，不难想象这样的场景，其中有一些p&#x2F;p操作的组合，它们相互阻塞，使它们等待发生的条件不可能发生。因此，例如，假设您有两个需要两个线程的线程，1和2需要两个不同的资源，a和b才能继续。所以他们必须获取，他们必须对互斥锁执行AP tex。与这两个资源相关联的互斥锁。</p>
<p>发言人   01:11:20<br>So let’s say process one acquires A, so it does AP on A’s mutex. That’s one, it succeeds, so it acquires that resource, and then it gets preempted by thread two, which acquires B first, instead of acquiring a thread B for some reason, acquires B, so now thread A holds the lock on resource A, and thread two holds the lock on resource B, and so now let’s say thread two gets preempted, so and thread one runs, and so now it’s waiting, it tries to acquire the lock on resource B, but thread two holds that lock, and at the same time, thread two tries to acquire the lock on resource A, but process one is holding that so each, So here’s a case where thread A is waiting for this semaphore associated with B to become nonzero, so it’s blocked in this p operation, and at the same time, thread 2 is blocked in the P operation for resource A, neither of those semaphores will ever be released, so thread one and two are deadlocked.<br>因此，假设进程1获取了A，因此它在A的互斥上执行AP。这是一个，它成功了，所以它获取了该资源，然后它被线程2抢占，线程2首先获取B，而不是由于某种原因获取线程B，获取B，所以现在线程a持有资源A的锁，而线程二持有资源B上的锁，所以现在假设线程二被抢占，所以线程一运行，现在它正在等待，它尝试获取资源B的锁，但线程二持有该锁，同时，线程二尝试获取资源A的锁。,但进程一持有它，所以这里有一个情况，线程a正在等待与B关联的信号量变为非零，因此它在此p操作中被阻塞，同时，线程2在资源A的P操作中被阻塞，这两个信号量都不会被释放，因此线程一和线程二被死锁了。</p>
<p>发言人   01:12:54<br>And it happened because just there was this innocuous little bug in this case where the threads acquired their resources in different orders.<br>之所以发生这种情况，是因为在这种情况下存在一个无害的小错误，即线程以不同的顺序获取资源。</p>
<p>发言人   01:13:09<br>So here’s an example of a program that deadlocks. And if you looked at this, the fact that it’s wrong and buggy doesn’t jump out at you, right? So this kind of stuff is very subtle. Here’s a program. We’re going to create two threads. We’ve got an array, and we have an array of mutexes. An array of 2 mutexes. So we create two threads, and we pass each thread. It’s a local thread ID of 0 and 1. And so here we’re avoiding the race. We’re just casting this thread ID to be to a pointer, which is a little strange, but it’s okay. And then we’re waiting for those threads to finish.<br>这里有一个程序死锁的例子。如果你看看这个，事实是错误的，越野车不会跳出你，对吧？所以这种东西非常微妙。这是一个程序。我们将创建两个线程。我们有一个数组，我们有一个互斥锁的数组。一个由2个互斥体组成的数组。所以我们创建了两个线程，并传递每个线程。它是一个由0和1账号的本地线程。所以在这里，我们要避免比赛。我们只是将这个线程账号投射到一个指针上，这有点奇怪，但没关系。然后我们等待这些线程完成。</p>
<p>发言人   01:14:03<br>Each thread. It’s going to acquire these two semaphores, these two mutexes. But it’s going to do it in a different order. So it’s going to do it as a function. It’s going to take the ID. So it’s so thread zero, we’ll first acquire mutex 0, and then acquire mutex 1 -0. So then it’ll acquire mutex 1, and thread one will first acquire mutex 1 and then acquire mutex 0. Okay, so if we were to draw that and then it’ll attempt to, then it’ll increment count. So this is totally bogus.<br>每个线程。它将获得这两个信号量，这两个互斥量。但它将以不同的顺序进行。所以它将作为一个功能来完成。这将需要账号。所以它是线程零，我们将首先获取互斥锁0，然后获取互斥锁1-0。所以它将获取互斥锁1，线程1将首先获取互斥锁1，然后获取互斥锁0。好的，所以如果我们要绘制它，然后它会尝试，然后它会增加计数。所以这完全是假的。</p>
<p>发言人   01:14:53<br>It’s just to illustrate the problem. So you can see thread 0 does AP on semaphore 0, followed by AP on semaphore 1, and thread one does AP on semaphore one, followed by AP on semaphore 0. And so we can see this, that this is a problem very clearly if we go back to our progress graphs. So if you look at thread 0, it’s doing AP on semaphore 1 and followed AP on semaphore one, followed by a V on semaphore 1. And thread one is doing AP on semaphore one, followed by a V on semaphore 1. So if you take.<br>这只是为了说明问题。所以你可以看到线程0在信号量0上执行AP，接着是信号量1上的AP，而线程1在信号量1上执行AP，然后是信号量0上的AP。因此，如果我们回到进度图表，我们可以非常清楚地看到这是一个问题。因此，如果您查看线程0，它在信号量1上执行AP，在信号量1上执行AP，然后在信号量1上执行V。线程一在信号量一上执行AP，接着在信号量一上执行V。所以，如果你接受。</p>
<p>发言人   01:15:45<br>The intersection of these two regions, you get the forbidden region for semaphore 1. So this is the region that enforces mutual exclusion. So on the resource associated with semaphore 1. And if you do the same thing for semaphore 0, so here in thread one, we’re acquiring semaphore 0 here, and we’re releasing it here. And in thread 0, we’re acquiring it here and releasing it here. So if you take the intersection of those two, you get this forbidden region 4 S 0.<br>这两个区域的交集处，您将获得信号量1的禁区。因此，这是强制执行互斥的区域。因此，在与信号量1关联的资源上。如果你对信号量0做同样的事情，那么在第一个线程中，我们将在这里获取信号量0，并在这里释放它。在线程0中，我们在这里获取它并在这里发布。所以如果你取这两者的交点，你会得到这个禁区4 S 0。</p>
<p>发言人   01:16:26<br>Okay, now the problem. Right, is this region here, this so called deadlock region. Because by the rules of time can’t go backwards. Once a trajectory enters into this deadlock region, then it’s doomed because once it enters, once it enters this deadlock region, there’s nowhere for it to go. Eventually, no matter how it progresses, every trajectory will lead to this point here where it’s boxed in and can’t and can no longer proceed.<br>好的，现在问题来了。对的，这个区域在这里吗，这个所谓的死锁区域。因为按照时间规则，不能倒退。一旦轨迹进入这个死锁区域，它就注定失败，因为一旦它进入，一旦它进入这个死锁区域，它就无处可去。最终，无论它如何进展，每个轨迹都将导致这个点，在那里它被装箱，不能再继续前进。</p>
<p>发言人   01:17:15<br>So? And interestingly, sort of this region back here on sort of the tail end of these two forbidden regions, this represents states that can never be reached. So these are unreachable states, which may or may not be interesting. And then what makes this so nasty is that it’s non-deterministic, right?<br>所以呢？有趣的是，这个区域位于这两个禁区的末端，代表着永远无法到达的状态。所以这些是无法到达的状态，可能是也可能不是很有趣。这是什么让它如此令人讨厌的是它是不确定的，对吧？</p>
<p>发言人   01:17:45<br>Some programs, some trajectories, if they get lucky, they’ll skirt this deadlock region and then the program will run fine. So if a projectories just by. Some arbitrary scheduling decision made by the kernel? The trajectory? Trajectory gets past the deadlock region in this direction, then it’ll eventually run without any problem. It’s just it’s only if a trajectory lands within the deadlock region, then that we’re in trouble.<br>一些程序，一些轨迹，如果幸运的话，他们会绕过这个死锁区域，然后程序会正常运行。所以，如果一个项目只是通过。内核做出的一些任意的调度决定？轨迹？轨迹在这个方向上经过死锁区域，然后它最终会毫无问题地运行。这只是如果一个轨迹落在死锁区域内，那么我们就有麻烦了。</p>
<p>发言人   01:18:31<br>So this is the really, the really nasty part is that you may run your program for a million times and every trajectory, every one of those million trajectories skirts the deadlock region. But on the million and first time that you run it, it enters the deadlock region and then deadlocks. So it’s a very tough problem to deal with.<br>所以这就是真正让人讨厌的部分，你可能会运行你的程序一百万次，而每一个轨迹，每一个都绕过了死锁区域。但是在百万次并且第一次运行它时，它进入死锁区域，然后死锁。所以这是一个非常棘手的问题。</p>
<p>发言人   01:18:59<br>Now, fortunately, it’s easy to avoid if threads that are acquiring locks on on resources acquire all those locks in the same order. Okay? So in our example, if we rewrite this program so that each thread, thread 0 and thread one, acquire their locks in the same order, semaphore zero first, followed by semaphore 1. Then if that happens, and you can see it eliminates the potential deadlock region. So now any trajectory that we take. Well? We’ll be fine because we’ve eliminated that deadlock region and the order that we release the locks doesn’t matter because that sort of affects.<br>现在幸运的是，如果正在获取资源锁的线程以相同的顺序获取所有这些锁，则很容易避免。好吗？因此，在我们的例子中，如果我们重写这个程序，使得每个线程，线程0和线程1，以相同的顺序获取它们的锁，首先是信号量0，然后是信号量1。如果发生这种情况，你可以看到它消除了潜在的死锁区域。所以现在我们采取的任何轨迹。好吗？我们会没事的，因为我们已经消除了死锁区域，而我们释放锁的顺序并不重要，因为这会产生影响。</p>
<p>发言人   01:20:05<br>It affects this unreachable region. The size and shape of this unreachable region. But the order that we release the locks can never introduce a deadlock region. Okay, so that’s it for today. Hope you all have a. Very nice Thanksgiving holiday, and we’ll see you on Tuesday.<br>它影响这个无法到达的区域。这个无法到达区域的大小和形状。但是我们释放锁的顺序永远不会引入死锁区域。好的，今天就到这里。希望大家都有一个。感恩节假期非常愉快，我们将在周二再见。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>深入理解计算机系统 026-Synchronization, Advanced</div>
      <div>http://example.com/2025/10/12/15213-026/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月12日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/18/6S081-001/" title="操作系统工程 001-Introductionan">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">操作系统工程 001-Introductionan</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/12/15213-025/" title="深入理解计算机系统 025-Synchronization, Basic">
                        <span class="hidden-mobile">深入理解计算机系统 025-Synchronization, Basic</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
