

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="发言人   00:07All right, can you guys hear me? Yes, excellent, thank you. All right, today I want to talk about talk mostly about micro kernels, but first a little bit of context to sort of help explain">
<meta property="og:type" content="article">
<meta property="og:title" content="操作系统工程 017-OS Organization">
<meta property="og:url" content="http://example.com/2025/10/18/6S081-017/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="发言人   00:07All right, can you guys hear me? Yes, excellent, thank you. All right, today I want to talk about talk mostly about micro kernels, but first a little bit of context to sort of help explain">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-18T02:00:17.000Z">
<meta property="article:modified_time" content="2025-10-19T11:16:51.144Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>操作系统工程 017-OS Organization - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="操作系统工程 017-OS Organization"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-18 10:00" pubdate>
          2025年10月18日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          22k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          183 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">操作系统工程 017-OS Organization</h1>
            
            
              <div class="markdown-body">
                
                <p>发言人   00:07<br>All right, can you guys hear me? Yes, excellent, thank you. All right, today I want to talk about talk mostly about micro kernels, but first a little bit of context to sort of help explain why people explored micro kernels in the first place.<br>好的，你们能听到我说话吗？是的，很好，谢谢。好的，今天我想谈论的主要是关于微内核，但首先要介绍一下背景，以帮助解释为什么人们首先探索微内核。</p>
<p>发言人   00:29<br>This people got the micro kernels by trying to think about a little more broadly about what kernels should actually do. Like we know with XV 6 sort of does the things that Unix does, and we kind of take that set of abstractions and system calls and sort of facilities inside the kernels kind of is for granted, is the target what we’re trying to design? But it’s totally worth wondering, gosh, what should a kernel do in the first place? Maybe, maybe the particular kind of stuff that XV six or Linux does is not really the best answer. Or maybe this. And of course, you know, we’re on somewhat treacherous ground here because now know what kernels are is kind of a development platform for programmers. As we know, programmers, different people have like very different sort of subjective preferences about what kind of infrastructure they like to program on. So we can’t necessarily expect a single best answer, but we can expect to maybe learn something and maybe make some progress by trying to think about what answers might be.<br>这些人通过尝试更广泛地思考内核实际上应该做什么来获得微内核。就像我们知道的XV 6一样，Unix所做的事情，我们把抽象和系统调用的集合以及内核内部的一些设施视为理所当然，我们试图设计的目标是什么？但这完全值得思考，天哪，内核首先应该做什么？也许，也许XV六或Linux所做的特定类型的东西并不是最好的答案。或者可能是这个。当然，你知道，我们在这里有点危险，因为现在知道内核是一种程序员的开发平台。正如我们所知，程序员，不同的人对于他们喜欢在什么样的基础设施上编程有非常不同的主观偏好。所以我们不一定期望一个最好的答案，但我们可以期望通过思考答案可能会学到一些东西，并取得一些进展。</p>
<p>发言人   01:40<br>So first of all, let me try to kind of crystallize what the traditional approach is to what kind of kernel interfaces we ought to be using. And Linux and Unix and XV 6 are all examples of of what I personally call the traditional design approach. But another word for it that kind of summarizes what this approach has ended up like is monolithic.<br>首先，让我尝试阐明传统方法是什么，以及我们应该使用什么样的内核接口。Linux、Unix和XV 6都是我个人称之为传统设计方法的例子。但它的另一个词，总结了这种方法最终的结果，就是单一的。</p>
<p>发言人   02:13<br>Monolithic and what that means is that the kernel is a single big program that does all kinds of things all within the same program. And indeed, this really reflects the way people thought about what kernels ought to be doing. A real hallmark of kernels like Linux is that they have, they provide powerful abstractions. You know, they choose things like file systems, and which is really a complicated item, and they present file systems and files and directories and file descriptors as their interface rather than, for example, presenting discard Warre as their interface to applications. So, and presenting powerful abstractions instead of very low level abstractions has some big advantages. So these monolithic kernels often have sort of big abstractions.<br>这意味着内核是一个单一的大程序，它在同一个程序中做各种各样的事情。事实上，这确实反映了人们对内核应该做什么的思考方式。像Linux这样的内核的真正标志是它们提供了强大的抽象。你知道，他们选择像文件系统这样的东西，这确实是一个复杂的项目，他们提供文件系统、文件、目录和文件描述符作为他们的接口，而不是例如提供丢弃警告作为他们的应用程序接口。因此，呈现强大的抽象而非非常低级的抽象有一些很大的优势。因此，这些单块内核通常具有某种大的抽象。</p>
<p>发言人   03:17<br>Like file the file system, 1 advantage of big abstractions is that they’re often portable files and directories. You can implement files and directories on all kinds of storage and file you. You can use files and directories without having to worry about what brand of disk drive it’s running on. Maybe it’s an SSD instead of a hard drive. Maybe it’s a network file system at all. It’s same interface because the file system interface is pretty high level, pretty abstract. So the advantage of this is that it’s a way to get portability and write an application and have it run on all kinds of different hardware without having to modify the application. Another example of this is that Unix Linux provides an address space abstraction something rather than providing.<br>就像文件系统一样，大抽象的一个优点是它们通常是可移植的文件和目录。您可以在各种存储和文件中实现文件和目录。您可以使用文件和目录，而不必担心它运行在什么品牌的磁盘驱动器上。也许这是一个固态硬盘，而不是一个硬盘。也许这完全是一个网络文件系统。它是相同的接口，因为文件系统接口非常高级，非常抽象。因此，这样做的优点是可以获得可移植性，编写应用程序并使其在各种不同硬件上运行，而无需修改应用程序。另一个例子是，Unix Linux提供了一个地址空间抽象，而不是提供。</p>
<p>发言人   04:10<br>Direct access to the MMU hardware. And that’s useful for portability and a sort of high complexity from applications.<br>直接访问MMU硬件。这对于可移植性和应用程序的高复杂性很有用。</p>
<p>发言人   04:20<br>So another big advantage here, these powerful abstractions, is that they tend to hide complexity from applications. The So for example, the file descriptor interface that XV 6 provides. It’s very simple interface, but you know, just read and write on file descriptors can’t much simpler. But behind it is some very complicated code for actually reading and writing the disk, the file system on disk. And so that’s nice for programmers, but it makes for a big complex kernel.<br>因此，这些强大的抽象还有一个很大的优势，那就是它们倾向于向应用程序隐藏复杂性。例如，XV 6提供的文件描述符接口。这是一个非常简单的界面，但是你知道，仅仅读写文件描述符是再简单不过了。但它背后是一些非常复杂的代码，用于实际读取和写入磁盘，磁盘上的文件系统。所以这对程序员来说很好，但它使得内核变得复杂起来。</p>
<p>发言人   04:55<br>These big abstractions also help the kernel manage and share resources to the kernel, things like memory management.<br>这些大的抽象也有助于内核管理和共享资源到内核，比如内存管理。</p>
<p>发言人   05:02<br>The kernel keeps track of what memory is free. We similarly, the kernel keeps track of what parts of the disk are free and what parts of the disk are in current use. So programs don’t get to think about it. And that, again, it simplifies programs. That also helps with robustness and security, even because if programs were allowed to decide what parts of the disc are free or not, then maybe one program could use a part of the disk that’s already being used by another program.<br>内核跟踪哪些内存是空闲的。我们同样，内核会跟踪磁盘的哪些部分是空闲的，以及磁盘的哪些部分当前正在使用。所以程序不会去想它。再次强调，它简化了程序。这也有助于稳健性和安全性，即使允许程序决定光盘的哪些部分是空闲的，那么也许一个程序可以使用另一个程序已经使用的磁盘部分。</p>
<p>发言人   05:30<br>So the fact that the kernel is in charge of resource management. Helps with sharing, it helps with security. But again, it’s a force that sort of causes the kernel to be big. So anyway, so having the colonel be in charge of all these sort of juicy abstractions that even if they have simple interfaces, have a lot of complexity inside, have lead kernels to be big and complex items.<br>因此，内核负责资源管理的事实。有助于共享，它有助于安全。但同样，这是一种力，会导致内核很大。无论如何，让上校负责所有这些多汁的抽象，即使它们有简单的接口，内部有很多复杂性，也会导致内核变得庞大而复杂。</p>
<p>发言人   06:06<br>Another aspect of this monolithic design approach is that because it’s all one program, all the different kernel systems like the file system and the memory allocator and the scheduler and the virtual memory system, they’re all part of one big integrated program. It means that they can peer into each other’s data structures. And so that’s just tended to make it much easier to implement facilities that are sort of parts of more than one or what you might think of as more than one kind of module or subsystem.<br>这种单体设计方法的另一个方面是，因为它是一个程序，所有不同的内核系统，如文件系统、内存分配器、调度程序和虚拟内存系统，它们都是一个大型集成程序的一部分。这意味着它们可以互相窥视彼此的数据结构。因此，这往往使得实现属于多个部分的设施变得更加容易，或者你可能认为是多个模块或子系统的设施变得更加容易。</p>
<p>发言人   06:35<br>So for example, a system called like exec exec has its fingers deeply into the file system because it’s reading binary images off the disk in order to load them into memory. It also has his fingers into the memory allocation and virtual memory paging system because it needs to set up the address space of the new process. But it’s really easy. There’s no problem with doing that in XV six or Linux because both law, the file system is right there in the same kernel program, and the virtual memory system is also right there as part of the same program. And if somehow there was a rigid split between the file system and the virtual memory system, it’d be much harder to implement something like exec that has sort of fingers in both of these pies. But in a monolithic system, just one big program, it’s much easier.<br>例如，一个名为exec exec的系统将其手指深深地插入文件系统，因为它正在从磁盘读取二进制图像，以便将它们加载到内存中。它还可以控制内存分配和虚拟内存分页系统，因为它需要设置新进程的地址空间。但这真的很简单。在XV 6或Linux中这样做没有问题，因为两个法则都是文件系统在同一个内核程序中，虚拟内存系统也在同一个程序中。如果文件系统和虚拟内存系统之间存在严格的分割，那么实现像exec这样在这两个馅饼中都有手指的东西就会困难得多。但在一个单一的系统中，只有一个大程序，这要容易得多。</p>
<p>发言人   07:24<br>Another thing that makes implementing software inside a monolithic kernel like XV 6 or Linux easy is that all the code runs with full hardware privileges, all of x.c. 6 runs in supervisor mode, for example, which means there’s no limits, There’s no irritating. Oh, you can’t, you know, read or write that memory here because you don’t have enough privilege. All the kernel code runs with sort of maximum privilege. The same is true of operating system like Linux, so this design strategy, very convenient for kernel developers and it’s made it easy to build these big abstractions which are convenient for application developers. However, there’s also a certain amount to criticize with the monolithic, the traditional monolithic approach. And this is starting to be part of the motivation for looking at other architectures like micro kernels. So you might ask, why not monolithic kernels?<br>使在XV 6或Linux等单体内核中实现软件变得容易的另一件事是，所有代码都以完整的硬件权限运行，所有的x.c。 例如，6个运行在主管模式下，这意味着没有限制，没有刺激性。哦，你不能在这里读或写那个记忆，因为你没有足够的特权。所有内核代码都以最大特权运行。这同样适用于像Linux这样的操作系统，因此这种设计策略对于内核开发人员来说非常方便，并且它使得构建这些大的抽象变得容易，这对于应用程序开发人员来说非常方便。然而，对于传统的单一方法，单一方法也有一定的批评。这开始成为寻找其他架构 (如微内核) 的动力之一。所以你可能会问，为什么不是单块内核呢？</p>
<p>发言人   08:43<br>So one is just that they’re big and complex. So anything that’s Linux is, depending on how you count.<br>所以其中一个原因就是它们又大又复杂。任何Linux的东西，都取决于你如何计数。</p>
<p>发言人   08:52<br>Linux is somewhere between many hundreds of thousands and a few million lines of code. And people really do take advantage of the fact that one part of Linux can sort of look at the data of another. That makes the programming here easier, but, but it also makes there be a lot of sort of interconnections and interrelationships between code. So it can be a bit challenging sometimes to look at Linux kernel code and figure out what it’s up to.<br>Linux的代码介于几十万到几百万行之间。人们确实利用了一个事实，即Linux的一部分可以查看另一部分的数据。这使得这里的编程更容易，但是，它也使得代码之间存在许多相互联系和相互关系。因此，有时查看Linux内核代码并弄清楚它的目的可能有点具有挑战性。</p>
<p>发言人   09:19<br>And anytime you get big programs, especially ones that are complex structure, you get bugs. And operating system kernels are no exceptions. And over the years, they’ve had all kinds of bugs, including bugs that are exploitable for security. So? So this is sort of a troubling set of relationships. If you allow a big model, if the kernel, you most certainly can’t avoid bugs and exploitable security problems. And that’s a real, I mean, that really is a problem.<br>每当你得到大型程序，特别是那些结构复杂的程序时，你就会遇到bug。操作系统内核也不例外。多年来，他们出现了各种各样的漏洞，包括可用于安全的漏洞。所以呢？所以这是一组令人不安的关系。如果你允许一个大模型，如果内核，你肯定无法避免错误和可利用的安全问题。这是一个真实的，我的意思是，这确实是一个问题。</p>
<p>发言人   10:00<br>Another reason why people are maybe not entirely happy with monolithic kernels is that they tend to just grow with all desirable features over time. And Linux is used for all kinds of different things, from telephone handsets to desktop workstations and laptops, to tablets, to servers on the internet to routers. And that’s caused Linux. It’s fantastic that Linux can support all those things, but it has caused it to be very general. It support in there for many, many different things. And any one application like me running my web server unlikely to need, for example Linux is very sophisticated sound card support. So there’s just a huge amount of stuff that’s there to allow Linux to be general purpose, which is good.<br>人们可能对单一内核并不完全满意的另一个原因是，随着时间的推移，它们往往会随着所有期望的功能而增长。Linux被用于各种不同的事物，从电话手机到台式工作站和笔记本电脑，到平板电脑，到互联网上的服务器到路由器。这就是Linux造成的。Linux能够支持所有这些东西真是太棒了，但这使得它非常通用。它支持许多不同的东西。并且像我这样运行我的web服务器的任何一个应用程序都不太可能需要，例如，Linux是非常复杂的声卡支持。因此，有大量的东西可以让Linux成为通用的，这很好。</p>
<p>发言人   10:53<br>There’s a worry that general purpose is going to tend to mean slow, that, you know, it may be good for all kinds of different things, but maybe not optimum for anything in particular. So it’s very hard, you know, when you’re trying to make something run really fast, it’s great to have it just only do 1 or two things. So you can focus on optimizing a single code path. But if your software needs to do any one of 1000 different things, it’s much harder to have focused optimization.<br>有人担心通用用途往往意味着缓慢，你知道，它可能对各种不同的事情都有好处，但可能对任何特定的事情都不是最优的。所以这很难，你知道，当你试图让某件事跑得非常快时，让它只做一两件事是很棒的。这样您就可以专注于优化单个代码路径。但是如果你的软件需要做1000种不同的事情中的任何一种，集中优化就更难了。</p>
<p>发言人   11:25<br>So this is Linux is not necessarily slow, but it’s, you know, you might wonder if it’s really as fast as it could possibly be for any given situation. And so if you think about, I mean, think if you think about almost anything in Linux or XP 6, you know, you may wonder whether it really needs to do everything it does. So for example, if you write a single byte over a pipe from one process to another, but there’s a lot of instructions that get executed even in XV 6, which is a simple kernel. There’s buffering, you know, there’s locking, there is could be a sleep and a wake up during a pipe read and write, There’s maybe a scheduler, maybe call it the scheduler or contact switch. That’s a lot of stuff that’s maybe not necessarily the absolute minimum that would be required to move a byte from 1 process to another.<br>所以这就是Linux并不一定很慢，但是你知道，你可能会想知道它是否在任何情况下都尽可能快。所以如果你想想，我是说，想想Linux或XP 6中的几乎所有东西，你可能会想知道它是否真的需要做它所做的一切。因此，例如，如果您通过管道从一个进程编写单个字节到另一个进程，但是即使在XV 6中也有许多指令被执行，XV 6是一个简单的内核。有缓冲，你知道，有锁定，在管道读写期间可能有睡眠和唤醒，可能有一个调度程序，可能称之为调度程序或接触开关。这是很多东西，可能不一定是将一个字节从一个进程移动到另一个进程所需的绝对最小值。</p>
<p>发言人   12:18<br>Another potential problem with these big kernels is that they, because they’re so big and they sort of intentionally bite off some very sophisticated abstractions, they tend to have a lot of design decisions kind of baked into the kernel. So, you know, in ways that you can’t, even if you disagree with them, you can’t really tough luck like applications, just have to live with it so. As opposed to you in some fantasy world, maybe the applications could make a lot more of the decisions.<br>这些大内核的另一个潜在问题是，因为它们太大了，而且它们有意地咬掉一些非常复杂的抽象概念，所以它们往往会将许多设计决策嵌入内核中。所以，你知道，以你不能的方式，即使你不同意他们，你也不能像应用程序那样真的很难运气，只是不得不忍受它。与幻想世界中的你不同，也许应用程序可以做出更多的决定。</p>
<p>发言人   12:54<br>Some examples of things where you may be bummed out by the way the API is designed.<br>一些事情的例子，你可能会对API的设计方式感到沮丧。</p>
<p>发言人   13:04<br>For example, in Unix, you can wait for a process your own children. If you fork, you can then wait for your children, but you can’t wait for some other process. And, you know, maybe you want to wait for a grandchild or an unrelated process, but that’s just not an option. It’s not the way things work, Even if it would be convenient for you, maybe you want to change the way another process is address space is set up, you know, maybe call a map on behalf of another process that you’re controlling. But again, that’s just not an option. You can and map change your own address space, but not change people. Other processes address space, maybe you’re a database and you have btree indexes on the disk and you may know a lot about it the fastest way to lay out a b-tree a disk, But if you’re reading and writing files with the file system, the file system has no idea that you’re actually writing a b-tree or how a b-tree ought to be laid out on a disk for fastest access. And so if you’re a database, you’re going to be kind of bum, you maybe you’re happy that you have this file system at your disposal, but it doesn’t really do what you want it to do.<br>例如，在Unix中，你可以等待你自己的子进程。如果你分叉，你可以等待你的孩子，但你不能等待其他过程。而且，你知道，也许你想等待孙子或一个不相关的过程，但这不是一种选择。这不是事情的运作方式，即使这对你来说方便，也许你想改变另一个进程的地址空间设置方式，你知道，也许代表你正在控制的另一个进程调用地图。但是再次强调，这不是一个选择。您可以使用地图更改自己的地址空间，但不能更改人员。其他进程地址空间，也许您是一个数据库，磁盘上有btree索引，您可能对它了解很多最快的布局b-tree磁盘的方法，但是如果您正在使用文件系统读取和写入文件，文件系统不知道你实际上正在编写一个b树，也不知道b树应该如何放置在磁盘上以实现最快的访问速度。所以如果你是一个数据库，你会有点流浪汉，也许你很高兴你可以使用这个文件系统，但它并没有真正做你想做的事情。</p>
<p>发言人   14:07<br>And that’s the sense in which design decisions are often baked in the big kernels.<br>这就是设计决策通常被烘焙在大内核中的感知。</p>
<p>发言人   14:13<br>And finally, a specific sort of issue that. So it came up in a big way in the 1990s, probably. It was the notion of extensibility. That it might be desirable for programs to be able to change the kernel on the fly, like to be able to download new code into the kernel or change the way it operates or something in order to do things like have databases be able to control the layout of data on the disk? And at least in decades past, monolithic kernels tended not to have any particular features that helped with this kind of extensibility.<br>最后，一个特定的问题。所以它可能在1990年代出现了很大的发展。这是可扩展性的概念。可能希望程序能够动态地更改内核，例如能够将新代码下载到内核中或更改其运行方式，以便做一些事情，例如使数据库能够控制磁盘上数据的布局？至少在过去的几十年里，单块内核往往没有任何特定的功能来帮助实现这种可扩展性。</p>
<p>发言人   14:56<br>You’re just stuck with whatever the kernel did. Okay, so these were sort of problems in the back of people’s minds. Led them to think about other kinds of other ways of designing other architectures for operating systems? And there were a number of ideas, some quite radically different, that people pursued. We’re going to talk about one of them, a particularly popular one today, and that’s the idea of micro kernels.<br>你只是被内核所做的事情困住了。好的，这些都是人们内心的问题。这促使他们考虑以其他方式设计操作系统的其他架构？并且有许多想法，有些完全不同，人们追求。我们今天要谈论其中一个特别流行的，那就是微内核的概念。</p>
<p>发言人   15:31<br>Micro kernels Although many of the ideas are back to the beginning of computer history, they became a sort of hot research topic starting in maybe the mid to late 1980s. And the big idea? So micro keel, this word, by the way, refers to a sort of general approach or concept, doesn’t refer to endings, specific artifact.<br>微内核尽管许多想法都可以追溯到计算机历史的开端，但它们可能从1980年代中期到晚期开始成为一种热门研究话题。伟大的想法？所以微龙骨，顺便说一下，这个词指的是一种普遍的方法或概念，并不指结局，具体的人工制品。</p>
<p>发言人   16:00<br>There were many people who designed and built operating systems that followed the general sort of plan for micro kernels. But, you know, each of these projects ended up designing and operating systems maybe quite different from the others.<br>有很多人设计和构建了遵循微内核一般计划的操作系统。但是，你知道，这些项目中的每一个最终的设计和操作系统可能与其他项目完全不同。</p>
<p>发言人   16:14<br>So the key idea was. Tiny kernel? That supported just IPC or interproces. And some sort of notion of threads or tasks. So you have a kernel that that, you know, provides you a notion of sort of process like abstraction and a way for processes to communicate each other with this in a process communication and nothing else. And everything else you might want to do that have a file system you’d implement as a process, as a task, as a user level code, not in the kernel at all. So a picture for that.<br>所以关键想法是。微小的内核？只支持IPC或interproces的。以及某种线程或任务的概念。所以你有一个内核，它提供了一种类似于抽象的过程的概念，以及一种过程之间进行通信的方式，而不是其他任何方式。你可能想做的其他事情都有一个文件系统，你可以将其实现为进程、任务、用户级代码，而不是在内核中。这就是一张图片。</p>
<p>发言人   17:01<br>Maybe now we’re going to use mu for micro kernel. We got the micro kernel down here, and we got user space processes up here. And we might have all the kind of usual processes run. Maybe we’re going to run VI, my favorite text editor, my compiler, my Windows system. But also up here, as user level processes, we’re going to have the file system just as a server process and user space.<br>也许现在我们要将mu用于微内核。我们在这里安装了微内核，在这里安装了用户空间进程。我们可能会运行所有常见的过程。也许我们要运行VI，这是我最喜欢的文本编辑器，我的编译器，我的Windows系统。而且在这里，作为用户级别的进程，我们将把文件系统作为服务器进程和用户空间。</p>
<p>发言人   17:32<br>Maybe we’re going to have a disk driver that knows how to talk to my disk hardware, maybe will have a network stack that. Knows how to talk TCP to my network interface card? Maybe we’ll have user level processes? Is in charge of doing fancy paging tricks like memory mapped files or implements, copy and write fork or something.<br>也许我们会有一个知道如何与我的磁盘硬件交流的磁盘驱动程序，也许会有一个网络堆栈。知道如何与我的网络接口卡进行TCP通信吗？也许我们会有用户级别的流程？负责执行花哨的分页技巧，如内存映射文件或实现、复制和编写分叉或其他操作。</p>
<p>发言人   18:01<br>And when my text editor needs to read a file, it needs to talk the file system, it’s going to send a. A message via IPC In our process, communication to the file system server, which has all the file system code in it, knows about files and directories, and the file system with server code may need to talk to the disk. So it might send a disk read or write to the another IPC, the disk driver, which somehow talks to the disk hardware. The disk driver may return, you know, a disk block to the file server after it does its thing. Maybe the file server finally returns the data you ask for, again, by Interpro communication messages back to my text editor.<br>当我的文本编辑器需要读取一个文件时，它需要与文件系统通信，它将发送一个。在我们的进程中，通过IPC发送一条消息，与文件系统服务器通信，后者包含所有文件系统代码，了解文件和目录，带有服务器代码的文件系统可能需要与磁盘通信。所以它可能会发送磁盘读取或写入到另一个IPC，即磁盘驱动程序，它以某种方式与磁盘硬件通信。磁盘驱动程序在执行操作后可能会将磁盘块返回给文件服务器。也许文件服务器最终会将您要求的数据返回，再次通过Interpro通信消息返回给我的文本编辑器。</p>
<p>发言人   18:50<br>But you, the critical thing to notice here is that the only stuff going down here in the kernel is support for these processes, tasks, or threads, or whatever they might be, and support for the inner process, communication, message passing, and nothing else. There’s no file system down here. There’s no device drivers necessarily down here in the kernel, there’s no network stack. All that stuff is up here is more or less ordinary user level processes. So that leads you to a very small kernel with relatively little code optimized, like you can optimize IPC and there’s not much else going on. And so this is the kind of picture we’re going to talk about for the rest of the lecture.<br>但是，在这里要注意的关键是，内核中唯一的东西就是支持这些进程、任务或线程，或者任何它们可能是的东西，以及对内部进程、通信、消息传递的支持，没有其他的。这里没有文件系统。在内核下面不一定有设备驱动程序，也没有网络堆栈。这里的所有东西或多或少都是普通的用户级进程。这样你就会得到一个非常小的内核，相对较少的代码优化，就像你可以优化IPC一样，其他事情就不多了。所以这就是我们要在讲座剩下的时间里谈论的图片。</p>
<p>发言人   19:39<br>And just to give you a taste of kind of where this ended up, there are actually still microkernels in use today. And indeed, the L 4 micro kernel, which is the topic of today’s paper, turns out to be used. There’s many instances, many, many instances of L 4 running because it’s used in a lot of cell phones too, in the little microcontrollers that control the cell phone radios. And it’s also apparently used in recent iPhones as the operating system that runs on the special dedicated Enclave processor in the iPhone that hides the secret cryptographic keys. So there’s a bunch of embedded where these micro kernels, one out is in little embedded specialized computer systems, not laptops, but computers sort of dedicated to single specialized tasks. Or you may not need the complexity of Linux, but you do need some operating system.<br>并且为了让您体验一下最终的结局，实际上今天仍在使用微核。事实上，今天论文的主题L 4微内核也被使用了。有许多实例，许多许多实例运行L 4，因为它也用于许多手机中，在控制手机收音机的小微控制器中。显然，它也在最近的iPhone中用作操作系统，运行在iPhone的特殊专用飞地处理器上，该处理器隐藏了秘密加密密钥。所以有一堆嵌入式内核，其中一个在小型的嵌入式专门计算机系统中，不是笔记本电脑，而是专门用于单个专门任务的计算机。或者你可能不需要Linux的复杂性，但你确实需要一些操作系统。</p>
<p>发言人   20:37<br>And the other thing that’s the other sort of final result from micro kernels is that the idea of user level services with with other programs talking to them with IPC, that also has made its way into a lot of operating systems like Mac OS, which I’m running right now to talk to you. You know, it’s as well as being a kind of ordinary monolithic kernel. It also has good support for user level services and IPC between Unix processes to talk to these services. So that idea also. Was a successful idea widely adopted?<br>而微内核的另一种最终结果是，用户级服务的理念与其他程序通过IPC与它们通信，这也已经进入了许多操作系统，如Mac OS，我现在正在运行它来与你交谈。你知道，它就像一种普通的单一内核一样。它还为Unix进程之间的用户级服务和IPC提供了良好的支持，以与这些服务进行通信。所以这个想法也是。一个成功的想法被广泛采用了吗？</p>
<p>发言人   21:17<br>Okay, so this is a basic architecture. And I’m going to go on and sort of talk about some ways, reasons why this might be attractive. But first, are there any just kind of high level questions about what it is I mean, by microkernel?<br>好的，这是一个基本的架构。我将继续谈论一些可能吸引人的方式和原因。但是首先，关于我所说的微内核，是否有任何高层次的问题？</p>
<p>发言人   21:38<br>Okay? So what is it that people are hoping for? When they started building micro kernels, one big motivation, although you wouldn’t necessarily see it written down much, is just a sense of aesthetics, right? I think just a lot of people feel that huge, complicated single programs like a Linux kernel or just not very elegant, that surely we can build something that’s much more, much, much smaller, is a much more focused design, Isn’t such a huge grab bag of random different features? So I think there was a strong sort of aesthetic feeling that surely we can do better than big kernels.<br>好吗？那么，人们所期望的是什么呢？当他们开始构建微内核时，一个很大的动力，虽然你不一定会看到它写下来，只是美学的感知，对吧？我想很多人觉得像Linux内核这样庞大、复杂的单一程序不是很优雅，我们肯定可以构建出更多、更小的东西，是一个更集中的设计，难道不是这么一个巨大的随机不同功能的抓包吗？所以我认为有一种强烈的美感，我们肯定可以比大内核做得更好。</p>
<p>发言人   22:33<br>But other sort of more specific things that you might be able to quantify are something, a kernel that’s small. Might be more secure? Fewer lines of code you have probably fewer bugs. You have less chance of somebody being able to exploit one of those bugs, break security.<br>但是其他更具体的东西，你可能能够量化的是一些小的内核。可能更安全？代码行越少，bug就越少。你不太可能有人能够利用这些漏洞，破坏安全。</p>
<p>发言人   22:55<br>And then the extreme of that, you could imagine an operating system that’s actually provably correct. Where somebody can sit down and write a proof that the operating system has no bugs or does exactly what it’s supposed to do and nothing else. And indeed, there is a at least one verified, proved correct, proved secure, a operating system named Sel 4, which is one of the many descendants of the L 4 micro kernel. That today’s paper? But you really, you know, people know how to verify sort of small to medium size programs, but they don’t know how to verify huge programs. And the fact that microkernels are small is sort of a critical ingredient and be able to prove they’re correct.<br>然后是极端情况，你可以想象一个实际上可以证明正确的操作系统。在这里，有人可以坐下来写一个证明，证明操作系统没有漏洞，或者完全按照它应该做的事情而不做其他事情。事实上，至少有一个经过验证的、被证明正确的、被证明安全的操作系统，名为Sel 4，它是L 4微内核的众多后代之一。今天的报纸？但是你真的知道，人们知道如何验证中小型程序，但他们不知道如何验证大型程序。而事实上，微核很小是一种关键因素，能够证明它们是正确的。</p>
<p>发言人   23:45<br>Another reason why you might like small is that small amount of code is often a lot easier to optimize than. And a huge program. Another reason why small might result in fast is that you don’t have to pay for a lot of features that you don’t use. If your micro kernel does hardly anything, then you’re not paying for a lot of features you’re not using. Another reason for small is that small kernel probably bakes in far fewer design decisions, forces fewer design decisions on application writers, and so it maybe it leaves them more flexible flexibility to make their own design decisions. So by the way, these are all, these are not necessary consequences of the microkernel approach.<br>你可能喜欢小的另一个原因是少量的代码通常比优化容易得多。一个庞大的计划。小巧可能导致快速的另一个原因是您不必为许多您不使用的功能付费。如果你的微内核几乎没有做任何事情，那么你就没有为你没有使用的许多功能付费。小的另一个原因是，小内核可能会导致更少的设计决策，迫使应用程序编写者做出更少的设计决策，因此这可能会让他们更灵活地做出自己的设计决策。所以顺便说一下，这些都是，这些不是微内核方法的必然结果。</p>
<p>发言人   24:37<br>These are things that people hoped for and tried to achieve by using microkernels.<br>这些是人们希望并试图通过使用微核函数来实现的事情。</p>
<p>发言人   24:42<br>Another set of reasons why microkernels seemed attractive has to do with the fact that a lot of the code was at user level. That is a lot of features and functions that we sort of have grown used to being inside the kernel were actually user level surfaces. So they hope that by sort of breaking the kernel apart and running the different parts, like user level service, like a file service, file service server, it might cause the. Code to be more modular might sort of encourage operating system designers to split up all these functions into many separate services. And that might, that might be a good thing.<br>微内核之所以看起来很有吸引力，另一组原因与很多代码都是用户级别的事实有关。这是许多特性和功能，我们已经习惯了在内核内部实际上是用户级别的表面。所以他们希望通过将内核拆分并运行不同的部分，比如用户级别服务、文件服务服务器，这可能会导致。代码更加模块化可能会鼓励操作系统设计师将所有这些功能拆分为许多单独的服务。那可能是件好事。</p>
<p>发言人   25:27<br>User level code is also possibly easier to modify. If that a user level, it’s usually easier to tweak it or replace it or modify it. Then doing the same stuff in the kernel. So maybe it’s easier to customize. Putting the operating systems at user levels might make them more robust.<br>用户级代码也可能更容易修改。如果是用户级别，通常更容易调整、替换或修改它。然后在内核中做同样的事情。所以也许更容易定制。将操作系统放在用户级别可能会使它们更健壮。</p>
<p>发言人   25:51<br>Can if the kernel something goes wrong with the kernel, you know, usually you have to panic and reboot because. You can’t necessarily trust what’s in kernel anymore, or if it’s had some bug that maybe causes it to overwrite a random part of its data. Whereas if you have a bunch of user level services and one of them malfunctions and divide by zero or references a wild pointer, maybe only that one server will crash and leaving the rest of the operating system intact. And maybe you can restart it, just that one server. So maybe user level, moving OS functionality, use of process, it might lead to more robustness. And this is probably particularly evident for drivers. Most bugs in the kernel are actually hardware device drivers if we can manage to move the device drivers out of the kernel, then we might have many fewer bugs and crashes in the kernel.<br>如果内核出现问题，你知道，通常你必须恐慌并重新启动，因为。你不一定再相信内核中的内容了，或者它有一些可能导致它覆盖其数据的随机部分的错误。然而，如果你有一堆用户级服务，其中一个出现故障并被零除或引用了一个野生指针，也许只有那一个服务器会崩溃并保持操作系统的其余部分完好无损。也许你可以重新启动它，就那一个服务器。可能是用户级别，移动操作系统功能，使用流程，这可能会导致更多的稳健性。这对司机来说可能尤其明显。内核中的大多数错误实际上是硬件设备驱动程序，如果我们可以设法将设备驱动程序移出内核，那么内核中的错误和崩溃可能会少得多。</p>
<p>发言人   26:46<br>And a final little advantage to the people who are thinking about back then is that you could emulate or run multiple operating system personalities on top of a micro kernel. So even though the micro kernel does hardly anything for you directly, you might be able to run a Unix server or something on top of it, maybe more than one on the same machine. And of course, that’s what today’s papers about, running a Unix, running Linux as a service on a micro kernel.<br>对于那些正在思考的人来说，最后一个小优势是你可以在微内核上模拟或运行多个操作系统。所以即使微内核几乎没有直接为你做任何事情，你也可以在它上面运行一个Unix服务器或其他东西，可能在同一台机器上运行多个。当然，这就是今天的报纸所谈论的，在微内核上运行Unix，将Linux作为服务运行。</p>
<p>发言人   27:16<br>These were all the set of things that people were hoping to be able to get some traction on by looking into microkernel signs. Of course, there’s some sort of puzzles. You have to think through some challenges. One challenge if you want to design your own microkernel is actually figuring out you want the API. You want the micro kernel system call interface to be as simple as possible because the whole point was to keep it small that what is the actual smallest set of useful system calls you can get away with. And you know, what does it look like? That’s not particularly clear. So looking at the minimum system call.<br>这些都是人们希望能够通过研究微核符号来获得一些关注的事情。当然，有一些谜题。你必须思考一些挑战。如果你想设计自己的微内核，一个挑战实际上是弄清楚你想要的API。你希望微内核系统调用接口尽可能简单，因为整个重点是保持它的小，以至于你可以逃脱的实际最小的有用系统调用集。你知道，它是什么样子的？这不是特别清楚。因此，请查看最小系统调用。</p>
<p>发言人   28:12<br>You need this minimum system can call API. It’s great if it’s simple, but you actually have to be able to build some pretty sophisticated features out of your minimum system called API. Because even if the kernel doesn’t do much, you know, in the end, you’ve got to be able to run programs. You got to maybe if you’re trying to run Unix on top of a microc, and you’ve got to be able to do things like fork and M map. As part of the system called an interface, simple low level system call interface, it has to be powerful enough to support. All the stuff people need to do, like exact and fork and heck, maybe even copy on right? Fork or memory mapping on disk files, but all in a kernel that has no idea about files or a file system, right? Needs to support exec, but with a kernel that knows nothing about files?<br>你需要这个最小系统可以调用API。如果简单的话很好，但实际上你必须能够从你的最小系统中构建一些非常复杂的功能，称为API。因为即使内核做不了什么，你知道，最终你必须能够运行程序。如果你想在microc上运行Unix，你必须能够做fork和M map之类的事情。作为称为接口的系统的一部分，简单的低级系统调用接口，它必须足够强大以支持。人们需要做的所有事情，比如精确、分叉等等，甚至可能是在上面复制吗？在磁盘文件上进行Fork或内存映射，但全部都在一个不知道文件或文件系统的内核中，对吗？需要支持exec，但内核对文件一无所知？</p>
<p>发言人   29:07<br>We need the rest of the. Of the operating system? Somehow? Not sure.<br>我们需要剩下的。关于操作系统？不知怎么的？不确定。</p>
<p>发言人   29:13<br>The microkernel may be very simple, but not now. We’re requiring the development of some set of user level servers that implement the rest of the operating system. So we need that has to get done at least, and may require some just solving design puzzles. And finally, this arrangement requires a lot of chit chat over interprocess communication over IPC. So there’s going to be great pressure to make the.<br>微核可能非常简单，但现在还不是这样。我们需要开发一些用户级服务器来实现操作系统的其余部分。所以我们需要至少完成，可能只需要解决一些设计难题。最后，这种安排需要在IPC的进程间通信上进行大量的聊天。所以会有很大的压力去做。</p>
<p>发言人   29:46<br>IPC very fast. So we’re going to wonder whether IPC can be made fast enough to keep microkernels competitive. Actually, not just IPC speed, but in general, there’s a lot of reason to believe that monolithic kernels derive some performance out of the fact that they’re integrated, that the file system code can talk to the virtual memory code and the memory allocation code. It’s all sort of one big, happy giant program. And if you require all those things to be split out into separate servers, or maybe split between a kernel user level, there may be fewer opportunities for optimization by way of integration. And that may or may not, and a hurting performance. Right, so these are sort of cross cutting, hoped for wins, challenges that all the many microkernel projects faced.<br>IPC非常快。所以我们会想知道IPC是否可以做得足够快，以保持微核的竞争力。实际上，不仅仅是IPC速度，而且一般来说，有很多理由相信单块内核可以从它们被集成的事实中获得一些性能，文件系统代码可以与虚拟内存代码和内存分配代码进行通信。这就像是一个大的、快乐的巨型程序。如果您需要将所有这些内容拆分到单独的服务器中，或者可能在内核用户级别之间拆分，则通过集成进行优化的机会可能较少。这可能是也可能不是，而且是一种有害的表现。对，所以这些都是交叉的，希望获胜，挑战，所有微内核项目都面临的挑战。</p>
<p>发言人   30:53<br>Because of today’s paper I’m going to tell you a bunch about L 4 specifically, which is the micro kernel that the authors of today’s paper developed and used. L four’s not certainly not the earliest micro kernel ever made, but it’s one of the sort of early micro kernels that came out of all the work in this starting in the 1980s. And it’s fairly representative as far as how it works. There’s been, it’s a bit of a moving target. It was a subject of intense development and evolution for many years, and it’s still going strong. If you look it up on Wikipedia, you’ll see that there’s maybe 15 or 20 different variants of L 4 that have kind of come and gone, and some are still here starting, I think, in the late 1980s. And what I’m going to going to try to explain to you is my understanding of how L 4 worked at about the time that today’s paper came out. All right so.<br>因为今天的论文，我将告诉你一些关于L 4的具体内容，这是今天论文的作者开发和使用的微内核。L four肯定不是有史以来最早制作的微内核，但它是从1980年代开始的所有工作中出现的早期微内核之一。就其工作原理而言，它相当具有代表性。已经有了，它有点像一个移动的目标。多年来，它一直是紧张发展和演变的主题，并且仍然很强大。如果你在维基百科上查找，你会发现可能有15或20种不同的L 4变体，它们来来去去，有些还在这里，我想是在1980年代末。我要试着向你们解释的是，大约在今天的论文发表时，我对l4工作原理的理解。好的，就这样。</p>
<p>发言人   32:02<br>Just at a high level, the Elf 4 was certainly micro in the sense that it was actually is a small kernel. It has only 7 system calls. Some of them are a little bit complex, but still, it only has 7 system calls. Whereas today’s Linux, the last time I counted, had in the mid 300 S and even XV 6, which is an extremely simple kernel. Even XV 6 has 21 system calls, so L 4. Is only 7. So by that metric, it’s simple. It’s also not very big.<br>只是在一个高层次上，精灵4在感知上肯定是微观的，它实际上只是一个小内核。它只有7个系统调用。其中一些有点复杂，但仍然只有7个系统调用。而今天的Linux，我最后一次计算，在300年代中期甚至是XV 6，这是一个非常简单的内核。即使XV 6也有21个系统调用，所以L 4。只有7。根据这个指标，它很简单。也不是很大。</p>
<p>发言人   32:38<br>I think as of the time this paper was written at about 13000 lines of code, which is not too much. XV 6 is smaller than that. I think XV 6 is maybe 6 or 7000 lines of code in the kernel, but still XV 6 is very as simple as kernels go. And so L 4, not much more complex than that.<br>我认为在这篇论文编写时，大约有13000行代码，这并不是太多。XV 6比这个小。我认为XV 6可能是内核中的6或7000行代码，但XV 6仍然像内核一样简单。所以L 4，没有比这更复杂的了。</p>
<p>发言人   32:57<br>This is no a 10th or 20th or a 30th as big as Linux is, this is pretty small. It had only a few basic abstractions. It had a notion of what they call tasks or address spaces. And these more or less correspond to what we would call a process in Unix, a bunch of memories, maps starting at 0. And you’re able to execute in here, just like in a process.<br>这没有Linux那么大，只有10号、20号或30号，非常小。它只有一些基本的抽象。它有一个他们称之为任务或地址空间的概念。而这些或多或少对应于我们在Unix中所称的进程，一堆内存，从0开始的映射。你可以在这里执行，就像在一个过程中一样。</p>
<p>发言人   33:36<br>One difference from XV 6 is that there can be multiple threads per task. And L 4 was in charge of scheduling multiple threads of execution within each task. And part of the reason for this is just that it’s very convenient to have.<br>与XV 6的一个区别是每个任务可以有多个线程。L 4负责在每个任务中调度多个执行线程。部分原因只是因为它非常方便。</p>
<p>发言人   33:58<br>Threads is a programming structuring program structuring tool. And it was also, I don’t know if they actually supported multicore or multiprocessor machines at the time the paper was written. They may well have. And threads are, of course, just what you need to be able to harness multiple cores, you know, executing in the same program.<br>Threads是一种编程结构化程序构建工具。而且在论文撰写时，我也不知道他们是否真的支持多核或多处理器机器。他们可能已经有了。当然，和线程正是你需要能够利用多个内核的，你知道，在同一个程序中执行。</p>
<p>发言人   34:19<br>So where there were threads supported by the L 4 kernel. So L 4 supported tasks. It knew about tasks, I knew about threads, and it also knew about address spaces in the sense that you could ask, tell L 4, look, here’s how I want pages mapped in my address space. And the other main thing that L 4 knew about is in-process. Every thread had an identifier. And one thread could say, look, I want to send a message, just some bytes to another thread. And here’s this identifier, please. Send a message to that other thread. So these are really the only.<br>因此，在l4内核支持线程的地方。所以L 4个支持任务。它知道任务，我知道线程，它也知道感知中的地址空间，你可以问，告诉L 4，看，这是我想要在地址空间中映射页面的方式。而L-4所知道的另一件主要事情是在进行中。每个线程都有一个标识符。一个线程可以说，看，我想向另一个线程发送一条消息，只是一些字节。这是标识符，请。发送一条消息到另一个线程。所以这些确实是唯一的。</p>
<p>发言人   35:12<br>Task threads, address spaces, and IPC were really the only abstractions the system calls. I don’t know if I can be able to list them all, but. The system calls were there was a thread create system call which also you gave it a dress based ID and a task to create a new thread. And if the address space or task didn’t already exist, it would create a new task for you sort of sort of combine thread and task creation.<br>任务线程、地址空间和IPC实际上是系统调用的唯一抽象。我不知道我是否能列出所有的东西，但是。系统调用是有一个线程创建系统调用，你也给它一个基于着装的账号和一个创建新线程的任务。如果地址空间或任务还不存在，它会为你创建一个新的任务，有点像合并线程和任务创建。</p>
<p>发言人   35:48<br>There’s send and receive various flavors of send and receive IPC system calls. There’s a way to map pages into your or other address spaces. You could ask the L 4 to change the way your address space was set up, the way your page table mappings were set up. But you could also ask L 4 if you had the right permissions to go and change the way another tasks address space was set up and.<br>有发送和接收各种风格的发送和接收IPC系统调用。有一种方法可以将页面映射到您的或其他地址空间。你可以要求L 4更改地址空间的设置方式，以及页表映射的设置方式。但是你也可以询问L 4是否有正确的权限去更改另一个任务地址空间的设置方式。</p>
<p>发言人   36:27<br>This was actually done through the IPC. Through the IPC interface, you would send a kind of special IPC message the kernel knew about to the target thread, and the kernel would modify the target threads at dress space. And this is if you are creating a new thread, this is actually new threads are created with no memory at all. So if you want to create a thread, you first call the thread create system called the create a new thread and task and address space. And then you send it, You make one of these magic Ipcs to send it some of your own memory, the map, some of your own memory that you’ve prepared with instructions or data to map that memory into the new, the new task address space. And then you send a special start IPC to this new task with the program counter and the stack pointer you want it to start executing with. It’ll start executing, and that memory you’ve set up at the program calendar that you ask it to start at.<br>这实际上是通过IPC完成的。通过IPC接口，你将向目标线程发送一种内核知道的特殊IPC消息，然后内核将在着装空间修改目标线程。如果你正在创建一个新线程，这实际上是在完全没有内存的情况下创建的新线程。因此，如果要创建线程，请首先调用名为 “创建新线程” 的线程创建系统以及任务和地址空间。然后你发送它，你制作这些神奇的ipc之一，将一些你自己的记忆，地图，一些你自己的记忆，你已经准备好了指令或数据，将这些记忆映射到新的任务地址空间。然后您发送一个特殊的开始IPC到这个新任务，带有程序计数器和堆栈指针，您希望它开始执行。它将开始执行，并且您在要求它开始的程序日历上设置的内存。</p>
<p>发言人   37:22<br>There’s a way not through system calls. In fact, I don’t know how it worked, but privileged tasks could map device hardware. Device control registers into their own address spaces. So L 4 didn’t really know much about devices like disks or network interface cards. User level software could get directly at use. A lot of the software that implemented device drivers at user level could get directly at device hardware.<br>有一种方法不是通过系统调用。事实上，我不知道它是如何工作的，但特权任务可以映射设备硬件。设备控制寄存器进入它们自己的地址空间。所以L 4并不真正了解像磁盘或网络接口卡这样的设备。用户级软件可以直接投入使用。很多在用户层面实现设备驱动程序的软件可以直接进入设备硬件。</p>
<p>发言人   37:57<br>There was a way to, you could tell L 4 to turn and interrupt. Any interrupt from any device? L 4 didn’t really know which device. It just turned a given interrupt into an IPC message. So a device driver task could not just read and write the device harder, but also tell L 4. Well, anytime that device interrupts, please send me an IPC message to notify me of the interrupt. And finally, 1 task could tell the colonel to, give it notifications of another task, page faults. So if this task page faults, L 4 would turn that into a IPC message and send it to another designated pager task, send the notification of the page-fault, a designated pager task, so every task had an associated It pager task that handled its page faults.<br>有一种方法，你可以让l4转弯并打断。任何设备的任何中断？我4真的不知道是哪个设备。它只是将给定的中断转换为IPC消息。因此，设备驱动程序任务不仅可以读取和写入设备，还可以告诉L 4。好的，每当那个设备中断时，请给我发一个IPC消息，通知我这个中断。最后，一个任务可以告诉上校，给它另一个任务的通知，页面错误。因此，如果此任务页面错误，l4会将其转换为IPC消息并将其发送到另一个指定的寻呼任务，发送页面错误通知，这是指定的寻呼任务，因此每个任务都有一个关联的寻呼任务，用于处理其页面错误。</p>
<p>发言人   38:57<br>And that’s the way you get hooks into the page faults in order to implement things like copy on write fork or lazy allocation. And that’s it for the kernel.<br>这就是你获取页面错误钩子的方式，以便实现像在写分叉时复制或懒惰分配这样的事情。内核就这样了。</p>
<p>发言人   39:10<br>There’s nothing else in L 4. There’s no file system. L 4 didn’t itself have support for things like fork or Ex, didn’t have any communication beyond these very simple IPC, like didn’t have pipes, didn’t have device drivers, no networking support, nothing. Everything else that you wanted it, you need to supply as user level services.<br>在L 4中没有别的了。没有文件系统。L 4本身不支持像fork或Ex这样的东西，除了这些非常简单的IPC之外没有任何通信，比如没有管道，没有设备驱动程序，没有网络支持，什么都没有。其他你想要的东西，你需要作为用户级别的服务提供。</p>
<p>发言人   39:37<br>Okay, so one thing that for does supply is switching among threads. L 4 would actually do the scheduling and context switches in order to multiplex a single CPU among multiple threads. And the way it did it, you would find completely unsurprising L 4 basically had saved registers for every task, for every thread.<br>好的，供应的一件事是在线程之间切换。L 4实际上会进行调度和上下文切换，以便在多个线程之间复用单个CPU。而且它的做法是，你会发现完全不足为奇的L 4基本上为每个任务、每个线程保存了寄存器。</p>
<p>发言人   40:04<br>When it had executed a thread, the execute thread, jump in user space and switch page table so that thread and that thread will execute for a while in user space. Then maybe the timer interrupt would go off. And that was actually a device L 4 knew about. A timer interrupt might go off after a while, interrupt into L 4. L 4 would save this tasks user registers in a per task An array of tasks. Thread structures would save this threads registers way. Pick a new task to run in a loop, much like the scheduling loop in XV 6. Restore this tasks registers from its previously saved registers, switch page tables, and then jump into this task and execute it for a while until the timer interrupt went off or until this task either did.<br>当它执行一个线程时，执行线程会在用户空间中跳转并切换页表，以便该线程和该线程将在用户空间中执行一段时间。那么定时器中断可能会停止。那实际上是L 4知道的一个设备。计时器中断可能会在一段时间后关闭，中断到L 4。L 4将把这些任务保存在每个任务一个任务数组中。线程结构将以寄存器的方式保存这些线程。挑选一个新任务循环运行，很像XV 6中的调度循环。从以前保存的寄存器恢复此任务寄存器，切换页表，然后跳入此任务并执行一段时间，直到计时器中断停止或此任务完成。</p>
<p>发言人   40:54<br>I think there’s also probably a yield system call or something like that. A task could yield the CPU, or a task could wait to receive an IPC, and in that case L 4 would jump back into L 4 and L 4 would save its registers, switch to a new task and whatnot, So that thread switching part of L 4 is very. Be very familiar.<br>我认为可能还有一个收益系统调用或类似的东西。一个任务可能会产生CPU，或者一个任务可能会等待接收IPC，在这种情况下，L 4会跳回L 4，并且L 4会保存它的寄存器，切换到一个新的任务等等，因此L 4的线程切换部分非常重要。非常熟悉。</p>
<p>发言人   41:25<br>The. I mentioned this before, but I just I want to because it comes up, I want to right here, this notion of a pager, the repeat if the process takes a page fault traps into the kernel and the kernel turns that page fault into an IPC message, a designated pager task, and the address tells this pager task which thread faulted and the address it faulted on. And then the pager task is people wants to say implement lazy allocation, maybe this thread red or writes a memory that wasn’t allocated yet think, but it sort of has to be lazily allocated its pager task would then be in charge of allocating some memory from L 4 sending one of these special Ipcs, the calls that caused the memory to be mapped into this task, and then sending an IPC to resume execution inside this thread. So there was this notion of your task to all this all stuff that V 6 or Linux implements in page-fault handlers like you could implement copy on write fork with this if you liked or memory mapped files, all using one of these pager tasks. So they were sort of powerful user level way to play tricks with driven by page faults.<br>这个。我之前提到过这个，但我只是想这样做，因为它出现了，我想就在这里，这个寻呼机的概念，如果进程将页面错误陷阱带入内核，并且内核将该页面错误转换为IPC消息，一个指定的寻呼机任务，地址告诉这个寻呼机任务哪个线程出现故障，以及故障所在的地址。然后寻呼机任务是人们想要说实现懒惰分配，也许这个线程是红色的，或者写了一个还没有分配的内存，但是它必须被懒惰地分配它的寻呼机任务，然后负责从l4中分配一些内存，发送其中一个特殊的ipc，导致内存被映射到此任务的调用，然后发送一个IPC以恢复此线程内的执行。因此，对于V 6或Linux在页面错误处理程序中实现的所有内容，如果您喜欢或内存映射文件，则可以使用这些页面导航任务之一来实现写复刻 (copy on write fork)。因此，它们是一种强大的用户级别的方式，可以通过页面错误驱动来玩弄技巧。</p>
<p>发言人   43:03<br>And so this is an example, one of many examples in which a micro kernel like L 4 might have been quite a bit more flexible for user programs than a conventional kernel if you think Linux ought to do some extra thing, like maybe some, if Linux didn’t already have copy on write fork, and you wanted to have copy on write fork, you really can’t implement the hand in Linux without modifying the kernel. There’s no way to write portable code, portable user level code for Linux that could implement something like copy and write fork. Because I’m not, that’s not quite true, but it would be very difficult. Whereas in L 4, it relatively straightforward. L 4 is like completely set up for you to be able to write user level code that gets the page faults that are required to drive copy on write fork all in user space without having to mess with the kernel.<br>所以这是一个例子，其中像L 4这样的微内核对于用户程序来说可能比传统内核更加灵活，如果你认为Linux应该做一些额外的事情，比如如果Linux还没有写分叉时的复制，如果你想要写复刻的副本，你真的无法在Linux中不修改内核就实现手。没有办法为Linux编写可移植的代码，可移植的用户级别代码，可以实现像复制和编写分叉这样的东西。因为我不是，这并不完全正确，但这将非常困难。而在L 4中，它相对简单。L 4就像完全设置好的一样，您可以编写用户级代码，这些代码可以获取在用户空间中驱动复制所需的页面错误，而不必干扰内核。</p>
<p>发言人   43:58<br>Okay, so any questions so far about how L 4 works? Oh sorry, can you just clarify the difference between a thread and a task? A task corresponds to it’s like a process in XP 6. It has a bunch of memory and an address space, and you can execute user code in it. XV 6, If you have a process in XV 6, it only there can only be one thread of control on a single executing inside a process in XV 6. But in modern operating systems and in L 4, in a single process in a single address space, you could have, if you have multiple cores, you can have multiple cores executing in a single task each, you know, typically, well, always each set up with its own stack inside that task address space. And so if you, that means you can, for example, write a single program that can get parallel speed up improved performance from multi-core hardware by running one thread on having multiple threads each running on a different core, thereby getting more work done. Okay thank you.<br>好的，到目前为止，有什么关于l4如何工作的问题吗？抱歉，你能澄清一下线程和任务之间的区别吗？任务对应于它就像XP 6中的一个进程。它有一堆内存和地址空间，您可以在其中执行用户代码。XV 6，如果您在XV 6中有一个进程，则在XV 6的进程内只能有一个控制线程在单个执行中。但在现代操作系统和L 4中，在单个地址空间的单个进程中，如果你有多个核心，你可以让多个核心在单个任务中执行，你知道，通常情况下，每个任务都在该任务地址空间内设置了自己的堆栈。因此，如果您这样做，这意味着您可以编写单个程序，通过在不同的内核上运行一个线程，每个线程运行多个线程，从而完成更多的工作，从而从多核硬件中并行加速提高性能。好的，谢谢。</p>
<p>发言人   45:22<br>Okay, so as you can see, this is a design that relies heavily on IPC because you’re going to want to talk to your file server. The file server is going to want to talk to the device driver server. You can have IPC messages flying back and forth for every system call, for every page fault for every device interrupt. IPC systems just has to be fast, however. Now we’re starting to.<br>好的，正如你所看到的，这是一个严重依赖于IPC的设计，因为你会想要与你的文件服务器通信。文件服务器将要与设备驱动程序服务器进行通信。您可以让IPC消息在每个系统调用、每个页面故障和每个设备中断之间来回飞扬。然而，IPC系统必须要快。现在我们开始了。</p>
<p>发言人   45:50<br>Sort of talk about a serious potential defect in the micro kernel’s story, So first, let me show you a straightforward but very slow design for IPC patterned off of Unix pipes. And I’m bringing this up because some early micro kernels. Worked in sort of a similar way to what I’m about to show you, which turned out to be slow.<br>有点谈论微内核故事中的一个严重的潜在缺陷，所以首先，让我向您展示一个简单但非常缓慢的基于Unix管道的IPC设计。我提起这个是因为一些早期的微内核。工作方式与我即将展示给你的方式有点类似，结果证明很慢。</p>
<p>发言人   46:25<br>Okay? So let’s suppose you have, you know, you have two processes. We got P1. P1 wants to send a message to P2. So how should that actually work? Well, one possibility is to have a send system call. And you give send system called the ID of the thread. You want to send the message to a pointer, to the message, to the bytes, maybe that you actually want to send to that process.<br>好吗？那么让我们假设你有，你知道，你有两个过程。我们得到了p1。P1想要向p2发送消息。那么这应该如何运作呢？好的，一种可能性是有一个发送系统调用。并且你给发送系统命名为线程的账号。你想将消息发送到指针、消息、字节，也许你实际上想发送到那个进程。</p>
<p>发言人   47:00<br>The system calls, you’re going to jump into the kernel. You know, maybe we design this patterned after pipes and XV 6. So you can imagine there being a buffer of messages waiting, or maybe P two’s doing something else right now, if it’s a server is serving somebody else’s request, so it’s not ready to handle your request, you can imagine maybe a buffer of waiting messages in the kernel, like a pipe buffer when you call send, appends your message to this buffer, waiting for p.u. to receive it. Now, in fact, almost always in these systems, you rarely just wanted to send a message, you almost always wanted to get a response to, you wanted an RPC or remote procedure call operation. So in fact, P1 would probably follow this immediately by a receive to try to get the response back. But in general, let’s just imagine we’re doing a one way IPC for the moment. So send would append your message to the in kernel buffer. We would have to copy the message bytes from user space into this buffer, and then return and process one can do something else, maybe prepare to receive the response.<br>系统调用后，您将跳转到内核。你知道，也许我们会在管道和XV 6之后设计这个图案。所以你可以想象有一个等待的消息缓冲区，或者现在正在做其他事情，如果它是一个服务器正在为别人的请求提供服务，所以它还没有准备好处理你的请求，你可以想象内核中有一个等待消息的缓冲区，就像一个管道缓冲区，当你调用send时，将你的消息附加到这个缓冲区，等待p.u. 接受它。实际上，现在在这些系统中，你很少只想发送消息，你几乎总是想获得响应，你想要一个RPC或远程过程调用操作。所以实际上，P1可能会立即通过receive来尝试获取响应。但总的来说，让我们想象一下我们目前正在做单向IPC。因此，send会将您的消息附加到内核缓冲区中。我们必须将消息字节从用户空间复制到此缓冲区，然后返回并处理，可以做其他事情，可能准备接收响应。</p>
<p>发言人   48:12<br>After a while. P2, it’s going to want to receive the next message is going to make the receive system call, and that’s going to return the ID of the sender and copy the message into P two’s memory. So it’s going to take the front message off the queue, copy into P two’s memory, and then return.<br>过了一会儿。P2，它将希望接收下一条消息，并进行接收系统调用，这将返回发送者的账号并将消息复制到P2的内存中。所以它将把前面的消息从队列中取出，复制到p2的内存中，然后返回。</p>
<p>发言人   48:46<br>This is called? Words for this, whose opposites you’ll see, you saw in today’s paper. This is called an asynchronous scheme because P1 sends a message without having to wait for anything. It just depends to this Q and returns. And it’s called a buffered system. Because the kernel copies these messages into the buffer, into its internal buffer, ascend, and then later, when the receive happens, it copies the message out of the buffer to the target. So this is asynchronous and buffered.<br>这叫做？这个词，你会看到谁的对立面，你在今天的报纸上看到了。这被称为异步的方案，因为P1无需等待任何事情就可以发送消息。这只取决于这个Q和回报。这就是所谓的缓冲系统。因为内核将这些消息复制到缓冲区，进入其内部缓冲区，然后上升，然后在接收发生时，将消息从缓冲区复制到目标。所以这是异步的和缓冲的。</p>
<p>发言人   49:27<br>If you’re doing a full request response pair, then P1 is going to call send, send is going to return. P1 is then immediately. Now let’s assume we’re going to assume that there’s really sort of two sets of buffers, one for each direction. P1 is immediately going to call receive. Is going to wait going to need to wait for something to appear in the reply buffer? So it’s going to have to yield the CPU.<br>如果您正在进行完整的请求响应对，则P1将调用send，send将返回。P1立即出现。现在让我们假设实际上有两组缓冲区，每个方向一个。P1将立即呼叫receive。等待是否需要等待回复缓冲区中出现某些内容？所以它必须产生CPU。</p>
<p>发言人   49:50<br>It’s going to have to do something like call sleep in XV 60, yield the CPU, and on a single CPU system, it may be only at this point that P1 gives up the CPU, and now P2 can run. And indeed, the hardware in this arrow is almost always single core. Certainly this paper ISS running on single core hardware. So P1 is, it’s going to be P1 executing and P1 not executing until P1 finally gives up the CPU and receive waiting for a message to appear here. And only then will p to be scheduled.<br>它将不得不做一些像在XV 60中调用sleep这样的事情，产生CPU，在单个CPU系统上，可能只有在此时P1放弃了CPU，现在P2可以运行了。事实上，这个箭头中的硬件几乎总是单核。当然，这篇论文是在单核硬件上运行的。所以P1是，它将执行P1，而P1直到P1最终放弃CPU并接收等待消息出现在这里才执行。只有到那时，才会安排p。</p>
<p>发言人   50:22<br>Maybe it’ll call receive s will copy the message, and then P2 will make its call a sand. To appendix reply and then the send system call will return to P2. And at some point, presumably P2 will give up the CPU. Maybe the timer will go off, then P1 will resume execution in the kernel, see that there’s a message there, and return it back to user space.<br>也许它会调用receive，s会复制消息，然后P2会将其调用变成沙。发送到附录回复，然后发送系统调用将返回到p2。在某个时候，假设P2会放弃CPU。也许计时器会停止，然后P1将继续在内核中执行，看到那里有一条消息，然后将其返回到用户空间。</p>
<p>发言人   50:50<br>And so that means that in this design, this slow design, there’s in order to have a request and a response, There’s 4 system calls, 2 sends, and two receives, and, you know, 8 user kernel crossings, each one of which is like reasonably expensive. There’s a need to sleep. This receive has to sleep waiting for data to appear, and there’s a full call to the scheduler loop and a context switch from P1 to P2 in order to make this. And you know, each of these kernel crossings and context switchings is potentially expensive because every time you cross the kernel user boundary, you switch page tables and that it has a near certainty of disturbing the CPU caches. Changing the page table probably flushes the TLB, the virtual memory lookup cache, which is going to slow things down. So this is a pretty slow way to go and involves a lot of kernel crossings, copying of messages between user and kernel, maybe allocation of buffers etc.<br>这意味着在这个缓慢的设计中，为了有一个请求和一个响应，有4个系统调用，2个发送，2个接收，以及8个用户内核交叉，每个都相当昂贵。有睡觉的必要。这个接收必须睡眠，等待数据出现，并且有一个完整的调度器循环调用和一个从P1到P2的上下文切换，以便实现这一点。而且你知道，这些内核交叉和上下文切换中的每一个都可能是昂贵的，因为每次跨越内核用户边界时，你都会切换页表，并且几乎肯定会干扰CPU缓存。更改页表可能会刷新TLB，即虚拟内存查找缓存，这会减慢速度。所以这是一个相当缓慢的方式，涉及到很多内核交叉，用户和内核之间的消息复制，可能还有缓冲区的分配等。</p>
<p>发言人   52:05<br>But it turns out that for the for this stylized case in which you’re sending a request and you want to get a response back, you can strip this down to a considerably simpler design.<br>但事实证明，对于这种样式化的情况，即您正在发送请求并希望获得响应，您可以将其简化为相当简单的设计。</p>
<p>发言人   52:18<br>And in fact, this is the way L 4 worked. And this was laid out in a famous paper called Improving IPC by Kernel Design published a few years before today’s paper. So it does a couple things differently. For one thing, it’s synchronous, that is. There’s none of this. There’s no. Dropping something off and returning and waiting. Letting the other guy, letting the other process pick up the data when it feels like it.<br>实际上，这就是l4运作的方式。这在今天的论文发表前几年发表的一篇著名论文 “通过内核设计改进IPC” 中有所阐述。所以它做了几件不同的事情。首先，它是同步的。这些都没有。没有。放下东西，然后回来等待。让另一个人，让其他进程在感觉到时拾取数据。</p>
<p>发言人   52:57<br>Instead, send waits for receive and receive waits for send. So if I’m process 1 and I want to send and I call send. It doesn’t copy my message into a buffer. It actually P1 will not immediately the P1 send in the L 4 kernel waits for P2 to call receive. And if P2 is already in the kernel waiting on a call to receive, well, P2 is either already in the kernel waiting in a call to receive, or P1 send, wait for it, waits for P two’s next call to receive.<br>相反，发送等待接收，接收等待发送。所以，如果我是进程1，我想发送，我称之为发送。它不会将我的消息复制到缓冲区中。实际上，P1不会立即在L 4内核中发送P1，等待P2调用receive。如果P2已经在内核中等待接收调用，那么，P2要么已经在内核中等待接收调用，要么P1发送等待它，等待P2的下一个调用接收。</p>
<p>发言人   53:39<br>When both have arrived here, when P1 is in the kernel, and it’s called the send and P2 is in the kernel, and it’s called to receive, only then does anything happen. And one reason this is fast is that if P2 is already in receive, then P1, when it’s executing send in the kernel, can just without a context switch or a general purpose scheduling, can just jump back into user space into P2 as if it was returning from this receive. And that’s a much faster path through the kernel than saving registers, giving up the CPU, calling the scheduler, and finding a new process to run. Instead. P1 send knows that there’s a waiting receive and just sort of immediately jumps into P2 as if it was returning from C?<br>当两者都到达这里，当P1在内核中，被称为send，而P2在内核中，被称为receive时，才会发生任何事情。这个速度快的一个原因是，如果P2已经在接收中，那么当P1在内核中执行send时，可以不需要上下文切换或通用调度，就可以跳回用户空间到P2，就像它从这个接收中返回一样。这比保存寄存器、放弃CPU、调用调度程序和找到一个新的进程运行要快得多。相反。P1发送知道有一个等待的接收，然后立即跳入P2，好像它是从C返回一样？</p>
<p>发言人   54:40<br>The scheme that they developed is also unbuffered. And it could do that partially because it’s synchronous when both the send and the receive are in the kernel, the message can be send is sending some message.<br>他们开发的方案也是无缓冲的。它可以部分做到这一点，因为当发送和接收都在内核中时，它是同步的，消息可以发送一些消息。</p>
<p>发言人   55:00<br>The kernel can directly copy the message from user space to user space without having to first copy it into the kernel and then back out of the kernel because because since know both sides wait for the other system called to happen, that means that they’ve waited for both pointers to be known. Receive specifies where it wants the message to be deposited. So at this point, we know both addresses. Then protocol just do the copy directly instead of through the kernel. And if the message is super small, like maybe only a few dozen bytes, then it can be passed in registers without any copy at all. What you might call zero copy.<br>内核可以直接将消息从用户空间复制到用户空间，而无需先将其复制到内核中，然后再从内核中复制回来，因为由于知道双方都等待另一个系统调用发生，这意味着他们已经等待两个指针已知。接收指定了消息要存放的位置。所以在这一点上，我们知道两个地址。然后协议直接进行复制，而不是通过内核。如果消息非常小，比如只有几十个字节，那么它可以在寄存器中传递，而不需要任何副本。你可以称之为零拷贝。</p>
<p>发言人   55:50<br>Remember, the send only proceeds if P2 is already in receive, and the send basically jumps directly to P2. Well, this code path through the kernel takes care to not disturb a bunch of the registers. And that means that P1 can put its system call. If the messages is short, it can put the message in certain designated registers. The kernel guarantees to preserve those registers on its way up to P2. And that means that when the kernel returns from the receive system call as a result of send, the contents of those designated registers hold the message and therefore never had to be copied at all from memory to memory, never had to be moved at all. They’re just sitting right in the registers where they can be accessed very quickly.<br>请记住，仅当P2已经在接收时发送才继续，并且发送基本上直接跳到P2。这个代码路径通过内核，注意不要干扰一堆寄存器。这意味着P1可以进行系统调用。如果消息很短，它可以将消息放入某些指定的寄存器中。内核保证在上升到p2的过程中保留这些寄存器。这意味着，当内核作为发送的结果从接收系统调用返回时，那些指定寄存器的内容保存着消息，因此根本不需要从内存复制到内存，也根本不需要移动。它们只是坐在寄存器中，可以非常快速地进行访问。</p>
<p>发言人   56:39<br>And of course, this only works for small messages. For very large messages L 4 could carry a page mapping in an IPC message. So for huge messages. Like the result of reading a block from a file or something. You could just send the page and it will be mapped into the targets address space, again without any copy. So this is done through page mapping. Give away the page or access to a Cop access permission to share the page. So small messages are fast, huge messages are pretty fast, and you still have to adjust the page table to target. That’s much faster than copying.<br>当然，这只适用于小信息。对于非常大的消息，l4可以在IPC消息中承载页面映射。所以对于巨大的信息。就像从文件或其他东西中读取块的结果一样。您可以只发送该页面，它将被映射到目标地址空间，而无需任何复制。这是通过页面映射完成的。放弃该页面或访问共享该页面的警察访问权限。所以小消息很快，大消息很快，你仍然需要调整页表以适应目标。这比复制快得多。</p>
<p>发言人   57:27<br>And a final trick that Elf Go played was noticing that if you’re doing an RPC with a request and a response, there’s a very stylized pairs of. System calls, and you may as well combine system calls. Send and receive system calls in order to reduced kernel crossing. So for the special case of RPC, which is almost always what people are doing when they’re using IPC, there was a call system call. And a call was basically a combined send plus receive. But without the return to user space and then re-entry into kernel space that a pair of system. Take and on the server side, there was a single call that would send the reply from one system call and then wait for the request message from anyone for the next system call. And this was basically a send of one response plus wait to receive the next request.<br>小精灵Go玩的最后一个把戏是注意到，如果你正在使用请求和响应进行RPC，则会有非常风格化的对。系统调用，你也可以组合系统调用。发送和接收系统调用以减少内核交叉。因此，对于RPC的特殊情况，即人们在使用IPC时几乎总是在做的事情，有一个呼叫系统调用。呼叫基本上是组合的发送加接收。但是没有返回用户空间，然后重新进入内核空间，这是一对系统。在服务器端，有一个调用将发送来自一个系统调用的回复，然后等待任何人的请求消息以进行下一个系统调用。这基本上是发送一个响应加上等待接收下一个请求。</p>
<p>发言人   58:40<br>And this again, cut in half the number of kernel crossings.<br>这一次，将内核交叉的数量减少一半。</p>
<p>发言人   58:44<br>And it turned out that the sum of all of these optimizations for the kind of short rpc’s which? Or, you know, one typical workload. All this led to a 20x speed up. This is what their paper reported, 20x speed up over their previous system, which was presumably a little bit more like what I showed in the previous design. And so this was an impressive. This paper came out a few years by some of the same authors. A few years before the people were reading, and this caused people to view microkernels a little bit more favorably that the IPC could actually be made quite fast.<br>结果发现，对于短rpc，所有这些优化的总和是什么？或者，你知道的，一个典型的工作量。所有这些都导致了20倍的速度提升。这是他们的论文所报道的，他们的系统速度比之前的系统快20倍，这大概更像我在之前的设计中展示的那样。所以这是令人印象深刻的。这篇论文是由一些相同的作者在几年前写出来的。在人们阅读之前的几年，这导致人们对微核的看法更加有利，以至于IPC实际上可以相当快地制造出来。</p>
<p>发言人   59:32<br>Any questions about these IPC tricks that L 4 plays? Yeah, I think I missed this, but when is the process sending or like receiving messages? Like when is it using that system call? Okay, actually, so for rpc’s for request response in fact. Processes use this pair of of system calls rather than send and receive. So yeah, call you really give it two arguments, a message you want to send and a place to put the response. And inside the kernel, it just combines these two. I mean, you could view this as a bit of a hack, but. Because IPC is so frequent, it’s worth a little bit of hackery in order to make me fast.<br>有关于l4所扮演的这些IPC技巧的问题吗？是的，我想我错过了这个，但是这个过程是什么时候发送或接收消息的？比如什么时候使用这个系统调用？好的，实际上，rpc的请求响应实际上是这样的。进程使用这对系统调用，而不是发送和接收。所以，是的，打电话给你真的给它两个参数，一个你想要发送的消息和一个放置响应的地方。在内核中，它只是将这两者结合在一起。我的意思是，你可以把这看作是一个小技巧，但是。因为IPC非常频繁，为了让我禁食，它值得一点点黑客攻击。</p>
<p>发言人   01:00:25<br>And in the diagram up there in the box where you have P2 sending that or like running the receive system call, like what prompted P2 to in my RPC world, we have clients and they’re sending requests to servers. And the server is going to do something in reply. So since P2 is a server, we imagine that P2 is sitting in a Y loop in which it in which it’s going to receive the next message from any client, do a little bit of work to process it, you know, look up some data in a database or something, and then send a reply and go back to the top of the loop and wait again. To a first approximation, we expect P2 to spend all that time waiting for the next message from anyone next request from anyone. This design, really? It does kind of rely on P2 always when it’s at rest, basically sitting in the kernel in a receive system call, waiting for the next request so that the next request can directly basically return from that system call. That’s the fast path that’s super efficient in this design.<br>在盒子里的图表中，你有P2发送这个或者运行接收系统调用，就像在我的RPC世界中提示P2的原因一样，我们有客户端，它们正在向服务器发送请求。服务器会回复一些事情。所以由于P2是一个服务器，我们想象P2坐在一个Y循环中，它将接收来自任何客户端的下一条消息，做一些工作来处理它，你知道，在数据库中查找一些数据或其他东西。然后发送回复并返回循环的顶部并再次等待。以第一次近似，我们预计P2将花费所有时间等待任何人的下一个消息和下一个请求。这个设计，真的？当它静止时，它总是依赖于P2，基本上在接收系统调用中坐在内核中，等待下一个请求，以便下一个请求可以直接从该系统调用返回。这是这个设计中非常高效的快速路径。</p>
<p>发言人   01:01:45<br>Cool, you. So just to follow up on that, that means that you said that it goes from P1 and like returns to P2. So like to come back, you would need to send the response. That’s right, we expect P2 to send a response and that sending of the response actually follows basically the same code path in reverse that when P2 sends a response. That effectively causes P1 to return from. I mean, p one’s actually making this call system call. So the delivery of P two’s response causes the return from from this system call back into P1.<br>酷，你。所以只是跟进一下，这意味着你说它从P1变成了像返回到p2一样的东西。所以想要回来，你需要发送响应。没错，我们期望P2发送响应，并且响应的发送实际上遵循与P2发送响应时基本相同的代码路径。这有效地导致P1返回。我的意思是，p one实际上正在进行这个呼叫系统调用。因此，传递P二的响应会导致从这个系统调用返回到p1。</p>
<p>发言人   01:02:29<br>Okay, I see, thank you. You know, this is a little bit different from the usual setup where you think of you jump into the kernel in a system call, and you execute that system call, and it returns sort of all working on the half a P1, which is the way pipe read and write work. Here. You know, P1 is entering the kernel, you know it’s P1 entering the kernel, but the return goes to P2. Kind of odd, but very fast.<br>好的，我明白了，谢谢。你知道，这与通常的设置有点不同，在通常的设置中，你认为你在系统调用中跳入内核，然后执行该系统调用，它返回半P1上的所有工作，这是管道读取和写入工作的方式。这里。你知道，P1正在进入内核，你知道这是P1进入内核，但是返回到p2。有点奇怪，但非常快。</p>
<p>发言人   01:03:02<br>Okay? So this was a big, big sort of contribution to people taking microkernels.<br>好吗？这是对人们服用微核的一个非常大的贡献。</p>
<p>发言人   01:03:13<br>People’s willingness to take microkernel seriously as potentially a replacement for monolithic kernels. However, you know, you still have to, you still leave open the question even if rpc’s fast, like where do you get the rest of the operating system? This kernel only has like a few percent of all the stuff like file systems and network stacks that we expect to be in a full operating system. What do we do about the rest? And this question is usually being the asked in the context of some university research project with relatively limited resources.<br>人们愿意认真对待微内核，作为单内核的潜在替代品。然而，你知道，即使rpc速度很快，你仍然需要保留一个问题，比如你从哪里得到操作系统的其余部分？这个内核只有我们期望在完整操作系统中包含的文件系统和网络堆栈等所有内容的百分之几。剩下的我们该怎么办？这个问题通常是在一些资源相对有限的大学研究项目的背景下提出的。</p>
<p>发言人   01:03:46<br>We need to get all those user level services from somewhere. Now, actually, there are specialized applications for which that’s not too much of a problem. If we’re running some sort of controller, maybe the ignition control system for your car is only running a few thousand lines of code anyway. Maybe it doesn’t need a file system. Then we can get away with like very little stuff at user level. And micro kernels totally makes sense for that kind of application.<br>我们需要从某个地方获取所有这些用户级别的服务。现在，实际上，有一些专门的应用程序，对于它们来说这不是太大的问题。如果我们运行某种控制器，也许你汽车的点火控制系统只运行了几千行代码。也许它不需要文件系统。那么我们就可以在用户层面上处理非常少的东西。而微内核完全为这种应用程序做出了感知。</p>
<p>发言人   01:04:19<br>But the people, you know, in these projects, really they had ambitions that, oh, gosh, we can totally replace existing operating systems. And they hoped that they could build something that people would want to run on their workstations and run on their servers and everywhere replace big monolithic kernels altogether. But for that, you know, you need a real, you need all the stuff that an operating system does. One possibility the most, maybe sort of philosophically consistent possibility, would be to, you know, we implement everything you need, but in a sort of micro kernel way as lots and lots of different user level processes. But that’s just actually people. There were projects that did that, but it’s a vast amount of work. And more specifically, people really want to run, you know, in order for me to use a laptop, it just has to run Emacs and it has to run my favorite C compiler, otherwise I’m just definitely not going to switch to your operating system.<br>但是，你知道，在这些项目中，他们真的有野心，哦，天啊，我们可以完全取代现有的操作系统。他们希望能够建立一些人们想要在他们的工作站上运行的东西，并在他们的服务器上运行，在任何地方都可以完全取代大的单体内核。但为此，你知道，你需要一个真正的，你需要操作系统能够做的所有事情。一种可能性，也许是哲学上一致的可能性，是我们实现你需要的一切，但以一种微内核的方式，作为许多不同的用户级别进程。但实际上那只是人。有一些项目做到了这一点，但这是一项巨大的工作。更具体地说，人们真的想要运行，你知道，为了让我使用笔记本电脑，它只需要运行Emacs和我最喜欢的C编译器，否则我肯定不会切换到你的操作系统。</p>
<p>发言人   01:05:22<br>And what that meant is that microkernels in order for to gain any kind of adoption, they had to be able to support existing applications. They had to be able to be compatible, provide identical at least at the system called a the higher level service API level, they have to be totally compatible with some existing operating system, like Unix, like Linux, in order for anybody to be willing to switch. So these projects face a more specific problem of how they were going to get, how are they going to attain compatibility with existing applications written for Linux or maybe Windows or something, but for this project it was Linux and rather than write their own totally new set of user level servers that mimic Linux, they decided to take the far easier path.<br>这意味着微内核要获得任何形式的采用，它们必须能够支持现有的应用程序。它们必须能够兼容，至少在称为更高级服务API级别的系统上提供相同的服务，它们必须与某些现有的操作系统完全兼容，如Unix、Linux，以便任何人愿意切换。因此，这些项目面临着一个更具体的问题，即他们将如何获得与为Linux或Windows编写的现有应用程序的兼容性，但对于这个项目来说，它是Linux，而不是编写自己的全新用户级服务器集，模仿Linux。他们决定走更容易的道路。</p>
<p>发言人   01:06:12<br>And many projects did this of simply directly running an existing monolithic kernel as on top of their micro kernel instead of reimplementing some new thing.<br>许多项目只是直接在微内核上运行现有的单体内核，而不是重新实现一些新的东西。</p>
<p>发言人   01:06:26<br>So that’s exactly what today’s paper is about. It has? Indeed, you know L 4 micro kernel down at the bottom, But also as like a pretty big server, they run a pretty full Linux kernel as a user level process. And so that may sound a little surprising. The kernel is not to use a level process, right? The kernel’s the kernel. You think of it as running on the hardware, but in fact, you know, the Linux kernel, as you can see from running XV 6 in QM, which is running a user space. After all, a kernel is just a program. And so with some modifications, it can be made to run at user level.<br>这正是今天报纸的重点。它有？确实，你知道在底部的4微内核，但就像一个相当大的服务器一样，它们作为用户级进程运行相当完整的Linux内核。所以这可能听起来有点令人惊讶。内核不是使用层次过程，对吗？内核就是内核。你认为它是在硬件上运行的，但实际上，你知道Linux内核，正如你在QM中运行XV 6所看到的，QM正在运行用户空间。毕竟，内核只是一个程序。因此，通过一些修改，它可以在用户级别运行。</p>
<p>发言人   01:07:19<br>And so they had to modify Linux. They took a lot of the low level stuff in Linux, for example, the code in Linux that expects to be able to directly modify page tables, read and write processor registers. There were some low level stuff they had to modify. So some parts of Linux they had to change in order to convert them to basically make system calls or send IPC messages through L 4 instead of directly get at hardware. But for the most part, they were able to directly run without change almost all of Linux. So that means they got as part of Linux, you know, a file system and a network support and all kinds of device drivers. And who knows what that comes with Linux without having to write their own version of this.<br>所以他们不得不修改Linux。他们采用了Linux中的许多低级东西，例如，希望能够直接修改页表、读取和写入处理器寄存器的Linux代码。他们必须修改一些低级别的东西。因此，他们不得不更改Linux的某些部分，以将其转换为基本上进行系统调用或通过L 4发送IPC消息，而不是直接在硬件上获取。但在大多数情况下，他们能够直接运行而不需要更改几乎所有的Linux。这意味着它们作为Linux的一部分得到了文件系统、网络支持和各种设备驱动程序。谁知道Linux会带来什么，而不必编写自己的版本。</p>
<p>发言人   01:08:08<br>Now, in fact, the way this was set up was that Linux, the Linux kernel ran as 1 L 4 task. And but each Linux process ran as a separate L 4 task. So when you log into this Linux and you ask it to run a shell for you, a terminal window or something, it’s going to fire up an L 4 task that’s going to run that Linux program at user level. So there were 1 task for Linux and 1 task for each Linux process that you fire up under Linux and Linux, instead of directly, you know, modifying the page table that VI, the VI process uses Linux is going to ask, send the right Ipcs to L 4 to cause L 4 to change Vi’s page table. Any questions about that? But the basic scheme here.<br>实际上，现在的设置方式是Linux，Linux内核运行为1 l 4任务。但是每个Linux进程都作为单独的L 4任务运行。因此，当您登录到此Linux并要求它为您运行shell、终端窗口或其他内容时，它将启动一个l4任务，该任务将在用户级别运行该Linux程序。因此，在Linux和Linux下，每个Linux进程都有1个任务和1个任务，而不是直接修改VI进程使用Linux的页表，将正确的ipc发送到L 4，以使L 4更改Vi的页表。有什么问题吗？但是这里的基本方案。</p>
<p>发言人   01:09:15<br>Another thing that was changed, many small things were changed. A specific thing of interest is that when VI wants to make a system call, if the doesn’t know it’s running on L 4. In this scheme, it’s really all these programs just think of themselves as running on Linux When VI wants to make a system call, you know L 4 does not support, it’s not making an L 4 system call, it’s making a Linux system call. So VI system calls is like fork, there’s a little library basically that was linked into these. Linux processes that would turn calls to things like fork or exec or pipe or read or write into IPC messages that it would send to the Linux task and wait for the response to the Linux task and then return as if the system called it return. So these little libraries would turn system calls into. IPC messages to Linux. And what that meant is that if the Linux kernel task isn isn’t doing anything else, it’s sitting in a call to receive waiting for the next system call request IPC from any one of these processes.<br>另一件事情发生了变化，许多小事情都改变了。一个特别有趣的事情是，当VI想要进行系统调用时，如果它不知道它正在L 4上运行。在这个方案中，当VI想要进行系统调用时，实际上所有这些程序都认为自己在Linux上运行，你知道L 4不支持，它不是在进行L 4系统调用，而是在进行Linux系统调用。因此，VI系统调用就像fork一样，基本上有一个小的库链接到这些库。Linux进程会将调用转向像fork、exec、pipe这样的东西，或者读取或写入IPC消息，将其发送给Linux任务并等待对Linux任务的响应，然后像系统调用它一样返回。所以这些小库会把系统调用变成。IPC消息到Linux。这意味着，如果Linux内核任务没有做任何其他事情，它就会坐在一个调用中，等待来自这些进程中的任何一个的下一个系统调用请求IPC。</p>
<p>发言人   01:10:39<br>That led to. That leads to a significant difference between how this Linux works and how ordinary Linux works. In ordinary Linux, just like XV 6, there’s a basically a kernel thread that corresponds to every user level process and went up program makes a system call. The kernel runs a thread on behalf of that system call.<br>这导致了。这导致了Linux的工作方式与普通Linux的工作方式之间存在显著差异。在普通的Linux中，就像XV 6一样，基本上有一个内核线程对应于每个用户级别的进程，并且程序会进行系统调用。内核代表该系统调用运行一个线程。</p>
<p>发言人   01:11:06<br>In ordinary Linux, when Linux switches between kernel threads, that basically implies a switch from 1 process to another. So there’s kind of one to one correspondence between. What kernel thread Linux kernel is running and what process is going to run When Linux is done? Here, that connection is broken. There were indeed, in this Linux server, a kernel thread corresponding to each. I’m sorry, let me start again. The Linux kernel server was running in a single L 4 thread, so there was only a single sort of thread of control executing in Linux at a time. However, just as an XV 6, this one thread of control would switch, you know, using a technique very much like.<br>在普通Linux中，当Linux在内核线程之间切换时，基本上意味着从一个进程切换到另一个进程。所以两者之间有一对一的对应关系。Linux内核正在运行什么内核线程，完成后将运行什么进程？在这里，这种联系被打破了。在这个Linux服务器上，确实有一个对应于每个线程的内核线程。对不起，让我重新开始。Linux内核服务器在单个L 4线程中运行，因此在Linux中每次只执行一种控制线程。然而，就像XV 6一样，这个控制线程会切换，你知道，使用一种非常类似的技术。</p>
<p>发言人   01:11:59<br>XV six’s context switch could switch between a kernel thread corresponding to each user process. However, which of these kernel threads were implemented purely within Linux had nothing to do with L 4 threads. There’s only 1 L 4 thread here, but which user process was running was determined by L 4. So in this setup Linux might be serving a request from executing the kernel thread for VI, serving a VI system call at the same time that L 4 is causing this shell to run in user space, which is very unlike what happens in XV 6 or Linux, where there’s a direct correspondence between the sort of active kernel thread and the corresponding user level thread.<br>XV six的上下文切换可以在每个用户进程对应的内核线程之间进行切换。然而，这些内核线程中哪些是纯粹在Linux中实现的，与L 4线程无关。这里只有1个L 4线程，但运行的用户进程是由L 4确定的。因此，在此设置中，Linux可能会为VI执行内核线程提供请求，同时为VI系统调用提供服务，同时L 4导致此shell在用户空间中运行，这与XV 6或Linux中发生的情况非常不同，其中，活动内核线程的种类和相应的用户级别线程之间存在直接对应关系。</p>
<p>发言人   01:12:50<br>Here Al fours off running, whatever it feels like. And these threads in the Linux kernel are really much more private and are just about Linux being able to concurrently execute system calls in different stages of execution where maybe one process is waiting for the disk in its thread. Linux can run a different processes, kernel thread to serve that processes system column.<br>在这里，四肢奔跑，无论感觉如何。而Linux内核中的这些线程实际上更加私有，并且只是Linux能够在不同的执行阶段并发执行系统调用，其中可能一个进程正在等待其线程中的磁盘。Linux可以运行不同的进程，内核线程来处理系统列。</p>
<p>发言人   01:13:20<br>So you might wonder why this design didn’t directly use L 4 threads, to implement the various different.<br>所以你可能想知道为什么这个设计没有直接使用L 4线程来实现各种不同的东西。</p>
<p>发言人   01:13:33<br>Kernel threads inside Linux you Why did Linux implement its own sort of internal threads package instead of using L 4 threads? And the answer was that in those days A, they didn’t have access to multi-core hardware. They were using single core hardware so they would know performance advantage to being able to execute multiple threads in the kernel at the same time because there was only one core, so a second thread couldn’t be executed. Only one threat could executed at the time due to the hardware.<br>你为什么Linux要实现自己的内部线程包而不是使用L 4线程？答案是，在那些日子里，他们无法访问多核硬件。他们使用单核硬件，因此他们会知道能够同时在内核中执行多个线程的性能优势，因为只有一个核心，因此第二个线程无法执行。由于硬件的原因，当时只能执行一个威胁。</p>
<p>发言人   01:14:03<br>And the other, maybe even more powerful reason is that in those days, the version of Linux they were using did not have the support that’s required, have multiple threads, multiple cores executing inside the kernel at the same time, they were using a uniprocessor Linux. It’s old enough Linux that it expected only one core in the kernel at a time. It didn’t have things like the spin locks that XV 6 has that would allow it to correctly execute multiple multiple cores inside the kernel. So there would have been no performance advantage in having multiple L 4 threads active inside the kernel, but it would have required adding in you for no performance win, adding in all the spin locks and other stuff that’s required to support currency. So they didn’t do it.<br>另一个可能更强大的原因是，在那些日子里，他们使用的Linux版本没有所需的支持，有多个线程，同时在内核中执行多个内核，他们使用的是单处理器Linux。它已经足够老的Linux，每次只期望内核中有一个核心。它没有像XV 6那样的旋转锁定功能，这样它就可以在内核中正确地执行多个内核。因此，在内核中拥有多个活动的L 4线程不会有性能优势，但需要添加您才能获得性能胜利，添加所有旋转锁和其他支持货币所需的东西。所以他们没有这样做。</p>
<p>发言人   01:14:58<br>A drawback of this arrangement is that in ordinary Linux, in native Linux, like you would run directly on your laptop. Linux has a lot of sophisticated scheduling machinery that can do things like impose priorities on different processes or ensure various kinds of fairness. And that works fine because on your laptop, because Linux is in control of what process is running on each core. But in this setup Linux is not controlling that at all. Linux has no control over what process is running because it’s L 4 that does this scheduling, not Linux. You know, these processes are scheduled by L 4, so they kind of lost the ability to have Linux be in charge of scheduling. You know, it’s a bit of a defect of this the although I’m sure leader versions of L 4 had some way for Linux or something like it to be able to tell the L 4 scheduler, oh, look, please give this process higher priority or whatever. So it’s a bit awkward.<br>这种安排的一个缺点是，在普通的Linux中，在本地的Linux中，就像直接在笔记本电脑上运行一样。Linux有许多复杂的调度机制，可以执行不同进程的优先级或确保各种公平性等操作。这很有效，因为在你的笔记本电脑上，Linux控制着每个内核上运行的进程。但是在这个设置中，Linux完全没有控制它。Linux无法控制哪个进程正在运行，因为执行此调度的是L 4，而不是Linux。你知道，这些进程是由L 4调度的，所以它们有点失去了由Linux负责调度的能力。你知道，这有点缺陷，尽管我确信L 4的领导版本有一些方法可以让Linux或类似的东西告诉L 4调度器，哦，看，请给这个进程更高的优先级或其他。所以这有点尴尬。</p>
<p>发言人   01:16:06<br>All right?<br>一切都好吗？</p>
<p>发言人   01:16:13<br>So they went to all this work to get this going. You should ask yourself, you know, what is, what’s the takeaway lesson from this paper about micro kernels? Now, one thing this so for us, you know, this paper has a lot of interesting tidbits about how micro kernels work and about how Linux works and how you set up how you can design a system like this. Maybe interesting, but the larger world, you know, people want to draw some lessons. They need to be able to present some lessons in this paper.<br>所以他们为了让这一切继续下去而进行了所有的工作。你应该问问自己，你知道，这篇论文关于微内核的主要教训是什么？现在，对我们来说有一件事，你知道，这篇论文有很多有趣的花絮，关于微内核的工作原理，Linux的工作原理，以及如何设置如何设计这样的系统。也许有趣，但更大的世界，你知道，人们想要吸取一些教训。他们需要能够在本文中提供一些经验教训。</p>
<p>发言人   01:16:53<br>The paper is not really answering the question, are micro kernels a good idea? That’s not really what’s going on here. The what the paper is is part of an argument about whether micro kernels have enough performance to be worth using. And the reason is that in sort of five years or five or 10 years before this paper came out, there was a famous set of measurements on one of the predecessor micro kernels, an earlier micro kernel called mock, basically running in very much this configuration, but a different, you know, totally different design internally, but kind of the same architecture. The name of this earlier microkernel project is Mach. There was measurements on Mach that showed that Mach was dramatically slower than just ordinary Unix when it was run in this configuration. And you know, there are a lot of reasons for that having to do with the IPC system not being as optimized as you might hope, there being just sort of more context switches, you know, user kernel crossings and cache misses and whatever.<br>这篇论文并没有真正回答这个问题，微内核是一个好主意吗？这并不是这里真正发生的事情。这篇论文是关于微内核是否有足够的性能值得使用的争论的一部分。原因是在这篇论文发表之前的五年、五年或十年里，有一组著名的测量方法，针对其前身微内核之一，早期的微内核mock，基本上运行在这种配置下，但是不同的，你知道的。内部设计完全不同，但架构有点相同。这个早期的微内核项目的名字是Mach。在Mach上的测量表明，在这种配置下运行时，Mach比普通的Unix慢得多。而且你知道，有很多原因与IPC系统没有像你希望的那样优化有关，只是有更多的上下文切换，你知道，用户内核交叉和缓存未命中等。</p>
<p>发言人   01:18:05<br>You know, there’s a whole lot of reasons why mock was slow. But many people saw those benchmark results showing that Mach was much slower than native operating systems and decided that microkernels were just hopeless, hopelessly inefficient, or unlikely ever to be fast enough to be competitive. You know, we should just all use monolithic kernels. Today’s paper is like answer basically to that argument. It’s sort of the rebuttal to that argument. And the point of this paper is to show that you can build this architecture and if you pay enough attention to optimizing performance, you can get competitive performance with native operating systems just directly running Unix. And therefore, you can’t dismiss microkernels simply on the basis of performance. You may not want them for other reasons, but you can’t use performance as the reason to reject them.<br>你知道，mock之所以慢有很多原因。但是很多人看到这些基准测试结果显示，马赫比本地操作系统慢得多，并认为微内核毫无希望，效率低下，或者不太可能快到具有竞争力。你知道，我们都应该使用单块内核。今天的论文基本上就像是对那个论点的回答。这是对那个论点的反驳。本文的重点是展示你可以构建这种架构，如果你足够注意优化性能，你可以直接运行Unix，就可以获得与本地操作系统竞争的性能。因此，您不能仅仅基于性能而忽略微内核。你可能出于其他原因不想要他们，但你不能以表现作为拒绝他们的理由。</p>
<p>发言人   01:19:02<br>Part of a huge part of the ingredients making that argument is that they made the IPC much faster with the techniques that I outlined a few minutes ago, and you can see this, I think, in a very simple benchmarking table too.<br>提出这个论点的很大一部分因素是，他们使用我几分钟前概述的技术使IPC变得更快，你也可以在一个非常简单的基准测试表中看到这一点。</p>
<p>发言人   01:19:17<br>If you have a copy of the paper with you. Table 2 as a measurements of just native Linux running in the ordinary way on hardware and on native Linux, they show that on their hardware and their version of Linux that a single simple system called get PID took 1.7 microseconds. They also show that the sort of equivalent thing in an L 4 setup where you have to send an IPC request and get an IPC response just for this get process ID system call that that took 4 microseconds under L 4 Linux, which is to say twice as long. There’s sort of twice as much work going on because you’re doing two sets of user kernel crossings instead of just a single simple system call. That is, they could claim that they have paired the expense of these IPC based system calls down to basically the minimum. That is twice the cost of a system called a native Linux. And therefore, they were doing roughly as good as you could possibly expect. Now, of course, their system goals are still half as fast as native Linux.<br>如果你有一份文件的副本。表2作为在硬件和本地Linux上以普通方式运行的本地Linux的测量值，他们表明在他们的硬件和Linux版本上，一个名为get PID的简单系统只需要1.7微秒。它们还显示了在L 4设置中类似的事情，你必须发送一个IPC请求并获得一个IPC响应，只是为了这个get process账号系统调用，在L 4 Linux下需要4微秒，也就是说需要两倍的时间。这意味着要进行两倍的工作，因为你正在进行两组用户内核交叉，而不仅仅是一个简单的系统调用。也就是说，他们可以声称他们已经将这些基于IPC的系统调用的费用降低到最低水平。这是本地Linux系统成本的两倍。因此，他们的表现大致与您可能期望的一样好。当然，现在他们的系统目标仍然是本地Linux的一半。</p>
<p>发言人   01:20:33<br>And, you know, it’s not clear unless you did some measurements, whether system calls taking twice or simple system calls taking twice as long as a disaster or not a problem. And in order to show that, you know, it might be a disaster if you do a lot of system calls, or it might be not a problem if you do relatively few system calls or there’s a lot of work per system call, because maybe your system calls are more complicated than get PID.<br>而且，你知道，除非你进行了一些测量，否则不清楚系统调用是否需要两次或简单的系统调用需要两倍时间才能成为灾难或不成问题。为了表明，你知道，如果你进行大量的系统调用，这可能是一场灾难，或者如果你进行相对较少的系统调用或每个系统调用有很多工作，这可能不是问题，因为您的系统调用可能比获取PID更复杂。</p>
<p>发言人   01:20:59<br>And their answer to that in the paper is the figure 8 benchmark using this benchmark called aim, which is just a more. It’s a benchmark that does all kinds of different system calls. It reads and writes files and creates processes, or does all the things with the kernel of it processes do. And they basically showed in Figure 8 that their setup running a much more full application that does much more than just get PID, runs only a few percent slower than native Linux, and that therefore, hopefully you could expect that whatever it is you wanted to run on a computer would run almost as fast under L 4 plus Linux as it does in a straight operating system under native operating system. Therefore, you know, they were basically to a first approximation is fast as just running straight Linux. Therefore, you should take them seriously, okay? So that was an impressive result by the way this is like somewhat unexpected and cool.<br>他们在论文中对这个问题的回答是图8的基准测试，使用这个称为目标的基准，这只是一个更多的基准。这是一个基准测试，可以进行各种不同的系统调用。它读取和写入文件并创建进程，或者执行内核进程所做的所有事情。他们基本上在图8中显示了他们的设置运行一个更完整的应用程序，它不仅仅是获得PID，运行速度只比本地Linux慢几个百分点，因此，希望你可以期望，无论你想在计算机上运行什么，在l4 plus Linux下运行的速度几乎和在本地操作系统下运行的一样快。因此，你知道，它们基本上是第一次近似，就像直接运行Linux一样快。因此，你应该认真对待它们，好吗？这是一个令人印象深刻的结果，顺便说一下，这有点出乎意料和很酷。</p>
<p>发言人   01:22:03<br>Just fast forwarding, 20 years where this ended up, as I mentioned before, people actually use L 4 in a bunch of embedded situations. Particularly it’s used a lot. There’s many instances of L 4 running in smartphones hidden from view, but nevertheless, you know, all running various kinds of custom software, not not running, you know, they don’t have to have compatibility with Unix in these situations, Micro kernels in other more general situations like workstations or servers never really caught on. And it’s not because there’s necessarily anything wrong with that design, it’s just they would have, in order to display some existing software, your new thing has to be, you know, like better, so people will be motivated to switch And these micro kernels were perfectly good, not certainly elegant, but it was hard to put for people to put their finger on why it was so much that they should go to the trouble of switching from Linux or whatever they were running to this. It never really caught on, not necessarily for good reasons, but because they weren’t like dramatically better on the other hand, many ideas from this architecture had a lasting impact. The people had to work out much more interesting and flexible ways of using virtual memory in order to support operating systems on their micro kernels. And those more sophisticated interfaces made their way through things like M map into mainstream operating systems like Linux.<br>只是快进，20年了，正如我之前提到的，人们实际上在许多嵌入式情况下使用L 4。特别是它经常被使用。有许多在智能手机中运行的L 4实例隐藏在视野之外，但是，你知道，所有这些实例都运行各种自定义软件，而不是不运行，你知道，在这些情况下，它们不必与Unix兼容。微内核在其他更普遍的情况下，如工作站或服务器，从未真正流行起来。这并不是因为这个设计一定有什么问题，只是为了展示一些现有的软件，你的新事物必须更好，这样人们就会有动力去切换，而这些微内核非常好。不一定优雅，但是很难让人们了解为什么这么多，他们应该去麻烦从Linux或他们正在运行的任何东西切换到这个。它从未真正流行起来，不一定是有充分的理由，但另一方面，因为它们并没有显著改善，这个架构中的许多想法产生了持久的影响。人们必须找出更有趣和灵活的使用虚拟内存的方法，以便在他们的微核上支持操作系统。那些更复杂的接口通过M map等东西进入Linux等主流操作系统。</p>
<p>发言人   01:23:39<br>This idea of running an operating system kind of on top as a server on top of a lower level operating system is extremely popular today in the form of virtual machine monitors, which is all over the place, sort of cloud hosting services, The desire for extensibility. You could modify a user level service, the way that played out in things like Linux was loadable kernel modules, which allow you to load, modify the way the Linux kernel works on the fly. The and of course, the a good support for this client server architecture also made its way into kernels like Mac OS, which has good IPC in good client server.<br>这种在底层操作系统上运行操作系统作为服务器的想法如今以虚拟机监视器的形式非常流行，虚拟机监视器遍布各地，是一种云托管服务，具有可扩展性的愿望。你可以修改用户级别的服务，就像Linux中的可加载内核模块一样，允许你动态加载、修改Linux内核的工作方式。当然，对这种客户端服务器架构的良好支持也进入了像Mac OS这样的内核，它在好的客户端服务器上具有良好的IPC。</p>
<p>发言人   01:24:25<br>And that’s all I have to say for this lecture, and I’m happy to stick around questions, thank you. Thank you? Oh, I wanted to ask. So the paper was talking about virtual, about page tables at I think 4.2.<br>这就是我本次讲座要说的全部内容，我很乐意坚持提问，谢谢。谢谢？哦，我想问一下。所以这篇论文谈论的是虚拟，关于页表，我认为是4.2。</p>
<p>发言人   01:24:51<br>And it was seeing how I think it was kind of what you mentioned before, where you said that there is the wrong way to do that, I think might kind of similar to that. But if you do this thing that you explain this in your picture now, would it be, I guess, how would the page tables work in this case? Well, are you may be referring to section 4.3, the dual space mistake. Oh yes, sorry, 4.3, I want no. Yeah, that’s a bit of a complicated story, but. Let’s see, part of the background is the way that Linux worked in those days, and indeed until recently, is that when you’re running at user level, the page table that’s active has both the processes pages, user level pages mapped in and all of the kernel mapped into that one page table on the x 80 86. Anyway, so when you made a system call and jumped into the kernel, the kernel was already mapped into the page table, and therefore no page table switch was required. So when you make a system call, it’s that much more expensive and much more cheaper because there was no page table switch, if you’re calling an XV six of the trampoline code switch is page table switch is an expensive thing to do because it flushes the TLB cache of virtual to physical mappings anyway.<br>它看到了我对你之前提到的事情的看法，你说这样做是错误的，我认为可能有点类似于那个。但是如果你现在做你在图片中解释的这件事，我猜页表在这种情况下会如何工作？好吧，你可能是指4.3节，即双重空间错误。哦，是的，对不起，4.3，我想不要。是的，这是一个有点复杂的故事，但是。让我们看看，部分背景是Linux当时的工作方式，事实上直到最近，当您在用户级别运行时，处于活动状态的页表具有两个进程页面，在x80 86上映射的用户级别页面以及所有内核都映射到该one page表中。无论如何，当您进行系统调用并跳入内核时，内核已经映射到页表中，因此不需要页表切换。所以当你进行系统调用时，它会变得越来越昂贵，因为没有页表切换，如果你正在调用xv6的蹦床代码切换是页表切换是一件昂贵的事情，因为它无论如何都会刷新虚拟到物理映射的TLB缓存。</p>
<p>发言人   01:26:23<br>So for efficiency Linux used to map kernel and user space in the same page table and had fast system calls as. A result, for reasons that aren’t very clear, decided to do this same thing to set up the mappings in the Unix server. Well, what they wanted was that when VI, when a process sent a system call over here, they wanted to have the page table that was active well in the Linux server while processing that system call, include all the virtual memory mappings for the process that sent the system call. And that at least would make it simpler to look up virtual addresses past as system call arguments like past or read.<br>因此，为了提高效率，Linux用于在同一页表中映射内核和用户空间，并具有快速的系统调用。结果，由于原因不是很清楚，决定做同样的事情在Unix服务器中设置映射。嗯，他们想要的是当VI，当一个进程向这里发送系统调用时，他们希望在处理该系统调用时，在Linux服务器中具有活动的页表，包括发送系统调用时的所有虚拟内存映射。这至少可以更简单地将虚拟地址过去作为系统调用参数 (如过去或读取) 进行查找。</p>
<p>发言人   01:27:15<br>The reason why this worked out poorly. There were a bunch of reasons. One is that L 4 which doesn’t know anything about any of this stuff. L 4 just knows there’s two processes. And so when you send an IPC from 1 process to another, L 4 just switches page tables. It always just switches page tables.<br>这个结果不好的原因。有很多原因。其中一个是L 4，它对这些东西一无所知。L 4只知道有两个过程。所以当你从一个进程向另一个进程发送IPC时，L 4只需切换页表。它总是只切换页表。</p>
<p>发言人   01:27:33<br>This guy at a page table, the VI at a page table. L 4 associates a page table with the Linux kernel, it just always switches page tables, so you couldn’t even due to L 4, due to the different ways system calls were implemented and the fact that L for was involved, there was no way to preserve the page table during a system call. That just wasn’t possible because all four always switched page tables when it’s switched from 1 process to another. So they were’t going to get the efficiency win of not having to switch page tables when sort of crossing from from user to kernel.<br>这个人在一个页表处，VI在一个页表处。L 4将一个页表与Linux内核相关联，它只是总是切换页表，所以由于L 4的原因，你甚至无法实现，由于系统调用的不同方式以及L for的参与，在系统调用期间无法保留页表。这是不可能的，因为四个进程在从一个进程切换到另一个进程时总是切换页表。因此，他们不会获得从用户到内核交叉时不必切换页表的效率优势。</p>
<p>发言人   01:28:10<br>But I think they wanted the convenience of being able to directly use users applied virtual addresses. But that meant that the mappings they needed to be active depended on which process they were executing a system call on behalf of. So there couldn’t be any one page table for Linux. The page table Linux server wanted to use depend on what process it sent the system called RPC, but L 4 did not know how to play that game.<br>但我认为他们想要的是能够直接使用用户应用的虚拟地址的便利性。但这意味着它们需要激活的映射取决于它们代表哪个进程执行系统调用。因此，对于Linux没有任何one page表。Linux服务器想要使用的页表取决于它向系统发送的名为RPC的进程，但是L 4不知道如何玩这个游戏。</p>
<p>发言人   01:28:39<br>I’ve associated a single page table with each process, with each task, and so in order, it would just switch to that page table. So tough lock Linux didn’t have any way to cause the page table to differ depending on who it sent the system call In order to deal with that, apparently they made a bunch of shared memory copies of the kernel, one for each process. And so each of these shared memory copies of the kernel had all of the kernel memory mapped into it. So they were all this was the same kernel data structures, but each process had a dedicated. Kernel tasks associated with it. And therefore, that basically allowed them to trick L 4 and to switch into the appropriate page table that included that process plus the kernel, you know, depending on which process sent the system called request.<br>我将一个页表与每个进程、每个任务相关联，因此按顺序，它只需切换到该页表。所以坚固的锁定Linux没有任何方法可以导致页表根据它发送系统调用的人而有所不同，以处理这个问题，显然他们制作了一堆内核的共享内存副本，每个进程一个。因此，内核的每个共享内存副本都将所有内核内存映射到其中。所以它们都是相同的内核数据结构，但每个进程都有一个专用的。与其关联的内核任务。因此，这基本上允许他们欺骗L 4，并切换到包含该进程和内核的适当页表，具体取决于哪个进程发送了称为request的系统。</p>
<p>发言人   01:29:34<br>And, you know, I think that kind of worked. But, or I don’t know if you what they said that worked, it was slow or something because there were a lot of tasks. Anyway, it’s like a complicated story, and I think it didn’t work out very well for them.<br>而且，你知道，我认为那种工作。但是，或者我不知道你是否知道他们说的有效，它很慢或其他什么，因为有很多任务。无论如何，这就像一个复杂的故事，我认为这对他们来说不是很好。</p>
<p>发言人   01:29:51<br>Okay, okay, I see, I think I think that explains, oh, well, why this thing is harder to do then what we do in XV 6. Yeah, because there’s not, yeah, this picture of XV 6 or even standard Linux is much simpler than this. You’re just jumping directly into the kernel, and the kernel has control over direct control over all the paging hardware, which it doesn’t have when it runs on all four, right? Okay, I see, thank you, thank you.<br>好的，好的，我明白了，我想这就解释了为什么这件事比我们在XV 6中所做的更难。是的，因为没有，是的，这张XV 6甚至标准Linux的图片比这简单得多。你只是直接跳入内核，内核可以直接控制所有分页硬件，而当它在所有四个上运行时，它没有控制，对吧？好的，我明白了，谢谢，谢谢。</p>
<p>发言人   01:30:21<br>We’re going to ask why it seems like some tasks are more appropriate to be put outside the kernel than others, but this else, like the approach with microcornea, is always seems to be either everything or nothing, or like either you have a monolithic kernel doing everything or nothing, just like I feel like paging and some other things could be very efficient inside the kernel and then maybe like file systems that things that need to be swappable could be outside and then even like you could maybe even have a kernel that has some functionality but you can opt to not use it and provide your own is there any everything you say is absolutely well taken and indeed there were there were a lot of micro kernel or micro kernel related projects and many of them built various kinds of hybrids like there are actually a couple different versions of Mach and some of them were sort of hybrid kernels in which yeah, there was this micro kernel that knew about IPC, but also in the kernel was a complete Unix, So for instance Mach 2.5 was this hybrid with, but micro kernel and Unix all sort of in the same kernel you could make system calls either and some stuff was built in the sort of microcos way, but some things they would just use it the kernel that was in mock that was built into the Mo kernel. The Unix kernel that was built into the mock kernel and modern Mac OS also is built in a way that like the way you describe Mac OS it has a complete operating system with a file system and everything inside it. But it also has good support for IPC and sort of mic threads or all the stuff you would want to build micro kernel style services.<br>我们会问为什么有些任务比其他任务更适合放在内核之外，但是其他的，就像微角膜的方法一样，似乎总是要么是一切，要么什么都不是，或者像你有一个单一的内核做一切或什么都不做。就像我觉得分页和其他一些东西在内核内可以非常有效，然后可能像文件系统一样，需要可交换的东西可以在外部，甚至你甚至可以有一个有一些功能的内核，但是你可以选择不使用它，并提供你自己的，如果有任何你说的东西都是绝对好的，确实有很多微内核或微内核相关的项目，其中许多项目建造了各种混合动力车。实际上有几个不同版本的Mach，其中一些是混合内核，其中有一个微内核知道IPC，但内核中也是一个完整的Unix，例如Mach 2.5是一个混合内核，但是微内核和Unix都在同一个内核中，你可以进行系统调用，有些东西是以微内核的方式构建的，但有些东西他们只会使用内置在莫内核。内置于模拟内核和现代Mac OS中的Unix内核也以一种类似于您描述Mac OS的方式构建，它具有完整的操作系统，其中包含文件系统和所有内容。但它也对IPC和一些微米线程或所有你想要构建微内核风格服务的东西有很好的支持。</p>
<p>发言人   01:32:13<br>I think Google’s Fuchsia I’m aware of also implement some of these ideas now as well. But yeah. So anyway, you know, there’s no one way that there were people who were sort of hoping that a pure, a very pure scheme could be made to work. Was not the only possible way forward? All right, thanks. Got around for my thanks lecture, but I’ll see you guys.<br>我认为谷歌的紫红色 (Fuchsia) 现在也实现了其中一些想法。但是，是的。所以无论如何，你知道，没有一种方式可以让人希望一个纯粹的，一个非常纯粹的计划能够起作用。这不是唯一可能的前进方向吗？好的，谢谢。我为我的感谢讲座四处走动，但我会见到你们。</p>
<p>发言人   01:32:52<br>Oh, I didn’t have, I have a remark. I think it’s fascinating that it’s ISS 5% slower, but it does so much more work. Yeah, was fascinating with that. You mean that even though it’s doing much more work, it’s only slightly slower? They really sweat blood over the IPC performance. And it’s another thing to remember, of course, is that if you start doing, if you’re doing significant amount of work per system, call, like, you know, looking at files and directories or something, then the cost of the system called the IPC itself starts to be less important. So the combination of faster system calls plus real programs do things other than making system calls.<br>哦，我没有，我有一句话。我认为它的速度慢了5%，但它做的工作要多得多，这很有趣。是的，这很吸引人。你的意思是，即使它做了更多的工作，它只是稍微慢了一点？他们真的为IPC的表现流汗。当然，要记住的另一件事是，如果你开始做，如果你在每个系统上做大量的工作，调用，你知道，查看文件和目录或其他东西，那么称为IPC的系统本身的成本开始不那么重要了。因此，更快的系统调用加上实际程序除了进行系统调用之外还可以做其他事情。</p>
<p>发言人   01:33:41<br>But you would also like switch page tables and the other. Yeah, although the paper I did not talk about it, but the paper had some clever tricks for avoiding the cost of switching page tape. I don’t know if you remember or some, like on page 6, we’re talking about supporting tag tobs or small spaces. They had some clever ideas for not which page tables, which I had not heard of before I read this paper. So this is pretty cool. Thank you so much, bye bye.<br>但您也想切换页表和其他。是的，虽然我没有谈论这篇论文，但这篇论文有一些聪明的技巧，可以避免切换页面磁带的成本。我不知道你是否还记得，像在第六页上，我们谈论的是支持标签或小空间。他们有一些聪明的想法可以不使用哪个页面表格，这是我在阅读本文之前没有听说过的。所以这很酷。非常感谢，再见。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>操作系统工程 017-OS Organization</div>
      <div>http://example.com/2025/10/18/6S081-017/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月18日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/18/6S081-018/" title="操作系统工程 018-Virtual Machines">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">操作系统工程 018-Virtual Machines</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/18/6S081-016/" title="操作系统工程 016-VirtualMemory for Applications">
                        <span class="hidden-mobile">操作系统工程 016-VirtualMemory for Applications</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
