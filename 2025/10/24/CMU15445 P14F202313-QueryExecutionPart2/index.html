

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="100:00:00,000 –&gt; 00:00:28,960Alright, so I have a question for Shubham. 200:00:28,960 –&gt; 00:00:30,960How do you get into Wu-Tang? 300:00:30,960 –&gt; 00:00:34,960To be honest, I was not that muc">
<meta property="og:type" content="article">
<meta property="og:title" content="CMU15445 P14F202313 QueryExecutionPart2">
<meta property="og:url" content="http://example.com/2025/10/24/CMU15445%20P14F202313-QueryExecutionPart2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="100:00:00,000 –&gt; 00:00:28,960Alright, so I have a question for Shubham. 200:00:28,960 –&gt; 00:00:30,960How do you get into Wu-Tang? 300:00:30,960 –&gt; 00:00:34,960To be honest, I was not that muc">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-24T12:00:44.467Z">
<meta property="article:modified_time" content="2025-10-24T12:06:28.540Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>CMU15445 P14F202313 QueryExecutionPart2 - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CMU15445 P14F202313 QueryExecutionPart2"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-24 20:00" pubdate>
          2025年10月24日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.9k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          33 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CMU15445 P14F202313 QueryExecutionPart2</h1>
            
            
              <div class="markdown-body">
                
                <p>1<br>00:00:00,000 –&gt; 00:00:28,960<br>Alright, so I have a question for Shubham.</p>
<p>2<br>00:00:28,960 –&gt; 00:00:30,960<br>How do you get into Wu-Tang?</p>
<p>3<br>00:00:30,960 –&gt; 00:00:34,960<br>To be honest, I was not that much into Wu-Tang before I met Andy.</p>
<p>4<br>00:00:34,960 –&gt; 00:00:36,960<br>I mean, I had an idea.</p>
<p>5<br>00:00:36,960 –&gt; 00:00:42,960<br>I used to listen to some build-up like old school stuff because my brother used to listen to it.</p>
<p>6<br>00:00:42,960 –&gt; 00:00:44,960<br>But I mean Andy is the one who’s got it.</p>
<p>7<br>00:00:44,960 –&gt; 00:00:46,960<br>Okay, okay. So he’s corrupted into that.</p>
<p>8<br>00:00:46,960 –&gt; 00:00:47,960<br>So awesome.</p>
<p>9<br>00:00:47,960 –&gt; 00:00:51,960<br>Yeah, add one to the list of long list of things that he’s done.</p>
<p>10<br>00:00:51,960 –&gt; 00:00:56,960<br>Alright, welcome everyone back from the fall break.</p>
<p>11<br>00:00:56,960 –&gt; 00:01:04,960<br>I know a lot of you have seen your exam scores on grade scope.</p>
<p>12<br>00:01:04,960 –&gt; 00:01:11,960<br>And let’s just jump into what the announcements are.</p>
<p>13<br>00:01:11,960 –&gt; 00:01:12,960<br>It requests.</p>
<p>14<br>00:01:12,960 –&gt; 00:01:18,960<br>So the usual protocol, but if you have questions, come talk to me and Andy during our office hours.</p>
<p>15<br>00:01:18,960 –&gt; 00:01:23,960<br>I know some of you already told me that you felt like the exam was a little tough.</p>
<p>16<br>00:01:23,959 –&gt; 00:01:29,959<br>And I gave a couple of tips to people who came into my office hours to as to how you might take such exams.</p>
<p>17<br>00:01:29,959 –&gt; 00:01:30,959<br>Right. So three things.</p>
<p>18<br>00:01:30,959 –&gt; 00:01:34,959<br>First is you don’t have to answer the question in the order in which they end the paper.</p>
<p>19<br>00:01:34,959 –&gt; 00:01:39,959<br>So for example, you say I glance to the topic storage management is something I’m really good at.</p>
<p>20<br>00:01:39,959 –&gt; 00:01:41,959<br>Go to that question first.</p>
<p>21<br>00:01:41,959 –&gt; 00:01:49,959<br>Second is as you read the question because they all have a little bit of a setup as to what’s the background that you need to know before you can answer the detailed question.</p>
<p>22<br>00:01:49,959 –&gt; 00:01:51,959<br>Don’t rush through it.</p>
<p>23<br>00:01:51,959 –&gt; 00:01:57,959<br>Try to skim through that question really quickly to get an understanding as to what someone what that question is about.</p>
<p>24<br>00:01:57,959 –&gt; 00:02:08,959<br>And as you write the as you assimilate that information, free free to use your pen or pencil and mark like for example, hey, these guys said, is a primary key that may be important.</p>
<p>25<br>00:02:08,959 –&gt; 00:02:10,959<br>So just note down what you have.</p>
<p>26<br>00:02:10,959 –&gt; 00:02:12,959<br>And 30 space yourself, right?</p>
<p>27<br>00:02:12,960 –&gt; 00:02:21,960<br>So if you get stuck on something, if you think that question number four is really difficult and you hit it right now, just if you think that’s really tough, go to the easy stuff.</p>
<p>28<br>00:02:21,960 –&gt; 00:02:22,960<br>Right. So you can make those changes.</p>
<p>29<br>00:02:22,960 –&gt; 00:02:26,960<br>So a little bit of what we are seeing is that there’s also some of that.</p>
<p>30<br>00:02:26,960 –&gt; 00:02:30,960<br>How do I take an exam and for modality that people are trying to get through.</p>
<p>31<br>00:02:30,960 –&gt; 00:02:32,960<br>So they’re going to be multiple components to this.</p>
<p>32<br>00:02:32,960 –&gt; 00:02:34,960<br>But hopefully that gives you.</p>
<p>33<br>00:02:34,960 –&gt; 00:02:36,960<br>Give you some idea. But come talk to me and Andy.</p>
<p>34<br>00:02:36,960 –&gt; 00:02:43,960<br>Alright, so today we are going to talk about query execution and picking up on what we discussed in the last class.</p>
<p>35<br>00:02:43,960 –&gt; 00:02:55,960<br>If you remember in the last class, we talked about how queries get converted into this internal representation of having operators that execute individual portions of that query.</p>
<p>36<br>00:02:55,960 –&gt; 00:03:04,960<br>And the data flows from these operators and each of these operators as we had discussed, you can think of that as being a producer consumer kind of a pipeline.</p>
<p>37<br>00:03:04,960 –&gt; 00:03:13,960<br>For example, here you have a selection and you have a projection and you can think of that as being of select project pipeline.</p>
<p>38<br>00:03:13,960 –&gt; 00:03:20,960<br>You’ll pick up on query execution. So as you can see over here, for example, this was an operator fee that you had looked at.</p>
<p>39<br>00:03:20,960 –&gt; 00:03:26,960<br>You can think of each pair of operators here as having a producer followed by a consumer right.</p>
<p>40<br>00:03:26,960 –&gt; 00:03:35,960<br>And the join is consuming input from the selection, but for it’s parent, the projection, it becomes a producer and the projection is the consumer right.</p>
<p>41<br>00:03:35,960 –&gt; 00:03:45,960<br>So operators like that are going to have this dual role. They’re going to consume from things that are below them and produce for the operators that are above them in this free representation.</p>
<p>42<br>00:03:45,960 –&gt; 00:03:52,960<br>Okay, so you’re going to go into the different ways in which you can deal with query execution.</p>
<p>43<br>00:03:52,960 –&gt; 00:03:58,960<br>And before we go into that, what are the different ways in which you can execute these queries.</p>
<p>44<br>00:03:58,960 –&gt; 00:04:13,960<br>So the first question is what else do we need to worry about? Well, yes, what do you need to worry about, which is processors these days, any computing platform these days today is going to have a lot of far from available at the heart rate.</p>
<p>45<br>00:04:13,960 –&gt; 00:04:21,960<br>So here’s a chart. It’s a little complicated, but the main part to focus on are these orange lines which stays from 1970 to this decade.</p>
<p>46<br>00:04:21,960 –&gt; 00:04:25,960<br>How much is in the word transistors that we be adding on processors.</p>
<p>47<br>00:04:25,960 –&gt; 00:04:38,960<br>And the y-axis is on exponential scale. So you can see that exponential growth continues where that’s basically at the heart of what most law in the not scaling is for those of you who can in pocket at that class.</p>
<p>48<br>00:04:38,959 –&gt; 00:04:44,959<br>And if you haven’t probably need to know is for the longest time, you should have processor transistor scaling happen, which is that orange line.</p>
<p>49<br>00:04:44,959 –&gt; 00:05:00,959<br>And the thing that has really changed is is the other two lines, which is single thread performance, which is how much can each thread of compute to as basically that blue line has started to flatten out, especially over the last decade.</p>
<p>50<br>00:05:00,959 –&gt; 00:05:11,959<br>Okay, so we will need a model to be able to deal with the hardware in which each single core is sort of become static in terms of what you can get out of it.</p>
<p>51<br>00:05:11,959 –&gt; 00:05:27,959<br>But what these extra transistor budgets allow you to do what the processor guys to do is this black line here, which still before this point middle of the early part of the century like 2005 or so, processors were largely single core.</p>
<p>52<br>00:05:27,959 –&gt; 00:05:34,959<br>And every generation of processor was doubling the clock frequency with stall because of our considerations.</p>
<p>53<br>00:05:34,959 –&gt; 00:05:42,959<br>And now what you start to see is a lot more cores, where we are now starting to get to tens of cores and maybe the trend will probably continue.</p>
<p>54<br>00:05:42,959 –&gt; 00:05:49,959<br>So today you can buy a processor that doesn’t have multiple cores, even your phone has multiple processing books.</p>
<p>55<br>00:05:49,959 –&gt; 00:06:03,959<br>So now we have to say how do we run all of this very processing machinery that we know in this hardware that by default now is going to be a parallel machine. So every processor is now a parallel data machine.</p>
<p>56<br>00:06:03,959 –&gt; 00:06:08,959<br>Okay, so we need to harness the full power of that hardware.</p>
<p>57<br>00:06:08,959 –&gt; 00:06:25,959<br>Things are also a little bit more complicated in the sense that we even within each processing core and within the context of each computing environment that we have, you have all kinds of different hardware parameters in there and not going to the details of this.</p>
<p>58<br>00:06:25,959 –&gt; 00:06:42,959<br>But just what you’ll be aware, this is a chart that is a reincarnation of various other charts that was started by Jeff Dean in 2017 and he was going around Google and looking at the new engineers and realizing they don’t really know how processors were and what are the trade offs includes a data access.</p>
<p>59<br>00:06:42,959 –&gt; 00:06:47,959<br>If you don’t know what the trade offs are, you won’t design the software correctly.</p>
<p>60<br>00:06:47,959 –&gt; 00:06:51,959<br>So let’s look at a couple elements of the chart.</p>
<p>61<br>00:06:51,959 –&gt; 00:07:00,959<br>Elbow cash is the small cash that sits right next to the processor, right? The processors and registers in which things get pulled in to compute.</p>
<p>62<br>00:07:00,959 –&gt; 00:07:07,959<br>But below that is the Elbow cash and accessing that is like one nano set, roughly a cycle or so.</p>
<p>63<br>00:07:07,959 –&gt; 00:07:17,959<br>And then as you get further down to run access, which is the D RAM, which is the above food sense, that’s going to be two hours of magnitude slower.</p>
<p>64<br>00:07:17,959 –&gt; 00:07:25,959<br>So if you have to fetch data from the buffer pool, it will get pulled into the cash is the value cash and the end one cash, which was its gets process.</p>
<p>65<br>00:07:25,959 –&gt; 00:07:34,959<br>And the difference over there is that far. So there’s need to have locality that you do in your great processing algorithms that you maintain across these cash.</p>
<p>66<br>00:07:34,959 –&gt; 00:07:40,959<br>And a lot of those algorithmic details we talk about in the advanced database class if you happen to think that in the spring semester.</p>
<p>67<br>00:07:40,959 –&gt; 00:07:46,959<br>But for today, what I need you to know is that memory is too much of magnitude away from the processor.</p>
<p>68<br>00:07:46,959 –&gt; 00:07:50,959<br>Okay, everything that you’ve done so far works of the buffer pool and that’s great.</p>
<p>69<br>00:07:50,959 –&gt; 00:07:58,959<br>But look at what happens to if you have to read data from an SSD and the buffer pool when you evict a page, it goes into this.</p>
<p>70<br>00:07:58,959 –&gt; 00:08:15,959<br>That this could be an SSD if you have high performance, high cost storage or it could be a disk which is at this level and between the RAM access, which is 100 nanoseconds and reading from an SSD that is 100 microseconds, you can see there’s three orders of magnitude difference.</p>
<p>71<br>00:08:15,959 –&gt; 00:08:25,959<br>So just three orders of having to slower to access something from memory, then it is to access it from SSD and four orders of magnitude that we have to go to a screen.</p>
<p>72<br>00:08:26,959 –&gt; 00:08:34,960<br>And so now we can start to see by this bubble pool is super important and by the way, so much time obsessing about bubble pool efficiency and things like that.</p>
<p>73<br>00:08:34,960 –&gt; 00:08:53,960<br>But as you go further down if your data processing hardware is not just a single process of which has multiple cores, but it is multiple machines as you talk about when you discuss today about talent and distributed systems, you may have to communicate across those different machines.</p>
<p>74<br>00:08:53,960 –&gt; 00:09:03,960<br>And sending data over the network, that’s about 10 microseconds right little bit faster than showing to the SSD one order of magnitude faster communicating from a node to node.</p>
<p>75<br>00:09:03,960 –&gt; 00:09:19,960<br>And now you have newer technologies like CSL, which allows a single node to access the data memory of the garden. So that’s going to go much faster than reading from a local disk even, but still going to be slower very likely that reading from the lab, directly from your local name.</p>
<p>76<br>00:09:19,960 –&gt; 00:09:22,960<br>Memory hierarchies are getting more complex.</p>
<p>77<br>00:09:22,960 –&gt; 00:09:34,960<br>Communicating to another machine is what’s going to be needed when we talk about talent and distributed database systems, distributed systems are going to communicate across servers that are much further about sometimes geographically spread apart.</p>
<p>78<br>00:09:34,960 –&gt; 00:09:38,960<br>And that’s what it’s going to cost you to go to different.</p>
<p>79<br>00:09:38,960 –&gt; 00:09:50,960<br>100 milliseconds and see the big difference between that and the ran access right one, two, three, four, five, six orders of quantity difference across that.</p>
<p>80<br>00:09:50,960 –&gt; 00:10:06,960<br>So now you can start to see why we need to understand this overall picture to really start to make high performance database systems, whether it’s on a single machine, we need to exploit all the course, but in parallel database machines, we need to be aware of these different costs.</p>
<p>81<br>00:10:06,960 –&gt; 00:10:09,960<br>Questions.</p>
<p>82<br>00:10:09,960 –&gt; 00:10:16,960<br>All right, so that’s why we care about parallel database machines, we need to exploit all of this hardware and do that well.</p>
<p>83<br>00:10:16,960 –&gt; 00:10:27,960<br>And if you do that expectation of that hardware, we are well, it also reduces the cost of ownership, because you may need fewer machines, for example, to serve that same workload.</p>
<p>84<br>00:10:27,960 –&gt; 00:10:38,960<br>And so there are multiple benefits for being efficient with the hardware you have, including fewer machines, smaller physical, and of course that comes with huge environmental benefit.</p>
<p>85<br>00:10:38,960 –&gt; 00:10:51,960<br>When we talk about improving the performance, we distinguish between latency improvement, how fast can we make a single query go as opposed to throughput improvements in which we say how fast can we make a batch of queries call.</p>
<p>86<br>00:10:51,960 –&gt; 00:11:04,960<br>And we’re seeking mechanisms that allow us to get both of those if you remember last class, we talked about the scheduler which broke everything into smaller units and allowed you to break up a query into even smaller chunks.</p>
<p>87<br>00:11:04,960 –&gt; 00:11:20,960<br>We’ll talk about some of the things that commercial systems do today, and then I would encourage you after this lecture to go back to that last part of last class’s discussion for the scheduler proposal that for the scheduler idea that we had mentioned from the quick set project, because that’s perhaps a more modern way to build a scheduler.</p>
<p>88<br>00:11:20,960 –&gt; 00:11:36,960<br>And there aren’t many systems who do that, quick step built it and hyper is another system that uses that type of scheduling, but the systems we’ll talk about today for scheduling and are going to be more traditional systems and I encourage you to go and compare and contrast that as after this class.</p>
<p>89<br>00:11:37,960 –&gt; 00:12:00,960<br>So we talk about these terms parallel and distributed data platforms and they both they’re different terms will distinguish what’s the set what’s the separation between those two in the next slide, but the commentalities across both of that is instead of having a database system work on a single server, a single node, you go you have a collection of these nodes.</p>
<p>90<br>00:12:00,960 –&gt; 00:12:08,960<br>And the database management system has to provide the illusion of a single node system to that end user to that end application.</p>
<p>91<br>00:12:08,960 –&gt; 00:12:18,960<br>So the end application is still going to send queries and the system now has to figure out how to break up that query and harvest the collective resources for the cross all of these machines.</p>
<p>92<br>00:12:19,960 –&gt; 00:12:43,960<br>So from the users perspective, it should feel like I’m just sitting a query I’m getting my answers faster lower latency, I have a whole batch of queries that I’ve sent all of them come back the queries are coming back a lot faster the queries per second the rate at which the system is retiring queries are producing answers to queries measured often as queries per second or queries per hour that’s a throughput measure is happening faster.</p>
<p>93<br>00:12:43,960 –&gt; 00:13:02,960<br>And then if you take the advanced database class, we talk about how there are certain properties about what that means of some sort of proportionality that these notion of linear scale up and linear speed up which have to do with terms like if I throw twice as hard work at the problem and expect my performance measures to get twice as fast.</p>
<p>94<br>00:13:02,960 –&gt; 00:13:12,960<br>And how do we get that linear linear behavior all of that we defer to the spring semester advanced database class today we just say how can we get some of this machinery off the ground.</p>
<p>95<br>00:13:14,960 –&gt; 00:13:31,960<br>So you’ll often hear the term parallel databases and distributed databases those names often come from technical jargon that evolved in the 80s and the 90s when these were getting separated today the distinction between that is a little arbitrary when people say some of these are not.</p>
<p>96<br>00:13:32,960 –&gt; 00:13:45,960<br>So I have a scalable cloud data platform like a snowflake or a data breaks or many of the Azure or AWS and Google data platforms they’re going to use a combination of both of these techniques.</p>
<p>97<br>00:13:46,960 –&gt; 00:14:01,960<br>So what’s the distinction the commonalities we talked about both of them want to work on a collection of machines and provide to the user the illusion that there’s a single machine that’s much faster than each individual machine in a parallel system typically those machines are more closely connected.</p>
<p>98<br>00:14:02,960 –&gt; 00:14:27,960<br>So the same data center they may be on the same rack so the communication between them is going to be if you go back to the previous slide here in a parallel database system you’re probably when you’re communicating across different servers or different notes it’s going to be in that 10 millisecond range and with C Excel it’s probably at the RAM layer you’re accessing the remote memory so it’s going to be even faster.</p>
<p>99<br>00:14:27,960 –&gt; 00:14:46,960<br>The distinction between the parallel and distributed system is that in a parallel system you have multiple machines but they’re close together they’re very fast network and you will assume that the network is reliable that means if I send a message or affect something from the from another node that that’s going to happen I don’t have to worry about oh did my message get lost.</p>
<p>100<br>00:14:47,960 –&gt; 00:14:56,960<br>And you know you’ll assume that the hardware kind of takes care of that in a distributed system the servers might be geographically distributed so you’re operating in this space you’re communicating to something else.</p>
<p>101<br>00:14:57,960 –&gt; 00:15:26,960<br>Modern cloud scenarios if you have snowflake installed or Databricks it’s only a cloud database like that you are essentially going to get a combination of both of it maybe that some of your nodes are local there might be georeplicated to some of the nodes and you might have a combination of that so quite processing tends to get even more sophisticated because that connectivity picture between the nodes is not just that I’m here or I’m at the network level it is a combination of that it’s asymmetric network communication.</p>
<p>102<br>00:15:26,960 –&gt; 00:15:34,960<br>So everything we talk about today gets even more complicated again that’s advanced database topic that gets covered in the spring semester graduate database class.</p>
<p>103<br>00:15:35,960 –&gt; 00:15:55,960<br>Okay so that’s the distinction between parallel and distributed database systems and that that whole trend is merging but the key part is in a distributed system you will assume that the communication costs are hired you do different algorithmic design differently and the messages could get lost as you’re going to build some sort of a reliable communication layer that if I’m sending to another node and saying hey.</p>
<p>104<br>00:15:55,960 –&gt; 00:16:10,960<br>Once you go process the selection query on the data fragment that you have you want to assume they got it you have to check with that you have to have layers about in your data layer in your database platform to be able to have that reliable communication and check with that.</p>
<p>105<br>00:16:10,960 –&gt; 00:16:39,960<br>Okay also things get more complicated as you talk about transactions which is coming in five wish lectures from now you have to commit the updates that are happening because in a parallel distributed system updates might have happened to data sitting on node here another in this other node they may be geographically spread how do you commit that transaction so that all those changes get committed and the state of the database is correct and you still have this illusion of a single large database that is being served by a collection of machines.</p>
<p>106<br>00:16:40,960 –&gt; 00:16:52,960<br>Okay so don’t worry about the distinction but you should be aware when someone says parallel is distributed they’re probably using these two terms in that sense.</p>
<p>107<br>00:16:52,960 –&gt; 00:17:08,960<br>So yeah so good example is a family database system is something that you would use for example if I’ve got you know hundreds of nodes sitting around in a single cluster I really get about high performance my data fits into all the nodes sitting in a rack.</p>
<p>108<br>00:17:08,960 –&gt; 00:17:33,960<br>A rack can take about 40-ish computers these days let’s say each computer each node in that rack might have a terabyte of memory might have tens of terabytes of storage and I can get to close to a half of that a by birth or a petabyte in fact there are models where people taking appliances like article has this appliance called exadata which is essentially a big rack they’ll roll that into a data center they just announce a partnership that Azure.</p>
<p>109<br>00:17:33,960 –&gt; 00:17:55,960<br>So you can get your Oracle parallel database system running in a rack in their specialized hardware plugged into the Azure network service by Azure for other things and distributed system would be where you end up a big for a distributed system that this wouldn’t survive is let’s say I’ve got my banking transaction data that master source for what’s in each and every account.</p>
<p>110<br>00:17:55,960 –&gt; 00:18:10,960<br>Changes are happening to that all the time you’re swiping cards you’re debiting money from it new deposits are coming into that I can keep that on a single node in a single rack in a single so let’s say that sitting in which word what did that data center catches on fire.</p>
<p>111<br>00:18:10,960 –&gt; 00:18:38,960<br>Orch bit sitting in California no one was data centers in California necessary use to expensive at times but there’s been in the middle of the desert like Arizona stuff like that or in ice to end you know that power is cheap usually data centers these days can be created by that but I can you can still have a single rack feel which is not a very low probability of that you have the entire center fail in the sense that network would get disconnected and even though probably even but not as evil probably.</p>
<p>112<br>00:18:38,960 –&gt; 00:18:45,960<br>And some of these things are not six in my life they can happen more often like a single node failure is a lot is very coffee.</p>
<p>113<br>00:18:45,960 –&gt; 00:19:04,960<br>And so I can need a copy of my data for my bank account which maybe it’s not a massive database of me not be affected by database it may be a terrible database but I want to keep three copies of it so I may keep one copy in Pittsburgh one in Arizona perhaps you know one in Minnesota and that way one feels like I can recover that.</p>
<p>114<br>00:19:04,960 –&gt; 00:19:20,960<br>When 911 happened a lot of the data was in those towers but most of nearly all of it was georeplicated and you could recover all of that information from the path of said like no one noticed anything different with the bank accounts that because replication was.</p>
<p>115<br>00:19:20,960 –&gt; 00:19:34,960<br>And then geographically spread across obviously it will be distributed because it has to be physically spread across for you to have this property of single site completely failing for some due some some environmental factors that coming to.</p>
<p>116<br>00:19:34,960 –&gt; 00:19:35,960<br>Okay.</p>
<p>117<br>00:19:35,960 –&gt; 00:19:49,960<br>And of course if you have a copy of the data you have queries running you probably want to use it sometimes you have copies of data you have this distributed stuff happening even for analytic work room so the whole world is changing paddle distributed hybrid systems like that is essentially what happened.</p>
<p>118<br>00:19:49,960 –&gt; 00:19:52,960<br>Is essentially what happens all the time.</p>
<p>119<br>00:19:52,960 –&gt; 00:19:56,960<br>Great question other questions.</p>
<p>120<br>00:19:56,960 –&gt; 00:20:18,960<br>Okay so we’re going to go into the process model the execution level parallelism and IOPadalism so the process model is a term that we refer to to talk about how is the system going to deal with this simple free representation that we talked about in the past class with the producer consumer stuff but now you.</p>
<p>121<br>00:20:18,960 –&gt; 00:20:47,960<br>Multiple levels of multiple foundation that’s available in the hardware and you’re going to call this thing of work you can think of worker as a unit of work in which some of the process same happens and then we talk about process model I say how is that worker allocated is that worker allocated to do we allocate it to a process is one worker map to a process every time a new worker to.</p>
<p>122<br>00:20:47,960 –&gt; 00:20:55,960<br>To start a new process like far from a new processing my code or is that worker thread.</p>
<p>123<br>00:20:55,960 –&gt; 00:21:16,960<br>Do you have a you had just started a new feature or do you have a collection of features and I just find one of the presets available and give it work is that a worker or an embedded system in which i’m running my code like in a Python notebook i’ve got some processing happening isn’t data be system running in that same context that’s called an embedded data.</p>
<p>124<br>00:21:16,960 –&gt; 00:21:19,960<br>Okay, and you talk about all these three different models right.</p>
<p>125<br>00:21:19,960 –&gt; 00:21:33,960<br>So let’s start with the process for worker this is the simplest model and those databases are not working the panel in this unit database system started in the 80s a lot of it was before friends were popular.</p>
<p>126<br>00:21:33,960 –&gt; 00:22:02,960<br>So in that form feature as a fact it’s been quite exist and so at that time the way in which you would start a query and you would start the processing as we talked about briefly i’m going to go back to slide number two here remember in the first query execution perhaps when we talked about we talked about how you can start this operator tree bottom up or top down and so the process model is like I start the process there that makes it call.</p>
<p>127<br>00:22:02,960 –&gt; 00:22:12,960<br>To another process maybe through some rdc mechanism or something like that and then that calls another process in the whole process free setup.</p>
<p>128<br>00:22:12,960 –&gt; 00:22:22,960<br>So that’s what the process model is it’s going to be a single process for each of those workers and in this case i’m just assuming a single worker per node.</p>
<p>129<br>00:22:22,960 –&gt; 00:22:34,960<br>So we’re going to start out into an intra query operation parallelism in which we might have multiple workers for each of those operations but the basic idea is that I get a query.</p>
<p>130<br>00:22:34,960 –&gt; 00:22:45,960<br>From an application that connects to the database system there’s a dispatcher i’m not calling it a scheduler dispatcher which is much simpler it’s going to call the worker process.</p>
<p>131<br>00:22:45,960 –&gt; 00:22:58,960<br>If one needs to be started up or if there’s a pool of process just waiting around for work to be given to them pull from that pool and then the work gets done work for that operator like a selection a projection a join an aggregation.</p>
<p>132<br>00:22:58,960 –&gt; 00:23:12,960<br>Okay so really simple add all the traditional database systems that have been around for a while including db2 oracle post press because they all started before p threads was popular will have this worker model.</p>
<p>133<br>00:23:12,960 –&gt; 00:23:28,960<br>What do modern data platforms do and you can see the longer list of systems that are over here including these traditional systems and pretty much every that slide over there the thread model we can fill up pretty much all the system that are out here today that are more modern.</p>
<p>134<br>00:23:28,960 –&gt; 00:23:49,960<br>The idea now is that instead of having a process which is a much more heavyweight abstraction for doing work use threads and threads are much cheaper compared to a process like spinning up a process and getting on the process is very more expensive than starting a new thread and.</p>
<p>135<br>00:23:49,960 –&gt; 00:24:04,960<br>So here the idea would be I have this operating systems gives me this process model which is heavyweight but within that process I can use p threads which is now pretty standard and spin up as many threads I want.</p>
<p>136<br>00:24:04,960 –&gt; 00:24:18,960<br>I can even control how many threads I want so if I’m working on a 40 core processor I might spin a 40 threads if I want to use something like hyper threading which allows multiple threads to work on a single core at the same time I might spin up 80 threads on a 40 core blocks.</p>
<p>137<br>00:24:19,960 –&gt; 00:24:33,960<br>Okay so now I just use the worker the process mechanism that the operating system gives me as a container and my parallelism that I’m orchestrating is inside the process by using threading.</p>
<p>138<br>00:24:33,960 –&gt; 00:24:52,960<br>Okay so pretty straightforward and it’s simpler from the perspective is lighter weight than the process model one downside to that is that if a thread crashes it could take the entire process down if I’ve got 40 threads running 100 crashes as a sit kill or a simple the entire process.</p>
<p>139<br>00:24:52,960 –&gt; 00:24:57,960<br>Unless you do special stuff to catch it and get you yet but you have to write that code.</p>
<p>140<br>00:24:58,960 –&gt; 00:25:02,960<br>Okay questions so far on these two models.</p>
<p>141<br>00:25:02,960 –&gt; 00:25:16,960<br>Okay and basically as the slide says you know every database has been created since p threads that became popular since about 20 years now uses a thread model because it just has way more advantage compared to the process model.</p>
<p>142<br>00:25:17,960 –&gt; 00:25:29,960<br>All right so and the idea is a similar application comes in now that unit of work is at the thread level and not at the process level.</p>
<p>143<br>00:25:29,960 –&gt; 00:25:45,960<br>Now some systems like SQL server which has been around Microsoft SQL server has been around for a little while they have to go through this pain point going from the process model to the worker model and doing different things and all of these systems are equal and SQL server and DB2 they now have.</p>
<p>144<br>00:25:46,960 –&gt; 00:25:51,960<br>This balance between the thread model and the worker model but what I wanted to zoom down into.</p>
<p>145<br>00:25:51,960 –&gt; 00:26:11,960<br>SQL server and talk about what they did at the dispatcher level and as you remember in the last class we talked about scheduling and and that unit based scheduling this is simpler than that but it’s practically what Microsoft did right so the idea is for each 20 plan.</p>
<p>146<br>00:26:11,960 –&gt; 00:26:30,960<br>The database system has to decide how to execute and you have to decide how many tasks in how many workers are going to get involved and you’re going to make that decision based on how many CPU cores you have and you might also decide which core should execute which task right and there’s sometimes advantages to picking the core as opposed to saying hey.</p>
<p>147<br>00:26:30,960 –&gt; 00:26:52,960<br>The thread manager or the operating system decide where this needs to be allocated and the big reason is the database system often knows a lot more than the operating system about the context so for example if I got a selection operator that had just produced the results and it was running on four four and now I need to schedule a subsequent aggregate operation.</p>
<p>148<br>00:26:52,960 –&gt; 00:27:10,960<br>I probably and court four is free I want to schedule that next worker on court or because very likely the data sits in the cash is like the L2 cash is for example right as you saw from the slide and two cash is an order of 92 cheaper to access then.</p>
<p>149<br>00:27:10,960 –&gt; 00:27:31,960<br>And often in most modern systems as an L3 cash and something that looks like a gigantic and four cash some of the newer internal and these systems have like a gate of S ran level four like cash sitting on the processor so the gate of that which is an amazing about so localities important and you want to take that into account yep.</p>
<p>150<br>00:27:31,960 –&gt; 00:27:59,960<br>Yeah some of them are L1 L2 typically are individual to each core and some of the bigger ones like the L3 or what looks like an L4 often shared cash is yep but even if you but you know often you also multiple processors right you buy a server today typically will have four processors and so you want to at least make sure it goes to the right process even if you’re using a low level cash question.</p>
<p>151<br>00:27:59,960 –&gt; 00:28:28,960<br>So the question is why don’t we just build a database system without the operating system and that’s been a perennial debate in the community including Mike Stronberger wrote a paper about 40 years ago say how database operating system just gets in the way but the practical reality is that you have a server.</p>
<p>152<br>00:28:28,960 –&gt; 00:28:42,960<br>It has to do a lot of low level device management stuff people write device drivers and tested at the operating system level so you want your networking card to work you want to storage device to work that service going to run a lot of applications databases maybe just one of them.</p>
<p>153<br>00:28:42,960 –&gt; 00:28:55,960<br>So you still need a layer that does the stuff that operating system does now the question the part of what the operating system does like a file crash that gets in the way for database systems because we like to do a buff a cool.</p>
<p>154<br>00:28:55,960 –&gt; 00:29:13,960<br>So often what database systems will do they will use some mechanism like direct map of file where the data is sitting in the file system for the operating from the operating systems perspective and tell us don’t cash any of this stuff I got a cash in there so the operating system does get in the way of things like.</p>
<p>155<br>00:29:13,960 –&gt; 00:29:42,960<br>Caching which the buffer pool does a much better job than the file system does it does get in the way of locking because the operating system will do its own types of stock you know locking at the five level is what you see is most Linux system for the one final grade locking as you talk about at the transaction that I’ve been talking about transactions so that gets in the way some of the mechanisms related to scheduling sometimes get in the way to operating systems have been getting much better by allowing the dispatcher to get.</p>
<p>156<br>00:29:42,960 –&gt; 00:29:44,960<br>Dispatcher to give this about.</p>
<p>157<br>00:29:44,960 –&gt; 00:29:55,960<br>But certainly there are portion of the operating system need me and most databases to state the stuff that they know don’t want the operating system but the operating system gets in the way and build it that starts a couple points of the exact.</p>
<p>158<br>00:29:55,960 –&gt; 00:30:09,960<br>But that’s an ongoing debate as to what should that boundary yeah if it’s the case the model like take the viewing what we’re doing the operating system yeah different you could see especially since like apparently it’s faster like yeah.</p>
<p>159<br>00:30:09,960 –&gt; 00:30:30,960<br>Yeah I would rephrase your question as saying why are operating systems so monolithic why can’t they be much simpler and this place has an tremendous history in researching that with the macOS and micro colonels and stuff like that awesome ideas and more and more absolutely means to happen in the operating system coming to be have what we have right now some of those ideas are there and be smell for but not in that.</p>
<p>160<br>00:30:30,960 –&gt; 00:30:36,960<br>Ultimate vision of operating systems in very modular and lightweight so you can just pick the faster.</p>
<p>161<br>00:30:36,960 –&gt; 00:30:48,960<br>Yeah I still an ongoing research topic and I encourage you to take the operating systems class if you’re interested in that they spend a fair amount of time talking about that stuff great question other questions.</p>
<p>162<br>00:30:48,960 –&gt; 00:30:53,960<br>Cool alright so yep you had a question.</p>
<p>163<br>00:30:53,960 –&gt; 00:31:14,960<br>Yeah we’ll get to that so we’ll talk talk about that we have a ton to cover so hold that question if I don’t answer it in 15 seconds stop me again so I’m going to go through the sequel the West part that Microsoft did very fast just to give you an idea on to say time for the core topic but what they did is essentially this type of an idea you remember Microsoft.</p>
<p>164<br>00:31:14,960 –&gt; 00:31:43,960<br>They’re very recently was only selling windows server everything work on windows windows and team the windows server stuff like that and so they had in mid 90s decided that they needed to build the database system because that was one of the most important applications for enterprises and so they started to build SQL server and the research certain point where they had to go make changes to SQL server and they added the SQL OS layer.</p>
<p>165<br>00:31:43,960 –&gt; 00:32:02,960<br>We’re learning some of the discussion we just had over here is like what’s the boundary and roles and responsibilities that we should allocate between the operating system and the database engine and they had the benefit of having gone through that debate a bunch of times so since they started in the mid 90s.</p>
<p>166<br>00:32:02,960 –&gt; 00:32:32,960<br>They had learned a bunch of lessons from before and so what they did is they had built something called SQL OS and that one they didn’t build in the first version of SQL server that happened much later and that was essentially an abstraction between the database engine and what it needs of the operating system and what the operating system that implements internally for that operating system and one of the big advantages of that was you know so these this management included things like Ios I’m going to send an I or request but don’t know what I’m going to do is I’m going to send an I or request but don’t know what I’m going to do is I’m going to send an I or request but don’t know what I’m going to do is I’m going to send an I or request but don’t know what I’m going to do is I’m going to send an I or request but don’t know what I’m going to do is I’m going to send an I or request but don’t know what I’m going to do is I’m going to do is I’m going to</p>
<p>167<br>00:32:32,960 –&gt; 00:32:58,960<br>don’t directly call the windows driver or the windows layer directly have an abstraction and have it call it and the big advantage of stuff like that was that when they had to move to Linux because we must in the cloud that’s what the servers run they lost me one Linux and Microsoft haven’t even before that with the subtraction there to address exactly the types of problems we just discussed.</p>
<p>168<br>00:33:02,960 –&gt; 00:33:29,960<br>That that moving over to Linux that massive piece of code was relatively easy to still a ton of work that was relatively easy essentially the key take a ways that it is an extremely good idea to have in your database layer of well defined interface to the operating system so that you can make that work in the ways that you want, especially as a boundary between what the operating system does and the database system is continually getting reimagined.</p>
<p>169<br>00:33:29,960 –&gt; 00:33:47,960<br>I’m not sure if you can understand what the SQL essence is a formal kernel by points with more so to be there in a base generic interface so it’s like I won’t call the I.O. Scheduler directly at the operating system level I’m going to call an internal schedule that does its own behavior for example it says I’m going to put all my requests into the I.O.</p>
<p>170<br>00:33:47,960 –&gt; 00:33:56,960<br>And before I actually call the operating system I’m going to see if there are eight features that are continuous in the physical this address space and make my I.O.</p>
<p>171<br>00:33:56,960 –&gt; 00:34:05,960<br>So now we can do all that type of algorithms in that layer as opposed to saying what the operating system does the right thing for.</p>
<p>172<br>00:34:05,960 –&gt; 00:34:20,960<br>Prefetching is another example scheduling affinities scheduling stuff like that are other examples of that but anyways the details are not as important as say yes that layer matters and it should be something we should consider.</p>
<p>173<br>00:34:20,960 –&gt; 00:34:32,960<br>I’ll go really quickly through that there’s one interesting design they made which I’m not sure makes a ton of sense now but you know they didn’t have the hindsight there is they still needed to figure out they had the process model.</p>
<p>174<br>00:34:32,960 –&gt; 00:35:01,960<br>I want to figure out how to make this process model work in the world ad-art faction you remember the discussion from last lecture where people this world order based on when he’s single worker was occupied with giving the value of the entire selection offer for a single operation the whole time which is operating on a block at the time of the day giving up themselves and saying tell me what to do next so they won’t like say I’ve got to find a file I’ll do the selection I’m starting now check back with me in an hour or two of the time right.</p>
<p>175<br>00:35:01,960 –&gt; 00:35:30,960<br>We’re not blocking the whole system blocking themselves from doing something else that was very short right that was what we discussed in the last class here what it is was something different because they didn’t have that luxury they actually went into the code for example if you’re doing a simple selection this is what sort of the code would look like right a for loop it reading over each record applying the evaluation predicate and emitting the record if it needed to so they actually went to each of the operators and put stuff in like this which is a lot of things that I’m going to do.</p>
<p>176<br>00:35:30,960 –&gt; 00:35:59,960<br>This which is to say every once in a while check how much time you’ve spent in that inner loop and yield explicitly so that the control can be given up obviously when they did this you know they didn’t have the advanced engineering mechanism that we talked about in the last class but that was a way they could not have a single worker block and you had this more agile way of building this system now I would suggest that what we talked about in the last class is the better way to go do this now but at least they got this thing to work.</p>
<p>177<br>00:36:00,960 –&gt; 00:36:30,940<br>In a way that makes sense for them okay so we talked about the process model you talked about the thread model the last model is an embedded database model and this is literally you can imagine the database is written as a library and you can imagine just calling that library to your code and essentially the database engine runs inside your application code and you think of just making calls to the database saying hey create a</p>
<p>178<br>00:36:30,960 –&gt; 00:36:51,960<br>table yes the schema is our records to a selection and the stuff. The database code is a library on embedded databases in the database lingo and effectively runs in the same worker space at the application so in a threat that the application is running.</p>
<p>179<br>00:36:51,960 –&gt; 00:37:19,960<br>And there are lots of examples of that the most famous example of that is SQLite but there was a database system called Berkeley DB that started about 5 10 years ago for SQLite at the toward the end of last century but SQLite is used in a large number of applications you probably have dozens of copies of SQLite compiled into applications on your phone most apps will have compiled that into the application right so it’s the most deployed database engine there are estimated.</p>
<p>180<br>00:37:19,960 –&gt; 00:37:45,960<br>10 billion copies of SQLite running across the plan because everyone’s carrying phones and many of them in a multiple copies of SQLite most apps when they need a database layer they embed it they just link into SQL library it’s pretty interesting if you go to SQLite they have a version of distribution which you get one single C file with the entire database system it’s very cool and you just compile that into your code and now you run that’s your database system.</p>
<p>181<br>00:37:45,960 –&gt; 00:37:57,960<br>Okay we could spend a whole lecture talking about SQLite and how it interacts with the OS and we’ve actually worked on SQLite and some of the code from the optimization stuff that we’ve developed in my research group ships and SQLite so all of you guys are running our code.</p>
<p>182<br>00:37:57,960 –&gt; 00:37:58,960<br>Okay question.</p>
<p>183<br>00:37:58,960 –&gt; 00:38:02,960<br>There is one I say for why isn’t dynamically linked more commonly.</p>
<p>184<br>00:38:02,960 –&gt; 00:38:16,960<br>I don’t think that is impossible I think you some applications might even be doing that I don’t know why you would not be able to quite do that this is notion of a global offer locking mechanism in there but that should still work with with dynamic.</p>
<p>185<br>00:38:17,960 –&gt; 00:38:26,960<br>Well no they have a buffer pool that they maintain and so I have to go and look at that and see if the buffer pool is allocated together it becomes a shared buffer pool which you may not want.</p>
<p>186<br>00:38:26,960 –&gt; 00:38:34,960<br>I think people compile it in because a lot of the assumptions probably don’t work if you start sharing the buffer pool across applications.</p>
<p>187<br>00:38:34,960 –&gt; 00:38:41,960<br>Okay cool okay all right so.</p>
<p>188<br>00:38:42,960 –&gt; 00:38:50,960<br>Just a summary of the process model we have these three different process models if you’re in the embedded space you’re really looking for a lightweight engine that makes sense.</p>
<p>189<br>00:38:50,960 –&gt; 00:39:02,960<br>The process model the workers with sitting a single process is essentially something that existed in the past but really most modern database systems use threads which is the better model to use.</p>
<p>190<br>00:39:02,960 –&gt; 00:39:29,960<br>What is not on this slide is that if you look at cloud databases what they often do is they’ll have a different notion for unit of note they effectively have essentially these containers like in where you are locating some compute container that might be four or eight codes that might be virtualized on top of a given hardware and you impose that on top of that.</p>
<p>191<br>00:39:29,960 –&gt; 00:39:37,960<br>The bottom line is that in the cloud environment is the most final range when in reality as to where are those resource coming from and people are going to allocate containers.</p>
<p>192<br>00:39:37,960 –&gt; 00:39:53,960<br>This containers might be units of two or four cores from a physical machine underlying that might have 64 cores of 100 cores that means sliced up virtually and in that you can put your process model thread model so there’s one more level of abstraction that you often see when you’re deploying in the cloud.</p>
<p>193<br>00:39:53,960 –&gt; 00:39:59,960<br>Okay, there’s just for you to know you still have to figure out within that container what brought what model are you going to use.</p>
<p>194<br>00:39:59,960 –&gt; 00:40:04,960<br>So the sign is that this is not by the giving us important to carry.</p>
<p>195<br>00:40:04,960 –&gt; 00:40:16,960<br>Yes, so we talk about intra quenny parallelism next which is just because someone is using threads doesn’t mean they have intra operator parallelism or inter operator parallelism which is exactly what we are going to talk about next.</p>
<p>196<br>00:40:16,960 –&gt; 00:40:27,960<br>So how far is that just wait for okay, so so far we talked about other questions related to the process model.</p>
<p>197<br>00:40:27,960 –&gt; 00:40:33,960<br>Okay, so so far we’ve said I need to do some work like on a selection operation.</p>
<p>198<br>00:40:33,960 –&gt; 00:40:44,960<br>Where do I make that allocation do I spin up a process do I use a thread or is it embedded to throw the embedded on the site for the thread in the process model.</p>
<p>199<br>00:40:44,960 –&gt; 00:40:55,960<br>I still have to decide oh, I have the selection operation should just one worker you got selection also that have 10 workers spin up and work on that selection.</p>
<p>200<br>00:40:55,960 –&gt; 00:41:05,960<br>You know, seeing a five with a million pages maybe a spin up 10 workers at each one of them work on the fence of the day that maybe interrupt operator path.</p>
<p>201<br>00:41:05,960 –&gt; 00:41:10,960<br>Okay, but I will get then millions of work for that selection operation.</p>
<p>202<br>00:41:10,960 –&gt; 00:41:16,960<br>I still have to decide whether that worker each worker now is a trigger process which we already discussed.</p>
<p>203<br>00:41:16,960 –&gt; 00:41:27,960<br>So now the discussion is what we’re going to teach operator and throw more of the workers many workers at intra operator paleness and inter query.</p>
<p>204<br>00:41:27,960 –&gt; 00:41:38,960<br>So how do I execute the operations so inter quit sorry for getting about intra operator right now we talk about intra query which will put the break it down into intra operator in a second.</p>
<p>205<br>00:41:38,960 –&gt; 00:41:50,960<br>So intra query is for a given query do I use multiple workers to do its work and inter query is if I’ve got multiple queries that are sent to my system.</p>
<p>206<br>00:41:50,960 –&gt; 00:42:04,960<br>Do I use multiple workers to go with that the simplest model would be I have one worker be the process or thread every if I’ve got 10 queries come into the system I’ll take the first query running on that worker till it’s done and take the second one.</p>
<p>207<br>00:42:04,960 –&gt; 00:42:08,960<br>Now you can go in two ways, once you say I’ve got 10 workers.</p>
<p>208<br>00:42:08,960 –&gt; 00:42:15,960<br>And they could either be speeding up a single query at a time or they could be speeding up.</p>
<p>209<br>00:42:15,960 –&gt; 00:42:25,960<br>So a collection of these ways together and of course there are things in between as we will see so let’s start with intra query far nism.</p>
<p>210<br>00:42:25,960 –&gt; 00:42:42,960<br>Basically we have multiple queries that are presented to the system and the are going to figure out how to read them to run and pass it requires a read only then this is an embarrassingly parallel task right if I’ve got 10 queries that are presented to the system or what 10 workers.</p>
<p>211<br>00:42:42,960 –&gt; 00:42:58,960<br>And each worker takes a query take one of the queries and start working on and that’s all fine that can go really well if some of those query shared the same set of data pages like all the queries start by reading the same table then one of them might bring into the bubble pool then everyone else.</p>
<p>212<br>00:42:58,960 –&gt; 00:43:07,960<br>And the bubble pool goes under this LRU to LRU key and I policy and that’s what you build it so that it’s sufficient to respect your sketch.</p>
<p>213<br>00:43:07,960 –&gt; 00:43:18,960<br>So might this right as long as everything is real only if there are multiple queries running simultaneously one is trying to read all the bank accounts and try to figure out what’s the total deposit for the bank.</p>
<p>214<br>00:43:18,960 –&gt; 00:43:34,960<br>The other one is trying to add interest to certain accounts they’re interfering with each other one is reading the other one is writing potential to that same data and there you have to start to worry about how to do that correctly and that’s we will cover all of the topic when we talk about transactions.</p>
<p>215<br>00:43:34,960 –&gt; 00:43:36,960<br>Okay.</p>
<p>216<br>00:43:36,960 –&gt; 00:43:38,960<br>So.</p>
<p>217<br>00:43:38,960 –&gt; 00:43:46,960<br>The buffer pool stuff that we talked about you and we’ll get into some of the more details about that in lecture 15.</p>
<p>218<br>00:43:46,960 –&gt; 00:43:50,960<br>Now, intro query pattern is.</p>
<p>219<br>00:43:50,960 –&gt; 00:43:57,960<br>I want to make it go faster by using more than one.</p>
<p>220<br>00:43:57,960 –&gt; 00:43:59,960<br>Okay, so how do I go do that?</p>
<p>221<br>00:43:59,960 –&gt; 00:44:21,960<br>So if you remember an operator free we’ve cast it as a producer consumer paradigm between each pairs of operators and we can now take each of the operators and to give it more than one worker so we’ll look at that next which is called intro operator patternism and the other part we can do is we can do multiple operators in parallel.</p>
<p>222<br>00:44:21,960 –&gt; 00:44:26,960<br>So before we go into the details of that let’s just take a simple example.</p>
<p>223<br>00:44:26,960 –&gt; 00:44:40,960<br>You can ask hash store in a specific flavor of hash store and if you remember in that what we did is we said you’re going to take the two tables that are being joined or in S.</p>
<p>224<br>00:44:40,960 –&gt; 00:44:51,960<br>We’re going to partition them to create partitions of R, HD, zero, one, two, up to some number of partitions.</p>
<p>225<br>00:44:51,960 –&gt; 00:44:59,960<br>We’re going to apply that same hash function h1 to the S slide get corresponding partitions in the second phase we’re going to join the partition pairs.</p>
<p>226<br>00:44:59,960 –&gt; 00:45:06,960<br>So we’ve been the hash table or partition zero or and probe it with S and basically we can get the whole joint using that divide and conquer that.</p>
<p>227<br>00:45:06,960 –&gt; 00:45:09,960<br>You know that there’s no like key lectures.</p>
<p>228<br>00:45:09,960 –&gt; 00:45:21,960<br>Right. So now if you want to take an operator like that there’s parallelism that you could exploit in that and you could basically say hey after I’ve done the partitioning step.</p>
<p>229<br>00:45:21,960 –&gt; 00:45:35,960<br>The first partition could be joined by worker one second partition could be joined by worker two and I can keep doing that in parallel right so we’ll take that and go further into that and break it down into these different level of parallelism.</p>
<p>230<br>00:45:35,960 –&gt; 00:45:43,960<br>Okay now you’re focused on this intra query parallelism a single query more than one worker is available how do you make that query.</p>
<p>231<br>00:45:43,960 –&gt; 00:45:52,960<br>You can think of it as I want to primarily reduce the latency of this way.</p>
<p>232<br>00:45:52,960 –&gt; 00:46:04,960<br>So we talk about intra operator parallelism which is using multiple operators to for example do that hash join better and you can think of it as a horizontal way to speed things up.</p>
<p>233<br>00:46:04,960 –&gt; 00:46:18,960<br>The other one we’ll talk about is vertical parallelism which is to look at the operators in a tree and do multiple operator simultaneously that’s orthogonal to it that will be the vertical way and the textbook talks about something called the bushy way which is like you can do a hybrid of both.</p>
<p>234<br>00:46:18,960 –&gt; 00:46:35,960<br>Really the first two ones are what’s important and if you look at the scheduler stuff we talked about in the last class it naturally will do both of those but we’ll very briefly talked about bushy just to keep it consistent with what the textbook says but you really need to know about intra and inter operator parallelism.</p>
<p>235<br>00:46:35,960 –&gt; 00:47:03,960<br>Okay all right so let’s start with the intra operator parallelism and the way to do this is to these in the hatching case as you saw we broke the table up into multiple pieces that divide these then allow it to conquer and do the individual foundation pairs and that hunger piece was the one in which we go exploit the balance.</p>
<p>236<br>00:47:03,960 –&gt; 00:47:22,960<br>Control multiple workers they were independent pieces of work and they could be done if that will without the future and so trying to take the structure whether divide in some of the work and then after that each of the pieces of work can be done independently at its own pace after all of that work is done we can get the final result.</p>
<p>237<br>00:47:22,960 –&gt; 00:47:32,960<br>Okay now how do you introduce that in a systematic way into your query operator tree which might have all kinds of complicated operations for him on.</p>
<p>238<br>00:47:32,960 –&gt; 00:48:01,960<br>So there’s a beautiful paper from good scruffy that talks about this exchange operator and i’ll just show that to you with an example postress by the way calls it a gather operation but here is how it works take the simplest case where I want to apply selection on a table again the table might have a large number of paper pages let’s say just five over here and i’ve got three units of hard work I want to exploit basically I can spin up key workers at the same time.</p>
<p>239<br>00:48:01,960 –&gt; 00:48:17,960<br>So what you can do is spin up worker one two and three and tell worker one go and work on page one independently of telling worker to go and work on page two and essentially there’s no interference in the work that is done by.</p>
<p>240<br>00:48:17,960 –&gt; 00:48:46,960<br>My each of these workers because what we do is we’ll introduce that exchange operator at the top postress calls a gather which probably might make more sense as a term it’s going to take the work that is done by each of these workers which of these workers will operate on an independent page apply the selection and then send that across and in the operator model if you’re doing this tree in a top down fashion the exchange will call the selection which will call the next which will call the next step.</p>
<p>241<br>00:48:46,960 –&gt; 00:49:01,960<br>And then start producing data for page one and it throws out the exchange slash gather operation is just going to collect that result and then send it upstream to the next operator.</p>
<p>242<br>00:49:01,960 –&gt; 00:49:26,960<br>And so right now this exchange is very simple we’ll build it up into something a little bit more complex and just go walk through this example worker one starts a one goes and does its work the others can start in parallel so at this point in time if I look at the machine these three workers are working on three independent pages and getting more.</p>
<p>243<br>00:49:26,960 –&gt; 00:49:42,960<br>As did is getting produced they’re all getting sent to that exchange operator that’s combining and collecting that producing its own pages for output that is going to create right so exchanges consuming from these three workers and then we’ll become a producer in a little bit.</p>
<p>244<br>00:49:42,960 –&gt; 00:50:11,960<br>These three workers can get done and then they can start to work on these other pages so in this example one and two got done maybe the few few records on that page to process for example and please work on it but that’s okay one and two are done so they can be allocated for in five because a selection is naturally divided by the pages so we didn’t have to do an explicit divide stack the conquer stuff is what we are doing over here in parallel and so.</p>
<p>245<br>00:50:12,960 –&gt; 00:50:22,960<br>So at the second instance in time now worker one and worker two are working on pages four and five worker three will eventually finish up and the exchange operator will get all of its result.</p>
<p>246<br>00:50:23,960 –&gt; 00:50:31,960<br>Okay the exchange operator has finished consuming that result but now it has to send it and do something upstream and so.</p>
<p>247<br>00:50:31,960 –&gt; 00:50:40,960<br>That simple version of exchange you can think of it as a pure gather operation right just gathering stuff and could send all of its stuff as a single output.</p>
<p>248<br>00:50:40,960 –&gt; 00:50:51,960<br>Now there’s a different type of an exchange which is a distance between so what we saw in the previous slide was just a simple gather version of the exchange operation.</p>
<p>249<br>00:50:51,960 –&gt; 00:51:09,960<br>It’s to be a portion of that operation says the following i’m going to take something as input and I’m going to distribute it across multiple outputs to get imagine the distance operation for example leads a table R applies that hash function h1 and produces the coefficients for k.</p>
<p>250<br>00:51:10,960 –&gt; 00:51:21,960<br>And the different distributes different distribute operator does the same for s and that’s how you would think the part is instead of that race hash.</p>
<p>251<br>00:51:21,960 –&gt; 00:51:29,960<br>So this is the opposite of gathered right the funnel is inverted a gather is going to take multiple inputs produce one output.</p>
<p>252<br>00:51:29,960 –&gt; 00:51:33,960<br>The distributors going to do the opposite.</p>
<p>253<br>00:51:33,960 –&gt; 00:51:48,960<br>Okay and when it does a distribution until use some function a hash function or round robin or range function again those will talk about when we get into both details in the advanced graduate class.</p>
<p>254<br>00:51:48,960 –&gt; 00:51:57,960<br>Then there’s a V partition component which is a combination of these two if I take a whole bunch of inputs.</p>
<p>255<br>00:51:57,960 –&gt; 00:52:08,960<br>Gather them but instead of saying I gather and I have a separate distributes out in which I like to pass data cost to operators I can just lend that function into one is more efficient.</p>
<p>256<br>00:52:08,960 –&gt; 00:52:14,960<br>What that will do is it might take input from three input workers.</p>
<p>257<br>00:52:14,960 –&gt; 00:52:21,960<br>I might produce output to do different work so it might apply a totally hash function for example.</p>
<p>258<br>00:52:21,960 –&gt; 00:52:32,960<br>Repotitions function is very general it could have inputs and outputs and now you can control the shape of that free and how parallel this needs to be.</p>
<p>259<br>00:52:32,960 –&gt; 00:52:50,960<br>And so that’s essentially the different flavors of the exchange operator that you see and of course if you look at the original paper exchange largely referred to that last portion and you can say if I have that operator the other two cases are special cases of it right which is a perfectly legitimate way to look at.</p>
<p>260<br>00:52:50,960 –&gt; 00:52:55,960<br>Right questions.</p>
<p>261<br>00:52:55,960 –&gt; 00:53:00,960<br>Where is all this which part of the database is where all of these files.</p>
<p>262<br>00:53:00,960 –&gt; 00:53:03,960<br>Yeah.</p>
<p>263<br>00:53:03,960 –&gt; 00:53:05,960<br>Yeah.</p>
<p>264<br>00:53:05,960 –&gt; 00:53:06,960<br>Yeah.</p>
<p>265<br>00:53:06,960 –&gt; 00:53:09,960<br>The dispensary no of all of this.</p>
<p>266<br>00:53:09,960 –&gt; 00:53:14,960<br>Yeah great question so who decides on this degree of parallelism my m versus n.</p>
<p>267<br>00:53:14,960 –&gt; 00:53:33,960<br>And it’s a complex question I’ll give you a quick answer which will be incomplete the optimizer often starts to make some of these first decisions and the trend more and more is to have smarter schedules like we talked about in the last class that can decide based on what hardware parallelism is available and the degree of hardware parallelism is also often changing on the fly.</p>
<p>268<br>00:53:33,960 –&gt; 00:53:35,960<br>Okay.</p>
<p>269<br>00:53:35,960 –&gt; 00:53:36,960<br>Great question.</p>
<p>270<br>00:53:36,960 –&gt; 00:53:37,960<br>Yep.</p>
<p>271<br>00:53:37,960 –&gt; 00:53:44,960<br>So we’ve taken this together effectively function as a signation from the send me.</p>
<p>272<br>00:53:44,960 –&gt; 00:53:46,960<br>Yeah, three of these finish then.</p>
<p>273<br>00:53:46,960 –&gt; 00:53:54,960<br>It does but we till I talk about the intro query parallelism because it can probably start to send in some cases things out but the simplest view of it is the.</p>
<p>274<br>00:53:54,960 –&gt; 00:54:03,960<br>So the question was whether it’s a synchronization barrier like primitive the simplest view of the exchange operator can be thought of that that’s a barrier stops all of the flow below.</p>
<p>275<br>00:54:03,960 –&gt; 00:54:09,960<br>So we talk about intro operator parallelism in some cases you get don’t need to do that you could pass things along if it is safe to do that.</p>
<p>276<br>00:54:09,960 –&gt; 00:54:13,960<br>So in a similar bank in the repetition.</p>
<p>277<br>00:54:13,960 –&gt; 00:54:20,960<br>Can it like receive one input is like oh that’s enough for this output so you know like it can it can synchronize on one thing and send one of the output.</p>
<p>278<br>00:54:20,960 –&gt; 00:54:28,960<br>Yeah so the question is can repotation take some input and send it to one and finish that before it does the other or no if it’s done.</p>
<p>279<br>00:54:28,960 –&gt; 00:54:50,960<br>The answer is that’s often hard because you don’t know where all of the input is if the repetition was repining a hash function and you knew everything below it was already hashed then yes you could do something like that but the need for something like that is probably low it’s better to do things in the smart scheduler like I was telling you in the last class which no one does except for a few systems but that would be the better way to do it.</p>
<p>280<br>00:54:50,960 –&gt; 00:54:56,960<br>So if you want to do that you could just add the what would be the output to the the change in place.</p>
<p>281<br>00:54:56,960 –&gt; 00:55:05,960<br>Yeah and that’s right so the question is like effectively all these questions about can I shape the fan out and fan in of these exchange operators in general.</p>
<p>282<br>00:55:05,960 –&gt; 00:55:16,960<br>Absolutely does the use implication on performance how to do that right burning questions especially in new environments where in a cloud environment a query may come you may have 10 notes to work on.</p>
<p>283<br>00:55:16,960 –&gt; 00:55:32,960<br>So if the query is running maybe it’s going to take a day to run it might have been given five of those resources might have been taken away 100 more have been added for a little bit of time and you have to work in that dynamic environment so lots of open research issues and we talk about a bunch of those in the database class.</p>
<p>284<br>00:55:32,960 –&gt; 00:55:33,960<br>Yep.</p>
<p>285<br>00:55:33,960 –&gt; 00:55:36,960<br>So operator on the same level.</p>
<p>286<br>00:55:36,960 –&gt; 00:55:41,960<br>Operating well I’m working on the same level operator on the same operation.</p>
<p>287<br>00:55:41,960 –&gt; 00:55:46,960<br>Yeah workers on the same level in that operator tree will be operating on the same operation.</p>
<p>288<br>00:55:46,960 –&gt; 00:55:48,960<br>Yes is that the question.</p>
<p>289<br>00:55:48,960 –&gt; 00:55:49,960<br>Yeah.</p>
<p>290<br>00:55:49,960 –&gt; 00:55:53,960<br>Combining like results from true development have like different.</p>
<p>291<br>00:55:53,960 –&gt; 00:55:57,960<br>It’s like Java and you know, yeah good question.</p>
<p>292<br>00:55:57,960 –&gt; 00:56:06,960<br>So I have a hash join for example I will have an exchange for the build site I will have an exchange for the pro site they’ll then go into the hash let me just go to that.</p>
<p>293<br>00:56:06,960 –&gt; 00:56:19,960<br>So here is a selection followed by a hash join let’s say we start three we have three units of parallelism for the a side and we start that they start producing the output we have.</p>
<p>294<br>00:56:19,960 –&gt; 00:56:31,960<br>In this case the projection has been pushed down below the joint which you can do query optimization when we talk about that will discuss that so here my pipeline is do the selection then the projection.</p>
<p>295<br>00:56:31,960 –&gt; 00:56:45,960<br>And now I’m going and sticking all of that into the big side of the hash did because I’m partitioning that and so I will have something on the build site I will go and exchange that feed that into the joint.</p>
<p>296<br>00:56:45,960 –&gt; 00:56:59,960<br>I can start the b site in a similar way and in this case is a three by three inputs on the exchange idea we’re having two different for them and the sheep of the three depending on the behavior panels and you are to allocate across the different operators.</p>
<p>297<br>00:56:59,960 –&gt; 00:57:12,960<br>The all legitimate and the end of the day however the join needs to end up having the input with a certain partition strategy if it doesn’t then has to re partition internally in the operator.</p>
<p>298<br>00:57:12,960 –&gt; 00:57:21,960<br>Again lots of interesting stuff many of those will be covered in the advanced database class let me keep moving.</p>
<p>299<br>00:57:21,960 –&gt; 00:57:35,960<br>So the other approach to things benefit from families and it’s going to come from what we just talk about that in drop operator of families and throwing more operate throwing more workers.</p>
<p>300<br>00:57:35,960 –&gt; 00:57:47,960<br>The giving off using this exchange offering for to make all of that pre logic work for us with those two mechanisms you get the most out of the hardware that you have.</p>
<p>301<br>00:57:47,960 –&gt; 00:58:04,960<br>Now a secondary mechanism which often gets used in streaming environments with this becomes a primary mechanism and it’s giving environment the application for these are cases they they just floating in for example you’re getting ticker prices you know you’re getting paper update for all for thousands of.</p>
<p>302<br>00:58:04,960 –&gt; 00:58:06,960<br>How is it so.</p>
<p>303<br>00:58:06,960 –&gt; 00:58:25,960<br>Access coming in every and every millisecond and every second and if I would compute a sliding window aggregate for that right in that case the work that you do is not a lot but you have lots and lots of pieces of information do work for that entire party look at all the figure prices and you’ll be out with you are</p>
<p>304<br>00:58:25,960 –&gt; 00:58:33,960<br>effectively a convenient environment called stealing a garden where I see what comes in your producing the output and the unit of work is somewhere so.</p>
<p>305<br>00:58:33,960 –&gt; 00:58:53,960<br>So this is just like storm play Kafka and I worked on both storm and heron that will do dead main parallelism is going to come from a combination of interim inter operator parallelism so what is this inter operator parallelism look like it’s essentially also called pipeline parallelism.</p>
<p>306<br>00:58:53,960 –&gt; 00:59:16,960<br>So if you know you next pipes you can connect to processes connect them by a process one is sending stuff is before it is finishing everything process to start working on what it is being sent to it both processes are operating on this free one data and you’re not waiting for everything to be done by the first process before saying same idea same idea here so if I’ve got this operator tree or</p>
<p>307<br>00:59:16,960 –&gt; 00:59:39,960<br>talking to doing all the inter operator parallelism what I can also do here is effectively say wrong let’s see a threat if I’m in the federal model that’s going to effectively go and do this joint but as it produces the joint after it gets a result over here the first time it produces a result it hasn’t finished.</p>
<p>308<br>00:59:39,960 –&gt; 01:00:08,960<br>So the first result is just send it and if that that operator over there will project the output and send it up so I don’t have to wait for all of this to be done to see my first result and now you can see how in the streaming environment this would be extremely useful and even in a regular system it helps because you can use and the other worker the doing one thing the projection is not sitting idle it can start to do it work while the downstream work is still happening.</p>
<p>309<br>01:00:09,960 –&gt; 01:00:17,960<br>So both of these are orthogonal mechanisms and you can use both of them in parallel but they’re very distinct mechanisms.</p>
<p>310<br>01:00:18,960 –&gt; 01:00:37,960<br>I’m just going to glance over there’s the text book talks about a bushy pipe parallelism which is a hybrid of both of those there’s really no magic to it is even unclear if you should call it that I think they’re the only ones who call it a bushy parallelism no other text but if you read the text book I don’t want you to get confused the main idea is if I’ve got three</p>
<p>311<br>01:00:38,960 –&gt; 01:01:07,960<br>often freeze of this time where you’re joining speeding into other joins is called the bushy three that you got those two joins up there feeling into an exchange operator right so the exchange is getting input from two different join operations and effectively what this is saying is I could do interoperator parallelism at some level like in the first join I could be using interoperator parallelism I could also be doing pipelining across each of these individual branches of the tree I can be working on multiple branches at the same time.</p>
<p>312<br>01:01:08,960 –&gt; 01:01:36,960<br>So it’s just a combination I don’t spend too much time on that but basically you could do that essentially if you look at the scheduler that we talked about in the previous class all of these that the bushy type of parallelism just implicitly comes for free so you don’t have to over engineer it there’s no rocket science to do it here okay and just hold that question till the end of the class I think I know what you’re going to ask but I don’t spend too much time on the bushy tree just want to dot the eye and cross the tea on that.</p>
<p>313<br>01:01:36,960 –&gt; 01:02:05,960<br>So what have we talked about so far that we want to have to use all the hardware parallelism that we and to do that we have all these complex worker models but now we’re going to look at go from that level to a slightly lower level which is you know so far essentially everything we’ve talked about is essentially say I’ve got 40 cores how do I use them oh I’ve got 100 nodes sitting in my rack how do I use them now.</p>
<p>314<br>01:02:06,960 –&gt; 01:02:35,960<br>But when an IO needs to get request said it needs to be sent to the IO subsystem more often than not that IO subsystem is going to be a complicated system might have multiple disks in it and so we’re going to start talking about IO parallelism right so we are now one level down and starting to make requests on the IO subsystem and they two are going to different parallel system right is the IO subsystem parallelism and a little different than some people.</p>
<p>315<br>01:02:36,960 –&gt; 01:02:39,960<br>So I think it’s going to be a little different than the one that we have to work together.</p>
<p>316<br>01:02:40,960 –&gt; 01:02:42,960<br>Does that make sense?</p>
<p>317<br>01:02:42,960 –&gt; 01:02:50,960<br>Because I’m not running on a single disk and probably running even on given server with a bunch of disks and this parallelism there too.</p>
<p>318<br>01:02:50,960 –&gt; 01:02:53,960<br>So this idea sorry.</p>
<p>319<br>01:02:53,960 –&gt; 01:03:05,960<br>And here to make these a parallel parallel parallelism you’re going to have some notion of how we have made out data across the different disks that we have.</p>
<p>320<br>01:03:05,960 –&gt; 01:03:14,960<br>There many ways of doing it I could say I may have four disks and four databases each database gets a disk obviously that’s pretty hard right.</p>
<p>321<br>01:03:14,960 –&gt; 01:03:24,960<br>I might say I’ve got you know four disks and every table sits on his own disk and some disks might have multiple tables and just write about a single database for example.</p>
<p>322<br>01:03:24,960 –&gt; 01:03:31,960<br>I could have multiple disks for database and then one of these or I could go to the extreme and say everything can be partitioned whichever way.</p>
<p>323<br>01:03:31,960 –&gt; 01:03:44,960<br>All I need to worry about is for each table if I’ve got four disks doesn’t go to four disks or three disks or two disks or one disk and I can control that and then I have mechanisms to go deal with that level of parallelism.</p>
<p>324<br>01:03:44,960 –&gt; 01:03:57,960<br>See if all the data was sitting only in one one disk and I thought worker threads running on it through interoperator parallelism they’re all going to hit that disk right that will become my bottle.</p>
<p>325<br>01:03:57,960 –&gt; 01:04:06,960<br>Okay so that’s why I was in important because if you don’t pay attention to that everything that you’re trying to do in that compute layer is going to get brought in a tier next.</p>
<p>326<br>01:04:06,960 –&gt; 01:04:11,960<br>Does that make sense how these two are connected.</p>
<p>327<br>01:04:11,960 –&gt; 01:04:16,960<br>Okay all right so what can we do.</p>
<p>328<br>01:04:16,960 –&gt; 01:04:27,960<br>So first is there’s a whole class that talks about things like data on disk doesn’t last forever.</p>
<p>329<br>01:04:27,960 –&gt; 01:04:35,960<br>I’m going to just present the high level over here and hopefully teach you take classes in the tunnel research that rush me does over here on duration coding.</p>
<p>330<br>01:04:35,960 –&gt; 01:04:45,960<br>It all has to do with this massive area say I’m stored data on a disk where the SSD or spinning this and bad things happen to it.</p>
<p>331<br>01:04:45,960 –&gt; 01:05:04,960<br>There are billions of bits sitting on these devices and some bits are going to get corrupted and maybe running some sort of checks on and stuff like that at the page level but disks but bits in a disk rock is called bit rotting and just like actual fungus and rot that rot tends to spread.</p>
<p>332<br>01:05:04,960 –&gt; 01:05:25,960<br>So essentially I have stored data on disk but I need to be able to get data back and it better be the data I wrote I don’t want to lose data I don’t get wrong information and I nearly always in the survey environment won’t have a single disk for the collection of disk that collectively behave like an IO subsystem so I need to work with that.</p>
<p>333<br>01:05:25,960 –&gt; 01:05:37,960<br>So there are three competing dimensions I want high performance if I’ve got four disks and I’m going to make a call to that can I get the collective power of those four disks on a single scan that performance.</p>
<p>334<br>01:05:37,960 –&gt; 01:05:52,960<br>You have lady I know disks are going to lose some bits are going to rot other bad things are going to happen to disk I want to replicate in that layer so that if something’s bad I can get the coffee and I need to keep the coffee consistent in just the IO subsystem.</p>
<p>335<br>01:05:52,960 –&gt; 01:06:19,960<br>So I want to use as much of the capacity of the disc collective storage capacity I have while getting these other properties and obviously they compete with each other as I said this is a whole area by itself so I can do justice to it but just want to appreciate have you appreciate that when you talk to an IO subsystem even on the local server is this this is the IO level and we need to be aware of it.</p>
<p>336<br>01:06:19,960 –&gt; 01:06:30,960<br>So I want to talk to you in the final or distributed system the IO subsystem is spread across and the cloud environment often the whole storage there is it’s in a different cloud and there’s a compute cloud and a storage cloud.</p>
<p>337<br>01:06:30,960 –&gt; 01:06:40,960<br>Okay, so it gets even more complicated but let’s just keep it simple so you can appreciate the basic and then gets you set up to do more advanced work in that area.</p>
<p>338<br>01:06:40,960 –&gt; 01:07:09,960<br>So I want to talk about a file of six pages to the database system this is a logical view I start a scan on this file I will say get me page one page two page three it comes to the buffer pool the buffer pool just gets about these page IDs but at the file system level or if we decide that we are going to take that over from the operating system we have to figure out how to lay these things out in many database systems will take this part over from the operating system and they things out you have certain properties at the database there.</p>
<p>339<br>01:07:09,960 –&gt; 01:07:14,960<br>Okay, okay, those are things we’ll talk about in the advanced class I just want to give you a flavor of that.</p>
<p>340<br>01:07:14,960 –&gt; 01:07:29,960<br>So imagine I’ve got three disks, three physical disks I could take those those pages and spread them across in the following grade one to three I’m just typing in a rob robin fashion first page on person second on the second and so on.</p>
<p>341<br>01:07:29,960 –&gt; 01:07:39,960<br>Another person say scan page one I start worker one to scan worker page one worker to start to work on page two worker people work on page.</p>
<p>342<br>01:07:39,960 –&gt; 01:07:58,960<br>I’m going to be able to leverage the entire panelism in the in the dissups system this disk and scan key pages at a given time because each this can serve up a single page I’m going to get that are your families and if I don’t pay attention to this and if I said for the first worker is going to page one same word is going to be page two that this could be the bottom.</p>
<p>343<br>01:07:59,960 –&gt; 01:08:14,960<br>But now perhaps this typing scheme which there used to be this old subsystem called rate which has been superseded by erasure codes this used to be called rate zero it’s simple striping other technical term for that is striping I’m just striping across the disk.</p>
<p>344<br>01:08:14,960 –&gt; 01:08:24,960<br>Okay, now I can get high performance high capacity but what do I not get durability page one is rotted I lost that information.</p>
<p>345<br>01:08:25,960 –&gt; 01:08:39,960<br>Okay, the other way is oh I really care about durability I’m going to mirror everything I’m really fine right about it or some of the form of the device getting corrupted and I’m going to make copy of page one three times.</p>
<p>346<br>01:08:39,960 –&gt; 01:09:08,960<br>And so every base has three pockets now we can still get time with them so if worker one starts I can say you go get it from here worker two starts and which we say you go work here worker three you can say go use this so I can still get performance I can get durability but I cannot get my capacity my capacity is good by a third because I’m making three copies now as I said this is a very simple technique two of the earlier numbers in rate mirroring and striping the more than we could do it is to work with the other person.</p>
<p>347<br>01:09:09,960 –&gt; 01:09:30,960<br>And this used to be done all in the hardware there’s to be hardware rate device drivers that do that that’s still deployed in the wild right now but the modern need to do it is to do all of this in software connecting to a question that was asked here is that you know take that over you know have this nice a boundary division of labor between hardware and software much better to write that stuff in software that to it.</p>
<p>348<br>01:09:31,960 –&gt; 01:09:46,960<br>And you can also do things like in software I don’t need a device driver that is tied to three controllers or four controllers that software could actually be controlling this set us spread across the planet in a distributed system and can build all kinds of fancy stuff at that.</p>
<p>349<br>01:09:46,960 –&gt; 01:09:55,960<br>They’re called racial codes they’ll do parity bits and then there’s a question of how many parity bits to a need for copy of the data to balance that trifecta of competing goals.</p>
<p>350<br>01:09:55,960 –&gt; 01:10:06,960<br>So again there’s a whole class on that and some cutting edge research happening here at CMU on how to do that well and I noticed this camera has been off the whole time now yep go for it.</p>
<p>351<br>01:10:07,960 –&gt; 01:10:15,960<br>Question yep does for example the these processing with BTRFS in ZFS to use a region.</p>
<p>352<br>01:10:16,960 –&gt; 01:10:27,960<br>Some of them use some version of that and have to look at the details for that but erasure codes are used all across in cloud file systems that’s the way modern cloud file systems are all right yeah.</p>
<p>353<br>01:10:28,960 –&gt; 01:10:30,960<br>Okay other questions.</p>
<p>354<br>01:10:33,960 –&gt; 01:10:40,960<br>Okay and make sure I get you guys out of here in time but as I said lots of really open and interesting questions here.</p>
<p>355<br>01:10:41,960 –&gt; 01:10:53,960<br>And most of the time when this is done at the file system or often there are these entire subsystems that do this software storage architecture.</p>
<p>356<br>01:10:54,960 –&gt; 01:10:58,960<br>This is often transferred to the database system but as I.</p>
<p>357<br>01:10:58,960 –&gt; 01:11:19,960<br>I do appreciate it is if the database system mean kind of what this was it can probably do a better job with what that allocation is so again this is one of those things that is still evolving say how transfer it should this be from the database system many of the storage vendors are right now building it as being pretty transparent but that battle is being fought as people try to figure out what’s that right control.</p>
<p>358<br>01:11:20,960 –&gt; 01:11:21,960<br>Okay.</p>
<p>359<br>01:11:23,960 –&gt; 01:11:24,960<br>All right.</p>
<p>360<br>01:11:24,960 –&gt; 01:11:25,960<br>Okay.</p>
<p>361<br>01:11:26,960 –&gt; 01:11:34,960<br>Database partitioning this relates to what we just talked about and should control the layout of that data of this.</p>
<p>362<br>01:11:34,960 –&gt; 01:11:48,960<br>Okay that’s a little bit of talk with you how many copies to any for the duty aspect and how do we piece all of these competing goals about of course databases by doing where the data is because that allows us to do things with locality.</p>
<p>363<br>01:11:49,960 –&gt; 01:12:12,960<br>And so many database systems will allow you that work in this parallel distributed environment will allow you to specify where this data goes either through hints or explicit mechanisms certainly in the embedded system you will be looking at allocating that in the local file system and post recipe install it will tell you hey which is the directory in the local file system because it’s just single node.</p>
<p>364<br>01:12:12,960 –&gt; 01:12:31,960<br>Do I store all my stuff that’s very simple but you know there are more complex mechanisms and database systems are going to use the buffer pool manager to map the pages to a disk in some sense insulating ourselves from that little pain saying oh if the I only are hasn’t given me control over where the pages are or knowing the location.</p>
<p>365<br>01:12:31,960 –&gt; 01:12:47,960<br>Fine at least if the pages are reactivized over and over again they’re in my buffer pool and I can insulate myself first I get a lot more efficiency because it’s way faster to access data in the buffer pool but it can mitigate some of that other challenges that come from not knowing precisely where things are.</p>
<p>366<br>01:12:47,960 –&gt; 01:13:16,960<br>When we talk about recovery and transactions you’ll notice that things like the recovery log that’s a place where we keep track of what changes have we made to records and pages in a transaction and that needs to be that needs to hit the storage device before that query that transaction query can be declared done and their complications over there too if that recovery log is in the file system and it gets cached in the file system but not actually.</p>
<p>367<br>01:13:17,960 –&gt; 01:13:29,960<br>Put to this we haven’t really committed the transaction and so there’s going to be that interplay that also happens between the file system and we’ll cover that as we talk about those discussions in.</p>
<p>368<br>01:13:29,960 –&gt; 01:13:46,960<br>When we get to the transactions are component of this class partitioning is is a rich topic you can take a table you’ll see many systems which will tell which will allow you to say things like here’s the table hash partition all the records in this class.</p>
<p>369<br>01:13:47,960 –&gt; 01:14:00,960<br>So, you can take a table based on certain set of keys I think that it’s relatively support that or partitioning may be transferred to the application and when you have a parallel distributed database system that will be a digital richness in the SQL schema that allows you to specify.</p>
<p>370<br>01:14:00,960 –&gt; 01:14:18,960<br>So, we’ll specify actually some of these partitioning countries and they obviously have big impact on what that performance of that end to end system looks like but those will get covered in the advanced database class so again this is a plug for those of you are really interested to take that in the next semester.</p>
<p>371<br>01:14:18,960 –&gt; 01:14:21,960<br>Okay, so we covered a lot today.</p>
<p>372<br>01:14:21,960 –&gt; 01:14:31,960<br>All of these techniques that we discussed are addressing the point that other families in this every bit you cannot buy a single core machine anymore.</p>
<p>373<br>01:14:31,960 –&gt; 01:14:47,960<br>The cloud system it’s a huge set of families in that’s a little bit locally in a single note single server to a customer service in Iraq, the rack sitting in a data center data centers being spread across the globe and how do we exploit all of this parallelism.</p>
<p>374<br>01:14:47,960 –&gt; 01:15:10,960<br>We need mechanisms for doing scheduling we talked about that in the previous class we need mechanisms to change the operator to introduce the exchange operator need to have interoperator table is in the big work force for getting a lot more performance of the hardware and of course we need to better understand how partitioning works so that we can work better with that underlying layer.</p>
<p>375<br>01:15:10,960 –&gt; 01:15:19,960<br>And so with that this is what we’ll talk about the next class will start going to query optimization and talk about the different steps for what makes a query optimizer works.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CMU15445 P14F202313 QueryExecutionPart2</div>
      <div>http://example.com/2025/10/24/CMU15445 P14F202313-QueryExecutionPart2/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月24日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/24/CMU15445%20P15F202314-QueryPlanningOptimization/" title="CMU15445 P15F202314 QueryPlanningOptimization">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CMU15445 P15F202314 QueryPlanningOptimization</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/24/CMU15445%20P13F202312-QueryExecutionPart1/" title="CMU15445 P13F202312 QueryExecutionPart1">
                        <span class="hidden-mobile">CMU15445 P13F202312 QueryExecutionPart1</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
