

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="100:00:00,000 –&gt; 00:00:06,000Canneke Mellon University’s advanced database systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio audience. 300:00:14,000 –&gt; 00:00:15,00">
<meta property="og:type" content="article">
<meta property="og:title" content="CMU15721 P17S202417 GoogleBigQuery⧸DremelCMUAdvancedDatabaseSystems">
<meta property="og:url" content="http://example.com/2025/10/24/CMU15721%20P17S202417-GoogleBigQuery%E2%A7%B8DremelCMUAdvancedDatabaseSystems/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="100:00:00,000 –&gt; 00:00:06,000Canneke Mellon University’s advanced database systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio audience. 300:00:14,000 –&gt; 00:00:15,00">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-24T11:57:41.746Z">
<meta property="article:modified_time" content="2025-10-24T12:06:28.545Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>CMU15721 P17S202417 GoogleBigQuery⧸DremelCMUAdvancedDatabaseSystems - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CMU15721 P17S202417 GoogleBigQuery⧸DremelCMUAdvancedDatabaseSystems"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-24 19:57" pubdate>
          2025年10月24日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          6.7k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          56 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CMU15721 P17S202417 GoogleBigQuery⧸DremelCMUAdvancedDatabaseSystems</h1>
            
            
              <div class="markdown-body">
                
                <p>1<br>00:00:00,000 –&gt; 00:00:06,000<br>Canneke Mellon University’s advanced database systems courses</p>
<p>2<br>00:00:06,000 –&gt; 00:00:09,000<br>filming front of the live studio audience.</p>
<p>3<br>00:00:14,000 –&gt; 00:00:15,000<br>Alright, yeah.</p>
<p>4<br>00:00:15,000 –&gt; 00:00:18,000<br>Just get through this. Get out early and go see the clubs.</p>
<p>5<br>00:00:18,000 –&gt; 00:00:22,000<br>So, for this point in the semester,</p>
<p>6<br>00:00:22,000 –&gt; 00:00:27,000<br>we’re going to start reading papers about individual systems.</p>
<p>7<br>00:00:28,000 –&gt; 00:00:31,000<br>Obviously, the paper you guys read today was on Google BigQuery.</p>
<p>8<br>00:00:31,000 –&gt; 00:00:37,000<br>But the purpose of this part of the semester is now to examine and look at how</p>
<p>9<br>00:00:37,000 –&gt; 00:00:41,000<br>companies were building real systems based on the technologies and methods and techniques</p>
<p>10<br>00:00:41,000 –&gt; 00:00:45,000<br>and algorithms that we talked about throughout the entire semester.</p>
<p>11<br>00:00:45,000 –&gt; 00:00:51,000<br>And the goal of this is to learn how we can read the papers from in industry.</p>
<p>12<br>00:00:51,000 –&gt; 00:00:55,000<br>Some of it’s a little bit marketing heavy. Some of it will be actual true systems discussions.</p>
<p>13<br>00:00:56,000 –&gt; 00:01:02,000<br>But basically understand how they apply the things that we’ve talked about to solve the real world problems.</p>
<p>14<br>00:01:02,000 –&gt; 00:01:08,000<br>And also for you to then be able to interpret and recalibrate maybe how they describe certain things</p>
<p>15<br>00:01:08,000 –&gt; 00:01:11,000<br>based on the fundamentals of what we talked about.</p>
<p>16<br>00:01:11,000 –&gt; 00:01:18,000<br>So, for example, sometimes you’ll see systems talk about technique ABC and slightly different language that we talked about.</p>
<p>17<br>00:01:18,000 –&gt; 00:01:20,000<br>But then you know because we’ve read certain papers.</p>
<p>18<br>00:01:20,000 –&gt; 00:01:22,000<br>Oh, it’s really describing this.</p>
<p>19<br>00:01:23,000 –&gt; 00:01:26,000<br>Not to pick on Dremio. We’ll see Dremio in a second.</p>
<p>20<br>00:01:26,000 –&gt; 00:01:29,000<br>They talk about having these things called reflections.</p>
<p>21<br>00:01:29,000 –&gt; 00:01:31,000<br>Like what the hell’s reflection? I’ve never heard of that before.</p>
<p>22<br>00:01:31,000 –&gt; 00:01:33,000<br>You go kind of read the documentation a bit more.</p>
<p>23<br>00:01:33,000 –&gt; 00:01:35,000<br>Oh, it’s just materialized views.</p>
<p>24<br>00:01:35,000 –&gt; 00:01:40,000<br>So the idea is like, again, now you have this core background knowledge about how these real world systems</p>
<p>25<br>00:01:40,000 –&gt; 00:01:41,000<br>are actually built.</p>
<p>26<br>00:01:41,000 –&gt; 00:01:44,000<br>Then you can cut through the BS and understand what’s going on.</p>
<p>27<br>00:01:44,000 –&gt; 00:01:45,000<br>They call reflections.</p>
<p>28<br>00:01:45,000 –&gt; 00:01:47,000<br>They call reflections.</p>
<p>29<br>00:01:47,000 –&gt; 00:01:48,000<br>Yes.</p>
<p>30<br>00:01:49,000 –&gt; 00:01:53,000<br>And then also too, now you also have this internal catalog you build and say,</p>
<p>31<br>00:01:53,000 –&gt; 00:01:56,000<br>okay, well, in the good papers, especially in the Databricks one,</p>
<p>32<br>00:01:56,000 –&gt; 00:01:58,000<br>they’ll talk about, oh, we have this problem.</p>
<p>33<br>00:01:58,000 –&gt; 00:01:59,000<br>So we solved it this way.</p>
<p>34<br>00:01:59,000 –&gt; 00:02:00,000<br>We have this problem solved it that way.</p>
<p>35<br>00:02:00,000 –&gt; 00:02:03,000<br>So again, when you go out in a real world, you’ll see how, you know,</p>
<p>36<br>00:02:03,000 –&gt; 00:02:09,000<br>apply those same lessons you’ve learned from reading these papers and other systems to different situations.</p>
<p>37<br>00:02:09,000 –&gt; 00:02:11,000<br>And of course, it’s always nice to understand that like,</p>
<p>38<br>00:02:11,000 –&gt; 00:02:13,000<br>I didn’t just make the stuff up.</p>
<p>39<br>00:02:13,000 –&gt; 00:02:16,000<br>Everything we talk about this semester is real.</p>
<p>40<br>00:02:17,000 –&gt; 00:02:22,000<br>So, because they said we’re going to start off with Google Dremel, a big query today.</p>
<p>41<br>00:02:22,000 –&gt; 00:02:25,000<br>Wednesday, we’ll talk about the Databricks, Spark SQL, and Photon,</p>
<p>42<br>00:02:25,000 –&gt; 00:02:28,000<br>and then Sniff Lake, Doug T. B, Yellow Brick, and Red Ship.</p>
<p>43<br>00:02:28,000 –&gt; 00:02:34,000<br>So it became because of the, whatever, the stomach virus we had to drop the last one.</p>
<p>44<br>00:02:34,000 –&gt; 00:02:38,000<br>And I decided to drop the Microsoft paper instead of the Yellow Brick one,</p>
<p>45<br>00:02:38,000 –&gt; 00:02:41,000<br>because as he was asking about before, the Yellow Brick one is wild,</p>
<p>46<br>00:02:41,000 –&gt; 00:02:44,000<br>because they do all sorts of low-level system optimizations.</p>
<p>47<br>00:02:44,000 –&gt; 00:02:47,000<br>And they report real numbers in their papers.</p>
<p>48<br>00:02:47,000 –&gt; 00:02:50,000<br>Whereas these other papers, you’re not going to see that.</p>
<p>49<br>00:02:50,000 –&gt; 00:02:55,000<br>Part of because these big companies don’t want any numbers they report in these papers,</p>
<p>50<br>00:02:55,000 –&gt; 00:02:59,000<br>they’re used against them in like a judo marketing move by their competitors.</p>
<p>51<br>00:02:59,000 –&gt; 00:03:02,000<br>But Yellow Bricks, they don’t give a f***, they put all the numbers in.</p>
<p>52<br>00:03:02,000 –&gt; 00:03:03,000<br>It’s awesome.</p>
<p>53<br>00:03:03,000 –&gt; 00:03:04,000<br>Okay.</p>
<p>54<br>00:03:04,000 –&gt; 00:03:08,000<br>So the reoccurring themes we’re going to have throughout the entire,</p>
<p>55<br>00:03:08,000 –&gt; 00:03:13,000<br>the papers are going to read, are again, all the things that we’ve talked about this semester.</p>
<p>56<br>00:03:13,000 –&gt; 00:03:16,000<br>So obviously the resource to segregation is separate and compute and storage.</p>
<p>57<br>00:03:16,000 –&gt; 00:03:20,000<br>Like this is the Lake Alice Data Lake model where a bunch of data is on S3,</p>
<p>58<br>00:03:20,000 –&gt; 00:03:24,000<br>whatever your object store is, and then we’re putting query engines on top of that.</p>
<p>59<br>00:03:24,000 –&gt; 00:03:27,000<br>We’re also going to see the challenge of dealing the lack of statistics.</p>
<p>60<br>00:03:27,000 –&gt; 00:03:29,000<br>Right? The big query papers certainly talked about this.</p>
<p>61<br>00:03:29,000 –&gt; 00:03:34,000<br>We’ll see this over and over again where it’s a bunch of files that were uploaded into the object store</p>
<p>62<br>00:03:34,000 –&gt; 00:03:36,000<br>outside of the control of the database system.</p>
<p>63<br>00:03:36,000 –&gt; 00:03:38,000<br>So now you get to be query shows up.</p>
<p>64<br>00:03:38,000 –&gt; 00:03:39,000<br>You want to start planning on it.</p>
<p>65<br>00:03:39,000 –&gt; 00:03:45,000<br>If a cost based optimizer, what are your costs going to be based on if you don’t have any statistics?</p>
<p>66<br>00:03:45,000 –&gt; 00:03:50,000<br>Obviously everything is going to be a kilometer, but we also want to handle the non-relational data,</p>
<p>67<br>00:03:50,000 –&gt; 00:03:54,000<br>like the JSON, in the case of the BigQuery, it was the protobuff files,</p>
<p>68<br>00:03:54,000 –&gt; 00:03:56,000<br>and then vectorize execution as we talked about before.</p>
<p>69<br>00:03:56,000 –&gt; 00:04:01,000<br>Again, these are our, like this thing is pretty much standard in every O-lapse system today.</p>
<p>70<br>00:04:01,000 –&gt; 00:04:05,000<br>So none of the papers are really going to talk about any unique aspects of what they’re doing,</p>
<p>71<br>00:04:05,000 –&gt; 00:04:08,000<br>which is it’ll always be that.</p>
<p>72<br>00:04:08,000 –&gt; 00:04:14,000<br>All right, so let’s talk about now the setup for Google’s Dremel BigQuery paper.</p>
<p>73<br>00:04:14,000 –&gt; 00:04:21,000<br>So maybe not in, you know, for the people that are younger here, maybe not now,</p>
<p>74<br>00:04:21,000 –&gt; 00:04:25,000<br>you don’t think of like Google as the hot tech company.</p>
<p>75<br>00:04:25,000 –&gt; 00:04:30,000<br>Maybe like OpenAI is what everyone’s excited about these days, something like that.</p>
<p>76<br>00:04:31,000 –&gt; 00:04:41,000<br>But in the 2000s, Google definitely had a sizeable influence on how people thought about and design and develop data systems.</p>
<p>77<br>00:04:41,000 –&gt; 00:04:49,000<br>And even to this day, I would say their pushes maybe, or influence in the menus, not as strong as it used to be,</p>
<p>78<br>00:04:49,000 –&gt; 00:04:54,000<br>simply because the technologies have been spread out for a little bit further.</p>
<p>79<br>00:04:54,000 –&gt; 00:04:56,000<br>Now obviously LLMs are the hot thing.</p>
<p>80<br>00:04:57,000 –&gt; 00:05:03,000<br>But back then, you know, pretty much any time Google put out a paper, a research paper that said, hey,</p>
<p>81<br>00:05:03,000 –&gt; 00:05:08,000<br>here’s this system we built internally at Google, that everyone read it, everyone got really excited,</p>
<p>82<br>00:05:08,000 –&gt; 00:05:11,000<br>and then people started building open source clones of these things.</p>
<p>83<br>00:05:11,000 –&gt; 00:05:15,000<br>Because the mindset in some ways was, well, Google’s super successful.</p>
<p>84<br>00:05:15,000 –&gt; 00:05:20,000<br>Google can operate at scale if we need, you know, if our company wants to be successful,</p>
<p>85<br>00:05:20,000 –&gt; 00:05:22,000<br>then we need basically the same stuff that they’re building.</p>
<p>86<br>00:05:23,000 –&gt; 00:05:29,000<br>Because Google didn’t release their things as open source, at least in the beginning didn’t release their things as services.</p>
<p>87<br>00:05:29,000 –&gt; 00:05:34,000<br>They just like, hey, here’s this paper written by Jeff Dean and others, who are obviously very smart people,</p>
<p>88<br>00:05:34,000 –&gt; 00:05:38,000<br>but everyone’s like, okay, they scram would then re-implement everything.</p>
<p>89<br>00:05:38,000 –&gt; 00:05:46,000<br>So this is sort of an incomplete list of a bunch of the database systems or data systems that Google has developed over the years that have been very influential.</p>
<p>90<br>00:05:46,000 –&gt; 00:05:49,000<br>And I’m sort of subdivided into two groups.</p>
<p>91<br>00:05:49,000 –&gt; 00:05:53,000<br>At the top here, you have all the no-sequel systems, right?</p>
<p>92<br>00:05:53,000 –&gt; 00:05:59,000<br>Because Google was the, you know, without maybe coming out and saying yes we’re a no-sequel company,</p>
<p>93<br>00:05:59,000 –&gt; 00:06:04,000<br>but they certainly were at the forefront of the vanguard of the no-sequel data movement, right?</p>
<p>94<br>00:06:04,000 –&gt; 00:06:07,000<br>And you saw this in the paper you guys read.</p>
<p>95<br>00:06:07,000 –&gt; 00:06:15,000<br>And then there’s all these other systems in the late early 2000s, you know, in sorry late 2010,</p>
<p>96<br>00:06:15,000 –&gt; 00:06:21,000<br>early 2010s and going forward where Google realized, oh, sequel was actually a good idea,</p>
<p>97<br>00:06:21,000 –&gt; 00:06:25,000<br>and then started adding, you know, starting building systems around this.</p>
<p>98<br>00:06:25,000 –&gt; 00:06:34,000<br>Again, in the paper you guys read, there’s this paragraph right here that talks about how the conventionalism Google was sequel didn’t scale, right?</p>
<p>99<br>00:06:34,000 –&gt; 00:06:40,000<br>And then again, everyone else sort of followed along with Bandwagon and was designing systems based on some of these early ideas.</p>
<p>100<br>00:06:40,000 –&gt; 00:06:44,000<br>Because Mongo is probably the biggest ones, right?</p>
<p>101<br>00:06:44,000 –&gt; 00:06:50,000<br>Or people were saying we don’t want to use sequel, sequel doesn’t scale, we don’t want to use joins.</p>
<p>102<br>00:06:50,000 –&gt; 00:06:52,000<br>And then now the tide has turned out.</p>
<p>103<br>00:06:52,000 –&gt; 00:06:57,000<br>And so the Dremel guys talk about how they see themselves as the ones that actually made Google,</p>
<p>104<br>00:06:57,000 –&gt; 00:07:04,000<br>which are made SQL cool again or important again or matter again in Google.</p>
<p>105<br>00:07:04,000 –&gt; 00:07:11,000<br>So a lot of these systems have size influence in a bunch of different data systems and other data systems.</p>
<p>106<br>00:07:11,000 –&gt; 00:07:16,000<br>And as I said, a lot of these things started off as early research papers that Google put out.</p>
<p>107<br>00:07:16,000 –&gt; 00:07:26,000<br>And typically the way industry companies put out papers is it’s usually about two or three, maybe even four or five years behind what actually the state of the art is.</p>
<p>108<br>00:07:26,000 –&gt; 00:07:31,000<br>So like they’ll build the system, you know, get it up and running, file the patents for it.</p>
<p>109<br>00:07:31,000 –&gt; 00:07:37,000<br>And then they’ll write the paper by the behind the paper comes out again, it’s a couple years old.</p>
<p>110<br>00:07:37,000 –&gt; 00:07:44,000<br>But Matt produced obviously was it was very influential, you know, maybe her dude made Spark.</p>
<p>111<br>00:07:44,000 –&gt; 00:07:48,000<br>Big table was clone, just eight basic human little hyper table and so forth.</p>
<p>112<br>00:07:48,000 –&gt; 00:07:51,000<br>Level DB was was this deal when they actually didn’t open the source.</p>
<p>113<br>00:07:51,000 –&gt; 00:07:55,000<br>And then got forked off as rocks DB box DB is more common now.</p>
<p>114<br>00:07:55,000 –&gt; 00:07:58,000<br>And then there’s other example, they almost down here.</p>
<p>115<br>00:07:58,000 –&gt; 00:08:03,000<br>And there’s a few other like for sellers from YouTube and there’s just the paper that’s not on source.</p>
<p>116<br>00:08:03,000 –&gt; 00:08:07,000<br>There’s a couple of systems like a list here that it sort of forgot or just having that in every room.</p>
<p>117<br>00:08:07,000 –&gt; 00:08:08,000<br>Yes.</p>
<p>118<br>00:08:08,000 –&gt; 00:08:12,000<br>Why is the we just look at Dremel and not the other ones actually look like others.</p>
<p>119<br>00:08:12,000 –&gt; 00:08:15,000<br>So why why are we looking at this one not these up?</p>
<p>120<br>00:08:16,000 –&gt; 00:08:21,000<br>Especially why we why we why do you care about Dremel in this class and not these other ones?</p>
<p>121<br>00:08:21,000 –&gt; 00:08:24,000<br>Because Dremel is the only one that’s doing analytics on the Olapp stuff.</p>
<p>122<br>00:08:24,000 –&gt; 00:08:29,000<br>Megastore was a early chart a version of my CO2 transactions.</p>
<p>123<br>00:08:29,000 –&gt; 00:08:31,000<br>The test is started my sequel out of YouTube.</p>
<p>124<br>00:08:31,000 –&gt; 00:08:33,000<br>Spanner I would just know about transaction.</p>
<p>125<br>00:08:33,000 –&gt; 00:08:34,000<br>These are all transactional ones.</p>
<p>126<br>00:08:34,000 –&gt; 00:08:39,000<br>Napa’s sort of getting into the realm of like doing analytics, but it does.</p>
<p>127<br>00:08:39,000 –&gt; 00:08:42,000<br>Well, we can talk a little bit maybe next class.</p>
<p>128<br>00:08:43,000 –&gt; 00:08:51,000<br>The context is like Delta Lake or iceberg, but Napa’s all about like doing Dremel style analytics on historical data,</p>
<p>129<br>00:08:51,000 –&gt; 00:08:54,000<br>but I’m also incorporating newly ingested data.</p>
<p>130<br>00:08:54,000 –&gt; 00:09:00,000<br>And I can make a trade off between how much do I’m going to read from the we only paid the cost of the road data.</p>
<p>131<br>00:09:00,000 –&gt; 00:09:04,000<br>I just just inserted versus like the historical data and then Delta Lake iceberg.</p>
<p>132<br>00:09:04,000 –&gt; 00:09:05,000<br>They’re all doing something.</p>
<p>133<br>00:09:05,000 –&gt; 00:09:06,000<br>Something’s in there.</p>
<p>134<br>00:09:06,000 –&gt; 00:09:07,000<br>Yeah.</p>
<p>135<br>00:09:07,000 –&gt; 00:09:11,000<br>But again, part of the reason I’m again, there’s a question why before using Dremel.</p>
<p>136<br>00:09:11,000 –&gt; 00:09:13,000<br>It’s a very influential paper.</p>
<p>137<br>00:09:13,000 –&gt; 00:09:18,000<br>This is pretty much how every lake house engines we built today.</p>
<p>138<br>00:09:18,000 –&gt; 00:09:19,000<br>Yes.</p>
<p>139<br>00:09:19,000 –&gt; 00:09:22,000<br>What do you mean only for market?</p>
<p>140<br>00:09:22,000 –&gt; 00:09:25,000<br>So I have an asterisk for Cochish DB and Title B on the market.</p>
<p>141<br>00:09:25,000 –&gt; 00:09:31,000<br>Because when these systems first came out, the Cochish DB guys certainly worked at Google, but they didn’t work on Spanner.</p>
<p>142<br>00:09:31,000 –&gt; 00:09:36,000<br>And then Title B gives nothing to do with Spanner.</p>
<p>143<br>00:09:36,000 –&gt; 00:09:39,000<br>Spanner was like the hot thing for transaction debuts.</p>
<p>144<br>00:09:39,000 –&gt; 00:09:50,000<br>So these guys were maybe they explicitly didn’t say it, but others are saying it and they didn’t correct them that these are open source variants of Spanner.</p>
<p>145<br>00:09:50,000 –&gt; 00:09:58,000<br>But the case of Cochish DB is and Title B is not the case because Spanner relies on this true time service with like GPS clocks and atomic clocks.</p>
<p>146<br>00:09:58,000 –&gt; 00:10:00,000<br>Part of the true time hardware service.</p>
<p>147<br>00:10:00,000 –&gt; 00:10:02,000<br>Cochish DB is doing everything in software.</p>
<p>148<br>00:10:02,000 –&gt; 00:10:13,000<br>So again, certainly now they’re not going to like say that they’re open source version of Spanner, but if you use Google open source Spanner, you probably get Cochish DB Title B.</p>
<p>149<br>00:10:13,000 –&gt; 00:10:14,000<br>Yes.</p>
<p>150<br>00:10:14,000 –&gt; 00:10:19,000<br>So he said that lots of like every link house is based on a lot of ideas from.</p>
<p>151<br>00:10:19,000 –&gt; 00:10:20,000<br>Yes.</p>
<p>152<br>00:10:20,000 –&gt; 00:10:26,000<br>I also know that data bridge is a cranny itself as the ones who are infected with them.</p>
<p>153<br>00:10:26,000 –&gt; 00:10:28,000<br>So wondering who you think is more important.</p>
<p>154<br>00:10:28,000 –&gt; 00:10:38,000<br>His question is like I’m saying that all these lake house systems are based on the high level architecture of Dremel, but then the data bridge guys they’re the one that branded the lake house term.</p>
<p>155<br>00:10:38,000 –&gt; 00:10:39,000<br>Right.</p>
<p>156<br>00:10:39,000 –&gt; 00:10:41,000<br>But that’s just marketing again.</p>
<p>157<br>00:10:41,000 –&gt; 00:10:42,000<br>Right.</p>
<p>158<br>00:10:42,000 –&gt; 00:10:44,000<br>The Trino guy say no, it’s not lake house.</p>
<p>159<br>00:10:44,000 –&gt; 00:10:45,000<br>You want an ice house.</p>
<p>160<br>00:10:45,000 –&gt; 00:10:46,000<br>Right.</p>
<p>161<br>00:10:46,000 –&gt; 00:10:55,000<br>So it’s just marketing, but the idea of a disaggregated storage, a vectorized execution engine that can read data that it’s never seen before.</p>
<p>162<br>00:10:55,000 –&gt; 00:11:00,000<br>All that is come from Dremel.</p>
<p>163<br>00:11:00,000 –&gt; 00:11:12,000<br>There’s other aspects though I think like, you know, it’s in case of snowflake like snowflake was doing the vector execution or actually more significantly like vector wise vector wise was doing the paper guys read all that vector execution stuff.</p>
<p>164<br>00:11:12,000 –&gt; 00:11:18,000<br>Like that’s prevalent and everything and that’s that’s common now in a lake house system, but they weren’t calling in a lake house back in the day.</p>
<p>165<br>00:11:18,000 –&gt; 00:11:20,000<br>It’s just a marketing term.</p>
<p>166<br>00:11:20,000 –&gt; 00:11:21,000<br>Yes.</p>
<p>167<br>00:11:21,000 –&gt; 00:11:31,000<br>So the original papers like 2011, right.</p>
<p>168<br>00:11:31,000 –&gt; 00:11:38,000<br>I think in the paper they say it came out it was like a side project in 2006 or something.</p>
<p>169<br>00:11:38,000 –&gt; 00:11:44,000<br>I think I’m putting the day here based on.</p>
<p>170<br>00:11:44,000 –&gt; 00:11:52,000<br>Yeah, some of the dates are inside of this like like this is one of his first known the pay to the paper you guys read is the 10 year rep perspective of the original paper.</p>
<p>171<br>00:11:52,000 –&gt; 00:12:00,000<br>The original paper is 2011, but I think that paper mentions that somebody was building in 2006 has originally shared nothing system.</p>
<p>172<br>00:12:00,000 –&gt; 00:12:01,000<br>Right.</p>
<p>173<br>00:12:01,000 –&gt; 00:12:06,000<br>It’s just like the test of time award you.</p>
<p>174<br>00:12:06,000 –&gt; 00:12:11,000<br>Yeah, so whatever I’m off by here, but again, even in the paper, he says they started earlier.</p>
<p>175<br>00:12:11,000 –&gt; 00:12:13,000<br>Yeah, okay.</p>
<p>176<br>00:12:13,000 –&gt; 00:12:14,000<br>Whatever.</p>
<p>177<br>00:12:14,000 –&gt; 00:12:25,000<br>Again, I, Napa, I know these guys are building this since 2017, 18, but I was under NDA and couldn’t say that but like the paper came out in 2021.</p>
<p>178<br>00:12:25,000 –&gt; 00:12:28,000<br>This is where the lake house is not.</p>
<p>179<br>00:12:28,000 –&gt; 00:12:29,000<br>Napa?</p>
<p>180<br>00:12:29,000 –&gt; 00:12:30,000<br>No, it’s.</p>
<p>181<br>00:12:30,000 –&gt; 00:12:31,000<br>It’s a gift.</p>
<p>182<br>00:12:31,000 –&gt; 00:12:33,000<br>You’re using it’s analytics and it does.</p>
<p>183<br>00:12:33,000 –&gt; 00:12:36,000<br>It does like you just, you won’t get the details.</p>
<p>184<br>00:12:36,000 –&gt; 00:12:39,000<br>You just data and it gets appended into the system.</p>
<p>185<br>00:12:39,000 –&gt; 00:12:42,000<br>I don’t know whether that ingestion process is just transactional not.</p>
<p>186<br>00:12:42,000 –&gt; 00:12:43,000<br>Okay.</p>
<p>187<br>00:12:43,000 –&gt; 00:12:55,000<br>And then they have this notion of like, do I care about when I run queries, I specify, do I want to run fastest possible and then maybe and then give up like reading the fresh data or do I give up?</p>
<p>188<br>00:12:55,000 –&gt; 00:13:00,000<br>Do I want to read the fresh data and pay them more extra money to get faster?</p>
<p>189<br>00:13:00,000 –&gt; 00:13:02,000<br>Like they have a gut, like a sort of.</p>
<p>190<br>00:13:02,000 –&gt; 00:13:08,000<br>You have the objective function like in terms of triangle not just cost and form but also like freshness.</p>
<p>191<br>00:13:08,000 –&gt; 00:13:09,000<br>Yeah.</p>
<p>192<br>00:13:09,000 –&gt; 00:13:13,000<br>Again, I mean, for this class, let’s focus on drumming.</p>
<p>193<br>00:13:13,000 –&gt; 00:13:19,000<br>They gave a talk a year or two ago during the pandemic with us about Napa.</p>
<p>194<br>00:13:19,000 –&gt; 00:13:21,000<br>It’s a good talk.</p>
<p>195<br>00:13:21,000 –&gt; 00:13:22,000<br>All right.</p>
<p>196<br>00:13:22,000 –&gt; 00:13:23,000<br>All right.</p>
<p>197<br>00:13:23,000 –&gt; 00:13:24,000<br>I already said 2006.</p>
<p>198<br>00:13:24,000 –&gt; 00:13:25,000<br>Right.</p>
<p>199<br>00:13:25,000 –&gt; 00:13:27,000<br>This was a side project.</p>
<p>200<br>00:13:27,000 –&gt; 00:13:31,000<br>I go with like 20% time, like one day a week, they were allowed to work on this.</p>
<p>201<br>00:13:31,000 –&gt; 00:13:45,000<br>And the idea, the original problem they were trying to solve was there’s all these artifacts being generated from different tools and services all throughout Google that is showing up on GFS and an internal file system.</p>
<p>202<br>00:13:45,000 –&gt; 00:13:54,000<br>And the idea was they want to be able to run queries on top of this data through SQL rather than writing this C++ map, reduced jobs.</p>
<p>203<br>00:13:54,000 –&gt; 00:14:01,000<br>Again, just going back to like the mid 2000s and Google saying that they don’t want to use SQL.</p>
<p>204<br>00:14:01,000 –&gt; 00:14:03,000<br>Everyone’s writing these map reduced jobs.</p>
<p>205<br>00:14:03,000 –&gt; 00:14:06,000<br>Hadoop, the open source version of map reduced was in Punjabah.</p>
<p>206<br>00:14:06,000 –&gt; 00:14:08,000<br>The Google version was all in C++.</p>
<p>207<br>00:14:08,000 –&gt; 00:14:14,000<br>So you have to write now C++ code to do scans and joins and data.</p>
<p>208<br>00:14:14,000 –&gt; 00:14:15,000<br>It’s terrible.</p>
<p>209<br>00:14:15,000 –&gt; 00:14:17,000<br>All right.</p>
<p>210<br>00:14:17,000 –&gt; 00:14:28,000<br>So the idea was they wanted to be able to just have a bunch of files sit around on disk, sorry, in shared storage and adjusted.</p>
<p>211<br>00:14:28,000 –&gt; 00:14:36,000<br>Although the first version actually was a shared nothing system, meaning like you had to ingest the data into the system and then it got in turn, you know, got cataloged.</p>
<p>212<br>00:14:36,000 –&gt; 00:14:51,000<br>And then the 2010 rewrite was, should we late 2000 or 2010s, they rewrote it and now be the disk I got to storage where just reading data directly off of Google files doesn’t work GFS.</p>
<p>213<br>00:14:51,000 –&gt; 00:14:55,000<br>And then this was the first paper came out and didn’t think 2010.</p>
<p>214<br>00:14:55,000 –&gt; 00:15:02,000<br>And then it was got commercialized in as BigQuery in 2012.</p>
<p>215<br>00:15:02,000 –&gt; 00:15:10,000<br>And the reason why I had you guys read the follow up paper rather than the original paper because the original paper doesn’t talk about the shuffle service, which this one does.</p>
<p>216<br>00:15:10,000 –&gt; 00:15:21,000<br>And that’s actually a key thing that separates BigQuery from other systems and allows them to do some interesting optimizations that other systems can’t can easily do.</p>
<p>217<br>00:15:21,000 –&gt; 00:15:28,000<br>Does anybody know what a drum is that’s kind of a database?</p>
<p>218<br>00:15:28,000 –&gt; 00:15:31,000<br>What is this hand gesture?</p>
<p>219<br>00:15:31,000 –&gt; 00:15:32,000<br>It’s a tool.</p>
<p>220<br>00:15:32,000 –&gt; 00:15:35,000<br>Yeah, it’s a tool, right? So there’s a footnote in the paper.</p>
<p>221<br>00:15:35,000 –&gt; 00:15:39,000<br>It’s a brand of power tools, primarily used of speed, a poster torque, right?</p>
<p>222<br>00:15:39,000 –&gt; 00:15:46,000<br>It’s more or less the minute or so. It’s just like a rotary drill, like a grinder and user things.</p>
<p>223<br>00:15:46,000 –&gt; 00:15:56,000<br>I’m always surprised that like their lawyers let them put out like a paper that says, hey, we have this, we have this internal service for a multi billion dollar company.</p>
<p>224<br>00:15:57,000 –&gt; 00:16:03,000<br>And we’ve named it after another company, right? That’s asking for a lawsuit, but they did it.</p>
<p>225<br>00:16:03,000 –&gt; 00:16:08,000<br>And again, but then the commercial version that they, they, they, they, you know, smartly renamed it as BigQuery.</p>
<p>226<br>00:16:08,000 –&gt; 00:16:17,000<br>So all the documentation you’ll see online for what drum was actually doing, you know, it’ll be called BigQuery, but for better reason the papers are still referred to as a drum.</p>
<p>227<br>00:16:17,000 –&gt; 00:16:22,000<br>All right, so this notion of in situ data processing, we’ve already covered this many times out of the semester.</p>
<p>228<br>00:16:23,000 –&gt; 00:16:32,000<br>It just means that I have a bunch of files that are sitting out in some storage that’s separate or not under the control of the database system.</p>
<p>229<br>00:16:32,000 –&gt; 00:16:38,000<br>And something else is going to be putting files there and people then want to run queries on top of them.</p>
<p>230<br>00:16:38,000 –&gt; 00:16:43,000<br>So obviously I need to be able to have a way to know what the files are in some kind of catalog, right?</p>
<p>231<br>00:16:43,000 –&gt; 00:16:47,000<br>And reference it to, you know, some table or some logical identifiers.</p>
<p>232<br>00:16:47,000 –&gt; 00:16:54,000<br>I say if you want to read this clutch and data or table food, whatever you want to call it, here’s the files where to go get it.</p>
<p>233<br>00:16:54,000 –&gt; 00:16:59,000<br>But other than that, the database system doesn’t necessarily need to know anything.</p>
<p>234<br>00:16:59,000 –&gt; 00:17:06,000<br>Now when we read snowflake next week, snowflake had what we call manage storage where you ingest data into the data system.</p>
<p>235<br>00:17:06,000 –&gt; 00:17:11,000<br>And then snowflake is responsible for deciding how to chop it up and where to store it and understand everything about it.</p>
<p>236<br>00:17:12,000 –&gt; 00:17:16,000<br>And the newer versions of snowflake now they have the support, you know, this link house architecture.</p>
<p>237<br>00:17:16,000 –&gt; 00:17:20,000<br>So they now support reading data from iceberg files.</p>
<p>238<br>00:17:20,000 –&gt; 00:17:23,000<br>Same thing with Redshift. They originally started out being shared nothing system.</p>
<p>239<br>00:17:23,000 –&gt; 00:17:27,000<br>Everything was all manage storage. Now with Athena, you can read files on S3.</p>
<p>240<br>00:17:27,000 –&gt; 00:17:28,000<br>Yes.</p>
<p>241<br>00:17:28,000 –&gt; 00:17:32,000<br>The snowflake may also charge the ETL cost between moving that into the app.</p>
<p>242<br>00:17:32,000 –&gt; 00:17:39,000<br>It’s question is to snowflake charge the ETL cost from getting data from like from from from from storage into prepared storage?</p>
<p>243<br>00:17:40,000 –&gt; 00:17:41,000<br>Sure they do, right?</p>
<p>244<br>00:17:41,000 –&gt; 00:17:46,000<br>Let me do a much more product because if they have extra steps that they have to do.</p>
<p>245<br>00:17:46,000 –&gt; 00:17:51,000<br>His question is, does it make it much work product that they actually have to do?</p>
<p>246<br>00:17:51,000 –&gt; 00:17:57,000<br>Well, the policy for that now, right?</p>
<p>247<br>00:17:57,000 –&gt; 00:18:01,000<br>Again, it’s not always like cost matters but performance matters.</p>
<p>248<br>00:18:01,000 –&gt; 00:18:03,000<br>There’s so many different factors to say like is it a bad product?</p>
<p>249<br>00:18:04,000 –&gt; 00:18:06,000<br>I’m not trying to be a cop out.</p>
<p>250<br>00:18:06,000 –&gt; 00:18:14,000<br>Like, you know, the corporate masters are just saying like, depending on different different scenarios, that may or may not be a good idea.</p>
<p>251<br>00:18:14,000 –&gt; 00:18:17,000<br>But the fact they do support it now is a good thing, right?</p>
<p>252<br>00:18:20,000 –&gt; 00:18:25,000<br>And as we’ve seen throughout the semester, all these systems, when they, you have a bunch of files in some format,</p>
<p>253<br>00:18:25,000 –&gt; 00:18:30,000<br>you know, we’ve seen this in the, the team is working on the, the caching server, the IO server is here.</p>
<p>254<br>00:18:31,000 –&gt; 00:18:34,000<br>You’re going to convert it into arrow or some other internal format, then process it anyway, right?</p>
<p>255<br>00:18:34,000 –&gt; 00:18:37,000<br>So, you know, who pays for that cost?</p>
<p>256<br>00:18:37,000 –&gt; 00:18:40,000<br>It depends on the pricing model.</p>
<p>257<br>00:18:41,000 –&gt; 00:18:50,000<br>So, again, this is, this is, this is the idea of Dremel, what they were trying to do was, you know, reading, eating files where they exist, right?</p>
<p>258<br>00:18:50,000 –&gt; 00:18:52,000<br>This is what we need my data lake or the lake house stuff.</p>
<p>259<br>00:18:52,000 –&gt; 00:18:55,000<br>Again, this is just a marketing term, but Dremel is doing it long ago.</p>
<p>260<br>00:18:56,000 –&gt; 00:19:02,000<br>And the paper they point out that one of the key reasons that they went with this, you know, trying to support this capability,</p>
<p>261<br>00:19:02,000 –&gt; 00:19:14,000<br>just reading files where they exist, is that it was better to have the, their users were willing to sacrifice performance of having like native, natively managed data.</p>
<p>262<br>00:19:16,000 –&gt; 00:19:24,000<br>They were rather sacrificed that performance in terms of the flexibility or the ease of use, meaning like, I don’t have to, you know, define a schema.</p>
<p>263<br>00:19:25,000 –&gt; 00:19:32,000<br>Then load a files into my schema, then run queries on it, right? Because that, there’s a human cost to that, like, you know, labor cost.</p>
<p>264<br>00:19:32,000 –&gt; 00:19:40,000<br>It’s rather, yeah, my queries are going to run a bit slower because I’m reading a bunch of files that maybe not being the best format for my data system, but that’s okay because I can just get to it really quickly.</p>
<p>265<br>00:19:41,000 –&gt; 00:19:47,000<br>And for my perspective, yes, I think this is the right trade-off, and then SQL is typically the right abstraction you would want to do this.</p>
<p>266<br>00:19:48,000 –&gt; 00:19:55,000<br>So for all the systems that we’re going to look at for the next few weeks, we’re going to need sort of this same kind of summary page like this.</p>
<p>267<br>00:19:55,000 –&gt; 00:20:01,000<br>We’re going to hit all the high-level aspects of the system as it relates to all the things we talked about throughout the semester.</p>
<p>268<br>00:20:01,000 –&gt; 00:20:08,000<br>So again, a lot of this is going to be cable sticks. There’s just things that you would expect in modern Lake House or O-Lat engine to be able to support.</p>
<p>269<br>00:20:09,000 –&gt; 00:20:16,000<br>So share disk aggregate storage. That’s to be expected, vectorized query processing, as we said, the papers aren’t going to say anything deep about it.</p>
<p>270<br>00:20:17,000 –&gt; 00:20:23,000<br>Other than I know that BigQuery is using Trenzix, because we asked them. The paper doesn’t say that though.</p>
<p>271<br>00:20:24,000 –&gt; 00:20:26,000<br>The shuffle-based distributed execution will get in a second.</p>
<p>272<br>00:20:29,000 –&gt; 00:20:35,000<br>Google is going to have their own proprietary format called Capacitor. We’ll see that in a second, although there’s not a lot of details about it.</p>
<p>273<br>00:20:36,000 –&gt; 00:20:41,000<br>But it’s basically going to look like Parking orc, and I’m sure people are generating Parking orc files internally at Google.</p>
<p>274<br>00:20:42,000 –&gt; 00:20:44,000<br>But for this common storage, they’re going to use all the tricks we talked about.</p>
<p>275<br>00:20:45,000 –&gt; 00:20:53,000<br>So zone maps, filters, dictionary and early compression. The only index they support in BigQuery, the service, is inverted search indexes.</p>
<p>276<br>00:20:54,000 –&gt; 00:20:59,000<br>Right to do like, you know, like in regular expression lookups on strings.</p>
<p>277<br>00:20:59,000 –&gt; 00:21:12,000<br>They’re all in the export hash joins, and then there’s a combination of a heuristic optimizer and a very light call-space optimizer when you have some statistics, and usually they don’t.</p>
<p>278<br>00:21:13,000 –&gt; 00:21:18,000<br>And they’re going to rely heavily on the ability to adapt the query plan while it’s running based on the data it sees.</p>
<p>279<br>00:21:20,000 –&gt; 00:21:24,000<br>So we’re saying those right time talking about this, because this is going to allow us to do things that we couldn’t easily before.</p>
<p>280<br>00:21:25,000 –&gt; 00:21:34,000<br>And this also is the transition to what we’ve been talking about in the entire semester, where we were talking about how do we build the single node execution in first, and then now start gluing it together.</p>
<p>281<br>00:21:35,000 –&gt; 00:21:38,000<br>And these systems, especially with this shovel, is one way to start gluing it together.</p>
<p>282<br>00:21:42,000 –&gt; 00:21:53,000<br>So when the query shows up, the data system is going to convert it into a logic plan and then divide that into stages, roughly correspond to pipelines, but not always necessarily.</p>
<p>283<br>00:21:55,000 –&gt; 00:22:00,000<br>And then within these stages, you’re going to have multiple parallel tasks that I’m going to distribute it across the workers.</p>
<p>284<br>00:22:01,000 –&gt; 00:22:15,000<br>And one key aspect of their query plan is that they need a guarantee that every task you would execute is going to be deterministic, meaning if I execute it over and over again with the same data, I should produce the exact same result.</p>
<p>285<br>00:22:16,000 –&gt; 00:22:23,000<br>And it’s going to be item potent, and this is going to allow them to have the ability to restart or kill a straggler or a task that’s running slow.</p>
<p>286<br>00:22:24,000 –&gt; 00:22:29,000<br>And then we execute on another task on another worker and be guaranteed to produce the same results.</p>
<p>287<br>00:22:30,000 –&gt; 00:22:42,000<br>So think of things like, if I have a random, call the random function in my query, I need a guarantee that no matter what worker I run on, when I invoke that random function, I get the same sequence of values.</p>
<p>288<br>00:22:43,000 –&gt; 00:22:45,000<br>Are times another one too?</p>
<p>289<br>00:22:45,000 –&gt; 00:22:54,000<br>Yes, deterministic in terms of how it’s going to run and also producing the same result.</p>
<p>290<br>00:22:55,000 –&gt; 00:22:56,000<br>So that’s item potent, yes.</p>
<p>291<br>00:22:58,000 –&gt; 00:23:05,000<br>So there’ll be a root node of the coordinator that’s going to be responsible for dispatching all the tasks.</p>
<p>292<br>00:23:06,000 –&gt; 00:23:11,000<br>They talk about having a centralized schedule, but the coordinator is sort of setting things up and then handing things off to the scheduler.</p>
<p>293<br>00:23:11,000 –&gt; 00:23:31,000<br>And what’s interesting they talk about, and we’ll see this, I think also in the snowflake paper as well, is that if you have all the workers going out to the catalog, the metadata, just sort of say, what’s the files that I need, when they start executing the task, then you could have thousands of workers all of a sudden flooding the catalog with all these requests.</p>
<p>294<br>00:23:31,000 –&gt; 00:23:41,000<br>So instead the root node is going to do a batch request to the catalog, be it all the metadata about the files is going to scan ahead of time, and then embed that nological plan.</p>
<p>295<br>00:23:42,000 –&gt; 00:23:48,000<br>So now when you hand the tasks off to the workers, they don’t have to do look at the catalog, they have everything they need to know of how to process the beginning.</p>
<p>296<br>00:23:50,000 –&gt; 00:24:00,000<br>Every worker at the node is going to have its own local memory and local disk, and then if they run out of memory while they’re processing that given task, they’ll be able to spill to that disk and spool it back in.</p>
<p>297<br>00:24:01,000 –&gt; 00:24:08,000<br>So this is going to be the same as needed, but then also we’ll see in a second they’re going to write out the results to a remote memory service.</p>
<p>298<br>00:24:09,000 –&gt; 00:24:18,000<br>So this is a really simple query plan, doing a look up to get the number of articles and Wikipedia with my name in it, or Pablo.</p>
<p>299<br>00:24:19,000 –&gt; 00:24:24,000<br>There’s some other asshole Greeks singer named Pablo too, and he might be in there.</p>
<p>300<br>00:24:24,000 –&gt; 00:24:29,000<br>I used to be when I was younger, like you just Google Pablo, I’d come up first, now the other guy is, but whatever.</p>
<p>301<br>00:24:29,000 –&gt; 00:24:32,000<br>So we have an distributive file system, a bunch of data we want to access.</p>
<p>302<br>00:24:33,000 –&gt; 00:24:37,000<br>In the first age of the corner it says, okay, I’m going to fire a bunch of these workers.</p>
<p>303<br>00:24:38,000 –&gt; 00:24:48,000<br>This is what I’m going to do on a partial group by, and then these workers are responsible for pulling the data that they need from the shared disk storage, and then doing what they’re processing on it.</p>
<p>304<br>00:24:49,000 –&gt; 00:24:55,000<br>And then now the output of these workers are not going to go to the next age of workers, instead they’re going to go to this in memory shuffle service.</p>
<p>305<br>00:24:55,000 –&gt; 00:25:05,000<br>So all of the worker nodes are going to be writing out their data to this, thinking this is like an in memory key value store that’s partitioned or scaled out horizontally.</p>
<p>306<br>00:25:06,000 –&gt; 00:25:12,000<br>So I can hash whatever the data I’m looking at and decide what, you know, send it to what shuffle node I need.</p>
<p>307<br>00:25:13,000 –&gt; 00:25:24,000<br>And then now the shuffle node can then send additional metadata about, here’s the data I saw for this first stage, for this query, to the coordinator, and the coordinator can slide on the fly, how many workers that it should use for the next stage.</p>
<p>308<br>00:25:25,000 –&gt; 00:25:30,000<br>And then it spins up, you know, it’s missed that request to the schedule that fires up these workers.</p>
<p>309<br>00:25:31,000 –&gt; 00:25:34,000<br>And then these guys are going to pull data from the memory shuffle.</p>
<p>310<br>00:25:35,000 –&gt; 00:25:42,000<br>So they’re not going to communicate from working worker to the next across the stages, they’re always going to use this in memory shuffle as an intermediary.</p>
<p>311<br>00:25:45,000 –&gt; 00:25:46,000<br>Yes.</p>
<p>312<br>00:25:46,000 –&gt; 00:25:47,000<br>Yes.</p>
<p>313<br>00:25:47,000 –&gt; 00:25:48,000<br>It’s a question.</p>
<p>314<br>00:25:48,000 –&gt; 00:25:49,000<br>Yes.</p>
<p>315<br>00:25:49,000 –&gt; 00:25:51,000<br>Is the in memory shuffle a single node thing or it’s like this very quick?</p>
<p>316<br>00:25:51,000 –&gt; 00:25:54,000<br>Is it single node or scaled out?</p>
<p>317<br>00:25:54,000 –&gt; 00:25:55,000<br>Scaled out.</p>
<p>318<br>00:25:55,000 –&gt; 00:25:56,000<br>Scale out.</p>
<p>319<br>00:25:56,000 –&gt; 00:26:00,000<br>Why not just make them like map or do you stuff it?</p>
<p>320<br>00:26:00,000 –&gt; 00:26:03,000<br>Which they, you can talk like workers talk to each other.</p>
<p>321<br>00:26:03,000 –&gt; 00:26:05,000<br>These questions, why am I doing this?</p>
<p>322<br>00:26:05,000 –&gt; 00:26:13,000<br>Like, why I had this extra step to go to this piece versus having to have the worker just pull the data from the worker itself?</p>
<p>323<br>00:26:13,000 –&gt; 00:26:23,000<br>Right? There’s performance implications of like, if now we’re thought, if I can kill all these guys and then reason the test for other things,</p>
<p>324<br>00:26:23,000 –&gt; 00:26:26,000<br>and this thing is just maintaining the data.</p>
<p>325<br>00:26:26,000 –&gt; 00:26:31,000<br>And then, you know, otherwise I got to keep this thing around so that make sure they get all the data that they need.</p>
<p>326<br>00:26:31,000 –&gt; 00:26:38,000<br>Because what happens if like, say one of these guys down, go down, then I got to go back to the previous worker and get the data again.</p>
<p>327<br>00:26:39,000 –&gt; 00:26:46,000<br>And then, as I said, we’ll see in a second, this having to step and then like get all the data I need from this first stage,</p>
<p>328<br>00:26:46,000 –&gt; 00:26:48,000<br>then I can decide what to do in the next stage.</p>
<p>329<br>00:26:48,000 –&gt; 00:26:50,000<br>Because I’ve seen the data.</p>
<p>330<br>00:26:50,000 –&gt; 00:26:54,000<br>Because I now have it in a sort of central location that I can pass along to the coordinator.</p>
<p>331<br>00:26:54,000 –&gt; 00:26:58,000<br>Does it live for longer than just one cycle?</p>
<p>332<br>00:26:58,000 –&gt; 00:27:01,000<br>Like, do you keep that memory for a very long time?</p>
<p>333<br>00:27:01,000 –&gt; 00:27:05,000<br>The question is, do I keep this memory around for a very long time?</p>
<p>334<br>00:27:05,000 –&gt; 00:27:09,000<br>What the content of the memory or the service itself?</p>
<p>335<br>00:27:09,000 –&gt; 00:27:11,000<br>The service is always running.</p>
<p>336<br>00:27:11,000 –&gt; 00:27:17,000<br>Right, but I mean the context of memory in a sense that, so we’re waiting for all of those three workers to finish, right?</p>
<p>337<br>00:27:17,000 –&gt; 00:27:21,000<br>So that we can then erase that in memory shop, where we have them there.</p>
<p>338<br>00:27:21,000 –&gt; 00:27:23,000<br>So when you get to the next stage?</p>
<p>339<br>00:27:23,000 –&gt; 00:27:25,000<br>Yes, and what point is this going away?</p>
<p>340<br>00:27:25,000 –&gt; 00:27:26,000<br>Yeah.</p>
<p>341<br>00:27:26,000 –&gt; 00:27:30,000<br>The coordinator would come back and say, all right, I’ve completed this stage, everybody’s got the data.</p>
<p>342<br>00:27:30,000 –&gt; 00:27:33,000<br>It’s still need to get past this stage, right?</p>
<p>343<br>00:27:33,000 –&gt; 00:27:36,000<br>Because again, these guys could crash and you need to go fetch it again.</p>
<p>344<br>00:27:36,000 –&gt; 00:27:40,000<br>But once you know that nobody else is going to go back to the data you need, you can leak them below their way.</p>
<p>345<br>00:27:40,000 –&gt; 00:27:50,000<br>No, that needs to be like a shift on the memory and also, like the worker can die, but this can die too, right?</p>
<p>346<br>00:27:50,000 –&gt; 00:27:54,000<br>But it’s just a key value store. There’s no techniques to replicate some scale itself, right?</p>
<p>347<br>00:27:54,000 –&gt; 00:27:56,000<br>It’s even crazier.</p>
<p>348<br>00:27:56,000 –&gt; 00:27:59,000<br>They actually fab custom hardware to make this go as fast as possible.</p>
<p>349<br>00:27:59,000 –&gt; 00:28:00,000<br>Oh.</p>
<p>350<br>00:28:00,000 –&gt; 00:28:01,000<br>Yeah.</p>
<p>351<br>00:28:01,000 –&gt; 00:28:02,000<br>It’s awesome.</p>
<p>352<br>00:28:02,000 –&gt; 00:28:03,000<br>Yes.</p>
<p>353<br>00:28:03,000 –&gt; 00:28:08,000<br>Why not just keep track of the method that actually changes the data and the data.</p>
<p>354<br>00:28:08,000 –&gt; 00:28:12,000<br>The question is, why not keep track of metadata rather than stream all the data here?</p>
<p>355<br>00:28:12,000 –&gt; 00:28:13,000<br>But where do you keep in the data?</p>
<p>356<br>00:28:13,000 –&gt; 00:28:15,000<br>Here, right?</p>
<p>357<br>00:28:15,000 –&gt; 00:28:21,000<br>And these guys can’t go away until you’ve gotten the data over there.</p>
<p>358<br>00:28:21,000 –&gt; 00:28:28,000<br>So in macro music, I get worse that you can get data getting much to this, because that speed of media results might be too big in size.</p>
<p>359<br>00:28:28,000 –&gt; 00:28:29,000<br>Yep.</p>
<p>360<br>00:28:29,000 –&gt; 00:28:34,000<br>What you’re doing as you may know that the memory is like this big enough to show all that.</p>
<p>361<br>00:28:34,000 –&gt; 00:28:37,000<br>And then, isn’t that what Spark also like does?</p>
<p>362<br>00:28:37,000 –&gt; 00:28:39,000<br>Like, is that information about view of Spark?</p>
<p>363<br>00:28:39,000 –&gt; 00:28:40,000<br>So there’s two things.</p>
<p>364<br>00:28:40,000 –&gt; 00:28:47,000<br>One is in Hadoop, the right to local disk here, because you might run a memory, whereas in this case here,</p>
<p>365<br>00:28:47,000 –&gt; 00:28:50,000<br>it’s just so massive that you’re not going to run out of memory.</p>
<p>366<br>00:28:50,000 –&gt; 00:28:53,000<br>This thing you can spell to disk too, we’ll see in a second, right?</p>
<p>367<br>00:28:54,000 –&gt; 00:28:58,000<br>And actually, it’ll spell to GFS or Colossus, the Google file system.</p>
<p>368<br>00:28:58,000 –&gt; 00:29:02,000<br>And then your second comment is, isn’t Spark is doing?</p>
<p>369<br>00:29:02,000 –&gt; 00:29:08,000<br>Spark still, I think, maintains the shuffle data on the worker nodes.</p>
<p>370<br>00:29:08,000 –&gt; 00:29:10,000<br>Yeah, it actually does the same thing in the map.</p>
<p>371<br>00:29:10,000 –&gt; 00:29:13,000<br>You just, the in and everything happens after station faces.</p>
<p>372<br>00:29:13,000 –&gt; 00:29:15,000<br>Because my purpose is do something like that.</p>
<p>373<br>00:29:15,000 –&gt; 00:29:19,000<br>Where it seems to be happening between maps, which is…</p>
<p>374<br>00:29:19,000 –&gt; 00:29:22,000<br>So this is not a…</p>
<p>375<br>00:29:22,000 –&gt; 00:29:29,000<br>Like, Dremel or even Matt Perdue’s who didn’t have this idea of this like shuffle step.</p>
<p>376<br>00:29:29,000 –&gt; 00:29:30,000<br>That’s the distributed data.</p>
<p>377<br>00:29:30,000 –&gt; 00:29:33,000<br>This is parallel to this from the 80s and 90s, right?</p>
<p>378<br>00:29:33,000 –&gt; 00:29:39,000<br>Which unique about BigQuery and Dremel is that they explicitly do this from every stage.</p>
<p>379<br>00:29:39,000 –&gt; 00:29:41,000<br>Snowflake does have a shuffle.</p>
<p>380<br>00:29:41,000 –&gt; 00:29:45,000<br>You can do shuffles as well, but they only use it as needed.</p>
<p>381<br>00:29:45,000 –&gt; 00:29:48,000<br>They do this for everything.</p>
<p>382<br>00:29:49,000 –&gt; 00:29:50,000<br>Yes.</p>
<p>383<br>00:29:50,000 –&gt; 00:29:52,000<br>Are there disadvantages to always doing this?</p>
<p>384<br>00:29:52,000 –&gt; 00:29:54,000<br>Like, what if you want the data on the same thing?</p>
<p>385<br>00:29:54,000 –&gt; 00:29:56,000<br>The question is, are there disadvantages of doing this?</p>
<p>386<br>00:29:56,000 –&gt; 00:29:58,000<br>What if you always want data on the same worker?</p>
<p>387<br>00:29:58,000 –&gt; 00:29:59,000<br>Ah, okay.</p>
<p>388<br>00:29:59,000 –&gt; 00:30:01,000<br>So this is what I’ve been saying before.</p>
<p>389<br>00:30:01,000 –&gt; 00:30:05,000<br>I think I said back here, the…</p>
<p>390<br>00:30:05,000 –&gt; 00:30:09,000<br>The call stages, they’re not always pipeline breakers though.</p>
<p>391<br>00:30:09,000 –&gt; 00:30:15,000<br>In some cases, you can have the second stage kick off</p>
<p>392<br>00:30:15,000 –&gt; 00:30:18,000<br>while this stage is still running, right?</p>
<p>393<br>00:30:18,000 –&gt; 00:30:21,000<br>And you can start processing ahead of time.</p>
<p>394<br>00:30:21,000 –&gt; 00:30:22,000<br>Right?</p>
<p>395<br>00:30:22,000 –&gt; 00:30:26,000<br>So that’s one advantage there that you could start doing this.</p>
<p>396<br>00:30:26,000 –&gt; 00:30:30,000<br>You could have this thing get fired up and certain reading the data before these guys even finish.</p>
<p>397<br>00:30:30,000 –&gt; 00:30:31,000<br>Right?</p>
<p>398<br>00:30:31,000 –&gt; 00:30:35,000<br>From a software engineering standpoint, also too, that now you no longer have to…</p>
<p>399<br>00:30:35,000 –&gt; 00:30:42,000<br>In bed logic of how to like scale up or scale down or do other organizations will see in a second,</p>
<p>400<br>00:30:42,000 –&gt; 00:30:45,000<br>at all your workers.</p>
<p>401<br>00:30:45,000 –&gt; 00:30:47,000<br>Because now, it’s just like…</p>
<p>402<br>00:30:47,000 –&gt; 00:30:51,000<br>It’s just a coordinator says, okay, I need more workers do this way or move the data here and there.</p>
<p>403<br>00:30:51,000 –&gt; 00:30:56,000<br>And from a software engineering perspective, like the worker implementation is much more simple now.</p>
<p>404<br>00:31:00,000 –&gt; 00:31:05,000<br>Right? So again, at the end, the last stage you’re doing certain limit, one worker can handle that.</p>
<p>405<br>00:31:05,000 –&gt; 00:31:09,000<br>As far as I know, these are just like containers and running in Bork,</p>
<p>406<br>00:31:09,000 –&gt; 00:31:12,000<br>which is the precursor to Kubernetes.</p>
<p>407<br>00:31:12,000 –&gt; 00:31:14,000<br>Right? So there’s meant to be stateless.</p>
<p>408<br>00:31:14,000 –&gt; 00:31:17,000<br>So these things can get Kaelin swapped out at any time.</p>
<p>409<br>00:31:17,000 –&gt; 00:31:22,000<br>Is that why they’re doing the memory shuffle because they’re supposed to be stateless and they don’t want to keep them alive?</p>
<p>410<br>00:31:22,000 –&gt; 00:31:24,000<br>Something stateless, alive?</p>
<p>411<br>00:31:24,000 –&gt; 00:31:29,000<br>It’s a question. Is that why they’re doing the memory shuffle because these things are stateless and don’t keep them alive?</p>
<p>412<br>00:31:29,000 –&gt; 00:31:32,000<br>Potential. I mean, it’s one of the ideas, yes.</p>
<p>413<br>00:31:32,000 –&gt; 00:31:38,000<br>But again, there’s database query plan advantages</p>
<p>414<br>00:31:38,000 –&gt; 00:31:43,000<br>that we can leverage if we have this extra stage. We’ll see in a second.</p>
<p>415<br>00:31:43,000 –&gt; 00:31:48,000<br>So the shuffle, it’s basically a producer consumer model.</p>
<p>416<br>00:31:48,000 –&gt; 00:31:52,000<br>It’s just a way to send the image results from one stage to the next.</p>
<p>417<br>00:31:52,000 –&gt; 00:32:01,000<br>Using this dedicated service, and then saying this in the paper talks about like this MRE service is used not just for Dremel.</p>
<p>418<br>00:32:01,000 –&gt; 00:32:06,000<br>I think Dremel is the main consumer of this service.</p>
<p>419<br>00:32:06,000 –&gt; 00:32:11,000<br>It’s used in other services within Google as well.</p>
<p>420<br>00:32:11,000 –&gt; 00:32:13,000<br>Right? So again, the workers just send their app into the shuffle nodes.</p>
<p>421<br>00:32:13,000 –&gt; 00:32:21,000<br>And then if the shuffle nodes get run out of space, they can spill to GFS if necessary.</p>
<p>422<br>00:32:21,000 –&gt; 00:32:30,000<br>And then the workers in the next stage is because they get, get, get, and get more data from the shuffle nodes.</p>
<p>423<br>00:32:30,000 –&gt; 00:32:36,000<br>Right? So in this case here, say that all the workers are consuming data from the previous stage.</p>
<p>424<br>00:32:36,000 –&gt; 00:32:43,000<br>In this case here, it could be from the distributed file system, the data reading, or could be from the shuffle service itself.</p>
<p>425<br>00:32:43,000 –&gt; 00:32:49,000<br>And then they’re processing the data and they’re doing end-wave traffic on the outside going out.</p>
<p>426<br>00:32:49,000 –&gt; 00:32:53,000<br>And if I run a memory, I can always spill to the distributed file system.</p>
<p>427<br>00:32:53,000 –&gt; 00:32:59,000<br>And then another key advantage of why you want to do this is that I no longer have to do sort of the end-to-end community service.</p>
<p>428<br>00:32:59,000 –&gt; 00:33:03,000<br>The end-to-end communication or end-to-end communication between the one stage to the next.</p>
<p>429<br>00:33:03,000 –&gt; 00:33:12,000<br>Because this data is going to be partitioned, that I only need to send, or get the data from a subset of the workers rather than sending it to all the possible workers.</p>
<p>430<br>00:33:12,000 –&gt; 00:33:19,000<br>Right? So without the shuffle service, without having to know exactly what, because the coordinator is going to tell us, here’s the data you need,</p>
<p>431<br>00:33:19,000 –&gt; 00:33:25,000<br>here’s the shuffle node to go get it from. Without that, potentially I have to pull all these guys and say, do you have any data that I could be consuming?</p>
<p>432<br>00:33:25,000 –&gt; 00:33:32,000<br>So from that perspective, this is way more efficient in terms of communication traffic.</p>
<p>433<br>00:33:32,000 –&gt; 00:33:44,000<br>And then I think also too they can pull from the distributed file system rather than having to go and get it from the shuffle service if it any gets filled with the disk.</p>
<p>434<br>00:33:45,000 –&gt; 00:33:57,000<br>So the shuffle is basically just like a checkpoint in the query plan. And so this part is actually unique to Dremel, because historically parallel to distributed databases didn’t do checkpoints,</p>
<p>435<br>00:33:57,000 –&gt; 00:34:08,000<br>and they weren’t fault-tolerant within the query itself, meaning if I had a two-hour query that was going to run, and one node happens to die partway through, then the whole query dies. And I got to restart.</p>
<p>436<br>00:34:09,000 –&gt; 00:34:21,000<br>From the database systems perspective, the disk was so slow that it was just not worth it to do right now, animate results.</p>
<p>437<br>00:34:21,000 –&gt; 00:34:30,000<br>Whereas Hadoop, as he mentioned, was doing that between every shuffle, was always writing things at the local disk, and then replicating out things on an A.C.F.S.</p>
<p>438<br>00:34:30,000 –&gt; 00:34:38,000<br>And that was really slow, because that was Google’s model of like, okay, we’re running on cheap pizza box machines that could, you know, thousands of machines that could die any time.</p>
<p>439<br>00:34:38,000 –&gt; 00:34:50,000<br>Whereas from the parallel database system perspective, it was better to design the system, assuming you’re running on, you know, not a thousand dollar rack machines, but like high M machines that aren’t going to crash that often.</p>
<p>440<br>00:34:50,000 –&gt; 00:34:57,000<br>And that, because you get better performance, but you’re not fault-tolerant to, if one of those nodes go down.</p>
<p>441<br>00:34:57,000 –&gt; 00:35:08,000<br>So the in-memory service allows them to get that fault-tolerant by taking a checkpoint between the different stages of the query plan, but because it’s an in-memory service, it’s in memory, it’s not really as slow as writing to disk.</p>
<p>442<br>00:35:08,000 –&gt; 00:35:19,000<br>Now with NDE drives, maybe less of an issue, because disk got really, really fast, but, you know, 10 years ago, that, you know, this obviously was a big concern.</p>
<p>443<br>00:35:20,000 –&gt; 00:35:34,000<br>So you get fault times, because at any time of a, of a node crashes, you know, you just get the data that you need from the in-memory shuffle and run the task on another service, because it had a potent, you can run it again without any side effects.</p>
<p>444<br>00:35:35,000 –&gt; 00:35:49,000<br>If a, you know, task is running too slow, because the worker node is for whatever reason slow. The big query guys told me that one big problem they faced is sometimes they’ll land on to run a query on a node that where another container is like doing encoding for YouTube.</p>
<p>445<br>00:35:49,000 –&gt; 00:35:58,000<br>And they can look at the traffic and actually know it’s YouTube, and that’s slowed down the query. So if you have a straggler, then you can go ahead and just kill it and assign the task to another worker that can run faster.</p>
<p>446<br>00:35:59,000 –&gt; 00:36:16,000<br>And then also to, as it’s so related to, it’s gonna allow, because we have this explicit stage, like, okay, we can take a step back, look what that query has done so far, look what the data looks like, and then decide at the next stage, we need to scale up or scale down the number of workers we need to process the query.</p>
<p>447<br>00:36:17,000 –&gt; 00:36:27,000<br>So look at both of these examples here. So the workers are running, they’re producing data that they’re sending to the shuffle nodes, and say for whatever reason this node is just falling behind, it can’t keep up.</p>
<p>448<br>00:36:27,000 –&gt; 00:36:43,000<br>So if we can start to go ahead and kill it, and it’s reassigned the task to this other worker here, who again is just getting the data either from the, from the distributed file system, it’ll be there or from the shuffle service, which again will always be there.</p>
<p>449<br>00:36:44,000 –&gt; 00:37:00,000<br>And then once I collect all my data in my, in my, in my storage, sorry, in the shuffle storage, I post some information to the coordinator, can look at the specifics of what the data actually looks like, and then decide based on what the SLI requirements are for the query.</p>
<p>450<br>00:37:01,000 –&gt; 00:37:15,000<br>Do I have too many workers or not enough workers, and then I can, you know, if I want to, I can regenerate, add more, and then I don’t have to move any of those data around, I just reassigned what workers going to mean, what data from the shuffle service.</p>
<p>451<br>00:37:16,000 –&gt; 00:37:17,000<br>Yes.</p>
<p>452<br>00:37:24,000 –&gt; 00:37:34,000<br>So checkpoint meaning, yeah, it’s very clear. Yes. So it’s not a checkpoint in that we think about in like the intro Davis class where I’m taking all the contents of memory, I’m writing at the desk.</p>
<p>453<br>00:37:35,000 –&gt; 00:37:48,000<br>I think it’s like, it’s a, I don’t want to do word staging point because these are already current stages, but it’s like a, it’s a pause is also not the right word because it’s not like you’re stopping anything, but it’s a, it’s not the word for checkpoint.</p>
<p>454<br>00:37:48,000 –&gt; 00:37:53,000<br>Say point, say point is there’s some explicit either. It’s hard to say again.</p>
<p>455<br>00:37:53,000 –&gt; 00:38:18,000<br>It’s more from a logistical standpoint that like I can, before I start executing the next stage, I can decide do I need to re change my query plan or change my topology of the query plan or the number of workers I have in the subsequent stages because I’ve seen the data that got generated from the previous stage.</p>
<p>456<br>00:38:19,000 –&gt; 00:38:26,000<br>Yeah, so checkpoint, I don’t mean that like everything in here always gets written the desk because they want to keep things in memory as possible.</p>
<p>457<br>00:38:26,000 –&gt; 00:38:33,000<br>In the intermediate results, who cares about like I don’t need the data beyond the query I’m actually trying to run right now.</p>
<p>458<br>00:38:33,000 –&gt; 00:38:42,000<br>There are some papers about how we use data structures from 20 to the next sort of like a mini materialized view like a hash table for a hash table for a join, kind of keep that hash table around from one query to the next.</p>
<p>459<br>00:38:42,000 –&gt; 00:38:50,000<br>They’re not doing that literally is just like I get all my data in this location. I can then have a global view of what’s going on and decide where to go next.</p>
<p>460<br>00:38:50,000 –&gt; 00:38:53,000<br>What to do next. Yes.</p>
<p>461<br>00:38:53,000 –&gt; 00:38:58,000<br>It’s question is different shuffle notes stored where?</p>
<p>462<br>00:38:58,000 –&gt; 00:39:07,000<br>This is just in memory. In memory hash table.</p>
<p>463<br>00:39:07,000 –&gt; 00:39:15,000<br>Oh, yeah, it’s questions like are these yes, so think of this as like.</p>
<p>464<br>00:39:15,000 –&gt; 00:39:30,000<br>Like I produce some work. I put my task process some data and then I have a key on the data I hash and I mod by the number of.</p>
<p>465<br>00:39:30,000 –&gt; 00:39:38,000<br>Yeah, it’s just a thing of like a consistent hash table.</p>
<p>466<br>00:39:38,000 –&gt; 00:39:48,000<br>Yes. This is a police ever service. This is a colossus GFS. This is this is their S3.</p>
<p>467<br>00:39:48,000 –&gt; 00:39:53,000<br>I guess like from the perspective of the work they don’t care.</p>
<p>468<br>00:39:53,000 –&gt; 00:39:58,000<br>I mean, you care about it’s in memory because you want to get handed to the person is quickly possible whoever asked for.</p>
<p>469<br>00:39:58,000 –&gt; 00:40:03,000<br>That’s the whole point of like these things. These are large memory machines.</p>
<p>470<br>00:40:03,000 –&gt; 00:40:07,000<br>This is this is Google’s not S3.</p>
<p>471<br>00:40:07,000 –&gt; 00:40:10,000<br>The lady said it’s an other store.</p>
<p>472<br>00:40:10,000 –&gt; 00:40:16,000<br>It’s an other store.</p>
<p>473<br>00:40:16,000 –&gt; 00:40:22,000<br>I thought I wanted to person. So many say S3 is at least the metadata is built on my SQL.</p>
<p>474<br>00:40:22,000 –&gt; 00:40:28,000<br>But again, that’s for this we don’t we don’t care.</p>
<p>475<br>00:40:28,000 –&gt; 00:40:42,000<br>Okay. So I keep alluding to it. Okay. Now that we have this staging area in the memory shuffle store and we’re now in</p>
<p>476<br>00:40:42,000 –&gt; 00:40:48,000<br>collect statistics about what the date they got we’ve gotten from the from the from the previous stage.</p>
<p>477<br>00:40:48,000 –&gt; 00:40:54,000<br>We start making decisions about what we want to do. But in the very very beginning obviously we don’t have any of that information.</p>
<p>478<br>00:40:54,000 –&gt; 00:40:58,000<br>Because we did some a bunch of files we may may not ever scan before.</p>
<p>479<br>00:40:58,000 –&gt; 00:41:07,000<br>I think the paper even said it says like a large percentage of the data that the Dremel queries are processing are files of the data is never seen before.</p>
<p>480<br>00:41:07,000 –&gt; 00:41:14,000<br>So there’s no statistics. So how can we actually try to generate optimal query plan without any of this.</p>
<p>481<br>00:41:14,000 –&gt; 00:41:21,000<br>They also talk about the ability to do queries against other data sources or data database systems.</p>
<p>482<br>00:41:21,000 –&gt; 00:41:32,000<br>This is oftentimes called connectors and we’ll see this in other system as we go along. But like the idea is that I have a single logical view within BigQuery Dremel to a bunch of different disparate database systems.</p>
<p>483<br>00:41:32,000 –&gt; 00:41:44,000<br>And now when I run my query I can say go read this postgres table. And then the system is responsible for then writing the the corresponding query to go against postgres and get the data that it needs.</p>
<p>484<br>00:41:44,000 –&gt; 00:41:52,000<br>But at that point if we’re running a query on another day you know our query gets generated to it gets converted to another query that runs another system.</p>
<p>485<br>00:41:52,000 –&gt; 00:42:01,000<br>We have no statistics. We have nothing. Right. The worst case scenario we we do like a select star gets some other table and then do processing what once we get into our system.</p>
<p>486<br>00:42:01,000 –&gt; 00:42:10,000<br>That’s case scenario we can do some kind of predicate push down to the other system. But again at this point again you don’t have any stats.</p>
<p>487<br>00:42:10,000 –&gt; 00:42:33,000<br>So the way Dremel’s new query optimization is a stratified approach with a rule based optimizer and a cost based optimizer that only does basic analysis of the cost based one only does basic analysis on the data trying to access if you have actually some information already about it.</p>
<p>488<br>00:42:33,000 –&gt; 00:42:43,000<br>So for the rules that’s all the classic stuff we talked about doing predicate push down primary key funky hints some very basic join ordering.</p>
<p>489<br>00:42:43,000 –&gt; 00:42:46,000<br>They have custom rules to do.</p>
<p>490<br>00:42:46,000 –&gt; 00:42:56,000<br>To constraint propagation for starts chemists like you could propagate the maybe the constraints from a dimension table into a fact table.</p>
<p>491<br>00:42:56,000 –&gt; 00:43:05,000<br>Or like if you’re doing you know if it’s a skin if the system will detect it if you have a star schema a fact table with all the dimension tables.</p>
<p>492<br>00:43:05,000 –&gt; 00:43:18,000<br>Then when it generates the the stages it knows to always generate the hash tables build the hash tables and the mention tables and then have this single pipeline when you write up the fact table all the way up and do probes and all these hash tables.</p>
<p>493<br>00:43:18,000 –&gt; 00:43:30,000<br>So they have basic rules to check these things but then they only trigger the cost based analysis and optimizations if you have some stats which they only generate if you have a materialized view.</p>
<p>494<br>00:43:30,000 –&gt; 00:43:38,000<br>But most of the queries are not materialize use or not the common case they have to deal with not having any statistics.</p>
<p>495<br>00:43:38,000 –&gt; 00:43:55,000<br>So instead to avoid any kind of bad cost model estimates that we saw before they’re going to apply adaptive query optimization techniques and they’re going to do this relying on that shuffle stage as a way to say okay stop the political is going on and then recalibrate as needed.</p>
<p>496<br>00:43:55,000 –&gt; 00:44:03,000<br>So we’ll see various other techniques that are used to do that adapt to query optimization in snowflake and Databricks and in so forth.</p>
<p>497<br>00:44:03,000 –&gt; 00:44:10,000<br>They’re not going to be as aggressive as or all encompassing the papers that we discussed.</p>
<p>498<br>00:44:10,000 –&gt; 00:44:17,000<br>They’re not doing plan stitching but they’re also not embedding those sort of trigger plan nodes that aside it go this query plan versus the other.</p>
<p>499<br>00:44:17,000 –&gt; 00:44:35,000<br>They’re going to be more like change the number of workers that they have and maybe change the what joint algorithm want to use based on the data that’s seen but not like recalibrate reorganize the entire query plan.</p>
<p>500<br>00:44:35,000 –&gt; 00:44:46,000<br>So when we can do this because we have the shuffle staging point where we can look at the data collected and the idea is in one of fixed things as we go along.</p>
<p>501<br>00:44:46,000 –&gt; 00:45:03,000<br>So obviously we’ve already seen how to change the number of workers in a stage like if I recognize that the data showing up is much larger smaller than I anticipated because maybe there’s a very selective filter that I didn’t anticipate was going to throw a most of the data then I can scale down the number of workers that have the next stage.</p>
<p>502<br>00:45:03,000 –&gt; 00:45:09,000<br>So you can decide whether you want to do a shuffle or a broadcast join based on the data that shows up in the shuffle phase we’ll see that in a second.</p>
<p>503<br>00:45:09,000 –&gt; 00:45:22,000<br>I also can change they have to go out you can change what the implementation of the operator you’re going to use this one I don’t fully understand what they because they don’t talk about the paper but like they have notions of like I have a</p>
<p>504<br>00:45:22,000 –&gt; 00:45:32,000<br>operator implementation for small partitions or large partitions I’m guessing things like on Roman loops and things like that if you know you’re only going to read small number data in each partition.</p>
<p>505<br>00:45:32,000 –&gt; 00:45:45,000<br>And then I’m going to partition is a way to just maybe split data up more if you have a hot bucket again based on the data seen I’ll go through I’ll go through these two examples.</p>
<p>506<br>00:45:45,000 –&gt; 00:46:04,000<br>So say I have a query here that’s going to read data from table A table B and I want to do a join so in the very first stage I have a bunch of workers read data from table A and a bunch of worker from table B from data from table B and again maybe there’s some filter I push down in these workers that start printing data.</p>
<p>507<br>00:46:04,000 –&gt; 00:46:24,000<br>And then the these workers are starting submitting all the results from these scans into the shuffle stage and if you think of like internally which is build a you know a history and our counter says how much data we put into for the partitions at these two tables and say for whatever reason table A is much smaller than the anticipated.</p>
<p>508<br>00:46:25,000 –&gt; 00:46:40,000<br>So in that case here you maybe we don’t want to actually do the shuffle join were just were repartitioning the data on the join key we could recognize that this data is actually small small to fit on every single node and we can change what join one use.</p>
<p>509<br>00:46:40,000 –&gt; 00:46:52,000<br>So again say the original idea was that we were going to do a hash the data and then send it to individual workers they’re more they’re pulling from this but I’m sharing the arrow of like here’s the data flow.</p>
<p>510<br>00:46:53,000 –&gt; 00:47:15,000<br>But again if this thing is super small then I can just change it to a broadcast join where now the the workers will every worker will go get the entire contents of table A from the shuffle service and then I still do the shuffle on B partition it up but now when I join B with a I have all the data I need to do the join locally.</p>
<p>511<br>00:47:16,000 –&gt; 00:47:35,000<br>The other choice is to do a dominant partition so say that I have I’m scanning my data right and say again for whatever reason partition one is much smaller than I can anticipate sorry take partition to is much larger than anticipated so this thing is going to spill the disk and that’s going to be slow.</p>
<p>512<br>00:47:35,000 –&gt; 00:47:46,000<br>So what I can do is you know as I’m running I’m passing statistics to the coordinator and then I can say okay great two new partitions that’s my biological daughter.</p>
<p>513<br>00:47:46,000 –&gt; 00:47:49,000<br>Sorry.</p>
<p>514<br>00:47:49,000 –&gt; 00:48:04,000<br>So the coordinator says okay well this partition is going to run out of space so now go send a message to the worker and say all right anything you’re going to partition to send a partition to hash it again and send it to these two new partitions I just added.</p>
<p>515<br>00:48:04,000 –&gt; 00:48:24,000<br>So basically recursive partitioning from the greatest hash to an algorithm we saw in the intro class right so then these guys start you know keep running and start filling up data from these two are partitions and then when this this stage is done I introduce a new task in my stage to re partition that then goes from trees data.</p>
<p>516<br>00:48:24,000 –&gt; 00:48:29,000<br>Sorry this is not professional.</p>
<p>517<br>00:48:29,000 –&gt; 00:48:37,000<br>I go read I go read your data from partition to and then it just rehashes it and puts it in partition three and four right.</p>
<p>518<br>00:48:37,000 –&gt; 00:48:40,000<br>In the beans one for the two should join.</p>
<p>519<br>00:48:40,000 –&gt; 00:48:43,000<br>Yes.</p>
<p>520<br>00:48:43,000 –&gt; 00:48:46,000<br>One of them always needs to be a broadcast right it’s just which one you want to make the one.</p>
<p>521<br>00:48:46,000 –&gt; 00:48:49,000<br>Yeah one doesn’t always have to be broadcast.</p>
<p>522<br>00:48:49,000 –&gt; 00:48:53,000<br>Sorry yeah it’s you can do.</p>
<p>523<br>00:48:53,000 –&gt; 00:49:08,000<br>So shuffle joints just like everything everything gets repartition on the hash key right the alternative is to do just a broadcast join or one of them gets broadcast everybody and then you don’t have to just scan locally.</p>
<p>524<br>00:49:08,000 –&gt; 00:49:20,000<br>Actually yeah so in the stage I’m missing if you do a broadcast join you don’t like I do broadcast a that I don’t need to do partition on B so you go ahead and kill these workers here.</p>
<p>525<br>00:49:20,000 –&gt; 00:49:25,000<br>And then in the next stage they’re just going to read the data from the record in the table the files.</p>
<p>526<br>00:49:25,000 –&gt; 00:49:28,000<br>Yes I’m missing arrows at draw that.</p>
<p>527<br>00:49:28,000 –&gt; 00:49:39,000<br>That’s how you do a broadcast join because the idea is like one small if you can send around everywhere and you leave the other table where we’re originally resided.</p>
<p>528<br>00:49:39,000 –&gt; 00:49:55,000<br>Okay so as I said before they’re they’re we’re going to rely on the internal just rate of file system called call call losses originally start off a GFS but then they switched to call us to do scale storage.</p>
<p>529<br>00:49:55,000 –&gt; 00:50:07,000<br>Again think of like it’s like an object store like S3 and other ones we’ve talked about the idea is that this is this is an external service to the data system you just let let them manage all the storage for us.</p>
<p>530<br>00:50:07,000 –&gt; 00:50:30,000<br>So the paper also talks about how they’re going to rely on a file format called capacitor which is internal to Google this link here take it to a blog article that mentions it there isn’t much documentation about it it’s not open source but it more or less looks like or can park a when you talk to them the Google people right.</p>
<p>531<br>00:50:31,000 –&gt; 00:50:44,000<br>One thing that capacitor does do that work in park a do not do is that you can do predicate push down and partial query evaluation or expression evaluation within the access library itself directly on the data.</p>
<p>532<br>00:50:44,000 –&gt; 00:50:58,000<br>So when S3 again you do some push down of some warehouses and select for select statements on the park a files or park a and CSV files or JSON files as well but it’s pretty limited and certainly in the case of.</p>
<p>533<br>00:50:59,000 –&gt; 00:51:11,000<br>Like you know if you just access park a through the arrow files like you had a decompress everything as you’re iterating over the data whereas this thing can do filtering directly on compressed data without decompressing it first.</p>
<p>534<br>00:51:11,000 –&gt; 00:51:23,000<br>There’s another file format called artists those for the YouTube or cello system that has similar capabilities but a high level this is just just going to look like park a network except that you can do better better better early filtering.</p>
<p>535<br>00:51:23,000 –&gt; 00:51:36,000<br>And we saw before how they’re going to handle repetition definition fields to deal with nested data like thing JSON data but again it’s Google world so it’s protocol buffers.</p>
<p>536<br>00:51:36,000 –&gt; 00:51:47,000<br>These file formats capacitor are going to self describing meaning again just like park a work there’ll be something in the folder that says here’s the schema that you expect to see.</p>
<p>537<br>00:51:47,000 –&gt; 00:52:12,000<br>And then they talk about how the data the the metadata for the schema is just stored as kilometer data as well so even though I may have 10,000 attributes in my in my file I don’t have to deseralize the entire thing like you have to do in park a work I can just do the the you know do all the opposition you look up on kilometer data directly on the metadata to find the things that I’m looking for.</p>
<p>538<br>00:52:12,000 –&gt; 00:52:26,000<br>Again this is not this itself is not like mind blowing the amazing but it’s certainly better than what’s in park a and what’s better than work in the current state of the art.</p>
<p>539<br>00:52:26,000 –&gt; 00:52:50,000<br>The last interesting to talk about in this paper is similar to what we saw in the Velox paper where they talked about how you know Dremel was the one of the big first systems that they Google built that brought back sequel and then once the call became in fashion again at Google there’s a bunch of these different random projects that people started adding their own you know for sequel.</p>
<p>540<br>00:52:50,000 –&gt; 00:53:17,000<br>The problem is all these different internal projects have their own dialect of sequel and so there was effort in the late 2010s to to to unify this across the entire corporation by having a single sequel dialect on Google sequel that that all these systems within incorporate so that way you didn’t have to deal with a weird nuances of one sequel to the another across the entire corporation everything was always the same.</p>
<p>541<br>00:53:17,000 –&gt; 00:53:31,000<br>So the this again in the Bellock’s world they talked about how like there’s all these like some string functions and everyone was re implementing the wheel over and over again and bellox invented standardized those invitations the same idea here.</p>
<p>542<br>00:53:31,000 –&gt; 00:53:46,000<br>So Google’s sequel itself is not open source but there’s an open source variant of it called Zeta sequel who has ever heard of Zeta sequel nobody okay so this thing here’s supposed to be the open source version of this and the idea was like okay yeah here’s you know.</p>
<p>543<br>00:53:46,000 –&gt; 00:54:09,000<br>People could start building you know Zeta sequel compatible database systems that would then smell a lot like Google’s sequel so like if you’re comfortable like running on whatever this one off system based on Zeta sequel you can easily transition your application over to to to BigQuery or Dremel or other spanner as well.</p>
<p>544<br>00:54:09,000 –&gt; 00:54:14,000<br>So this thing is basically as far as I tell is dead like there was like there’s new updates.</p>
<p>545<br>00:54:14,000 –&gt; 00:54:30,000<br>There’s some update like a month ago on GitHub but like it says at the bottom it’s not officially supported by Google there’s a bunch of pull requests and issues that aren’t being responded to or answered right so as far as I can tell this thing is dead like and there’s only one data system I know that actually supports Zeta sequel it’s called Apache beam.</p>
<p>546<br>00:54:30,000 –&gt; 00:54:59,000<br>There’s like a stream processing system but nobody else is actually using this and what’s to me this is interesting because again Google was or Google still is huge right Google still is very influential in the tech community but if they’re putting out a sequel dialect they say this is what the standard should be no one follows it like you know this shows you like that the sequel marketplace is so diverse and fracture that like no one is going to be able to do that.</p>
<p>547<br>00:54:59,000 –&gt; 00:55:11,000<br>There’s no one company even a major tech giant can can sort of bend the you know the bend the physics of bend the world to you know to to their to the winds.</p>
<p>548<br>00:55:11,000 –&gt; 00:55:39,000<br>Last time this was actually done was IBM right IBM came out 82 83 said okay we’re putting out a new Davis system and it’s going to be based on sequel sequels the standard and everyone sort of got mine follow along and sequel became what it is today but now things are so diverse that I don’t think you could ever do that again the close you get to I think a true I mean there’s the sequel standard the ice or sequel standard but as I said nobody actually follows that the close you get to a dialect that everyone is based on is postgres.</p>
<p>549<br>00:55:39,000 –&gt; 00:55:46,000<br>Because everyone takes the postgres parser the grammar file uses it like ducty be did did this a bunch of other system did this.</p>
<p>550<br>00:55:46,000 –&gt; 00:55:58,000<br>Google you know not saying that they try and fail but like you know like it knows we use that as a sequel.</p>
<p>551<br>00:55:58,000 –&gt; 00:56:16,000<br>Okay so there’s again since the 2011 paper as I mentioned there’s a bunch of systems that have come out that are some cases like wholesale they claim it’s a copy off of the architecture but other ones is more likely to say that there’s inspired.</p>
<p>552<br>00:56:16,000 –&gt; 00:56:31,000<br>So I want to go through these four here and then what’s also interesting about this is that they’re in the last three or four years there are now separate shuffle as a service components or architecture systems that you could then use.</p>
<p>553<br>00:56:31,000 –&gt; 00:56:44,000<br>That maybe don’t exactly replicate all the capabilities of the the the dremels in memory shuffle service certainly not using hardware acceleration but now that again there’s separate projects that do nothing but shuffles.</p>
<p>554<br>00:56:44,000 –&gt; 00:56:53,000<br>Which I think is kind of cool and so we’ll talk about the celebrate one from Ali Baba because that one’s that one’s the farthest along the uniform and the the over one.</p>
<p>555<br>00:56:53,000 –&gt; 00:57:00,000<br>I mean I’m sure the store using this uniform is still and again Apache incubator projects so so early this is the big one.</p>
<p>556<br>00:57:00,000 –&gt; 00:57:07,000<br>So again let me go through each of these systems and I’ll cover this one and we’ll finish up go out for the for the eclipse.</p>
<p>557<br>00:57:08,000 –&gt; 00:57:18,000<br>All right Apache drill is again this one claim to be to begin straight up copy like Dremel is a drill Apache drill right no imagination there.</p>
<p>558<br>00:57:18,000 –&gt; 00:57:30,000<br>So this started as a right after the triple people came out as a way to build up a query engine on top of hgfs and this is started at a tech company called map r.</p>
<p>559<br>00:57:31,000 –&gt; 00:57:42,000<br>This was in the in the late 2000 to 2010 there was three major Hadoop companies or map reduced companies there was a cloud era Horton works in map are.</p>
<p>560<br>00:57:42,000 –&gt; 00:57:52,000<br>Cloud era and Horton works are based on the open source version Hadoop the Java one map are had the room for proprietary simple those version that was meant to be faster and so map are built a.</p>
<p>561<br>00:57:52,000 –&gt; 00:57:58,000<br>The building of Apache drill actually this was in Java.</p>
<p>562<br>00:57:58,000 –&gt; 00:58:12,000<br>So it’s interesting about this is that they are going to code gen query compilation using this thing called geneno which is basically it’s some kind of embedded Java compiler where you can give it Java code and it converts it in process.</p>
<p>563<br>00:58:13,000 –&gt; 00:58:22,000<br>So this project is not dead but certainly the number of commits and engagement and usage of it has gone down.</p>
<p>564<br>00:58:22,000 –&gt; 00:58:34,000<br>Map reduce or map are was on the market a couple of times finally got acquired for not much by HPE and the HPE announced in 2020 that they’re basically stopping all development on this.</p>
<p>565<br>00:58:34,000 –&gt; 00:58:40,000<br>At least their HPE is not paying for the developers to work on this but other people are still still working on it.</p>
<p>566<br>00:58:40,000 –&gt; 00:58:51,000<br>So I would say that this is not you know there’s better alternatives now especially in the open source world but you know this was the first one that sort of came out directly after the Dremel paper came out.</p>
<p>567<br>00:58:51,000 –&gt; 00:58:53,000<br>Can I do the shoppers in the memory shop?</p>
<p>568<br>00:58:53,000 –&gt; 00:58:55,000<br>I think this one did the memory shop with us.</p>
<p>569<br>00:58:55,000 –&gt; 00:58:58,000<br>Again not the hardware.</p>
<p>570<br>00:58:59,000 –&gt; 00:59:03,000<br>The next one is presto DB.</p>
<p>571<br>00:59:03,000 –&gt; 00:59:05,000<br>This was started at Facebook.</p>
<p>572<br>00:59:05,000 –&gt; 00:59:11,000<br>I wouldn’t say this is like directly inspired by Dremel because I think they were working on this.</p>
<p>573<br>00:59:11,000 –&gt; 00:59:21,000<br>They were already working this when the Dremel paper came out but they were building Facebook was building this to replace hive which was a.</p>
<p>574<br>00:59:21,000 –&gt; 00:59:32,000<br>Which is way to do sequence up a map reduce it would take your sequel query and then convert it literally into map reduce Java jobs and run those and obviously that would be super slow because map reduce was slow.</p>
<p>575<br>00:59:32,000 –&gt; 00:59:41,000<br>But the idea again same same motivation that they have a bunch of files sort of data lakes this case it’s a TFS or I think Facebook has their own internal distributed file system.</p>
<p>576<br>00:59:42,000 –&gt; 00:59:48,000<br>And they had a way to do a bunch of connectors and different storage system and data systems similar to Dremel.</p>
<p>577<br>00:59:48,000 –&gt; 00:59:57,000<br>And in a few years ago Facebook announced that they’re getting off of the Java based runtime engine and they’re switching everything everything to the VeloX.</p>
<p>578<br>00:59:57,000 –&gt; 01:00:08,000<br>The VeloX paper talks about this project called Pratissimo. This is one of the the the the targets they were building VeloX work to replace the Java engine with this equalist engine and VeloX.</p>
<p>579<br>01:00:09,000 –&gt; 01:00:21,000<br>There’s also another version of presto called Treno previously called presto sequel so the first version of the project was called presto then it was called presto DB and then there was a fork called presto sequel.</p>
<p>580<br>01:00:21,000 –&gt; 01:00:26,000<br>Then they got renamed to Treno and this is done by the starburst guys that came out of.</p>
<p>581<br>01:00:27,000 –&gt; 01:00:30,000<br>Carid data.</p>
<p>582<br>01:00:32,000 –&gt; 01:00:36,000<br>No they by aster data as they’re part of aster data and aster data acquired.</p>
<p>583<br>01:00:36,000 –&gt; 01:00:40,000<br>As the project got a doob DB is a company called had apt.</p>
<p>584<br>01:00:41,000 –&gt; 01:00:48,000<br>Adapt got acquired by aster data and an aster data got acquired by terror data and then tear data spun out this as starburst there we go.</p>
<p>585<br>01:00:49,000 –&gt; 01:00:59,000<br>And so they didn’t like how Facebook wouldn’t give up the control of the source code like high came out of Facebook and that’s a patchy project for whatever reason presto was still not a patchy project and it wasn’t.</p>
<p>586<br>01:00:59,000 –&gt; 01:01:17,000<br>Facebook wasn’t giving up control so these guys forked it renamed it as Treno and I think this went to the cloud computing foundation and then Facebook then converted gave up source control and gave presto DB to the Linux computing foundation right so not a patchy with these these other similar kind of foundations.</p>
<p>587<br>01:01:17,000 –&gt; 01:01:41,000<br>So what’s interesting about this in presto or presto DB Facebook is trying to get rid of the job of stuff in place of the locks but Treno guys they’re very explicit saying they don’t want to give up job on they have a blog article or they got a podcast a year ago and they talk about here is explicitly that like they rather spend the time trying to make the query plan query optimizer better.</p>
<p>588<br>01:01:41,000 –&gt; 01:01:51,000<br>Then then try to spend a much engineering effort to replace the execution engine with something like Bellox or even data fusion.</p>
<p>589<br>01:01:51,000 –&gt; 01:01:59,000<br>So is the query engine or is that the high is the query engine the same way the drum was a query engine pressors query engine Trenus query engine.</p>
<p>590<br>01:01:59,000 –&gt; 01:02:02,000<br>So how is press so high connection.</p>
<p>591<br>01:02:02,000 –&gt; 01:02:04,000<br>The question is why how is press so high connection.</p>
<p>592<br>01:02:04,000 –&gt; 01:02:14,000<br>Facebook first built hive because they were like okay they had all this all this map produced stuff infrastructure map produces slow and people writing Java code running queries instead of SQL.</p>
<p>593<br>01:02:14,000 –&gt; 01:02:25,000<br>So then they build hive where’s a front end query engine that can take your SQL query and convert it to a map produced job and run that that’s slow because map produces slow in the Hadoop model slow.</p>
<p>594<br>01:02:25,000 –&gt; 01:02:37,000<br>So then they said okay let’s get rid of that and let’s have build keep HDFS or the short file system and let’s build a query engine that takes SQL and run the actual query plans directly to SQL that’s presto.</p>
<p>595<br>01:02:37,000 –&gt; 01:02:39,000<br>Simulta Dremel.</p>
<p>596<br>01:02:39,000 –&gt; 01:02:42,000<br>And that one also has the immigration.</p>
<p>597<br>01:02:42,000 –&gt; 01:02:45,000<br>Actually I pressed so I don’t know.</p>
<p>598<br>01:02:45,000 –&gt; 01:02:47,000<br>I should look a bit out. I don’t know.</p>
<p>599<br>01:02:47,000 –&gt; 01:02:49,000<br>Go question.</p>
<p>600<br>01:02:49,000 –&gt; 01:03:01,000<br>All right another project again that came with definitely definitely definitely inspired by Dremel was the thing called impala came out of Claudero.</p>
<p>601<br>01:03:01,000 –&gt; 01:03:11,000<br>And so this was founded by people that Claudero hired from Google who didn’t work on Dremel what what used it and what inspired about it.</p>
<p>602<br>01:03:11,000 –&gt; 01:03:30,000<br>But the the the key thing that they did that they didn’t that an impala did I think it still works this way that rather than have the query engine and the workers pull the data from the shared storage and then do the processing on the worker nodes.</p>
<p>603<br>01:03:30,000 –&gt; 01:03:38,000<br>They want to do more credit push down than you can do on S3 or the GFS at the time.</p>
<p>604<br>01:03:38,000 –&gt; 01:03:43,000<br>So what they would do is that on the true file system you actually install a little execution engine down there.</p>
<p>605<br>01:03:43,000 –&gt; 01:03:54,000<br>I think this all written in Java so this is like the JVM so then work could then do a credit push down another another push downs and that would run that part of the query directly where the data was being stored.</p>
<p>606<br>01:03:54,000 –&gt; 01:04:03,000<br>This was HDFS at the time so then like on your HDFS node also install the some pile X-puter node who then take the queries and process the data locally before sending it back.</p>
<p>607<br>01:04:03,000 –&gt; 01:04:13,000<br>So that’s not a true disaggregated storage the way that we’ve been talking about the entire semester but they did this because they wanted to do the the the predicate push down.</p>
<p>608<br>01:04:13,000 –&gt; 01:04:27,000<br>I think they also did query compilation but they did the compala it actually see back this was not Java this was in C++ and I think they were doing like predicate compilation on like ware clauses and they could do that down there.</p>
<p>609<br>01:04:27,000 –&gt; 01:04:30,000<br>And see CSV parsing and other things.</p>
<p>610<br>01:04:31,000 –&gt; 01:04:40,000<br>Well see we’ll see more about impala next class when we talk about data bricks right because cloud air was the big big map reduce company.</p>
<p>611<br>01:04:40,000 –&gt; 01:04:46,000<br>And they were pushing impala very heavily but then everyone started asking for spark.</p>
<p>612<br>01:04:46,000 –&gt; 01:04:54,000<br>So they also had to start supporting spark but then sparks like hey let’s add SQL and clander didn’t like that because they want to keep it a buy impala.</p>
<p>613<br>01:04:54,000 –&gt; 01:04:59,000<br>And then the spark guys do this one trick we’ll see next class how they got SQL in the spark.</p>
<p>614<br>01:04:59,000 –&gt; 01:05:04,000<br>I mean basically they invented it instead of having them being a middleware and it did it.</p>
<p>615<br>01:05:04,000 –&gt; 01:05:11,000<br>Databricks basically destroyed cloud air will come that well we’ll discuss the more next class.</p>
<p>616<br>01:05:11,000 –&gt; 01:05:30,000<br>Dremel is probably the evolved open source ones we talked about again directly inspired by Dremel and actually is backed by a you know VC backed company actually found a by seeming to see me alum I think it is master’s here but it wasn’t my student as far as I know.</p>
<p>617<br>01:05:30,000 –&gt; 01:05:54,000<br>And the Z before they’re doing all the things we talked about very similar to Dremel but one of the things we’re going to do to speed things up is direct to access what we call the reflections as far as we can tell they’re just materialize views right they’re doing on on Java based co gen I think for the entire query not just where causes and vectorization as we talked about before.</p>
<p>618<br>01:05:54,000 –&gt; 01:06:12,000<br>And then the last one is Apache celebrant celebrant this again shuffle as a service came out of a Baba ideas that in spark and flink you can actually specify what shuffle service you want like there’s a beat default built in one where the worker notes and the data directly to you know to other worker nodes.</p>
<p>619<br>01:06:12,000 –&gt; 01:06:41,000<br>You can actually have use this as a standalone service as the intermediary and it can do all the things that we talked about so far like I can spell the disc on that run out of memory they can actually do block compression of the data when they put it down the desk and so forth and again it’s just a key value store that’s fault tolerant and think this one is using wrapped internally does another one the unifool that’s based on suit keeper right it’s just a key value store that’s but it’s only meant for moving data back and forth between the different stages of queries.</p>
<p>620<br>01:06:43,000 –&gt; 01:07:06,000<br>All right so the finish up. Dremel is very very influential as I said in the combination of vector wise for like the single no query processing plus the Dremel for like to overall architecture not everyone does the shuffle as we’ll see is going along with the combination these two things gives you what what we call a modern lake house.</p>
<p>621<br>01:07:06,000 –&gt; 01:07:24,000<br>And although the shovel stuff seems way for with baseball it is actually make things better because I keep I keep as much memory as much as possible I can disconnect the warm workers at one stage of the next right there are a bunch of advantages to this not just performance also from engineering because it simplifies the implementation of all the workers.</p>
<p>622<br>01:07:25,000 –&gt; 01:07:45,000<br>And this is another good example to of of sort of the projects you guys are working on based on it’s like by decoupling the system architecture and having one group to spend as much time to optimize this one piece that then be taken advantage of by other parts of the system I think that’s the right way to to build a modern cloud native system today.</p>
<p>623<br>01:07:45,000 –&gt; 01:08:02,000<br>Okay. All right. Again next class will talk about sparks equal and photon this is going to be different than the Dremel paper because the journey papers of the entire system the photon you’re going to see it’s going to look like Velox right.</p>
<p>624<br>01:08:02,000 –&gt; 01:08:10,000<br>It’s something you bed inside the gvm of and for this this rock runtime rather than being so in one system.</p>
<p>625<br>01:08:10,000 –&gt; 01:08:15,000<br>Okay. All right. So that’s the stop now. Let’s go inside and check out the clips.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CMU15721 P17S202417 GoogleBigQuery⧸DremelCMUAdvancedDatabaseSystems</div>
      <div>http://example.com/2025/10/24/CMU15721 P17S202417-GoogleBigQuery⧸DremelCMUAdvancedDatabaseSystems/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月24日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/24/CMU15721%20P16S202415-QueryOptimizerImplementation3CMUAdvancedDatabaseSystems/" title="CMU15721 P16S202415 QueryOptimizerImplementation3CMUAdvancedDatabaseSystems">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CMU15721 P16S202415 QueryOptimizerImplementation3CMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/24/CMU15721%20P14S202413-QueryOptimizerImplementation1CMUAdvancedDatabaseSystems/" title="CMU15721 P14S202413 QueryOptimizerImplementation1CMUAdvancedDatabaseSystems">
                        <span class="hidden-mobile">CMU15721 P14S202413 QueryOptimizerImplementation1CMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
