

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="100:00:00,000 –&gt; 00:00:06,000Kaniki Melon University’s advanced database systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio board. 300:00:09,000 –&gt; 00:00:11,000I do">
<meta property="og:type" content="article">
<meta property="og:title" content="CMU15721 P21S202421 YellowbrickDataWarehouseSystemCMUAdvancedDatabaseSystems">
<meta property="og:url" content="http://example.com/2025/10/24/CMU15721%20P21S202421-YellowbrickDataWarehouseSystemCMUAdvancedDatabaseSystems/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="100:00:00,000 –&gt; 00:00:06,000Kaniki Melon University’s advanced database systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio board. 300:00:09,000 –&gt; 00:00:11,000I do">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-24T11:57:41.748Z">
<meta property="article:modified_time" content="2025-10-24T12:06:28.547Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>CMU15721 P21S202421 YellowbrickDataWarehouseSystemCMUAdvancedDatabaseSystems - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CMU15721 P21S202421 YellowbrickDataWarehouseSystemCMUAdvancedDatabaseSystems"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-24 19:57" pubdate>
          2025年10月24日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          13k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          112 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CMU15721 P21S202421 YellowbrickDataWarehouseSystemCMUAdvancedDatabaseSystems</h1>
            
            
              <div class="markdown-body">
                
                <p>1<br>00:00:00,000 –&gt; 00:00:06,000<br>Kaniki Melon University’s advanced database systems courses</p>
<p>2<br>00:00:06,000 –&gt; 00:00:09,000<br>filming front of the live studio board.</p>
<p>3<br>00:00:09,000 –&gt; 00:00:11,000<br>I don’t want to know what kind of club.</p>
<p>4<br>00:00:11,000 –&gt; 00:00:13,000<br>I don’t want to know what kind of club.</p>
<p>5<br>00:00:13,000 –&gt; 00:00:14,000<br>They’re watching.</p>
<p>6<br>00:00:14,000 –&gt; 00:00:17,000<br>I don’t want to know what kind of club.</p>
<p>7<br>00:00:17,000 –&gt; 00:00:20,000<br>All right, so today we’re going to talk about Yellowbrick.</p>
<p>8<br>00:00:20,000 –&gt; 00:00:24,000<br>And to me, this is a very, very fascinating system.</p>
<p>9<br>00:00:24,000 –&gt; 00:00:27,000<br>You know, I probably recognize that they’re not a really</p>
<p>10<br>00:00:27,000 –&gt; 00:00:29,000<br>well-known company.</p>
<p>11<br>00:00:29,000 –&gt; 00:00:32,000<br>You might have come give a talk two years ago.</p>
<p>12<br>00:00:32,000 –&gt; 00:00:34,000<br>And then I told the CTO, like, you guys got to write a paper</p>
<p>13<br>00:00:34,000 –&gt; 00:00:37,000<br>with all this great stuff, and being the cider paper you guys</p>
<p>14<br>00:00:37,000 –&gt; 00:00:39,000<br>are reading.</p>
<p>15<br>00:00:39,000 –&gt; 00:00:41,000<br>And it’s just, it’s wild, right?</p>
<p>16<br>00:00:41,000 –&gt; 00:00:43,000<br>It does bunch of stuff that nobody else does.</p>
<p>17<br>00:00:43,000 –&gt; 00:00:46,000<br>And I think it’s a good way to think about, like, you know,</p>
<p>18<br>00:00:46,000 –&gt; 00:00:49,000<br>like, for you guys to say, oh, like, yeah, you can just write</p>
<p>19<br>00:00:49,000 –&gt; 00:00:52,000<br>your own PCIe driver, right?</p>
<p>20<br>00:00:52,000 –&gt; 00:00:53,000<br>Like insane things like that.</p>
<p>21<br>00:00:53,000 –&gt; 00:00:55,000<br>And so from a system perspective, I think it’s super</p>
<p>22<br>00:00:55,000 –&gt; 00:00:57,000<br>fascinating.</p>
<p>23<br>00:00:57,000 –&gt; 00:00:59,000<br>But before we jump into that, again, for the administrative</p>
<p>24<br>00:00:59,000 –&gt; 00:01:01,000<br>stuff, this is the last week, last week classes.</p>
<p>25<br>00:01:01,000 –&gt; 00:01:03,000<br>Again, the final presentations for the project.</p>
<p>26<br>00:01:03,000 –&gt; 00:01:06,000<br>So be next Thursday in this room at 9 a.m.</p>
<p>27<br>00:01:06,000 –&gt; 00:01:09,000<br>And then if you go to this post on the outside that I put out</p>
<p>28<br>00:01:09,000 –&gt; 00:01:13,000<br>last night, there’s a poll there, select what you want for breakfast</p>
<p>29<br>00:01:13,000 –&gt; 00:01:15,000<br>on Thursday.</p>
<p>30<br>00:01:15,000 –&gt; 00:01:17,000<br>And there’ll be information that’ll link to the course</p>
<p>31<br>00:01:17,000 –&gt; 00:01:19,000<br>website that says what’s expected or what the deliverables</p>
<p>32<br>00:01:19,000 –&gt; 00:01:24,000<br>are required for when you show up on that day.</p>
<p>33<br>00:01:24,000 –&gt; 00:01:26,000<br>OK?</p>
<p>34<br>00:01:26,000 –&gt; 00:01:28,000<br>And the presentations will be 10 minutes.</p>
<p>35<br>00:01:28,000 –&gt; 00:01:30,000<br>It’s supposed to be like a lighthearted thing.</p>
<p>36<br>00:01:30,000 –&gt; 00:01:32,000<br>We meant to have fun.</p>
<p>37<br>00:01:32,000 –&gt; 00:01:36,000<br>And then the final exam, I’ll give it out in class this Wednesday.</p>
<p>38<br>00:01:36,000 –&gt; 00:01:43,000<br>And then it’ll be due the same day on the final presentation.</p>
<p>39<br>00:01:43,000 –&gt; 00:01:46,000<br>I haven’t decided whether I should do show with the PDF or email me</p>
<p>40<br>00:01:46,000 –&gt; 00:01:48,000<br>PDF or print it out.</p>
<p>41<br>00:01:48,000 –&gt; 00:01:49,000<br>I’ll probably do PDFs.</p>
<p>42<br>00:01:49,000 –&gt; 00:01:51,000<br>But I’ll clarify on Wednesday.</p>
<p>43<br>00:01:51,000 –&gt; 00:01:53,000<br>OK?</p>
<p>44<br>00:01:53,000 –&gt; 00:01:56,000<br>Any questions about any of this?</p>
<p>45<br>00:01:56,000 –&gt; 00:01:57,000<br>Yes?</p>
<p>46<br>00:01:57,000 –&gt; 00:01:58,000<br>How long is the final exam?</p>
<p>47<br>00:01:58,000 –&gt; 00:02:00,000<br>Questions, how long is the final exam?</p>
<p>48<br>00:02:00,000 –&gt; 00:02:05,000<br>It’s take home, right?</p>
<p>49<br>00:02:05,000 –&gt; 00:02:08,000<br>So if you do it while you’re watching TV, maybe it takes longer.</p>
<p>50<br>00:02:08,000 –&gt; 00:02:10,000<br>I know.</p>
<p>51<br>00:02:10,000 –&gt; 00:02:13,000<br>But again, it’s not questions like, what does this paper say?</p>
<p>52<br>00:02:13,000 –&gt; 00:02:14,000<br>What does that paper say?</p>
<p>53<br>00:02:14,000 –&gt; 00:02:16,000<br>Because that’s just wrote memorization.</p>
<p>54<br>00:02:16,000 –&gt; 00:02:19,000<br>It’s really internalizing the various things we’ve talked about</p>
<p>55<br>00:02:19,000 –&gt; 00:02:22,000<br>and then seeing in different contexts and then applying</p>
<p>56<br>00:02:22,000 –&gt; 00:02:23,000<br>to new situations.</p>
<p>57<br>00:02:23,000 –&gt; 00:02:25,000<br>That’s what I care about.</p>
<p>58<br>00:02:25,000 –&gt; 00:02:28,000<br>Any other questions?</p>
<p>59<br>00:02:28,000 –&gt; 00:02:29,000<br>All right.</p>
<p>60<br>00:02:29,000 –&gt; 00:02:31,000<br>So, right.</p>
<p>61<br>00:02:31,000 –&gt; 00:02:34,000<br>In the last class, last Wednesday, we talked about duct-DB,</p>
<p>62<br>00:02:34,000 –&gt; 00:02:36,000<br>which is a fascinating system that’s been widely used.</p>
<p>63<br>00:02:36,000 –&gt; 00:02:40,000<br>But it’s obviously not a sort of scale-out,</p>
<p>64<br>00:02:40,000 –&gt; 00:02:44,000<br>overlap system as we talked about this entire semester.</p>
<p>65<br>00:02:44,000 –&gt; 00:02:47,000<br>And then someone suggested, oh, can’t they,</p>
<p>66<br>00:02:47,000 –&gt; 00:02:50,000<br>or the mother-duck stuff, can’t they take the fragments</p>
<p>67<br>00:02:50,000 –&gt; 00:02:52,000<br>and scale it out across multiple nodes?</p>
<p>68<br>00:02:52,000 –&gt; 00:02:54,000<br>Again, looking back on mother-duck, as far as they can tell,</p>
<p>69<br>00:02:54,000 –&gt; 00:02:55,000<br>they’re not doing that.</p>
<p>70<br>00:02:55,000 –&gt; 00:02:58,000<br>They rather scale vertically than horizontally</p>
<p>71<br>00:02:58,000 –&gt; 00:03:02,000<br>for duct-DB running on the cloud.</p>
<p>72<br>00:03:02,000 –&gt; 00:03:03,000<br>All right.</p>
<p>73<br>00:03:03,000 –&gt; 00:03:08,000<br>So today to sort of lead us into the discussion about yellow brick,</p>
<p>74<br>00:03:08,000 –&gt; 00:03:13,000<br>we haven’t really talked about specialized holograms for databases.</p>
<p>75<br>00:03:13,000 –&gt; 00:03:16,000<br>There are databases that are run on GPUs,</p>
<p>76<br>00:03:16,000 –&gt; 00:03:18,000<br>databases that are run on FPGAs.</p>
<p>77<br>00:03:18,000 –&gt; 00:03:20,000<br>Yellow brick is actually one of them.</p>
<p>78<br>00:03:20,000 –&gt; 00:03:22,000<br>And then we’ll see in Redshift next week,</p>
<p>79<br>00:03:22,000 –&gt; 00:03:24,000<br>they’ll have their own, because this Amazon,</p>
<p>80<br>00:03:24,000 –&gt; 00:03:26,000<br>they’ve fad their own hardware called Aqua,</p>
<p>81<br>00:03:26,000 –&gt; 00:03:28,000<br>where they have custom silicon that they’re using.</p>
<p>82<br>00:03:28,000 –&gt; 00:03:31,000<br>At least, I think they’ve discontinued it, though,</p>
<p>83<br>00:03:31,000 –&gt; 00:03:33,000<br>to accelerate queries.</p>
<p>84<br>00:03:33,000 –&gt; 00:03:35,000<br>And we’ll see how they’re doing that.</p>
<p>85<br>00:03:35,000 –&gt; 00:03:40,000<br>But there’s a long history for databases going back to the 1970s,</p>
<p>86<br>00:03:40,000 –&gt; 00:03:45,000<br>where vendors have tried to rely on hardware accelerators</p>
<p>87<br>00:03:45,000 –&gt; 00:03:48,000<br>to make the database system go faster.</p>
<p>88<br>00:03:48,000 –&gt; 00:03:52,000<br>In the 1970s and the 80s, these were called the 80s machines.</p>
<p>89<br>00:03:52,000 –&gt; 00:03:57,000<br>Think of, again, custom chips to make queries run faster</p>
<p>90<br>00:03:57,000 –&gt; 00:03:59,000<br>or network communication run faster.</p>
<p>91<br>00:03:59,000 –&gt; 00:04:02,000<br>The challenge in the 1980s was that if you built custom hardware,</p>
<p>92<br>00:04:02,000 –&gt; 00:04:04,000<br>by the time you designed it, fad it,</p>
<p>93<br>00:04:04,000 –&gt; 00:04:09,000<br>and put it out for customers, the next version of the Motorola 680,000</p>
<p>94<br>00:04:09,000 –&gt; 00:04:12,000<br>or Intel’s next CPU came out,</p>
<p>95<br>00:04:12,000 –&gt; 00:04:14,000<br>and all the benefits you got from custom hardware</p>
<p>96<br>00:04:14,000 –&gt; 00:04:16,000<br>were defeated.</p>
<p>97<br>00:04:16,000 –&gt; 00:04:18,000<br>And so people haven’t really tried custom,</p>
<p>98<br>00:04:18,000 –&gt; 00:04:20,000<br>another in the major cloud vendors,</p>
<p>99<br>00:04:20,000 –&gt; 00:04:22,000<br>there’s not a lot of people fadding custom hardware today.</p>
<p>100<br>00:04:22,000 –&gt; 00:04:26,000<br>Typically, you see people leveraging FPGAs and GPUs,</p>
<p>101<br>00:04:26,000 –&gt; 00:04:29,000<br>commodity hardware accelerators.</p>
<p>102<br>00:04:29,000 –&gt; 00:04:34,000<br>But another way that people sort of put together databases</p>
<p>103<br>00:04:34,000 –&gt; 00:04:37,000<br>and sold them on a prem are what are called appliances.</p>
<p>104<br>00:04:37,000 –&gt; 00:04:40,000<br>And basically, the thing of like, you’re buying rack units</p>
<p>105<br>00:04:40,000 –&gt; 00:04:43,000<br>where, even though maybe using off-the-shelf hardware,</p>
<p>106<br>00:04:43,000 –&gt; 00:04:45,000<br>like SSDs, or CPUs, and so forth,</p>
<p>107<br>00:04:45,000 –&gt; 00:04:48,000<br>but it’s set up and tuned and configured such a way</p>
<p>108<br>00:04:48,000 –&gt; 00:04:52,000<br>that the data is designed to be optimized for it.</p>
<p>109<br>00:04:52,000 –&gt; 00:04:55,000<br>So as a bunch of vendors that I’ll sell you big rack units</p>
<p>110<br>00:04:55,000 –&gt; 00:04:58,000<br>that look like this, Oracle’s pie the strongest one in this game</p>
<p>111<br>00:04:58,000 –&gt; 00:05:02,000<br>with something like X-Data or their data’s appliance.</p>
<p>112<br>00:05:02,000 –&gt; 00:05:04,000<br>And again, it’s like, you buy the hardware from them</p>
<p>113<br>00:05:04,000 –&gt; 00:05:07,000<br>and the data system has been optimized and tuned specifically</p>
<p>114<br>00:05:07,000 –&gt; 00:05:09,000<br>for the machine-ger running one.</p>
<p>115<br>00:05:09,000 –&gt; 00:05:11,000<br>Versus like, just downloading the binary, setting it up</p>
<p>116<br>00:05:11,000 –&gt; 00:05:15,000<br>and you can configure it correctly on whatever</p>
<p>117<br>00:05:15,000 –&gt; 00:05:17,000<br>your running one.</p>
<p>118<br>00:05:17,000 –&gt; 00:05:19,000<br>So this is how Yellowbrick started.</p>
<p>119<br>00:05:19,000 –&gt; 00:05:22,000<br>Yellowbrick was an appliance.</p>
<p>120<br>00:05:22,000 –&gt; 00:05:25,000<br>So this is from the TTO, the author of the paper you guys read.</p>
<p>121<br>00:05:25,000 –&gt; 00:05:28,000<br>He had to talk with us, CMU, a few years ago,</p>
<p>122<br>00:05:28,000 –&gt; 00:05:30,000<br>and he’s had a screenshot here of like,</p>
<p>123<br>00:05:30,000 –&gt; 00:05:33,000<br>this is the Yellowbrick appliance, obviously,</p>
<p>124<br>00:05:33,000 –&gt; 00:05:34,000<br>the things with the yellow.</p>
<p>125<br>00:05:34,000 –&gt; 00:05:36,000<br>So a bunch of SSDs and a bunch of CPUs,</p>
<p>126<br>00:05:36,000 –&gt; 00:05:39,000<br>and the system was designed to be statistically around it.</p>
<p>127<br>00:05:39,000 –&gt; 00:05:42,000<br>And one of the things they did that was really fastening,</p>
<p>128<br>00:05:42,000 –&gt; 00:05:44,000<br>again, it wasn’t just off the shelf,</p>
<p>129<br>00:05:44,000 –&gt; 00:05:47,000<br>or it was all off the shelf hardware,</p>
<p>130<br>00:05:47,000 –&gt; 00:05:49,000<br>but it wasn’t just like CPU disk and memory.</p>
<p>131<br>00:05:49,000 –&gt; 00:05:52,000<br>They actually used FPGA accelerations,</p>
<p>132<br>00:05:52,000 –&gt; 00:05:54,000<br>built inside of it to do things like,</p>
<p>133<br>00:05:54,000 –&gt; 00:05:59,000<br>do hashing for bloom filters,</p>
<p>134<br>00:05:59,000 –&gt; 00:06:02,000<br>do decompression of the data coming off the disk,</p>
<p>135<br>00:06:02,000 –&gt; 00:06:07,000<br>to do transposing from the data from rows and columns.</p>
<p>136<br>00:06:08,000 –&gt; 00:06:13,000<br>So in the paper you guys read, that was the cloud version.</p>
<p>137<br>00:06:13,000 –&gt; 00:06:15,000<br>And the reason why I’m bringing this up,</p>
<p>138<br>00:06:15,000 –&gt; 00:06:17,000<br>I think you can still buy this on-prem hardware,</p>
<p>139<br>00:06:17,000 –&gt; 00:06:19,000<br>this appliance from Yellowbrick,</p>
<p>140<br>00:06:19,000 –&gt; 00:06:21,000<br>the reason why I’m bringing this up,</p>
<p>141<br>00:06:21,000 –&gt; 00:06:23,000<br>because the motivation of the paper,</p>
<p>142<br>00:06:23,000 –&gt; 00:06:26,000<br>why they did all the insane engineering stuff that they did,</p>
<p>143<br>00:06:26,000 –&gt; 00:06:30,000<br>was they were trying to transition their system</p>
<p>144<br>00:06:30,000 –&gt; 00:06:34,000<br>from an on-prem custom hardware to a cloud setting</p>
<p>145<br>00:06:35,000 –&gt; 00:06:37,000<br>and still get all the benefits of acceleration</p>
<p>146<br>00:06:37,000 –&gt; 00:06:39,000<br>that they were getting when they were running</p>
<p>147<br>00:06:39,000 –&gt; 00:06:41,000<br>their own silicon.</p>
<p>148<br>00:06:41,000 –&gt; 00:06:44,000<br>So that’s the background of what Yellowbrick is.</p>
<p>149<br>00:06:44,000 –&gt; 00:06:48,000<br>So even though, again, the company and the product started in 2014,</p>
<p>150<br>00:06:48,000 –&gt; 00:06:52,000<br>I think the cloud version came out in 2020 or 2021.</p>
<p>151<br>00:06:52,000 –&gt; 00:06:55,000<br>So again, I find this system very, very fast-sating.</p>
<p>152<br>00:06:55,000 –&gt; 00:06:59,000<br>To me, when I first started learning about it,</p>
<p>153<br>00:06:59,000 –&gt; 00:07:02,000<br>Yellowbrick, when they came out of stealth,</p>
<p>154<br>00:07:02,000 –&gt; 00:07:05,000<br>it reminded me also when I first saw a click house,</p>
<p>155<br>00:07:05,000 –&gt; 00:07:07,000<br>when you read the things that they were like,</p>
<p>156<br>00:07:07,000 –&gt; 00:07:09,000<br>oh, we do this, this, this, this, this, this, this.</p>
<p>157<br>00:07:09,000 –&gt; 00:07:10,000<br>And it’s like, this is insane.</p>
<p>158<br>00:07:10,000 –&gt; 00:07:12,000<br>They’re doing so many different things.</p>
<p>159<br>00:07:12,000 –&gt; 00:07:14,000<br>And that was back when they were just doing FPGA stuff.</p>
<p>160<br>00:07:14,000 –&gt; 00:07:17,000<br>And then when the cloud version came out</p>
<p>161<br>00:07:17,000 –&gt; 00:07:19,000<br>and they were like, oh yeah, we’re doing kernel bypass with this</p>
<p>162<br>00:07:19,000 –&gt; 00:07:22,000<br>and custom device drivers for that,</p>
<p>163<br>00:07:22,000 –&gt; 00:07:24,000<br>that’s insane.</p>
<p>164<br>00:07:24,000 –&gt; 00:07:26,000<br>You almost wonder sometimes whether it’s actually true</p>
<p>165<br>00:07:26,000 –&gt; 00:07:29,000<br>and then you meet the people, understand what’s actually going on.</p>
<p>166<br>00:07:29,000 –&gt; 00:07:33,000<br>And then you see that it is actually all real.</p>
<p>167<br>00:07:33,000 –&gt; 00:07:38,000<br>And it’s unclear whether the other major database vendors,</p>
<p>168<br>00:07:38,000 –&gt; 00:07:42,000<br>particularly the Amazon, the Oracle, the Microsoft,</p>
<p>169<br>00:07:42,000 –&gt; 00:07:46,000<br>whether they’re doing the same kind of low-level system</p>
<p>170<br>00:07:46,000 –&gt; 00:07:48,000<br>optimizations that Yellowbrick is doing.</p>
<p>171<br>00:07:48,000 –&gt; 00:07:50,000<br>For some things, I know that Microsoft makes heavy use</p>
<p>172<br>00:07:50,000 –&gt; 00:07:56,000<br>of running FPGA’s or down on the Nix to do filtering and other things.</p>
<p>173<br>00:07:56,000 –&gt; 00:07:59,000<br>But again, I don’t know whether anybody else is doing</p>
<p>174<br>00:07:59,000 –&gt; 00:08:01,000<br>the amount of work that they’ve done.</p>
<p>175<br>00:08:01,000 –&gt; 00:08:03,000<br>And certainly it’s risky.</p>
<p>176<br>00:08:03,000 –&gt; 00:08:05,000<br>Like if you’re looking at brand new database startup</p>
<p>177<br>00:08:05,000 –&gt; 00:08:08,000<br>to say, hey, I’m going to write my own PCIU drivers.</p>
<p>178<br>00:08:08,000 –&gt; 00:08:10,000<br>It’s not something I would actually recommend,</p>
<p>179<br>00:08:10,000 –&gt; 00:08:12,000<br>but they pulled it off.</p>
<p>180<br>00:08:12,000 –&gt; 00:08:14,000<br>All right, so it’s an O-Lap database system.</p>
<p>181<br>00:08:14,000 –&gt; 00:08:18,000<br>That was originally designed for a sort of classic shared</p>
<p>182<br>00:08:18,000 –&gt; 00:08:19,000<br>nothing architecture.</p>
<p>183<br>00:08:19,000 –&gt; 00:08:21,000<br>And then when they transitioned to the cloud,</p>
<p>184<br>00:08:21,000 –&gt; 00:08:25,000<br>they obviously had to switch to a shared disk architecture.</p>
<p>185<br>00:08:25,000 –&gt; 00:08:28,000<br>And they’re going to employ a client-side caching mechanism</p>
<p>186<br>00:08:28,000 –&gt; 00:08:30,000<br>similar to what we saw in Snowflake.</p>
<p>187<br>00:08:30,000 –&gt; 00:08:32,000<br>So everything’s in C++.</p>
<p>188<br>00:08:32,000 –&gt; 00:08:35,000<br>It’s Sarah Luff as a fork of Postgres 9.5.</p>
<p>189<br>00:08:35,000 –&gt; 00:08:37,000<br>So they’re still going to use the Postgres front end</p>
<p>190<br>00:08:37,000 –&gt; 00:08:40,000<br>for the ODBC, GDBC, Wire Protocol.</p>
<p>191<br>00:08:40,000 –&gt; 00:08:42,000<br>They’re still going to use the Postgres parser</p>
<p>192<br>00:08:42,000 –&gt; 00:08:45,000<br>and the catalog, and basically handle all the incoming</p>
<p>193<br>00:08:45,000 –&gt; 00:08:46,000<br>SQL queries.</p>
<p>194<br>00:08:46,000 –&gt; 00:08:50,000<br>But then once you get past the parser,</p>
<p>195<br>00:08:50,000 –&gt; 00:08:53,000<br>you still use remnants of the Postgres query optimizer,</p>
<p>196<br>00:08:53,000 –&gt; 00:08:56,000<br>but they’re going to inject their own optimization</p>
<p>197<br>00:08:56,000 –&gt; 00:09:00,000<br>passes in there to handle the yellow brick specific things.</p>
<p>198<br>00:09:00,000 –&gt; 00:09:02,000<br>And then they hand it off to the query app,</p>
<p>199<br>00:09:02,000 –&gt; 00:09:06,000<br>the compiler, we’ll get to that in a second.</p>
<p>200<br>00:09:06,000 –&gt; 00:09:09,000<br>So the other thing about it in this paper is that,</p>
<p>201<br>00:09:09,000 –&gt; 00:09:12,000<br>and we sort of speculated before we talked about Dremel</p>
<p>202<br>00:09:12,000 –&gt; 00:09:15,000<br>or Snowflake, like, oh, I went over there using Kubernetes</p>
<p>203<br>00:09:15,000 –&gt; 00:09:18,000<br>or how they’re actually running the actual worker nodes themselves,</p>
<p>204<br>00:09:18,000 –&gt; 00:09:22,000<br>they’re heavily based on Kubernetes.</p>
<p>205<br>00:09:22,000 –&gt; 00:09:25,000<br>Like, at all the components in running inside of the system,</p>
<p>206<br>00:09:25,000 –&gt; 00:09:32,000<br>I’m going to set up as services running in Kubernetes.</p>
<p>207<br>00:09:32,000 –&gt; 00:09:34,000<br>And again, the paper talks about how they were surprised</p>
<p>208<br>00:09:34,000 –&gt; 00:09:36,000<br>to see that even though they were running containers,</p>
<p>209<br>00:09:36,000 –&gt; 00:09:39,000<br>they still could have the load level control over the system hardware</p>
<p>210<br>00:09:39,000 –&gt; 00:09:42,000<br>to do the kind of optimizations that they want to do,</p>
<p>211<br>00:09:42,000 –&gt; 00:09:44,000<br>that they’ll still all available to them,</p>
<p>212<br>00:09:44,000 –&gt; 00:09:48,000<br>even though it was running in containers.</p>
<p>213<br>00:09:48,000 –&gt; 00:09:50,000<br>So again, everything we’ll talk about today</p>
<p>214<br>00:09:50,000 –&gt; 00:09:51,000<br>will be the cloud version.</p>
<p>215<br>00:09:51,000 –&gt; 00:09:53,000<br>So we’re not going to talk about the FBGA,</p>
<p>216<br>00:09:53,000 –&gt; 00:09:58,000<br>we’re not going to talk about the custom stuff they were doing</p>
<p>217<br>00:09:58,000 –&gt; 00:10:00,000<br>on the on-prem version.</p>
<p>218<br>00:10:00,000 –&gt; 00:10:04,000<br>It just be all about the cloud.</p>
<p>219<br>00:10:04,000 –&gt; 00:10:05,000<br>All right, so here’s again,</p>
<p>220<br>00:10:05,000 –&gt; 00:10:07,000<br>a high level overview of the key aspects of it,</p>
<p>221<br>00:10:07,000 –&gt; 00:10:10,000<br>going similar to the text on what we’ve used for the other systems</p>
<p>222<br>00:10:10,000 –&gt; 00:10:12,000<br>we talked about so far.</p>
<p>223<br>00:10:12,000 –&gt; 00:10:14,000<br>So as I said, start off as shared nothing,</p>
<p>224<br>00:10:14,000 –&gt; 00:10:16,000<br>but now as a shared disk system,</p>
<p>225<br>00:10:16,000 –&gt; 00:10:18,000<br>which is, we’re going to compute and storage,</p>
<p>226<br>00:10:18,000 –&gt; 00:10:21,000<br>there will be a push-based vectorized query processing.</p>
<p>227<br>00:10:21,000 –&gt; 00:10:23,000<br>They’re going to make heavy use,</p>
<p>228<br>00:10:23,000 –&gt; 00:10:27,000<br>or the entirely code generation of query compilation</p>
<p>229<br>00:10:27,000 –&gt; 00:10:30,000<br>using the transpiration method we talked about with HIGHQ,</p>
<p>230<br>00:10:30,000 –&gt; 00:10:32,000<br>so they’re going to take the query plan and convert it into</p>
<p>231<br>00:10:32,000 –&gt; 00:10:35,000<br>SQL++ code and then go ahead and compile that.</p>
<p>232<br>00:10:35,000 –&gt; 00:10:37,000<br>They’re going to compute side caching,</p>
<p>233<br>00:10:37,000 –&gt; 00:10:40,000<br>similar to we saw on Snowflake.</p>
<p>234<br>00:10:40,000 –&gt; 00:10:44,000<br>They’re going to have a separate row and column storage opponent.</p>
<p>235<br>00:10:45,000 –&gt; 00:10:48,000<br>This is a lot of them to do ingestion of new rights,</p>
<p>236<br>00:10:48,000 –&gt; 00:10:49,000<br>in a row-oriented manner,</p>
<p>237<br>00:10:49,000 –&gt; 00:10:51,000<br>and then the background process is going to convert them</p>
<p>238<br>00:10:51,000 –&gt; 00:10:54,000<br>into a Pax columnar format.</p>
<p>239<br>00:10:54,000 –&gt; 00:10:56,000<br>There will be short merge joins, hash joins,</p>
<p>240<br>00:10:56,000 –&gt; 00:10:58,000<br>the also support nested loop joins.</p>
<p>241<br>00:10:58,000 –&gt; 00:11:00,000<br>And as I said, they’re going to rely heavily still</p>
<p>242<br>00:11:00,000 –&gt; 00:11:01,000<br>on the post-corti-aprimerser,</p>
<p>243<br>00:11:01,000 –&gt; 00:11:06,000<br>but in POS or install their own additional optimization passes</p>
<p>244<br>00:11:06,000 –&gt; 00:11:13,000<br>for, you know, for, you know, for the yellow brick stuff.</p>
<p>245<br>00:11:13,000 –&gt; 00:11:16,000<br>And, and then, you know, say this multiple times,</p>
<p>246<br>00:11:16,000 –&gt; 00:11:19,000<br>like co-king level and seeing system engineering</p>
<p>247<br>00:11:19,000 –&gt; 00:11:21,000<br>from the optimizations.</p>
<p>248<br>00:11:21,000 –&gt; 00:11:23,000<br>So we’re going to talk about all of these,</p>
<p>249<br>00:11:23,000 –&gt; 00:11:26,000<br>but I think these two ones are probably the most interesting ones,</p>
<p>250<br>00:11:26,000 –&gt; 00:11:28,000<br>these are the ones we can talk about the most,</p>
<p>251<br>00:11:28,000 –&gt; 00:11:31,000<br>but what’s most publicly available.</p>
<p>252<br>00:11:31,000 –&gt; 00:11:33,000<br>All right, so the nomically,</p>
<p>253<br>00:11:33,000 –&gt; 00:11:37,000<br>they’re going to use it, say, that there’ll be a data warehouse instance,</p>
<p>254<br>00:11:37,000 –&gt; 00:11:41,000<br>and that’ll be the front end service of the database system</p>
<p>255<br>00:11:41,000 –&gt; 00:11:44,000<br>that’s going to manage all the, you sort of have the complete purview</p>
<p>256<br>00:11:44,000 –&gt; 00:11:46,000<br>of the worker nodes and the,</p>
<p>257<br>00:11:46,000 –&gt; 00:11:50,000<br>and other additional services running for a single customer.</p>
<p>258<br>00:11:50,000 –&gt; 00:11:53,000<br>So the, the, the front end piece is going to have, again,</p>
<p>259<br>00:11:53,000 –&gt; 00:11:58,000<br>that, that, that, that portion of Postgres that’s going to take incoming connections,</p>
<p>260<br>00:11:58,000 –&gt; 00:12:00,000<br>do the parsing, the plan, the optimization.</p>
<p>261<br>00:12:00,000 –&gt; 00:12:03,000<br>It also is where they’re going to run the,</p>
<p>262<br>00:12:03,000 –&gt; 00:12:05,000<br>the Rostro portion of the system,</p>
<p>263<br>00:12:05,000 –&gt; 00:12:08,000<br>but then they’re still going to use Postgres for the catalogs.</p>
<p>264<br>00:12:08,000 –&gt; 00:12:12,000<br>They’re just going to use PG catalog inside of the,</p>
<p>265<br>00:12:12,000 –&gt; 00:12:14,000<br>Postgres keeps track of where, what data is available,</p>
<p>266<br>00:12:14,000 –&gt; 00:12:17,000<br>where, you know, we’re, how to map the shards to,</p>
<p>267<br>00:12:17,000 –&gt; 00:12:19,000<br>the files to, to different nodes.</p>
<p>268<br>00:12:19,000 –&gt; 00:12:22,000<br>They’re going to make heavy use of caching for this and other nodes</p>
<p>269<br>00:12:22,000 –&gt; 00:12:25,000<br>because they hit up PG catalog for every single time we want to do a request,</p>
<p>270<br>00:12:25,000 –&gt; 00:12:27,000<br>would be too slow,</p>
<p>271<br>00:12:27,000 –&gt; 00:12:29,000<br>and for transaction management as well,</p>
<p>272<br>00:12:29,000 –&gt; 00:12:31,000<br>which we’re not really talking about,</p>
<p>273<br>00:12:31,000 –&gt; 00:12:32,000<br>but they try to mimic,</p>
<p>274<br>00:12:32,000 –&gt; 00:12:37,000<br>or try to follow Postgres’ style at MVCC to do, to do transactions.</p>
<p>275<br>00:12:38,000 –&gt; 00:12:40,000<br>The worker nodes are going to be single containers,</p>
<p>276<br>00:12:40,000 –&gt; 00:12:43,000<br>and this is, they’re, they’re, as he, as he describes it,</p>
<p>277<br>00:12:43,000 –&gt; 00:12:45,000<br>they’re pretty lightweight or dumb.</p>
<p>278<br>00:12:45,000 –&gt; 00:12:48,000<br>There’s, you know, they just, given the task of, here’s Kapal query,</p>
<p>279<br>00:12:48,000 –&gt; 00:12:49,000<br>and they go ahead and run it,</p>
<p>280<br>00:12:49,000 –&gt; 00:12:51,000<br>and know how to move the data back and forth between,</p>
<p>281<br>00:12:51,000 –&gt; 00:12:53,000<br>between the hardware and the different nodes.</p>
<p>282<br>00:12:53,000 –&gt; 00:12:57,000<br>But they’re also going to maintain their own local cache using,</p>
<p>283<br>00:12:57,000 –&gt; 00:13:01,000<br>you know, MVME drives to write things out as,</p>
<p>284<br>00:13:01,000 –&gt; 00:13:02,000<br>as needed if they have this built to it.</p>
<p>285<br>00:13:02,000 –&gt; 00:13:04,000<br>And we’ll talk about that in a second.</p>
<p>286<br>00:13:04,000 –&gt; 00:13:06,000<br>And then there’s separate additional services</p>
<p>287<br>00:13:06,000 –&gt; 00:13:08,000<br>that are running background tasks or maintenance tasks.</p>
<p>288<br>00:13:08,000 –&gt; 00:13:13,000<br>This is for compilation, they run analyze in these background tasks,</p>
<p>289<br>00:13:13,000 –&gt; 00:13:16,000<br>bulk loading, they actually do a bypass of the row store,</p>
<p>290<br>00:13:16,000 –&gt; 00:13:18,000<br>so you can just take, you know, large files and dump them directly,</p>
<p>291<br>00:13:18,000 –&gt; 00:13:19,000<br>a column store.</p>
<p>292<br>00:13:21,000 –&gt; 00:13:23,000<br>So the high level architecture looks like this,</p>
<p>293<br>00:13:23,000 –&gt; 00:13:25,000<br>right, again, so this is the front end,</p>
<p>294<br>00:13:25,000 –&gt; 00:13:26,000<br>and then you have the worker nodes,</p>
<p>295<br>00:13:26,000 –&gt; 00:13:29,000<br>and then the object store is just s3, or whatever Azure has.</p>
<p>296<br>00:13:29,000 –&gt; 00:13:32,000<br>Query shows up, it’s going to go through this Postgres front layer,</p>
<p>297<br>00:13:32,000 –&gt; 00:13:35,000<br>again, through all the parsing and planning there,</p>
<p>298<br>00:13:35,000 –&gt; 00:13:37,000<br>and then it’s going to hand things off to the scheduler,</p>
<p>299<br>00:13:37,000 –&gt; 00:13:40,000<br>and the compiler service, we’ll talk about each of these in a second,</p>
<p>300<br>00:13:40,000 –&gt; 00:13:43,000<br>but they have a centralized scheduler that then is going to hand out</p>
<p>301<br>00:13:43,000 –&gt; 00:13:45,000<br>the tasks to the different worker nodes,</p>
<p>302<br>00:13:45,000 –&gt; 00:13:47,000<br>and then every 100 milliseconds they’re going to coordinate</p>
<p>303<br>00:13:47,000 –&gt; 00:13:50,000<br>and synchronize and say what task should we work on next.</p>
<p>304<br>00:13:50,000 –&gt; 00:13:52,000<br>And these guys are all going to run as co-routines.</p>
<p>305<br>00:13:52,000 –&gt; 00:13:54,000<br>Compiler service, we’ll see in a second,</p>
<p>306<br>00:13:54,000 –&gt; 00:13:56,000<br>and it’s just converting the query plan into</p>
<p>307<br>00:13:56,000 –&gt; 00:13:58,000<br>C++ and go ahead and compile it,</p>
<p>308<br>00:13:58,000 –&gt; 00:14:03,000<br>but they’ll do some tricks to make this run faster in parallel with the LLVM.</p>
<p>309<br>00:14:04,000 –&gt; 00:14:06,000<br>Schedule hands out the tasks to the worker nodes,</p>
<p>310<br>00:14:06,000 –&gt; 00:14:09,000<br>assuming that the worker nodes’ caches are empty,</p>
<p>311<br>00:14:09,000 –&gt; 00:14:12,000<br>if they need any data that go out to the object store and get it stored</p>
<p>312<br>00:14:12,000 –&gt; 00:14:17,000<br>in its local cache, they’ll do an approximate version of LLVK,</p>
<p>313<br>00:14:17,000 –&gt; 00:14:21,000<br>which we’ll cover in a second, to decide when to evict things from memory,</p>
<p>314<br>00:14:21,000 –&gt; 00:14:25,000<br>or things from the cache, and then they can also move data</p>
<p>315<br>00:14:25,000 –&gt; 00:14:28,000<br>back and forth between different worker nodes.</p>
<p>316<br>00:14:28,000 –&gt; 00:14:31,000<br>And then as I said, if you’re doing a bulk loading of like,</p>
<p>317<br>00:14:31,000 –&gt; 00:14:34,000<br>I have terabytes of files, I don’t want to go through the row store,</p>
<p>318<br>00:14:34,000 –&gt; 00:14:37,000<br>and then have it get compacted to the object store,</p>
<p>319<br>00:14:37,000 –&gt; 00:14:40,000<br>you can have this bulk load of service just right directly out to</p>
<p>320<br>00:14:40,000 –&gt; 00:14:43,000<br>the object store all in a transactional manner.</p>
<p>321<br>00:14:44,000 –&gt; 00:14:45,000<br>Yes?</p>
<p>322<br>00:14:45,000 –&gt; 00:14:48,000<br>So the paper says that the old brick is shared nothing.</p>
<p>323<br>00:14:48,000 –&gt; 00:14:50,000<br>Yeah, that parts a little bit confusing.</p>
<p>324<br>00:14:50,000 –&gt; 00:14:53,000<br>So again, historically it was a shared nothing system.</p>
<p>325<br>00:14:53,000 –&gt; 00:14:58,000<br>And I think they’re incorrect because like,</p>
<p>326<br>00:14:58,000 –&gt; 00:15:01,000<br>these things are caches, right?</p>
<p>327<br>00:15:01,000 –&gt; 00:15:04,000<br>The primary location of the database at rest is over here.</p>
<p>328<br>00:15:04,000 –&gt; 00:15:07,000<br>Now, ignoring the row store, but it doesn’t really make a difference.</p>
<p>329<br>00:15:07,000 –&gt; 00:15:10,000<br>And then they talk about how, okay, if any time I need to get data,</p>
<p>330<br>00:15:10,000 –&gt; 00:15:12,000<br>I go fetch it from the object store, right?</p>
<p>331<br>00:15:12,000 –&gt; 00:15:17,000<br>So it’s like a, it’s a right back caches that are right through cache,</p>
<p>332<br>00:15:17,000 –&gt; 00:15:20,000<br>meaning I get data and I want to get it to the object store,</p>
<p>333<br>00:15:20,000 –&gt; 00:15:24,000<br>and then I pull in and then I fill my caches as needed when I run queries.</p>
<p>334<br>00:15:25,000 –&gt; 00:15:28,000<br>If it was shared nothing, then I would not never have to do that,</p>
<p>335<br>00:15:28,000 –&gt; 00:15:30,000<br>because I would always populate this stuff here.</p>
<p>336<br>00:15:30,000 –&gt; 00:15:33,000<br>Is it like, would the decision be that like,</p>
<p>337<br>00:15:33,000 –&gt; 00:15:35,000<br>whatever we take the data from the object store,</p>
<p>338<br>00:15:35,000 –&gt; 00:15:37,000<br>will all the queries running and never go back?</p>
<p>339<br>00:15:37,000 –&gt; 00:15:39,000<br>So that could be seen as something like fill,</p>
<p>340<br>00:15:39,000 –&gt; 00:15:43,000<br>they’ll use the industry as like intermediate.</p>
<p>341<br>00:15:43,000 –&gt; 00:15:47,000<br>So his comment is like,</p>
<p>342<br>00:15:47,000 –&gt; 00:15:49,000<br>is it the case where they will never have,</p>
<p>343<br>00:15:49,000 –&gt; 00:15:51,000<br>like, is it the case that they have to spill the disk,</p>
<p>344<br>00:15:51,000 –&gt; 00:15:54,000<br>if they run out of disk space on the worker nodes,</p>
<p>345<br>00:15:54,000 –&gt; 00:15:56,000<br>then they don’t write to the object store.</p>
<p>346<br>00:15:56,000 –&gt; 00:15:59,000<br>As far as I can know from the paper, they don’t write to the object store.</p>
<p>347<br>00:15:59,000 –&gt; 00:16:01,000<br>And then it’ll do back pressure to say,</p>
<p>348<br>00:16:01,000 –&gt; 00:16:03,000<br>okay, we can’t process this as fast as possible,</p>
<p>349<br>00:16:03,000 –&gt; 00:16:06,000<br>so everyone sort of slows down that way.</p>
<p>350<br>00:16:06,000 –&gt; 00:16:11,000<br>Again, like, historically it was a shared nothing system when it was on prem,</p>
<p>351<br>00:16:11,000 –&gt; 00:16:13,000<br>but what they described in the paper,</p>
<p>352<br>00:16:13,000 –&gt; 00:16:16,000<br>smells like snowflake and snowflake is a shared disk system.</p>
<p>353<br>00:16:16,000 –&gt; 00:16:18,000<br>So I think that’s incorrect.</p>
<p>354<br>00:16:21,000 –&gt; 00:16:25,000<br>All right, so again, the insane parts of here for these guys is that</p>
<p>355<br>00:16:25,000 –&gt; 00:16:28,000<br>all of this, like, these lines are drawing here.</p>
<p>356<br>00:16:28,000 –&gt; 00:16:32,000<br>This is all being custom stuff, either custom drivers or custom API limitations,</p>
<p>357<br>00:16:32,000 –&gt; 00:16:37,000<br>custom protocols to talk to either internally between the client,</p>
<p>358<br>00:16:37,000 –&gt; 00:16:39,000<br>internally between the network nodes,</p>
<p>359<br>00:16:39,000 –&gt; 00:16:42,000<br>and also within the object store itself.</p>
<p>360<br>00:16:42,000 –&gt; 00:16:46,000<br>But the outbound stuff going back to the client,</p>
<p>361<br>00:16:46,000 –&gt; 00:16:49,000<br>that’s just going to be the regular GDBC wire protocol,</p>
<p>362<br>00:16:49,000 –&gt; 00:16:52,000<br>the ODPC wire protocol that Postgres has.</p>
<p>363<br>00:16:54,000 –&gt; 00:16:56,000<br>So I’ve said some of this already.</p>
<p>364<br>00:16:56,000 –&gt; 00:17:00,000<br>They’re going to be heavily based on Kubernetes.</p>
<p>365<br>00:17:00,000 –&gt; 00:17:02,000<br>So all the different pieces that I’m showing here,</p>
<p>366<br>00:17:02,000 –&gt; 00:17:04,000<br>they’re going to run separate microservices.</p>
<p>367<br>00:17:04,000 –&gt; 00:17:06,000<br>When I think of microservices,</p>
<p>368<br>00:17:06,000 –&gt; 00:17:09,000<br>I think that like something like a Lambda function comes in and comes out,</p>
<p>369<br>00:17:09,000 –&gt; 00:17:11,000<br>like, you know, does some processing goes away.</p>
<p>370<br>00:17:11,000 –&gt; 00:17:13,000<br>These things are always running, so,</p>
<p>371<br>00:17:13,000 –&gt; 00:17:15,000<br>but they’re using the term microservices.</p>
<p>372<br>00:17:15,000 –&gt; 00:17:19,000<br>Basically, they’ve broken up and things are running separately.</p>
<p>373<br>00:17:19,000 –&gt; 00:17:23,000<br>And so they’re going to leverage Kubernetes to do all the state management,</p>
<p>374<br>00:17:23,000 –&gt; 00:17:25,000<br>of like, what machines are up, when one goes down,</p>
<p>375<br>00:17:25,000 –&gt; 00:17:28,000<br>how to do failover, how to provision new machines.</p>
<p>376<br>00:17:28,000 –&gt; 00:17:30,000<br>All of that’s going to be done through Kubernetes.</p>
<p>377<br>00:17:30,000 –&gt; 00:17:34,000<br>So that sort of control plane of the backbone of the system itself.</p>
<p>378<br>00:17:34,000 –&gt; 00:17:36,000<br>They don’t write from scratch the way, like,</p>
<p>379<br>00:17:36,000 –&gt; 00:17:38,000<br>snowflake and others had to do.</p>
<p>380<br>00:17:38,000 –&gt; 00:17:42,000<br>There’s an leverage Kubernetes for all of this.</p>
<p>381<br>00:17:43,000 –&gt; 00:17:47,000<br>And as far as I know, I mean, I’m not saying that they’re the only system running Kubernetes,</p>
<p>382<br>00:17:47,000 –&gt; 00:17:51,000<br>but there’s only these paper that publicly talks about how they’re using Kubernetes</p>
<p>383<br>00:17:51,000 –&gt; 00:17:54,000<br>to organize a scale-out system like this.</p>
<p>384<br>00:17:54,000 –&gt; 00:17:57,000<br>But if you ever use Kubernetes,</p>
<p>385<br>00:17:57,000 –&gt; 00:18:00,000<br>it’s depending on how to set up.</p>
<p>386<br>00:18:00,000 –&gt; 00:18:03,000<br>And so you’d have to write the Helm chart to figure out how to,</p>
<p>387<br>00:18:03,000 –&gt; 00:18:05,000<br>you know, solve the puzzle and whatnot.</p>
<p>388<br>00:18:05,000 –&gt; 00:18:09,000<br>So they’re going to hide all of the Kubernetes operations behind SQL,</p>
<p>389<br>00:18:09,000 –&gt; 00:18:11,000<br>which is fantastic.</p>
<p>390<br>00:18:11,000 –&gt; 00:18:14,000<br>So you can call it create cluster or create instance or whatever,</p>
<p>391<br>00:18:14,000 –&gt; 00:18:17,000<br>and not all done through SQL, and then it’s translating that into the Kubernetes commands</p>
<p>392<br>00:18:17,000 –&gt; 00:18:20,000<br>to apply the changes.</p>
<p>393<br>00:18:20,000 –&gt; 00:18:21,000<br>It’s nice.</p>
<p>394<br>00:18:21,000 –&gt; 00:18:29,000<br>Obviously not groundbreakingly, but it’s a quality of life feature that I like.</p>
<p>395<br>00:18:29,000 –&gt; 00:18:34,000<br>The other thing they’re going to do that sort of goes against the way you’re expected</p>
<p>396<br>00:18:34,000 –&gt; 00:18:38,000<br>to run or operate Kubernetes is that when the worker pods come up,</p>
<p>397<br>00:18:38,000 –&gt; 00:18:41,000<br>they want to make sure they have a one-to-one mapping between a worker pod and a worker node</p>
<p>398<br>00:18:41,000 –&gt; 00:18:44,000<br>because when a worker pod lands on the node,</p>
<p>399<br>00:18:44,000 –&gt; 00:18:47,000<br>they want to take over everything.</p>
<p>400<br>00:18:47,000 –&gt; 00:18:52,000<br>And again, this is why I’m saying it’s not truly like a microservice where,</p>
<p>401<br>00:18:52,000 –&gt; 00:18:54,000<br>because this worker pod,</p>
<p>402<br>00:18:54,000 –&gt; 00:18:56,000<br>it’s going to allocate all the memory that it wants,</p>
<p>403<br>00:18:56,000 –&gt; 00:19:01,000<br>you just want to run forever because you don’t want to have the spin-up cost</p>
<p>404<br>00:19:01,000 –&gt; 00:19:05,000<br>of starting a stopping worker pod over and over again.</p>
<p>405<br>00:19:05,000 –&gt; 00:19:08,000<br>And again, the paper talks about how they thought to be trouble,</p>
<p>406<br>00:19:08,000 –&gt; 00:19:10,000<br>they have troubles with the worker node,</p>
<p>407<br>00:19:10,000 –&gt; 00:19:14,000<br>starting to work on pod, taking control of all the hardware,</p>
<p>408<br>00:19:14,000 –&gt; 00:19:17,000<br>at least through the container, and that seemed to work,</p>
<p>409<br>00:19:17,000 –&gt; 00:19:19,000<br>but if you had two containers running at the same node,</p>
<p>410<br>00:19:19,000 –&gt; 00:19:21,000<br>then they would conflict and try to get to the hardware,</p>
<p>411<br>00:19:21,000 –&gt; 00:19:23,000<br>and that would break things.</p>
<p>412<br>00:19:23,000 –&gt; 00:19:29,000<br>So that’s why they have a one-to-one mapping for the worker pod and the worker node.</p>
<p>413<br>00:19:29,000 –&gt; 00:19:32,000<br>All right, so the execution engine is the push-based model we’ve talked about before.</p>
<p>414<br>00:19:32,000 –&gt; 00:19:34,000<br>They’re doing vectorized query processing,</p>
<p>415<br>00:19:34,000 –&gt; 00:19:36,000<br>when they’re operating on a clonear data,</p>
<p>416<br>00:19:36,000 –&gt; 00:19:38,000<br>but as I said, in some cases,</p>
<p>417<br>00:19:38,000 –&gt; 00:19:41,000<br>they’re actually going to convert things into row store,</p>
<p>418<br>00:19:41,000 –&gt; 00:19:46,000<br>row-oriented data, essentially doing early materialization in some cases.</p>
<p>419<br>00:19:46,000 –&gt; 00:19:50,000<br>So like, all the scans we vectorized based on clonear data,</p>
<p>420<br>00:19:50,000 –&gt; 00:19:52,000<br>but then if you feed that up into a join,</p>
<p>421<br>00:19:52,000 –&gt; 00:19:55,000<br>they’re going to convert it to a row store,</p>
<p>422<br>00:19:55,000 –&gt; 00:19:58,000<br>right, they call it a transpose operator,</p>
<p>423<br>00:19:58,000 –&gt; 00:20:01,000<br>they’re going to convert, they have a injector transpose operator in the query plan</p>
<p>424<br>00:20:01,000 –&gt; 00:20:05,000<br>to convert it, but fully pass it up into the hash join.</p>
<p>425<br>00:20:05,000 –&gt; 00:20:08,000<br>And likewise, when they send data from one node to the next,</p>
<p>426<br>00:20:08,000 –&gt; 00:20:12,000<br>they’re going to convert it into a row-oriented manner</p>
<p>427<br>00:20:12,000 –&gt; 00:20:17,000<br>because they want to send smaller chunks to make sure that the data you’re sending over</p>
<p>428<br>00:20:17,000 –&gt; 00:20:20,000<br>all sits in L3 cache on the other side,</p>
<p>429<br>00:20:20,000 –&gt; 00:20:22,000<br>and you can process things very efficiently.</p>
<p>430<br>00:20:22,000 –&gt; 00:20:25,000<br>And then when the overarching themes we’ll see over and over again,</p>
<p>431<br>00:20:25,000 –&gt; 00:20:28,000<br>is that the goal of the system that they designed it,</p>
<p>432<br>00:20:29,000 –&gt; 00:20:32,000<br>is that you want to, everything sitting in L3 is much as possible.</p>
<p>433<br>00:20:32,000 –&gt; 00:20:36,000<br>Right? It’s not like when we talk about other systems like,</p>
<p>434<br>00:20:36,000 –&gt; 00:20:38,000<br>oh yeah, you know, keep things in memory, don’t go to disk,</p>
<p>435<br>00:20:38,000 –&gt; 00:20:41,000<br>these guys are insane, they want to keep everything in L3.</p>
<p>436<br>00:20:41,000 –&gt; 00:20:45,000<br>And so that’s why they’re doing all the custom optimizations that they’re doing,</p>
<p>437<br>00:20:45,000 –&gt; 00:20:48,000<br>and their architect of the system says that,</p>
<p>438<br>00:20:48,000 –&gt; 00:20:50,000<br>you know, by converting to a row store,</p>
<p>439<br>00:20:50,000 –&gt; 00:20:54,000<br>you know that all the data you can need to process a single tuple before you can move on to the next tuple,</p>
<p>440<br>00:20:54,000 –&gt; 00:20:56,000<br>is going to sit in L3.</p>
<p>441<br>00:21:00,000 –&gt; 00:21:02,000<br>As I said before, they’re going to do holistic query compilation,</p>
<p>442<br>00:21:02,000 –&gt; 00:21:05,000<br>doing source search translation of generating into the C++.</p>
<p>443<br>00:21:06,000 –&gt; 00:21:08,000<br>And this is going to be done through a separate service.</p>
<p>444<br>00:21:08,000 –&gt; 00:21:11,000<br>Now, it’s debating whether we should be read shift first versus this paper,</p>
<p>445<br>00:21:11,000 –&gt; 00:21:14,000<br>because read shift is basically going to do the same thing.</p>
<p>446<br>00:21:14,000 –&gt; 00:21:16,000<br>Right? They’re going to do code gen into C++,</p>
<p>447<br>00:21:16,000 –&gt; 00:21:19,000<br>and they’re going to have a separate service that’s compiler service</p>
<p>448<br>00:21:19,000 –&gt; 00:21:23,000<br>that you send queries to, and then you get back the binary.</p>
<p>449<br>00:21:24,000 –&gt; 00:21:29,000<br>But I wanted to cover the yellow bit optimizations first before we took out a read shift.</p>
<p>450<br>00:21:30,000 –&gt; 00:21:34,000<br>So this idea is not novel, again, what they’re doing is standalone service.</p>
<p>451<br>00:21:34,000 –&gt; 00:21:36,000<br>This is what read shift is going to do.</p>
<p>452<br>00:21:37,000 –&gt; 00:21:40,000<br>But one of the things that’s interesting about this is that they talk about,</p>
<p>453<br>00:21:40,000 –&gt; 00:21:45,000<br>if you have a query plan and use generate, you know, a plan fragment as like a giant,</p>
<p>454<br>00:21:45,000 –&gt; 00:21:52,000<br>just one giant file, then the core compiler component inside L of the M is single thread.</p>
<p>455<br>00:21:53,000 –&gt; 00:21:57,000<br>Like you can compile multiple files at the same time in parallel,</p>
<p>456<br>00:21:57,000 –&gt; 00:22:02,000<br>but within a single thread it can only compile a single, you know, single piece of code.</p>
<p>457<br>00:22:02,000 –&gt; 00:22:09,000<br>So they’re going to artificially break up a query plan into different fragments,</p>
<p>458<br>00:22:09,000 –&gt; 00:22:13,000<br>let them be compiled separately into multiple threads,</p>
<p>459<br>00:22:13,000 –&gt; 00:22:18,000<br>and then they’re going to stitch things back together doing just dynamic linking.</p>
<p>460<br>00:22:19,000 –&gt; 00:22:23,000<br>It’s similar to the plan stitching stuff we talked about when we talked about a depth of query optimization,</p>
<p>461<br>00:22:23,000 –&gt; 00:22:28,000<br>they’re sort of breaking things up into the smaller chunks that may not be a single pipeline,</p>
<p>462<br>00:22:28,000 –&gt; 00:22:31,000<br>but again, you can compile those in parallel and then put things back together.</p>
<p>463<br>00:22:33,000 –&gt; 00:22:38,000<br>The compiler service is also going to maintain a cache so that they recognize that I’ve seen this fragment before.</p>
<p>464<br>00:22:38,000 –&gt; 00:22:44,000<br>I have a cache version of it, and it’s the right version of the run time of the database system</p>
<p>465<br>00:22:45,000 –&gt; 00:22:48,000<br>and the right hardware and the other dependencies you may have,</p>
<p>466<br>00:22:48,000 –&gt; 00:22:52,000<br>then it can reuse that, give you that shared object, instead of compiling from scratch.</p>
<p>467<br>00:22:52,000 –&gt; 00:22:55,000<br>Again, we’ll see this again in redshift. Redshift’s insane.</p>
<p>468<br>00:22:55,000 –&gt; 00:22:58,000<br>They have a, it’s not the same, it’s amazing.</p>
<p>469<br>00:22:58,000 –&gt; 00:23:02,000<br>They have a cache for your database, so they see the same plan,</p>
<p>470<br>00:23:02,000 –&gt; 00:23:05,000<br>plan, plan, plan, open over again for within your database, they’ll reuse that,</p>
<p>471<br>00:23:05,000 –&gt; 00:23:10,000<br>but they can also have a shared cache across the entire fleet.</p>
<p>472<br>00:23:11,000 –&gt; 00:23:16,000<br>If some other customer has a table that has integer column and a string column that are,</p>
<p>473<br>00:23:16,000 –&gt; 00:23:21,000<br>and then you’re doing a less than on it, their query, they might have a compile version</p>
<p>474<br>00:23:21,000 –&gt; 00:23:25,000<br>of a plan fragment that processes that data and you have the same basic layout.</p>
<p>475<br>00:23:25,000 –&gt; 00:23:28,000<br>The name of the top columns may be different, or the distribution might be different,</p>
<p>476<br>00:23:28,000 –&gt; 00:23:33,000<br>but who cares? They actually data itself, the underlying physical representation of the data is the same,</p>
<p>477<br>00:23:33,000 –&gt; 00:23:35,000<br>so I can reuse that plan cache.</p>
<p>478<br>00:23:35,000 –&gt; 00:23:38,000<br>So we’ll talk about what Amazon does in X-Class,</p>
<p>479<br>00:23:38,000 –&gt; 00:23:44,000<br>but in this version, because they don’t talk about whether they can cache things across different customers,</p>
<p>480<br>00:23:44,000 –&gt; 00:23:48,000<br>I think everything is still isolated to a single customer,</p>
<p>481<br>00:23:48,000 –&gt; 00:23:51,000<br>but Amazon will be more aggressive when reusing things.</p>
<p>482<br>00:23:56,000 –&gt; 00:24:01,000<br>So the optimizer itself, again, they’re just going to rely on the Postgres stratified optimizer,</p>
<p>483<br>00:24:01,000 –&gt; 00:24:05,000<br>a bunch of rules to do predicate pushdown and other optimizations and rewriting,</p>
<p>484<br>00:24:05,000 –&gt; 00:24:08,000<br>then you switch into the dynamic programming section where they’re doing the join ordering,</p>
<p>485<br>00:24:08,000 –&gt; 00:24:12,000<br>and then at least in Postgres they have to pop out of that, they do some final cleanup.</p>
<p>486<br>00:24:12,000 –&gt; 00:24:19,000<br>But they’re going to inject, they’re going to have their own cost model extensions to Postgres</p>
<p>487<br>00:24:19,000 –&gt; 00:24:23,000<br>and some additional passes they’ll do to do additional optimizations based on,</p>
<p>488<br>00:24:23,000 –&gt; 00:24:25,000<br>again, the yellow brick architecture.</p>
<p>489<br>00:24:25,000 –&gt; 00:24:30,000<br>And one of the things they’re going to introduce is more aggressive file filtering,</p>
<p>490<br>00:24:30,000 –&gt; 00:24:34,000<br>or data file filtering from the object store, by doing lookups and zone maps and other things like that.</p>
<p>491<br>00:24:35,000 –&gt; 00:24:41,000<br>Identifying these are the files I know I never need to read because my predicate can’t, can never be satisfied.</p>
<p>492<br>00:24:43,000 –&gt; 00:24:47,000<br>The other thing though that’s going to be different than all the other systems we’ve talked about before is that</p>
<p>493<br>00:24:47,000 –&gt; 00:24:51,000<br>they’re going to have their own managed storage.</p>
<p>494<br>00:24:51,000 –&gt; 00:24:57,000<br>So unlike in Dremel and Databricks and Snowflake, at Icebroke,</p>
<p>495<br>00:24:57,000 –&gt; 00:25:03,000<br>they’re not going to let you just have a bunch of arbitrary files in S3</p>
<p>496<br>00:25:03,000 –&gt; 00:25:06,000<br>and then write whatever SQL queries you want to top of them.</p>
<p>497<br>00:25:06,000 –&gt; 00:25:10,000<br>You have to bulk load them and import them into the yellow brick system</p>
<p>498<br>00:25:10,000 –&gt; 00:25:12,000<br>and be managed by yellow brick.</p>
<p>499<br>00:25:12,000 –&gt; 00:25:18,000<br>And then at that point yellow brick can then run analyze on it and collect statistics as if it was a regular database system.</p>
<p>500<br>00:25:18,000 –&gt; 00:25:20,000<br>Not a lake house.</p>
<p>501<br>00:25:20,000 –&gt; 00:25:24,000<br>So again, so the current version can’t take arbitrary files in your data lake.</p>
<p>502<br>00:25:24,000 –&gt; 00:25:29,000<br>Everything’s got to be loaded into the system, then at which point you can collect statistics.</p>
<p>503<br>00:25:29,000 –&gt; 00:25:33,000<br>Now when you’re block loading too, you can collect some basic things like some zone maps and whatnot.</p>
<p>504<br>00:25:33,000 –&gt; 00:25:41,000<br>But they’ll do a heavyweight analyze pass, like a single node system to go get histograms, heavy hitter listings.</p>
<p>505<br>00:25:41,000 –&gt; 00:25:45,000<br>They make heavy use of hyper log log, number of null values or sync values.</p>
<p>506<br>00:25:45,000 –&gt; 00:25:50,000<br>All these things they’re going to maintain, store that in the Postgres catalog on the front end service.</p>
<p>507<br>00:25:50,000 –&gt; 00:25:56,000<br>And then their extensions to the query optimizer will be able to use them in their cost model estimates.</p>
<p>508<br>00:25:57,000 –&gt; 00:26:00,000<br>Again, that’s different than everything we’ve seen so far.</p>
<p>509<br>00:26:00,000 –&gt; 00:26:07,000<br>And as far as they can tell, they don’t have any adaptive optimizations that we’ve seen in Dremel and Snowflake and Databricks.</p>
<p>510<br>00:26:07,000 –&gt; 00:26:13,000<br>They’re going to assume everything is going to be whatever the plan that you get out from the query optimizer, that’s it.</p>
<p>511<br>00:26:13,000 –&gt; 00:26:16,000<br>I’m not going to adapt them to fly.</p>
<p>512<br>00:26:16,000 –&gt; 00:26:19,000<br>They’re going to do the classic optimization we’ve seen many times before.</p>
<p>513<br>00:26:20,000 –&gt; 00:26:25,000<br>Through the sideways information passing for the Bloomfotters on the build side of the hash join.</p>
<p>514<br>00:26:25,000 –&gt; 00:26:30,000<br>So as in building hash table, build a Bloomfotter and then send that over on the probe side to do early filtering that way.</p>
<p>515<br>00:26:33,000 –&gt; 00:26:43,000<br>So that part again, other than the, from a non-Lakeout system, this is sort of a textbook implementation of how to collect statistics in your query optimizer.</p>
<p>516<br>00:26:44,000 –&gt; 00:26:53,000<br>I think I asked them to when you get, they gave a talk of like, sort of related what you, what you were bringing up.</p>
<p>517<br>00:26:53,000 –&gt; 00:27:08,000<br>I was like, well, how good, you know, how good is your query optimizer and how often do you make changes because there’s things that’s missing because, you know, the Postgres optimizer doesn’t know about yellow brick stuff versus like the Postgres optimizer just does something stupid.</p>
<p>518<br>00:27:09,000 –&gt; 00:27:17,000<br>And you got to add rules in or add changes to the, to the optimizer’s logic to account for, you know, to avoid bad plans.</p>
<p>519<br>00:27:17,000 –&gt; 00:27:22,000<br>And they mentioned that what they’re doing is basically adding hard coded rules per customer.</p>
<p>520<br>00:27:22,000 –&gt; 00:27:25,000<br>That’s their ideal situation.</p>
<p>521<br>00:27:25,000 –&gt; 00:27:31,000<br>And that they, as far as I know, they haven’t pushed any changes back into the Postgres mainline branch.</p>
<p>522<br>00:27:32,000 –&gt; 00:27:39,000<br>Again, when you’re a small startup, actually, I don’t know how big they are. Medium size startup.</p>
<p>523<br>00:27:39,000 –&gt; 00:27:45,000<br>Yeah, I pick and choose your battles and they chose to optimize in low level OS stuff versus the query optimizer.</p>
<p>524<br>00:27:45,000 –&gt; 00:27:49,000<br>Although, again, they claim that this is what they’re focusing on now the most.</p>
<p>525<br>00:27:49,000 –&gt; 00:27:54,000<br>Now that they’ve gotten sort of the OS level stuff taken care of.</p>
<p>526<br>00:27:55,000 –&gt; 00:28:07,000<br>All right. So the, as I said before, the yellow brick, even though it’s a shared disk system, it is going to have its own managed storage.</p>
<p>527<br>00:28:07,000 –&gt; 00:28:10,000<br>So you tell it, here’s my S3 bucket so I can store stuff.</p>
<p>528<br>00:28:10,000 –&gt; 00:28:16,000<br>And then yellow bricks is going to be in charge of, you know, writing, writing, reading, writing data into those files.</p>
<p>529<br>00:28:16,000 –&gt; 00:28:20,000<br>And they’re going to be using their own proprietary format.</p>
<p>530<br>00:28:21,000 –&gt; 00:28:26,000<br>I know they’re doing dictionary encoding. I don’t know what other additional optimizations that they’re doing.</p>
<p>531<br>00:28:26,000 –&gt; 00:28:33,000<br>And when you load a file or sorry, load a table and start adding data to it, you can specify the charting key or the partitioning key.</p>
<p>532<br>00:28:33,000 –&gt; 00:28:39,000<br>In anticipation of doing joins, you’re also going to do local sorting attributes.</p>
<p>533<br>00:28:39,000 –&gt; 00:28:43,000<br>So within the file itself, you can have them sort of on a single column.</p>
<p>534<br>00:28:43,000 –&gt; 00:28:48,000<br>So they won’t be globally sort across all the files, but within the file it’ll be sorted.</p>
<p>535<br>00:28:48,000 –&gt; 00:28:53,000<br>So the files themselves, they’ll be roughly 100 megabytes with two megabyte chunks.</p>
<p>536<br>00:28:53,000 –&gt; 00:29:00,000<br>And there’s two megabyte numbers going to be special for us later on. So we’ll just keep that in the back of your mind.</p>
<p>537<br>00:29:00,000 –&gt; 00:29:07,000<br>So you sort of think of like the row groups are two megabytes for these guys.</p>
<p>538<br>00:29:07,000 –&gt; 00:29:11,000<br>They do, again, they say they do support bulk loading, parquet files.</p>
<p>539<br>00:29:11,000 –&gt; 00:29:14,000<br>And I don’t think they support ORC or other formats.</p>
<p>540<br>00:29:14,000 –&gt; 00:29:20,000<br>But you can’t take the, how does this?</p>
<p>541<br>00:29:20,000 –&gt; 00:29:23,000<br>They can’t support all possible variations of the things you can do in parquet.</p>
<p>542<br>00:29:23,000 –&gt; 00:29:26,000<br>Like I know they claim they can’t do nested data.</p>
<p>543<br>00:29:26,000 –&gt; 00:29:30,000<br>They say they can’t break up your JSON information and store that as columns that we saw in Dremel.</p>
<p>544<br>00:29:30,000 –&gt; 00:29:35,000<br>And there’s certain data types that they don’t support.</p>
<p>545<br>00:29:35,000 –&gt; 00:29:39,000<br>And again, there’s going to be this row store, which the original version was just Postgres.</p>
<p>546<br>00:29:39,000 –&gt; 00:29:43,000<br>So when you inserted new data, landed in this Postgres version, they were running the front end.</p>
<p>547<br>00:29:43,000 –&gt; 00:29:46,000<br>That turned it too slow, they got rid of that.</p>
<p>548<br>00:29:46,000 –&gt; 00:29:48,000<br>And now they have their own custom row store.</p>
<p>549<br>00:29:48,000 –&gt; 00:29:52,000<br>And then every cell wall from the background task is going to go through, pull things out of the row store,</p>
<p>550<br>00:29:52,000 –&gt; 00:29:58,000<br>put it into a columnar format, and then write that out to S3.</p>
<p>551<br>00:29:58,000 –&gt; 00:30:03,000<br>They also like you to do transactional updates on the column store data as well,</p>
<p>552<br>00:30:03,000 –&gt; 00:30:06,000<br>which is something we haven’t really seen in the other systems.</p>
<p>553<br>00:30:06,000 –&gt; 00:30:13,000<br>This is what iceberg and hoodie can handle for you in the front, as well as Delta Lake, or yeah, that’s the big data version.</p>
<p>554<br>00:30:13,000 –&gt; 00:30:20,000<br>You can do, you know, update, insert update deletes on those data if you use those front ends,</p>
<p>555<br>00:30:20,000 –&gt; 00:30:24,000<br>but in yellow brick you can go directly and just call insert update lead.</p>
<p>556<br>00:30:24,000 –&gt; 00:30:33,000<br>And then they maintain a, basically a log record of like here’s all the changes that were made to change your data file,</p>
<p>557<br>00:30:34,000 –&gt; 00:30:37,000<br>so that when you read in, you know, a data potentially need to skip.</p>
<p>558<br>00:30:37,000 –&gt; 00:30:44,000<br>And then periodically they’ll go through and do compaction of combined multiple model files together and try to produce,</p>
<p>559<br>00:30:44,000 –&gt; 00:30:51,000<br>print out the old data and reduce it down to smaller file sources.</p>
<p>560<br>00:30:51,000 –&gt; 00:31:00,000<br>So the technique they’re going to use to assign files to workers is when we different than what we saw in snowflake.</p>
<p>561<br>00:31:01,000 –&gt; 00:31:11,000<br>We’re going to call that snowflake was using consistent hashing basically the ring approach to figure out what file belongs to what worker node for the local cache.</p>
<p>562<br>00:31:11,000 –&gt; 00:31:26,000<br>And then when the number of worker node changed, the node goes down, you add a new node, you could reorganize the mapping of files to worker nodes without having to shuffle everything as you numerally with sort of naive hashing.</p>
<p>563<br>00:31:27,000 –&gt; 00:31:31,000<br>So yellow brick is going to use another technique that’s very similar called run of you hashing.</p>
<p>564<br>00:31:31,000 –&gt; 00:31:38,000<br>Who here is, I don’t think we teach run, do they teach run of you hashing and distributed it is or not distributed systems.</p>
<p>565<br>00:31:41,000 –&gt; 00:31:45,000<br>It’s a variation of consistent hashing that actually came up before.</p>
<p>566<br>00:31:45,000 –&gt; 00:31:47,000<br>It’s a lot simpler.</p>
<p>567<br>00:31:47,000 –&gt; 00:31:52,000<br>It’s just consistent hashing is more well known, more popular than run of you hashing.</p>
<p>568<br>00:31:52,000 –&gt; 00:31:59,000<br>So the system is like druid or patchy ignite, which is a key value store in memory store that do use this.</p>
<p>569<br>00:31:59,000 –&gt; 00:32:04,000<br>The idea is really simple. The basic idea is like you have whatever you want to hash.</p>
<p>570<br>00:32:04,000 –&gt; 00:32:07,000<br>So it would say for us it’s either files like the file name.</p>
<p>571<br>00:32:07,000 –&gt; 00:32:10,000<br>And then you have the worker node you want to assign them to.</p>
<p>572<br>00:32:10,000 –&gt; 00:32:21,000<br>So all you need to do is just hash each file by some key multiple times and then come up with a rank list of the assignment of a file to a worker.</p>
<p>573<br>00:32:22,000 –&gt; 00:32:26,000<br>And then you just pick whatever one has the comes first based on some leading.</p>
<p>574<br>00:32:26,000 –&gt; 00:32:31,000<br>So say I have three files and say I have three workers.</p>
<p>575<br>00:32:31,000 –&gt; 00:32:38,000<br>So I’ll take the first file here and as I say I pick the file name and then I pen the worker identifier to it.</p>
<p>576<br>00:32:38,000 –&gt; 00:32:42,000<br>And I hashed that and I can come up with some number.</p>
<p>577<br>00:32:42,000 –&gt; 00:32:52,000<br>Again, it’s somehow the number it corresponds to how important it is for this file to be on this worker worker to be random.</p>
<p>578<br>00:32:52,000 –&gt; 00:32:58,000<br>So now I have a rank list of the workers and then I’m picking whatever the first one is.</p>
<p>579<br>00:32:58,000 –&gt; 00:33:00,000<br>And then I do the same thing for the other ones.</p>
<p>580<br>00:33:00,000 –&gt; 00:33:07,000<br>They rank ordering and then the first one is the one I’m going to assign them to.</p>
<p>581<br>00:33:07,000 –&gt; 00:33:20,000<br>So for this I don’t need to maintain any sort of immediate global state or routing information between the different nodes because along with everyone has the same hash function and the same hash and protocol.</p>
<p>582<br>00:33:20,000 –&gt; 00:33:22,000<br>Everyone can everyone can compute this.</p>
<p>583<br>00:33:22,000 –&gt; 00:33:26,000<br>Why is this different that there’s a fashion?</p>
<p>584<br>00:33:26,000 –&gt; 00:33:31,000<br>Fashion the file name and then the module number of workers.</p>
<p>585<br>00:33:31,000 –&gt; 00:33:33,000<br>What if I add a new worker?</p>
<p>586<br>00:33:33,000 –&gt; 00:33:38,000<br>It’s the same reason why it’s solving the same problem as this and hashing.</p>
<p>587<br>00:33:38,000 –&gt; 00:33:42,000<br>New file comes along, do the same thing, hash it, come on the list.</p>
<p>588<br>00:33:42,000 –&gt; 00:33:47,000<br>But now to his point what if I come along with the new worker node?</p>
<p>589<br>00:33:47,000 –&gt; 00:33:51,000<br>Because I want to scale out or worker node goes down, I got to take one away.</p>
<p>590<br>00:33:51,000 –&gt; 00:33:59,000<br>So say I had a new worker worker for and so if I hashed it for the first one, it just ends up being on the end of the rank list.</p>
<p>591<br>00:33:59,000 –&gt; 00:34:05,000<br>So nothing changes because it was assigned to worker one that’s still at highest priority so it stays there.</p>
<p>592<br>00:34:05,000 –&gt; 00:34:09,000<br>Say for this one worker four ends up between three and one.</p>
<p>593<br>00:34:09,000 –&gt; 00:34:11,000<br>Again, nothing changes to stays there.</p>
<p>594<br>00:34:11,000 –&gt; 00:34:13,000<br>For this one a worker four is here.</p>
<p>595<br>00:34:13,000 –&gt; 00:34:18,000<br>These guys slide over nothing changes but for this one here it becomes the this has the highest priority.</p>
<p>596<br>00:34:18,000 –&gt; 00:34:25,000<br>So now when I add this new node, I need to transfer file for his ownership to from where it was two to four.</p>
<p>597<br>00:34:25,000 –&gt; 00:34:28,000<br>And I didn’t have to reshuffle everything.</p>
<p>598<br>00:34:28,000 –&gt; 00:34:31,000<br>Same idea as assistant hashing, yes.</p>
<p>599<br>00:34:31,000 –&gt; 00:34:34,000<br>So I can call off in this check.</p>
<p>600<br>00:34:34,000 –&gt; 00:34:37,000<br>So question how often is this check for each individual node?</p>
<p>601<br>00:34:37,000 –&gt; 00:34:40,000<br>Well if the topology of the cluster never changes, you don’t need to rerun it.</p>
<p>602<br>00:34:40,000 –&gt; 00:34:43,000<br>Right? Because the hash function is not going to change.</p>
<p>603<br>00:34:43,000 –&gt; 00:34:48,000<br>So it’s only when like it’s only my membership changes in the cluster.</p>
<p>604<br>00:34:48,000 –&gt; 00:34:54,000<br>So what’s the drawback of using this or is this assistant hashing?</p>
<p>605<br>00:34:54,000 –&gt; 00:34:57,000<br>So consistent hashing is going to have faster lookups.</p>
<p>606<br>00:34:57,000 –&gt; 00:34:59,000<br>It’s going to be a log n.</p>
<p>607<br>00:34:59,000 –&gt; 00:35:00,000<br>Right?</p>
<p>608<br>00:35:00,000 –&gt; 00:35:03,000<br>Versus this is o n because I got a hash, we’re n’s the number of nodes.</p>
<p>609<br>00:35:03,000 –&gt; 00:35:05,000<br>I got a hash for each one.</p>
<p>610<br>00:35:05,000 –&gt; 00:35:07,000<br>All right, well that’s a hash function.</p>
<p>611<br>00:35:07,000 –&gt; 00:35:08,000<br>It’s not that expensive.</p>
<p>612<br>00:35:08,000 –&gt; 00:35:11,000<br>When you’re doing a profile, that’s not in a big of a deal.</p>
<p>613<br>00:35:11,000 –&gt; 00:35:15,000<br>You know, this is simpler, this is much simpler to to implement than consistent hashing.</p>
<p>614<br>00:35:15,000 –&gt; 00:35:20,000<br>And consistent hashing is going to have a slower initialization and a slower rebalancing.</p>
<p>615<br>00:35:20,000 –&gt; 00:35:21,000<br>Right?</p>
<p>616<br>00:35:21,000 –&gt; 00:35:28,000<br>Depends on the number of nodes I would have in the ring for consistent hashing.</p>
<p>617<br>00:35:28,000 –&gt; 00:35:29,000<br>Right?</p>
<p>618<br>00:35:29,000 –&gt; 00:35:31,000<br>This is simple and easy.</p>
<p>619<br>00:35:31,000 –&gt; 00:35:38,000<br>And it’s consistent hashing is better for I think also if the topology is more volatile.</p>
<p>620<br>00:35:38,000 –&gt; 00:35:40,000<br>Like people come and go in all the time.</p>
<p>621<br>00:35:40,000 –&gt; 00:35:44,000<br>Which again for distributed database system where people paying thousands of dollars.</p>
<p>622<br>00:35:44,000 –&gt; 00:35:47,000<br>The nodes aren’t flipping, flipping one and off all the time.</p>
<p>623<br>00:35:47,000 –&gt; 00:35:49,000<br>So this is fine.</p>
<p>624<br>00:35:49,000 –&gt; 00:35:50,000<br>Yes.</p>
<p>625<br>00:35:50,000 –&gt; 00:35:53,000<br>How many files can you assign to a single node?</p>
<p>626<br>00:35:53,000 –&gt; 00:36:07,000<br>So this gets that to this weight thing here, not really talking about, but like you could have the weight be based on the capacity of a node.</p>
<p>627<br>00:36:07,000 –&gt; 00:36:14,000<br>So if these are heterogeneous nodes, one machine has more compute capacity or storage capacity than others.</p>
<p>628<br>00:36:14,000 –&gt; 00:36:23,000<br>You would you could skew it such that it’s given maybe a higher priority sometimes versus more likely to give a higher priority than the other times.</p>
<p>629<br>00:36:23,000 –&gt; 00:36:24,000<br>Right?</p>
<p>630<br>00:36:24,000 –&gt; 00:36:33,000<br>So like another way to handle that is you could have virtual nodes and say like everybody gets one, but if you are a big machine then you’ll get two virtual nodes.</p>
<p>631<br>00:36:33,000 –&gt; 00:36:36,000<br>So I’m more likely to get assigned to that larger node.</p>
<p>632<br>00:36:36,000 –&gt; 00:36:39,000<br>There’s various schemes you can use to balance things out.</p>
<p>633<br>00:36:40,000 –&gt; 00:36:49,000<br>This question is what happens if you have two different files that are actually the same worker?</p>
<p>634<br>00:36:49,000 –&gt; 00:36:50,000<br>That’s not a problem.</p>
<p>635<br>00:36:50,000 –&gt; 00:36:51,000<br>It cares.</p>
<p>636<br>00:36:51,000 –&gt; 00:36:54,000<br>They’re going to have thousands of files and like 10 workers.</p>
<p>637<br>00:36:54,000 –&gt; 00:36:57,000<br>So of course you can have workers multiple files that sound the same worker.</p>
<p>638<br>00:36:57,000 –&gt; 00:37:01,000<br>Same thing we saw in snowflake.</p>
<p>639<br>00:37:01,000 –&gt; 00:37:03,000<br>Right? There are files or 100 megabytes.</p>
<p>640<br>00:37:03,000 –&gt; 00:37:07,000<br>So if I have a 10 petabyte database, then I have a lot of files.</p>
<p>641<br>00:37:07,000 –&gt; 00:37:12,000<br>Right?</p>
<p>642<br>00:37:12,000 –&gt; 00:37:14,000<br>Again, it’s a nice easy trick.</p>
<p>643<br>00:37:14,000 –&gt; 00:37:15,000<br>It’s from the 90s.</p>
<p>644<br>00:37:15,000 –&gt; 00:37:21,000<br>It’s a simpler implementation than consistent hashing.</p>
<p>645<br>00:37:21,000 –&gt; 00:37:23,000<br>It just surprised me when I read this.</p>
<p>646<br>00:37:23,000 –&gt; 00:37:24,000<br>I’m like, oh, okay, yeah.</p>
<p>647<br>00:37:24,000 –&gt; 00:37:28,000<br>Because most systems would do consistent hashing as we saw in snowflake.</p>
<p>648<br>00:37:28,000 –&gt; 00:37:36,000<br>And I think this is because it’s just a similar hell like for distributed consensus protocols people pick raft or packs those over a view stamp river.</p>
<p>649<br>00:37:36,000 –&gt; 00:37:38,000<br>And they’re all fundamentally equivalent.</p>
<p>650<br>00:37:38,000 –&gt; 00:37:39,000<br>Yes.</p>
<p>651<br>00:37:39,000 –&gt; 00:37:46,000<br>I think the hash function in the Qt change any bit in the age of knowing where you’re going to be messed up the up with.</p>
<p>652<br>00:37:46,000 –&gt; 00:37:48,000<br>You’re going to change it drastically.</p>
<p>653<br>00:37:48,000 –&gt; 00:37:49,000<br>Yes.</p>
<p>654<br>00:37:49,000 –&gt; 00:37:56,000<br>So that kind of ensures that you kind of have an even distribution when you’re just doing regular hashing.</p>
<p>655<br>00:37:56,000 –&gt; 00:38:01,000<br>It’s not here at like like pre-bending like this worker in the fire.</p>
<p>656<br>00:38:01,000 –&gt; 00:38:05,000<br>How do you make your most, not going to be as cute and like like maybe one more time.</p>
<p>657<br>00:38:05,000 –&gt; 00:38:12,000<br>Like maybe one worker gets a time to like more and more files on the other.</p>
<p>658<br>00:38:12,000 –&gt; 00:38:19,000<br>His question is like how do you ensure that the hash function doesn’t introduce skew?</p>
<p>659<br>00:38:19,000 –&gt; 00:38:22,000<br>I mean the hash function like hashing.</p>
<p>660<br>00:38:22,000 –&gt; 00:38:28,000<br>Putting the the worker ID as a suffix to like the file name.</p>
<p>661<br>00:38:28,000 –&gt; 00:38:32,000<br>That’s not going to introduce skew in any meaningful way.</p>
<p>662<br>00:38:32,000 –&gt; 00:38:35,000<br>We’re not hashing in the contents of the file.</p>
<p>663<br>00:38:35,000 –&gt; 00:38:38,000<br>We’re hashing in the name identifier.</p>
<p>664<br>00:38:38,000 –&gt; 00:38:44,000<br>So that string itself with the good hash function to be low, low collocologians.</p>
<p>665<br>00:38:44,000 –&gt; 00:38:47,000<br>And again, think of like thousands and thousands of files.</p>
<p>666<br>00:38:47,000 –&gt; 00:38:50,000<br>It’ll be okay.</p>
<p>667<br>00:38:50,000 –&gt; 00:38:53,000<br>The thing that they care about again was solving the problem.</p>
<p>668<br>00:38:53,000 –&gt; 00:38:55,000<br>The classic trap he walked into.</p>
<p>669<br>00:38:55,000 –&gt; 00:38:58,000<br>Like okay, well like if I had a new note, if I’m assuming naive hashing,</p>
<p>670<br>00:38:58,000 –&gt; 00:39:03,000<br>I got to rebalance everything, this avoids that problem.</p>
<p>671<br>00:39:03,000 –&gt; 00:39:07,000<br>And then obviously if you drop in a worker goes down, you got to re-sign things.</p>
<p>672<br>00:39:07,000 –&gt; 00:39:10,000<br>It’s just the reverse of this.</p>
<p>673<br>00:39:10,000 –&gt; 00:39:12,000<br>Okay?</p>
<p>674<br>00:39:12,000 –&gt; 00:39:14,000<br>So that’s the core architecture of yellow brick.</p>
<p>675<br>00:39:14,000 –&gt; 00:39:18,000<br>As I said, it basically looks a lot like what we talked about so far.</p>
<p>676<br>00:39:18,000 –&gt; 00:39:23,000<br>And you see bits and pieces from snowflake, bits and pieces from Dremel.</p>
<p>677<br>00:39:23,000 –&gt; 00:39:25,000<br>And the other systems.</p>
<p>678<br>00:39:26,000 –&gt; 00:39:28,000<br>And they’re doing like the high q query compilation.</p>
<p>679<br>00:39:28,000 –&gt; 00:39:33,000<br>They’re doing the vector wise vector wise query processing, the hyper push based model.</p>
<p>680<br>00:39:33,000 –&gt; 00:39:36,000<br>You can see again, all the papers we talked about through the semester,</p>
<p>681<br>00:39:36,000 –&gt; 00:39:39,000<br>just seeing these patterns repeated over and over again.</p>
<p>682<br>00:39:39,000 –&gt; 00:39:40,000<br>Yes.</p>
<p>683<br>00:39:40,000 –&gt; 00:39:42,000<br>Oh, the other system is not running on containers, right?</p>
<p>684<br>00:39:42,000 –&gt; 00:39:46,000<br>The other system is not running on containers.</p>
<p>685<br>00:39:46,000 –&gt; 00:39:48,000<br>That you know about.</p>
<p>686<br>00:39:48,000 –&gt; 00:39:49,000<br>Like so hyper no, right?</p>
<p>687<br>00:39:49,000 –&gt; 00:39:53,000<br>Because hyper was a single node system, high q with before containers are invented.</p>
<p>688<br>00:39:54,000 –&gt; 00:39:56,000<br>But like is Dremel running on containers?</p>
<p>689<br>00:39:56,000 –&gt; 00:39:57,000<br>Yes.</p>
<p>690<br>00:39:57,000 –&gt; 00:39:58,000<br>Oh, the other.</p>
<p>691<br>00:39:58,000 –&gt; 00:39:59,000<br>Yeah.</p>
<p>692<br>00:39:59,000 –&gt; 00:40:02,000<br>It’s running on board.</p>
<p>693<br>00:40:02,000 –&gt; 00:40:03,000<br>Right?</p>
<p>694<br>00:40:03,000 –&gt; 00:40:04,000<br>But the paper doesn’t like talk about that.</p>
<p>695<br>00:40:04,000 –&gt; 00:40:06,000<br>Is snowflake running on containers?</p>
<p>696<br>00:40:06,000 –&gt; 00:40:07,000<br>I don’t know.</p>
<p>697<br>00:40:07,000 –&gt; 00:40:08,000<br>We’d have to go email.</p>
<p>698<br>00:40:08,000 –&gt; 00:40:09,000<br>They ask them.</p>
<p>699<br>00:40:09,000 –&gt; 00:40:11,000<br>I don’t think they’re running on their own systems.</p>
<p>700<br>00:40:11,000 –&gt; 00:40:14,000<br>Like they have their own machine.</p>
<p>701<br>00:40:14,000 –&gt; 00:40:16,000<br>Is it snowflake?</p>
<p>702<br>00:40:16,000 –&gt; 00:40:19,000<br>No snowflakes running on the AWS or whatever cloud plumber.</p>
<p>703<br>00:40:19,000 –&gt; 00:40:21,000<br>They’re like, they have.</p>
<p>704<br>00:40:22,000 –&gt; 00:40:30,000<br>Yeah, so like you can get bare mental instances, but like do you want to have again, like going back to.</p>
<p>705<br>00:40:30,000 –&gt; 00:40:31,000<br>The same before.</p>
<p>706<br>00:40:37,000 –&gt; 00:40:39,000<br>Like there’s a bunch of stuff Kubernetes is going to do for you.</p>
<p>707<br>00:40:39,000 –&gt; 00:40:42,000<br>Like fault tolerance, provisioning, system state management.</p>
<p>708<br>00:40:42,000 –&gt; 00:40:44,000<br>What nodes are up?</p>
<p>709<br>00:40:44,000 –&gt; 00:40:49,000<br>If you don’t use Kubernetes, you got to roll that all yourself using like LCD or bookkeeper or zookeeper or whatever, right?</p>
<p>710<br>00:40:49,000 –&gt; 00:40:52,000<br>Kubernetes, like Kubernetes, just runs the LCD, right?</p>
<p>711<br>00:40:52,000 –&gt; 00:40:55,000<br>Like it just does it for you.</p>
<p>712<br>00:40:55,000 –&gt; 00:40:58,000<br>So like all of that, you don’t have to write if you just run Kubernetes.</p>
<p>713<br>00:40:58,000 –&gt; 00:41:04,000<br>And the big thing that paper talks about is for all the low-level optimizations that they implemented, which we’ll talk about next,</p>
<p>714<br>00:41:04,000 –&gt; 00:41:08,000<br>the containers did actually not impose a problem.</p>
<p>715<br>00:41:10,000 –&gt; 00:41:15,000<br>Which to me, that was that was that was surprising.</p>
<p>716<br>00:41:16,000 –&gt; 00:41:18,000<br>Okay.</p>
<p>717<br>00:41:20,000 –&gt; 00:41:29,000<br>So I’ve said this many, many times that the entire semester that the operating system is not our friend, it is our enemy, right?</p>
<p>718<br>00:41:29,000 –&gt; 00:41:33,000<br>Every day when you’re data system, you wake up and you’re like, oh man, I hope I don’t talk about operating systems.</p>
<p>719<br>00:41:33,000 –&gt; 00:41:36,000<br>It’s always going to cause problem, right?</p>
<p>720<br>00:41:37,000 –&gt; 00:41:51,000<br>So if now we want to build a data system where we avoid the operating system as much as possible, in the extreme case, in the way that we haven’t really talked about before,</p>
<p>721<br>00:41:51,000 –&gt; 00:42:00,000<br>and we basically want to just, our data process is to boot up, maybe tell the OS, like, oh yeah, give us some of this, give us that,</p>
<p>722<br>00:42:00,000 –&gt; 00:42:08,000<br>and push it aside, like, and drive off without it, and not, you know, not ever talk to it again, what do we need to do to actually achieve that?</p>
<p>723<br>00:42:08,000 –&gt; 00:42:11,000<br>And that’s what this paper shows describes how to do this.</p>
<p>724<br>00:42:11,000 –&gt; 00:42:23,000<br>So what they are basically built a unicernal for their data system, where upon boot up, the data based system talks to the OS a little bit, makes six to ten system calls,</p>
<p>725<br>00:42:23,000 –&gt; 00:42:29,000<br>gets some memory, gets some allocation to talk to the hardware, yadda yadda, installs whatever drivers it needs, and then that’s it.</p>
<p>726<br>00:42:29,000 –&gt; 00:42:38,000<br>And no point, will we ever have to go back and talk to the operating system for anything, which is the way it should be in life.</p>
<p>727<br>00:42:38,000 –&gt; 00:42:49,000<br>And so they list a bunch of things that they do, which again, some of these things you probably would not do now, like, build on custom memory allocator, we can talk about that a little bit.</p>
<p>728<br>00:42:50,000 –&gt; 00:43:01,000<br>But like, all the other stuff, again, it’s basically had interact with hardware, and manage our own runtime environment of the system, we don’t want to use the opposite system for any of that.</p>
<p>729<br>00:43:01,000 –&gt; 00:43:17,000<br>And they’ll talk about in the paper, or another public service that they’ve given, about the performance benefit that they’re getting by re-implementing a bunch of the stuff that the operating system is trying to, you know, sort of a general purpose in the notation, but they’re going to have something that’s highly tuned for their database system.</p>
<p>730<br>00:43:18,000 –&gt; 00:43:35,000<br>And the reoccurring pattern, I’m going to say the word overnight in custom, because they custom this, custom that, custom this, and again, they’re trying to avoid the OS, they end up re-implementing a bunch of the OS functionality inside of their database system, but if you want to get the extreme performance numbers that they’re going to get, you have to do this.</p>
<p>731<br>00:43:35,000 –&gt; 00:43:43,000<br>So they’re going to lie, they’re making heavy use on asynchronous architecture using covert teams, and they want to do this because they want to maximize the hardware utilization.</p>
<p>732<br>00:43:43,000 –&gt; 00:43:54,000<br>At no point do they want any thread to stall, like I said, because it’s got to get something from memory, they want to keep everything in CPU caches, which is not easy to do, it’s very impressive.</p>
<p>733<br>00:43:54,000 –&gt; 00:44:05,000<br>You see this kind of architecture also too, alpha databases, this is all what the high-future trading guys are doing, like the quants and things like that.</p>
<p>734<br>00:44:05,000 –&gt; 00:44:19,000<br>Those guys are super hardcore about performance, and they’re basically doing the same kind of thing, whatever the trading system they’re building, you know, for event notifications or triggers or all that is going to run in user space, and they don’t want the colonel, the OS to do anything.</p>
<p>735<br>00:44:19,000 –&gt; 00:44:28,000<br>So they’re applying a lot of the tricks that are being done in like the Wall Street world and the FinTech world, but for a database system.</p>
<p>736<br>00:44:28,000 –&gt; 00:44:32,000<br>All right, so the first thing you’re going to do is build a memory allocator.</p>
<p>737<br>00:44:32,000 –&gt; 00:44:42,000<br>So they have a custom newerware allocator that sometimes they say they say it was latch or lock free, other times they say, I think the paper says it does minimal locks.</p>
<p>738<br>00:44:42,000 –&gt; 00:45:11,000<br>I would assume they’re taking some locks in there or some latches in there, but what’s going to happen is when you boot up the operating boot up the the Davis system, it goes to the OS doesn’t, doesn’t, doesn’t use an M app for malloc, like the allocator, all the memory they’re ever going to want, and then manage all that in user space, and they’re going to M lock it in the kernel to vent it from getting evicted and written out the disk, because they keep all the memory pages pin pin in memory and never written back to disk.</p>
<p>739<br>00:45:11,000 –&gt; 00:45:32,000<br>And then when they hand out allocations to queries, whether running, they’re going to do one trick to make sure that like, well, they’re only going to run one query at a time, the entire cluster, we’ll talk about that why in a second, but they’re going to make sure that the chunks that they’re allocating are, are contiguous as much as possible to avoid fragmentation.</p>
<p>740<br>00:45:32,000 –&gt; 00:45:43,000<br>So you don’t want to have like, you know, one query allocated, one big bit chunk, and then start freeing up little bits and pieces inside of it, because then you can’t reuse it without, to the query finishes.</p>
<p>741<br>00:45:43,000 –&gt; 00:45:47,000<br>So you’re going to hand things out in larger chunks and try to keep things aligned nicely.</p>
<p>742<br>00:45:47,000 –&gt; 00:45:53,000<br>So they claimed that their allocator is a 100x faster than the Libsy malloc. That’s not surprising.</p>
<p>743<br>00:45:53,000 –&gt; 00:46:05,000<br>When I asked them when they gave a talk, like, what if you compare against like, G email or TC malloc, I forgot the other one is from Microsoft, it’s called me malloc, which is especially better than the G emailoc now.</p>
<p>744<br>00:46:05,000 –&gt; 00:46:16,000<br>And they haven’t done experiments, but when they were building this in 2016, they were getting, you know, 100x faster than the Libsy malloc.</p>
<p>745<br>00:46:17,000 –&gt; 00:46:26,000<br>In addition to this memory allocator, every worker is also going to have its own bufferable manager using all the techniques that we talked about in the interclass.</p>
<p>746<br>00:46:26,000 –&gt; 00:46:39,000<br>But for their eviction policy, they’re going to use basically a primitive version of LUK where it’s basically two cues, two linked lists.</p>
<p>747<br>00:46:39,000 –&gt; 00:46:44,000<br>And this is the basic, the same technique that MySQL is doing. Yes.</p>
<p>748<br>00:46:44,000 –&gt; 00:46:48,000<br>Is this a specific, do we know allocations, like a lot of work, like, what does that mean?</p>
<p>749<br>00:46:48,000 –&gt; 00:46:52,000<br>It’s a question, is this a rena allocation? Yes, where the rena is a query.</p>
<p>750<br>00:46:52,000 –&gt; 00:46:59,000<br>Postgres calls it a pool, like a memory pool where the query, same idea.</p>
<p>751<br>00:46:59,000 –&gt; 00:47:05,000<br>But the key thing is like, when it boots up, it gets all the memory. Whereas like, G malloc and other ones, I think they do it come out.</p>
<p>752<br>00:47:06,000 –&gt; 00:47:10,000<br>Which are your questions? What’s our, sorry?</p>
<p>753<br>00:47:10,000 –&gt; 00:47:12,000<br>It’s only doing it once in the beginning.</p>
<p>754<br>00:47:12,000 –&gt; 00:47:15,000<br>You know, it’s a question, like, it’s a question, why do I care about how fast it is if it’s only doing once in the beginning?</p>
<p>755<br>00:47:15,000 –&gt; 00:47:25,000<br>It’s a 100x faster at runtime. So I do the giant memory allocation in the beginning, and then they, their own custom allocators, like when you call, say you called new, whatever, right?</p>
<p>756<br>00:47:25,000 –&gt; 00:47:31,000<br>And then you get some chuckle memory. It’s going through their code getting from this giant chunk of memory that they’ve already pre-allocated. Yes.</p>
<p>757<br>00:47:31,000 –&gt; 00:47:39,000<br>So, I remember you saying that all of the queries, sorry, like each of the clusters, like, running one query? Yes.</p>
<p>758<br>00:47:39,000 –&gt; 00:47:45,000<br>So, like, that’s one query at a time, but they can be like, concurrently doing a cluster of them.</p>
<p>759<br>00:47:45,000 –&gt; 00:47:53,000<br>So the paper says that, either, I’m conflating papers and talks in the way they’re, as they have.</p>
<p>760<br>00:47:53,000 –&gt; 00:47:57,000<br>At some point, they’re saying that they’re only around one query at a time in a cluster.</p>
<p>761<br>00:47:57,000 –&gt; 00:48:03,000<br>But they talk about the memory allocator that, like, they keep track of memory on a per-query lifetime.</p>
<p>762<br>00:48:03,000 –&gt; 00:48:10,000<br>So that part’s sort of confusing, I agree, that, like, if there’s only one query running, then, like, you just give all the memory back when the query is done.</p>
<p>763<br>00:48:10,000 –&gt; 00:48:16,000<br>Right? And this is what they said, sort of says. So I don’t know exactly what they’re doing.</p>
<p>764<br>00:48:16,000 –&gt; 00:48:22,000<br>Similar question or Patrick or no? Oh, no. Okay. Right.</p>
<p>765<br>00:48:22,000 –&gt; 00:48:28,000<br>So, I want to go back to this thing, huge pages. Who are you here? Sorry, yes.</p>
<p>766<br>00:48:28,000 –&gt; 00:48:32,000<br>The technology is happening from the OS at the beginning to when you need to use more of these questions?</p>
<p>767<br>00:48:32,000 –&gt; 00:48:41,000<br>You don’t get more. The machine has, say, a machine has 100 gigs. You take 99 gigs. That’s it.</p>
<p>768<br>00:48:41,000 –&gt; 00:48:51,000<br>So, I say, this idea is not novel to them. So the embedded, some of the embedded data is that run on the stream hardware.</p>
<p>769<br>00:48:51,000 –&gt; 00:48:57,000<br>I think we, I think I mentioned ExtremeDB. They’re running on, like, low-level embedded devices.</p>
<p>770<br>00:48:57,000 –&gt; 00:49:05,000<br>Like, for, in those real-time environments, real-time operating systems, when you boot up, you have to get all your memory at the very beginning.</p>
<p>771<br>00:49:05,000 –&gt; 00:49:10,000<br>You can never go ask for more again to the OS. The OS actually doesn’t allow you to do that. Right?</p>
<p>772<br>00:49:10,000 –&gt; 00:49:16,000<br>Because they don’t want, like, you know, the missile will be flying and then you try to do malloc and malloc fails and you have to handle that.</p>
<p>773<br>00:49:16,000 –&gt; 00:49:22,000<br>So they allocate all the memory in the beginning. There’s another system out of South Africa called Tiger Beetle. They do the same thing.</p>
<p>774<br>00:49:22,000 –&gt; 00:49:31,000<br>System boots up to get all the memory in the beginning. They never call malloc again. Everything’s happening in Deuserspace.</p>
<p>775<br>00:49:31,000 –&gt; 00:49:46,000<br>So from a system design structure, it’s actually a good idea. Like, taking how many times you call, you know, how many times you, if you called malloc or new inside your code, C++ or C code, you’re not checking with the memory allocation fails.</p>
<p>776<br>00:49:46,000 –&gt; 00:49:49,000<br>Right?</p>
<p>777<br>00:49:49,000 –&gt; 00:49:53,000<br>Yes? You’re laughing. Yes.</p>
<p>778<br>00:49:53,000 –&gt; 00:50:06,000<br>Again, like, a lot of code is written like that. If you’re trying to make something super fault-ton, and resilient, especially for embedded devices, you don’t want your car calling malloc and failing and then drive you off the highway.</p>
<p>779<br>00:50:06,000 –&gt; 00:50:14,000<br>Right? So they do all, for safety reasons, they do all this allocation up front. And they’re doing this for performance reasons.</p>
<p>780<br>00:50:15,000 –&gt; 00:50:29,000<br>Right? So now, when you, when I call, you know, I want to get memory from this allocator, if the allocations going to fail, you know, you, you, you could have the allocator implement something to like, okay, I know I’m running this query. It’s asked for this memory.</p>
<p>781<br>00:50:29,000 –&gt; 00:50:35,000<br>I can’t get the memory that it needs. You know, you could alert another part of the system to start spilling other things up the disk.</p>
<p>782<br>00:50:36,000 –&gt; 00:50:49,000<br>Okay, we saw something similar in photon, right? We’re photon had essentialized allocators sitting in the Java that could in spark sequel that could then recognize that you can’t allocate, you know, you know, you know, you need to decide who to start freeing.</p>
<p>783<br>00:50:49,000 –&gt; 00:50:53,000<br>You could do the same thing here.</p>
<p>784<br>00:50:53,000 –&gt; 00:50:56,000<br>Okay, so quick show of hands. Who here knows what the huge pages are?</p>
<p>785<br>00:50:57,000 –&gt; 00:51:07,000<br>Okay, so that’s not bad. So less than half, but more than I thought, but okay. So this is quick diverges. I don’t think we teach us in the intro class.</p>
<p>786<br>00:51:07,000 –&gt; 00:51:20,000<br>We probably should. So, you know, when we talk about pages in a data system, we said that there’s like, there’s a horror page that’s simply four kilobytes. That’s the smallest size that you can guarantee or the larger size, you can guarantee the hardware to write out something atomically.</p>
<p>787<br>00:51:21,000 –&gt; 00:51:34,000<br>Then the OS has its own page size and the database system has its page size. And in an OTP system like Postgres and SQL item, I SQL, you know, we’re worried about, you know, making changes in the page and writing it out. So we don’t want our pages to be really big.</p>
<p>788<br>00:51:34,000 –&gt; 00:51:38,000<br>But now in the read only system.</p>
<p>789<br>00:51:39,000 –&gt; 00:51:47,000<br>You know, the four kilobytes page size is actually kind of small, especially if we’re processing, you know, terabytes of data.</p>
<p>790<br>00:51:47,000 –&gt; 00:52:04,000<br>So the reason why this is going to be bad for us is because the CPU is going to main its own translation, local side buffer using some portion of the URL one cache that basically is a mapping from a, like a virtual memory page to a physical page.</p>
<p>791<br>00:52:04,000 –&gt; 00:52:16,000<br>And so if I have all these four kilobyte pages as I’m ripping through, I’m getting a large chunk of data, then that TLB is going to have a bunch of misses because it has to go fetch things that then aren’t going to fit.</p>
<p>792<br>00:52:16,000 –&gt; 00:52:18,000<br>Right.</p>
<p>793<br>00:52:18,000 –&gt; 00:52:30,000<br>So with huge pages, you can tell the operating system that instead of allocating things in four kilobytes, I want to allocate memory in larger chunks.</p>
<p>794<br>00:52:30,000 –&gt; 00:52:35,000<br>So in Linux, it’s going to be two megabytes or one gigabyte.</p>
<p>795<br>00:52:35,000 –&gt; 00:52:43,000<br>And so now the, on the actual physical hardware, these blocks have to be contiguous.</p>
<p>796<br>00:52:43,000 –&gt; 00:52:56,000<br>But now I can address a block in my, or page in my TLB, which is, you know, a single, single virtual memory address can now point to two megabytes instead of four kilobytes.</p>
<p>797<br>00:52:56,000 –&gt; 00:53:04,000<br>So that’s less pressure on my TLB. And that means I’m going to have fewer L1 cache misses in my TLB. And I can rip through things much, much faster.</p>
<p>798<br>00:53:04,000 –&gt; 00:53:12,000<br>So you don’t get this by default in Linux. Right. By Linux default, again, you call, you call Malik, about allocate something in an MAP, you’re going to get four kilobytes.</p>
<p>799<br>00:53:12,000 –&gt; 00:53:17,000<br>But you can pass flags and say, I want this to be using, using huge pages.</p>
<p>800<br>00:53:18,000 –&gt; 00:53:35,000<br>So there’s a paper from Google, which I think is two years ago, for their custom memory allocator that found that when they switched over to huge pages for Spanner, you know, you got a 65, 6.5% improvement just for switching to huge pages.</p>
<p>801<br>00:53:35,000 –&gt; 00:53:47,000<br>And again, Spanner is an OTP system. So it’s not like you’re doing huge massive table scans. It’s still doing, you know, point-query lookups. But it’s, you know, that’s a pretty big win for just changing this flag in the US and Linux.</p>
<p>802<br>00:53:47,000 –&gt; 00:53:54,000<br>So huge pages are going to make a lot of sense for an MLB system like yellow brick and others that we’ve talked about.</p>
<p>803<br>00:53:55,000 –&gt; 00:54:07,000<br>And so that’s why they were setting the page size within a file to be two megabytes, because that aligns with the two megabyte size you would get with huge pages in the US.</p>
<p>804<br>00:54:07,000 –&gt; 00:54:15,000<br>And also to its small enough where it could sit around in your L3 cache, because that’s going to be tens or dozens of megabytes.</p>
<p>805<br>00:54:15,000 –&gt; 00:54:22,000<br>So these chunks can sit in your L3 cache and your CPU, and you can process it very quickly without going out to memory.</p>
<p>806<br>00:54:23,000 –&gt; 00:54:31,000<br>So for this reason, that’s why they’re using two megabytes size, and they’re going to turn on huge pages for in your operating system.</p>
<p>807<br>00:54:31,000 –&gt; 00:54:37,000<br>And the other papers don’t talk about this, but I guarantee other systems are doing something very similar.</p>
<p>808<br>00:54:38,000 –&gt; 00:54:44,000<br>Okay, so half of you have heard of huge pages. How many of you have heard of transparent huge pages?</p>
<p>809<br>00:54:45,000 –&gt; 00:54:48,000<br>Nobody. Good. Excellent.</p>
<p>810<br>00:54:48,000 –&gt; 00:54:55,000<br>Okay, so do not do this. I’m putting this warning here, so you don’t forget to remember this.</p>
<p>811<br>00:54:55,000 –&gt; 00:54:59,000<br>Okay? This is the worst thing you can do for databases for memory. Do not do this.</p>
<p>812<br>00:54:59,000 –&gt; 00:55:04,000<br>Okay? If you do this, and I’m dead, I’ll haunt you or I’ll figure something out.</p>
<p>813<br>00:55:05,000 –&gt; 00:55:16,000<br>So Linux had this feature called transparent huge pages. I came out in, I think, 2007, but it wasn’t on default for a while.</p>
<p>814<br>00:55:16,000 –&gt; 00:55:28,000<br>And what this does is the OS in the background starts looking at your pages in memory and says, oh, I see that these guys are being accessed together very frequently, and they’re both four kilobytes.</p>
<p>815<br>00:55:28,000 –&gt; 00:55:33,000<br>So we find a bunch of ones that I can stick together for your process and to make a two megabyte page.</p>
<p>816<br>00:55:33,000 –&gt; 00:55:43,000<br>And then on the other covers, I’ll make a hide that from you that now the virtual memory address, you know, now points to this two megabyte pages to a bunch of four kilobytes pages.</p>
<p>817<br>00:55:43,000 –&gt; 00:55:51,000<br>And then it goes even bigger, like, you find a bunch of two megabyte pages, it’ll try to put them into one gigabyte pages.</p>
<p>818<br>00:55:52,000 –&gt; 00:56:20,000<br>So they turn this on my default at some point in the 2010s, and then they revert it back and a bunch of things, because this is a terrible thing to do, because what happens is, when it’s trying to do this realization, if you do a look up now on that address in memory, it’s going to block your process because it’s doing this reorganization and reorganizing things.</p>
<p>819<br>00:56:20,000 –&gt; 00:56:38,000<br>Now further also more, like, if now that if you’re doing a bunch of small rights into these files, which again for O-Lapso, we’re not really doing, now you’re going to have invalidation of the memory across this one gigabyte file, even though I may have done a small change.</p>
<p>820<br>00:56:39,000 –&gt; 00:56:46,000<br>So this is a classic example of like the operating system getting in the way from the database system, you definitely don’t want to do this.</p>
<p>821<br>00:56:46,000 –&gt; 00:56:51,000<br>But they thought, okay, for most applications this is okay, but from a database perspective, this is actually terrible.</p>
<p>822<br>00:56:51,000 –&gt; 00:56:57,000<br>Now for O-Lap, if everything’s read-only, maybe it might be okay.</p>
<p>823<br>00:56:58,000 –&gt; 00:57:06,000<br>Vertica’s an O-Lap system, they say you can turn it along, if you have run newer Linux distros, because the stalling has gotten much less.</p>
<p>824<br>00:57:06,000 –&gt; 00:57:11,000<br>But pretty much every single database is not there, it will tell you to make sure you turn this off.</p>
<p>825<br>00:57:12,000 –&gt; 00:57:23,000<br>So if you just Google Transparent Huge Pages, so here, here’s what it is, here’s how to turn it along.</p>
<p>826<br>00:57:23,000 –&gt; 00:57:35,000<br>But in the scroll pass all this, then here’s all the database vendors saying, hey, don’t do this, long it to be, disable trans- Huge Pages, pincap or tidy B, why we disable it, oracle, disable it, splunk, disable it, right?</p>
<p>827<br>00:57:35,000 –&gt; 00:57:41,000<br>So this is terrible, do not do this, right?</p>
<p>828<br>00:57:41,000 –&gt; 00:57:44,000<br>This is, as I said, this is cancer for databases.</p>
<p>829<br>00:57:44,000 –&gt; 00:57:48,000<br>But for regular Huge Pages, that’s okay, because we can control that.</p>
<p>830<br>00:57:48,000 –&gt; 00:57:59,000<br>And again, we know that the access pattern, the query that we’re running for this particular data, we might be fetching from the object store, that can be in two megabytes chunks, and that’s fine.</p>
<p>831<br>00:57:59,000 –&gt; 00:58:02,000<br>And we’ll get the win from it, okay?</p>
<p>832<br>00:58:02,000 –&gt; 00:58:09,000<br>Yeah, this is kind of like, you know, read NC17, so we can probably don’t want to teach it to the undergrad, but for you guys, it’s okay.</p>
<p>833<br>00:58:09,000 –&gt; 00:58:11,000<br>But don’t do it, okay?</p>
<p>834<br>00:58:11,000 –&gt; 00:58:20,000<br>So going back to the Ruffle Manager, as I was saying, they’re doing an approximate LUK, and they don’t say this explicitly, but it looks from their description, it smells a lot like what my Seaco does.</p>
<p>835<br>00:58:20,000 –&gt; 00:58:33,000<br>So in the way my Seaco does the eviction policy, they actually just maintain a single link list of pages as they’re being accessed, going from news to odys.</p>
<p>836<br>00:58:33,000 –&gt; 00:58:37,000<br>But they’re actually going to maintain two heads into this link list.</p>
<p>837<br>00:58:37,000 –&gt; 00:58:40,000<br>They’ll have a young list, and then the old list.</p>
<p>838<br>00:58:40,000 –&gt; 00:58:48,000<br>So the young list would be the ones that are like, are, are, are, are, have recently been added and recently being accessed over and over again.</p>
<p>839<br>00:58:48,000 –&gt; 00:58:56,000<br>And then the old list are ones where once you access it the first time, you put it in this old list, so that’s more likely to get evicted.</p>
<p>840<br>00:58:56,000 –&gt; 00:59:10,000<br>The idea is that if I read it once, but then never read it again, like what happened in the Scantral Scantral Scam flood, then it’ll get evicted more quickly than it would otherwise with a regular LRU.</p>
<p>841<br>00:59:10,000 –&gt; 00:59:17,000<br>But then when, if it gets access to the game when it’s in the old list, then it gets promoted to the young list, right? Because it’s more likely to be accessed again.</p>
<p>842<br>00:59:17,000 –&gt; 00:59:25,000<br>So here, query one touches page one. Page one is not in our, in our buffer pool. So we go ahead and put it here at the, the head of the old list.</p>
<p>843<br>00:59:25,000 –&gt; 00:59:28,000<br>Evict page eight, everything slides over and that’s fine.</p>
<p>844<br>00:59:28,000 –&gt; 00:59:32,000<br>Query one goes away, the queue two shows up, query two shows up, it accesses page one again.</p>
<p>845<br>00:59:32,000 –&gt; 00:59:41,000<br>We recognize that it is in our old list, so go, we go ahead and now promote it to the front of the new list, right? And everything’s sort of slides over.</p>
<p>846<br>00:59:41,000 –&gt; 00:59:48,000<br>So then I don’t think they’re, they’re not, I don’t know whether maintaining the old list in the young list, but this is basically what they say.</p>
<p>847<br>00:59:48,000 –&gt; 00:59:57,000<br>Like every time you access the file the first time in the cache, then it’s put in this sort of special place where it’s more likely to get evicted.</p>
<p>848<br>00:59:57,000 –&gt; 01:00:04,000<br>But then if it’s accessed again while still in the cache, then it gets promoted to, to the regular LRU.</p>
<p>849<br>01:00:04,000 –&gt; 01:00:11,000<br>Right? Pretty, you know, pretty standard check is, and it’s, and it’s simple.</p>
<p>850<br>01:00:11,000 –&gt; 01:00:17,000<br>All right, so the next thing that we also implement is given to the OS is the task scheduler, the thread scheduler.</p>
<p>851<br>01:00:17,000 –&gt; 01:00:31,000<br>So everything’s based on covert teams. And they’re going to do basically in crop to multitasking with non-pimter threading where the, the task or the thread can go get something, you know, test execute.</p>
<p>852<br>01:00:31,000 –&gt; 01:00:41,000<br>If it needs something, then it’ll just, the language control of it and return back to the, you know, get the next task to work on for that particular query.</p>
<p>853<br>01:00:41,000 –&gt; 01:00:50,000<br>So for this, it smells a lot like the Microsoft SQL server or SQL OS, but in that world they were doing like, I think five or 10 millisecond quantum’s.</p>
<p>854<br>01:00:50,000 –&gt; 01:01:00,000<br>And this one, I think they’re doing 100 millisecond quantum’s where they have this centralized scheduler for the cross the entire cluster that they have to then do a heartbeat and synchronize with every 100 milliseconds.</p>
<p>855<br>01:01:00,000 –&gt; 01:01:07,000<br>So every 100 milliseconds, say, are we all doing the right thing, or you know, the right set of tasks, the right operations right now.</p>
<p>856<br>01:01:07,000 –&gt; 01:01:15,000<br>And as I said before, they want to have all the work, all the, the only one query can run out of time in, in the cluster.</p>
<p>857<br>01:01:15,000 –&gt; 01:01:29,000<br>And that all of the threads on a single worker node will be executing the same task, just on different pieces of the data at the same time, so that all the, the code, you know,</p>
<p>858<br>01:01:29,000 –&gt; 01:01:36,000<br>the code space, the code memory for the actual program instructions, all that’s going to reside in the cache and you’re not ping-ponging back and forth by running different tasks.</p>
<p>859<br>01:01:36,000 –&gt; 01:01:45,000<br>They’re all basically running the same code, just in different pieces of data and you’re going to maximize your, your cache usage.</p>
<p>860<br>01:01:45,000 –&gt; 01:01:52,000<br>The, right, this is basically what I said before, because again, they want to have everything be able to process on L, L3. Yes.</p>
<p>861<br>01:01:52,000 –&gt; 01:02:06,000<br>So, like, good question. So this goes back to like the having multiple queries on the same thing. So like, are the, sorry, are the tasks, like, let’s say, one from one with queries or like, the query had multiple tasks?</p>
<p>862<br>01:02:06,000 –&gt; 01:02:13,000<br>So actually, like, our tasks one in one query are queries on multiple tasks. They can have multiple tasks, sort of, like different fragments on it, you know.</p>
<p>863<br>01:02:13,000 –&gt; 01:02:23,000<br>So, yeah, the second thing, what, what’s part of this is cooperating? Does this seem like an interrupt your event scheduler, right? So every 100 seconds, I’m just going to push to a different query.</p>
<p>864<br>01:02:23,000 –&gt; 01:02:39,000<br>Like, it’s not every 100 milliseconds, I switch another query, every 100 milliseconds. Right, but it may be like the, I have, I have 10, 100, 100 milliseconds chunks or, our, cues of data.</p>
<p>865<br>01:02:39,000 –&gt; 01:02:52,000<br>So I could still be working the same task as in processing a query, right? So every 100 milliseconds, you go back and you go back to the, the essential schedule and say, you know, what’s the next thing I should do?</p>
<p>866<br>01:02:52,000 –&gt; 01:02:59,000<br>Right? And it may be like, oh yeah, keep running the same task, we’re just running, but here’s the next set of follow, you should process on. Right?</p>
<p>867<br>01:02:59,000 –&gt; 01:03:07,000<br>So that’s not like, that sounds like a normal, like, interrupt your event scheduler.</p>
<p>868<br>01:03:07,000 –&gt; 01:03:16,000<br>I think if there’s something you need that’s not there, you you’ll yield back and get the next task for yourself. I think that’s the difference.</p>
<p>869<br>01:03:16,000 –&gt; 01:03:25,000<br>And it goes back to like how do you keep all the memory like on the same worker? I’m sorry, all the data on the same worker.</p>
<p>870<br>01:03:25,000 –&gt; 01:03:28,000<br>What kind of context would happen?</p>
<p>871<br>01:03:28,000 –&gt; 01:03:30,000<br>Like a…</p>
<p>872<br>01:03:30,000 –&gt; 01:03:32,000<br>OS level context switch or query.</p>
<p>873<br>01:03:32,000 –&gt; 01:03:34,000<br>Like how does a context switch, query context switch?</p>
<p>874<br>01:03:34,000 –&gt; 01:03:36,000<br>Do you need like grab data from somewhere else?</p>
<p>875<br>01:03:36,000 –&gt; 01:03:41,000<br>Or is it just like five sets of query data on a single worker?</p>
<p>876<br>01:03:41,000 –&gt; 01:03:43,000<br>I’m not sure what you’re asking.</p>
<p>877<br>01:03:43,000 –&gt; 01:03:46,000<br>So it seems like we have like, look at we have four tasks here.</p>
<p>878<br>01:03:46,000 –&gt; 01:03:50,000<br>Let’s say they’re different queries on the same worker.</p>
<p>879<br>01:03:50,000 –&gt; 01:03:54,000<br>Again, from what they say, I think it’s the same query.</p>
<p>880<br>01:03:54,000 –&gt; 01:03:57,000<br>But it could be different fragments of the same query plan.</p>
<p>881<br>01:03:57,000 –&gt; 01:04:00,000<br>Right.</p>
<p>882<br>01:04:00,000 –&gt; 01:04:05,000<br>So how can you execute one query at once if you want to maximize?</p>
<p>883<br>01:04:05,000 –&gt; 01:04:08,000<br>Right, so to say it’s correct.</p>
<p>884<br>01:04:08,000 –&gt; 01:04:12,000<br>Like why would you only want to execute one query at a time in the cluster?</p>
<p>885<br>01:04:12,000 –&gt; 01:04:20,000<br>If you want to maximize your research utilization because there’s points where I need to call us a bunch of data.</p>
<p>886<br>01:04:20,000 –&gt; 01:04:23,000<br>And I’m not going to have all the query, you know, all…</p>
<p>887<br>01:04:23,000 –&gt; 01:04:25,000<br>And that’s a run as a single thread.</p>
<p>888<br>01:04:25,000 –&gt; 01:04:27,000<br>I don’t know, I don’t know how to handle that.</p>
<p>889<br>01:04:27,000 –&gt; 01:04:30,000<br>So I think that…</p>
<p>890<br>01:04:30,000 –&gt; 01:04:32,000<br>I think what they probably do is…</p>
<p>891<br>01:04:32,000 –&gt; 01:04:34,000<br>Actually, I’m speculating, I don’t know.</p>
<p>892<br>01:04:34,000 –&gt; 01:04:39,000<br>But you can imagine like, I know at the last stage I’m going to just combine the results back to it’s…</p>
<p>893<br>01:04:39,000 –&gt; 01:04:42,000<br>Because combine the results back to a single…</p>
<p>894<br>01:04:42,000 –&gt; 01:04:45,000<br>You know, result set from multiple threads.</p>
<p>895<br>01:04:45,000 –&gt; 01:04:48,000<br>That single thread in…</p>
<p>896<br>01:04:48,000 –&gt; 01:04:52,000<br>I have my work or start running out the next query.</p>
<p>897<br>01:04:52,000 –&gt; 01:04:55,000<br>But I don’t know.</p>
<p>898<br>01:04:55,000 –&gt; 01:04:58,000<br>Is it not my two or three long tryout?</p>
<p>899<br>01:04:58,000 –&gt; 01:05:01,000<br>Like what if you install a…</p>
<p>900<br>01:05:01,000 –&gt; 01:05:03,000<br>Is it 100 milliseconds a long time?</p>
<p>901<br>01:05:03,000 –&gt; 01:05:05,000<br>Yes.</p>
<p>902<br>01:05:05,000 –&gt; 01:05:06,000<br>But I think…</p>
<p>903<br>01:05:06,000 –&gt; 01:05:10,000<br>Again, they don’t talk about this, but I think the centralized schedule could be like, here’s task 1, 2, 3.</p>
<p>904<br>01:05:10,000 –&gt; 01:05:13,000<br>And then each task might take, you know, some 10 milliseconds,</p>
<p>905<br>01:05:13,000 –&gt; 01:05:16,000<br>some subset of the 100 milliseconds.</p>
<p>906<br>01:05:16,000 –&gt; 01:05:18,000<br>It’s not like every 100 milliseconds.</p>
<p>907<br>01:05:18,000 –&gt; 01:05:20,000<br>Okay, we’re all noticing things.</p>
<p>908<br>01:05:20,000 –&gt; 01:05:22,000<br>You could say, you know, here’s enough work to do for 100 milliseconds.</p>
<p>909<br>01:05:22,000 –&gt; 01:05:25,000<br>I don’t know, the paper doesn’t specify.</p>
<p>910<br>01:05:28,000 –&gt; 01:05:29,000<br>Okay.</p>
<p>911<br>01:05:29,000 –&gt; 01:05:36,000<br>So they claim that with not the centralized schedule part, but the thread schedule on the node itself,</p>
<p>912<br>01:05:36,000 –&gt; 01:05:40,000<br>that it’s 500x faster than the Linux thread schedule.</p>
<p>913<br>01:05:40,000 –&gt; 01:05:43,000<br>Again, doesn’t surprise me.</p>
<p>914<br>01:05:43,000 –&gt; 01:05:45,000<br>But it’s…</p>
<p>915<br>01:05:45,000 –&gt; 01:05:48,000<br>Again, 500x faster than what?</p>
<p>916<br>01:05:48,000 –&gt; 01:05:50,000<br>Like the…</p>
<p>917<br>01:05:50,000 –&gt; 01:05:53,000<br>The query is not going to run 500x faster.</p>
<p>918<br>01:05:53,000 –&gt; 01:05:56,000<br>It’s more like the portion that you’re spending…</p>
<p>919<br>01:05:56,000 –&gt; 01:06:01,000<br>The time you spent on scheduling itself, they’ve reduced that down by 500 milliseconds.</p>
<p>920<br>01:06:01,000 –&gt; 01:06:02,000<br>But…</p>
<p>921<br>01:06:02,000 –&gt; 01:06:04,000<br>Is the context switched at the 100 milliseconds?</p>
<p>922<br>01:06:04,000 –&gt; 01:06:05,000<br>The context…</p>
<p>923<br>01:06:05,000 –&gt; 01:06:09,000<br>So the normal, the West context switch would take easily less than…</p>
<p>924<br>01:06:10,000 –&gt; 01:06:11,000<br>Yeah.</p>
<p>925<br>01:06:11,000 –&gt; 01:06:12,000<br>…100 nanoseconds.</p>
<p>926<br>01:06:12,000 –&gt; 01:06:13,000<br>Yeah.</p>
<p>927<br>01:06:13,000 –&gt; 01:06:16,000<br>But again, like that’s like a…</p>
<p>928<br>01:06:16,000 –&gt; 01:06:19,000<br>…head of citizen.</p>
<p>929<br>01:06:19,000 –&gt; 01:06:21,000<br>It’s Omnus Law, right?</p>
<p>930<br>01:06:21,000 –&gt; 01:06:24,000<br>The slowest part of your query isn’t the context switch.</p>
<p>931<br>01:06:24,000 –&gt; 01:06:25,000<br>Right?</p>
<p>932<br>01:06:25,000 –&gt; 01:06:27,000<br>It can be if you’re flipping around all the time, but like…</p>
<p>933<br>01:06:27,000 –&gt; 01:06:29,000<br>It wouldn’t be that bad.</p>
<p>934<br>01:06:31,000 –&gt; 01:06:32,000<br>All right.</p>
<p>935<br>01:06:32,000 –&gt; 01:06:34,000<br>So the other crazy thing they’re doing again is that they’re going to run their own…</p>
<p>936<br>01:06:34,000 –&gt; 01:06:36,000<br>They build their own device drivers.</p>
<p>937<br>01:06:36,000 –&gt; 01:06:39,000<br>So they’re going to have custom NMVME and custom Nick drivers…</p>
<p>938<br>01:06:39,000 –&gt; 01:06:42,000<br>That are all going to run in user space using their memory allocator.</p>
<p>939<br>01:06:42,000 –&gt; 01:06:44,000<br>And they want to do this because they want to reduce the…</p>
<p>940<br>01:06:44,000 –&gt; 01:06:48,000<br>The kernel copying of a memory going in from the hardware…</p>
<p>941<br>01:06:48,000 –&gt; 01:06:51,000<br>From the hardware to the device to the OS…</p>
<p>942<br>01:06:51,000 –&gt; 01:06:53,000<br>To the…</p>
<p>943<br>01:06:53,000 –&gt; 01:06:55,000<br>You know, to user space.</p>
<p>944<br>01:06:55,000 –&gt; 01:06:58,000<br>So everything’s going to be running down…</p>
<p>945<br>01:06:58,000 –&gt; 01:07:00,000<br>Running on their own custom drivers.</p>
<p>946<br>01:07:00,000 –&gt; 01:07:04,000<br>And then they have their own custom network protocol that they build.</p>
<p>947<br>01:07:04,000 –&gt; 01:07:06,000<br>And there’s a byproduct of…</p>
<p>948<br>01:07:06,000 –&gt; 01:07:08,000<br>The reason they had to do this is because when it was on-prem…</p>
<p>949<br>01:07:08,000 –&gt; 01:07:11,000<br>They were running with a Finneban using RDMA.</p>
<p>950<br>01:07:11,000 –&gt; 01:07:15,000<br>And they were getting insane numbers because it’s a Finneban, but it’s expensive.</p>
<p>951<br>01:07:15,000 –&gt; 01:07:17,000<br>But you don’t get…</p>
<p>952<br>01:07:17,000 –&gt; 01:07:20,000<br>I don’t think you can now, but at the time when they were switching to cloud…</p>
<p>953<br>01:07:20,000 –&gt; 01:07:21,000<br>I don’t think…</p>
<p>954<br>01:07:21,000 –&gt; 01:07:23,000<br>I don’t think you could get that on Amazon.</p>
<p>955<br>01:07:23,000 –&gt; 01:07:24,000<br>I don’t know.</p>
<p>956<br>01:07:24,000 –&gt; 01:07:25,000<br>But…</p>
<p>957<br>01:07:25,000 –&gt; 01:07:27,000<br>So instead of relying on TCP, which they found was too slow…</p>
<p>958<br>01:07:27,000 –&gt; 01:07:30,000<br>That they built their own network protocol based on UDP…</p>
<p>959<br>01:07:30,000 –&gt; 01:07:34,000<br>And did all the reliability checks themselves in their driver…</p>
<p>960<br>01:07:34,000 –&gt; 01:07:38,000<br>And to get things from the NIC up into user space as fast as possible…</p>
<p>961<br>01:07:38,000 –&gt; 01:07:40,000<br>They’re going to be using the DVDK.</p>
<p>962<br>01:07:40,000 –&gt; 01:07:42,000<br>Where they’re actually going to maintain the…</p>
<p>963<br>01:07:42,000 –&gt; 01:07:45,000<br>The queues on the hardware themselves…</p>
<p>964<br>01:07:45,000 –&gt; 01:07:47,000<br>Every thread…</p>
<p>965<br>01:07:47,000 –&gt; 01:07:49,000<br>Worker thread is going to have its own receive and transmit queue…</p>
<p>966<br>01:07:49,000 –&gt; 01:07:52,000<br>They’re going to pull asynchronously to see whether it’s any work to be done.</p>
<p>967<br>01:07:52,000 –&gt; 01:07:54,000<br>And then this part is bit confusing…</p>
<p>968<br>01:07:54,000 –&gt; 01:07:58,000<br>But they talk about how they have what I’ll call partners CPU thread…</p>
<p>969<br>01:07:58,000 –&gt; 01:08:01,000<br>Or partner thread CPU running in another worker…</p>
<p>970<br>01:08:01,000 –&gt; 01:08:04,000<br>That if that thread ever needs to send…</p>
<p>971<br>01:08:04,000 –&gt; 01:08:07,000<br>If I’m a thread and need to send data to another worker…</p>
<p>972<br>01:08:07,000 –&gt; 01:08:11,000<br>I can’t send it to any generic mailbox on the other side…</p>
<p>973<br>01:08:11,000 –&gt; 01:08:15,000<br>I’m only sending it directly to that CPU.</p>
<p>974<br>01:08:15,000 –&gt; 01:08:17,000<br>Some orchestration has to be done…</p>
<p>975<br>01:08:17,000 –&gt; 01:08:19,000<br>A coordination has to be done to recognize that…</p>
<p>976<br>01:08:19,000 –&gt; 01:08:21,000<br>Okay, this data press was this CPU on this worker…</p>
<p>977<br>01:08:21,000 –&gt; 01:08:24,000<br>And it needs to get processed by that worker on that CPU on that worker…</p>
<p>978<br>01:08:24,000 –&gt; 01:08:28,000<br>And so they wrap things in the correct way.</p>
<p>979<br>01:08:28,000 –&gt; 01:08:32,000<br>They talk about how they can introduce a distribution operator…</p>
<p>980<br>01:08:32,000 –&gt; 01:08:35,000<br>This is basically a shuffle stuff we talked about before…</p>
<p>981<br>01:08:35,000 –&gt; 01:08:38,000<br>And in that case, again, I think it’s…</p>
<p>982<br>01:08:38,000 –&gt; 01:08:42,000<br>I don’t know how that’s going to fit into this model…</p>
<p>983<br>01:08:42,000 –&gt; 01:08:45,000<br>Unless you’re doing the shuffle locally to read issue with things…</p>
<p>984<br>01:08:45,000 –&gt; 01:08:49,000<br>And because you know that the memory address for this worker…</p>
<p>985<br>01:08:49,000 –&gt; 01:08:52,000<br>That’s going to be responsible for some portion of the data…</p>
<p>986<br>01:08:52,000 –&gt; 01:08:55,000<br>It’s going to then be sent to the other worker…</p>
<p>987<br>01:08:55,000 –&gt; 01:08:58,000<br>The other thread on the other node that I know is going to be able to process it…</p>
<p>988<br>01:08:58,000 –&gt; 01:09:02,000<br>So there’s a little extra work they have to do to make this guarantee…</p>
<p>989<br>01:09:02,000 –&gt; 01:09:06,000<br>But now, if you have this where you know that my CPU can only write to the…</p>
<p>990<br>01:09:06,000 –&gt; 01:09:10,000<br>To the other worker on its one queue…</p>
<p>991<br>01:09:10,000 –&gt; 01:09:15,000<br>Then that can reduce the amount of latching or locking up the do on the hover device itself.</p>
<p>992<br>01:09:17,000 –&gt; 01:09:19,000<br>They also built their own client side…</p>
<p>993<br>01:09:19,000 –&gt; 01:09:22,000<br>Their custom client side S3 library…</p>
<p>994<br>01:09:22,000 –&gt; 01:09:25,000<br>Because the one Amazon gave them was too slow…</p>
<p>995<br>01:09:25,000 –&gt; 01:09:27,000<br>Again, they used the DVDK for that…</p>
<p>996<br>01:09:27,000 –&gt; 01:09:30,000<br>And they said it was 3x faster than when Amazon did.</p>
<p>997<br>01:09:30,000 –&gt; 01:09:33,000<br>So again, when they gave a talk…</p>
<p>998<br>01:09:33,000 –&gt; 01:09:35,000<br>With us a few years ago…</p>
<p>999<br>01:09:35,000 –&gt; 01:09:37,000<br>I don’t think they showed these numbers…</p>
<p>1000<br>01:09:37,000 –&gt; 01:09:39,000<br>And I asked them about it…</p>
<p>1001<br>01:09:39,000 –&gt; 01:09:42,000<br>How much better are you getting if you use DVDK?</p>
<p>1002<br>01:09:42,000 –&gt; 01:09:46,000<br>And so this is running the TPCDS workload…</p>
<p>1003<br>01:09:46,000 –&gt; 01:09:48,000<br>On different cluster sizes…</p>
<p>1004<br>01:09:48,000 –&gt; 01:09:50,000<br>And they’re showing…</p>
<p>1005<br>01:09:50,000 –&gt; 01:09:53,000<br>If you use regular TCP versus their optimized DVDK version…</p>
<p>1006<br>01:09:53,000 –&gt; 01:09:57,000<br>It’s around roughly about 20% improvement…</p>
<p>1007<br>01:09:57,000 –&gt; 01:10:01,000<br>Overrego TCP.</p>
<p>1008<br>01:10:01,000 –&gt; 01:10:06,000<br>You know, that’s not like mind-blowingly faster…</p>
<p>1009<br>01:10:06,000 –&gt; 01:10:09,000<br>But I think in the paper talk about…</p>
<p>1010<br>01:10:09,000 –&gt; 01:10:11,000<br>There was one query that was 70% faster…</p>
<p>1011<br>01:10:11,000 –&gt; 01:10:13,000<br>Because it was transferring a lot of data.</p>
<p>1012<br>01:10:13,000 –&gt; 01:10:15,000<br>It depends on the workload…</p>
<p>1013<br>01:10:15,000 –&gt; 01:10:17,000<br>How much does cash, of course…</p>
<p>1014<br>01:10:17,000 –&gt; 01:10:21,000<br>But again, these guys really care about performance…</p>
<p>1015<br>01:10:21,000 –&gt; 01:10:23,000<br>And it’s 20% matters.</p>
<p>1016<br>01:10:23,000 –&gt; 01:10:25,000<br>I also asked them to…</p>
<p>1017<br>01:10:25,000 –&gt; 01:10:27,000<br>Between the custom S3…</p>
<p>1018<br>01:10:27,000 –&gt; 01:10:29,000<br>All the PCIe drivers…</p>
<p>1019<br>01:10:29,000 –&gt; 01:10:31,000<br>The custom drivers…</p>
<p>1020<br>01:10:31,000 –&gt; 01:10:33,000<br>The allocator…</p>
<p>1021<br>01:10:33,000 –&gt; 01:10:35,000<br>Which one has the biggest win…</p>
<p>1022<br>01:10:35,000 –&gt; 01:10:37,000<br>And they didn’t have an answer…</p>
<p>1023<br>01:10:37,000 –&gt; 01:10:39,000<br>You’re going to build a new company…</p>
<p>1024<br>01:10:39,000 –&gt; 01:10:41,000<br>And pick one of those techniques…</p>
<p>1025<br>01:10:41,000 –&gt; 01:10:42,000<br>Which one should you target…</p>
<p>1026<br>01:10:42,000 –&gt; 01:10:44,000<br>A lot of the stuff they built…</p>
<p>1027<br>01:10:44,000 –&gt; 01:10:46,000<br>2015, 2016…</p>
<p>1028<br>01:10:46,000 –&gt; 01:10:48,000<br>And they haven’t really…</p>
<p>1029<br>01:10:48,000 –&gt; 01:10:50,000<br>Going back and revisited it…</p>
<p>1030<br>01:10:50,000 –&gt; 01:10:52,000<br>And I think other things like the memory allocator…</p>
<p>1031<br>01:10:52,000 –&gt; 01:10:54,000<br>Like Jemal or Memeal or TC Mal…</p>
<p>1032<br>01:10:54,000 –&gt; 01:10:56,000<br>They’ve gotten a lot better…</p>
<p>1033<br>01:10:56,000 –&gt; 01:10:58,000<br>And they’re not going to be worth it to build it from scratch now…</p>
<p>1034<br>01:10:58,000 –&gt; 01:11:00,000<br>Yes?</p>
<p>1035<br>01:11:00,000 –&gt; 01:11:02,000<br>You said they used to use them…</p>
<p>1036<br>01:11:02,000 –&gt; 01:11:04,000<br>And what would it cost a lot of money…</p>
<p>1037<br>01:11:04,000 –&gt; 01:11:06,000<br>So is this really…</p>
<p>1038<br>01:11:06,000 –&gt; 01:11:08,000<br>Do you think like…</p>
<p>1039<br>01:11:08,000 –&gt; 01:11:10,000<br>Smoker than Fanyback would be interested in cost saving money?</p>
<p>1040<br>01:11:10,000 –&gt; 01:11:12,000<br>But you can only get that on prem…</p>
<p>1041<br>01:11:12,000 –&gt; 01:11:14,000<br>Is UDP…</p>
<p>1042<br>01:11:14,000 –&gt; 01:11:18,000<br>Is this UDP thing they’re doing?</p>
<p>1043<br>01:11:18,000 –&gt; 01:11:22,000<br>Are they doing this because it’s cheaper?</p>
<p>1044<br>01:11:22,000 –&gt; 01:11:26,000<br>I think no, I think they’re doing this because if you just use like…</p>
<p>1045<br>01:11:26,000 –&gt; 01:11:28,000<br>Out of the box OS networking stack…</p>
<p>1046<br>01:11:28,000 –&gt; 01:11:30,000<br>It’s going to be slower.</p>
<p>1047<br>01:11:30,000 –&gt; 01:11:32,000<br>So like by building something custom…</p>
<p>1048<br>01:11:32,000 –&gt; 01:11:34,000<br>They get better performance.</p>
<p>1049<br>01:11:34,000 –&gt; 01:11:36,000<br>I don’t think it achieves the incentive end performance…</p>
<p>1050<br>01:11:36,000 –&gt; 01:11:38,000<br>But it moves you closer to it.</p>
<p>1051<br>01:11:38,000 –&gt; 01:11:42,000<br>Because 20% best on TCP doesn’t sound like…</p>
<p>1052<br>01:11:42,000 –&gt; 01:11:44,000<br>Crazy at all.</p>
<p>1053<br>01:11:44,000 –&gt; 01:11:46,000<br>How slow DCP is?</p>
<p>1054<br>01:11:46,000 –&gt; 01:11:48,000<br>It doesn’t sound like crazy…</p>
<p>1055<br>01:11:48,000 –&gt; 01:11:50,000<br>You think you could do better or…</p>
<p>1056<br>01:11:50,000 –&gt; 01:11:52,000<br>Well DCP does like so much stuff in this communication…</p>
<p>1057<br>01:11:52,000 –&gt; 01:11:54,000<br>Like UDP would be basically just blasting fights in a wire…</p>
<p>1058<br>01:11:54,000 –&gt; 01:11:56,000<br>Like you would think that…</p>
<p>1059<br>01:11:56,000 –&gt; 01:11:58,000<br>It would be much higher.</p>
<p>1060<br>01:11:58,000 –&gt; 01:12:00,000<br>Plus the DPDK did doing kernel bypass anyway.</p>
<p>1061<br>01:12:00,000 –&gt; 01:12:02,000<br>I mean it depends on the query.</p>
<p>1062<br>01:12:02,000 –&gt; 01:12:04,000<br>Again there’s one query in the TPCDS workload…</p>
<p>1063<br>01:12:04,000 –&gt; 01:12:06,000<br>Where they were getting a 77% improvement…</p>
<p>1064<br>01:12:06,000 –&gt; 01:12:08,000<br>Because I think transfer to a ton of data.</p>
<p>1065<br>01:12:08,000 –&gt; 01:12:10,000<br>On average it’s 20%.</p>
<p>1066<br>01:12:12,000 –&gt; 01:12:22,000<br>So your question is…</p>
<p>1067<br>01:12:22,000 –&gt; 01:12:24,000<br>Can we take a little bit to show that…</p>
<p>1068<br>01:12:24,000 –&gt; 01:12:26,000<br>Yeah so it’s quite…</p>
<p>1069<br>01:12:26,000 –&gt; 01:12:28,000<br>So your question is…</p>
<p>1070<br>01:12:28,000 –&gt; 01:12:30,000<br>How do you show a reliable delivery…</p>
<p>1071<br>01:12:30,000 –&gt; 01:12:32,000<br>Do you still do sequence numbers?</p>
<p>1072<br>01:12:32,000 –&gt; 01:12:34,000<br>Yes, but…</p>
<p>1073<br>01:12:34,000 –&gt; 01:12:36,000<br>I can’t say how they do it…</p>
<p>1074<br>01:12:36,000 –&gt; 01:12:38,000<br>But you can imagine like…</p>
<p>1075<br>01:12:38,000 –&gt; 01:12:40,000<br>Okay, I know I need to send 10 packets…</p>
<p>1076<br>01:12:40,000 –&gt; 01:12:42,000<br>So when I start sending you data…</p>
<p>1077<br>01:12:42,000 –&gt; 01:12:44,000<br>Here’s one of 10…</p>
<p>1078<br>01:12:44,000 –&gt; 01:12:46,000<br>Two of 10, three of 10…</p>
<p>1079<br>01:12:46,000 –&gt; 01:12:48,000<br>If I don’t see four of 10…</p>
<p>1080<br>01:12:48,000 –&gt; 01:12:50,000<br>I go back and ask where back.</p>
<p>1081<br>01:12:50,000 –&gt; 01:12:54,000<br>Yes, they are re-implementing something that looks like TCP…</p>
<p>1082<br>01:12:54,000 –&gt; 01:12:58,000<br>But TCP sends acts for everything.</p>
<p>1083<br>01:12:58,000 –&gt; 01:13:00,000<br>Here’s a packet.</p>
<p>1084<br>01:13:00,000 –&gt; 01:13:02,000<br>Did you get it?</p>
<p>1085<br>01:13:02,000 –&gt; 01:13:04,000<br>Yes, I got it.</p>
<p>1086<br>01:13:04,000 –&gt; 01:13:06,000<br>And this is like…</p>
<p>1087<br>01:13:06,000 –&gt; 01:13:08,000<br>Okay, I should have sent you 10.</p>
<p>1088<br>01:13:08,000 –&gt; 01:13:10,000<br>Did you get 10? Yes.</p>
<p>1089<br>01:13:10,000 –&gt; 01:13:12,000<br>Yeah, I don’t know…</p>
<p>1090<br>01:13:12,000 –&gt; 01:13:14,000<br>You know, simple ways to do it…</p>
<p>1091<br>01:13:14,000 –&gt; 01:13:16,000<br>Like some…</p>
<p>1092<br>01:13:16,000 –&gt; 01:13:18,000<br>Just try and see if you can see…</p>
<p>1093<br>01:13:18,000 –&gt; 01:13:20,000<br>Like just moving everything you got…</p>
<p>1094<br>01:13:20,000 –&gt; 01:13:22,000<br>Like that.</p>
<p>1095<br>01:13:22,000 –&gt; 01:13:24,000<br>Hey…</p>
<p>1096<br>01:13:24,000 –&gt; 01:13:26,000<br>Because like…</p>
<p>1097<br>01:13:26,000 –&gt; 01:13:28,000<br>What they just do…</p>
<p>1098<br>01:13:28,000 –&gt; 01:13:30,000<br>You need the best moment in time…</p>
<p>1099<br>01:13:30,000 –&gt; 01:13:32,000<br>In five years ago was the only choice…</p>
<p>1100<br>01:13:32,000 –&gt; 01:13:34,000<br>It’s for EPBF.</p>
<p>1101<br>01:13:34,000 –&gt; 01:13:36,000<br>Right?</p>
<p>1102<br>01:13:36,000 –&gt; 01:13:40,000<br>Or say you can go back to like 2016-2017.</p>
<p>1103<br>01:13:40,000 –&gt; 01:13:44,000<br>I think there’s papers around…</p>
<p>1104<br>01:13:44,000 –&gt; 01:13:46,000<br>Even 2020, the show like…</p>
<p>1105<br>01:13:46,000 –&gt; 01:13:48,000<br>In like the Linux networking stack…</p>
<p>1106<br>01:13:48,000 –&gt; 01:13:50,000<br>We just hammered the hell out of it…</p>
<p>1107<br>01:13:50,000 –&gt; 01:13:52,000<br>Fifty percent of the time it spent men copying in the kernel.</p>
<p>1108<br>01:13:52,000 –&gt; 01:13:54,000<br>So DBDK will avoid that.</p>
<p>1109<br>01:13:54,000 –&gt; 01:13:56,000<br>So now if you want to reduce them on round trips…</p>
<p>1110<br>01:13:56,000 –&gt; 01:13:58,000<br>I have to do…</p>
<p>1111<br>01:13:58,000 –&gt; 01:14:00,000<br>You have to use something…</p>
<p>1112<br>01:14:00,000 –&gt; 01:14:02,000<br>Like with the UDP thing.</p>
<p>1113<br>01:14:02,000 –&gt; 01:14:04,000<br>And you can only do this for…</p>
<p>1114<br>01:14:04,000 –&gt; 01:14:06,000<br>In a network communication…</p>
<p>1115<br>01:14:06,000 –&gt; 01:14:08,000<br>Like obviously Amazon is not going to know your funky UDP protocol…</p>
<p>1116<br>01:14:08,000 –&gt; 01:14:10,000<br>When you start having S3…</p>
<p>1117<br>01:14:10,000 –&gt; 01:14:12,000<br>You have to use what they want for that…</p>
<p>1118<br>01:14:12,000 –&gt; 01:14:16,000<br>But you can use DBDK to get that data up…</p>
<p>1119<br>01:14:16,000 –&gt; 01:14:18,000<br>Quickly as possible…</p>
<p>1120<br>01:14:18,000 –&gt; 01:14:20,000<br>On the client side.</p>
<p>1121<br>01:14:22,000 –&gt; 01:14:24,000<br>So, as David is…</p>
<p>1122<br>01:14:24,000 –&gt; 01:14:26,000<br>He’s skeptical…</p>
<p>1123<br>01:14:26,000 –&gt; 01:14:28,000<br>He’s thinking…</p>
<p>1124<br>01:14:28,000 –&gt; 01:14:30,000<br>Three more slides…</p>
<p>1125<br>01:14:30,000 –&gt; 01:14:32,000<br>We can revisit this.</p>
<p>1126<br>01:14:32,000 –&gt; 01:14:34,000<br>Yes.</p>
<p>1127<br>01:14:34,000 –&gt; 01:14:36,000<br>Other questions.</p>
<p>1128<br>01:14:36,000 –&gt; 01:14:38,000<br>Alright, so as I said, I like to pay it because they actually have benchmark results…</p>
<p>1129<br>01:14:38,000 –&gt; 01:14:40,000<br>Which I think there are other ones that actually do…</p>
<p>1130<br>01:14:40,000 –&gt; 01:14:42,000<br>With absolute numbers, not relative numbers.</p>
<p>1131<br>01:14:42,000 –&gt; 01:14:46,000<br>And so for this, they’re running all TPC-DS scale factor 1…</p>
<p>1132<br>01:14:46,000 –&gt; 01:14:48,000<br>So it’s not that big…</p>
<p>1133<br>01:14:48,000 –&gt; 01:14:50,000<br>But TPC-DS has 100 queries…</p>
<p>1134<br>01:14:50,000 –&gt; 01:14:52,000<br>And they show the total runtime of the workload…</p>
<p>1135<br>01:14:52,000 –&gt; 01:14:54,000<br>And so you can see…</p>
<p>1136<br>01:14:54,000 –&gt; 01:14:56,000<br>In the Paragons, Yellowbricks, Snowflick, Redshift,</p>
<p>1137<br>01:14:56,000 –&gt; 01:14:58,000<br>BigQuery, Synapse, and Databricks…</p>
<p>1138<br>01:14:58,000 –&gt; 01:15:02,000<br>Again, Redshift will cover on Wednesday…</p>
<p>1139<br>01:15:02,000 –&gt; 01:15:06,000<br>Synapse is from Microsoft and Azure…</p>
<p>1140<br>01:15:06,000 –&gt; 01:15:08,000<br>We were supposed to cover that…</p>
<p>1141<br>01:15:08,000 –&gt; 01:15:10,000<br>And then…</p>
<p>1142<br>01:15:10,000 –&gt; 01:15:12,000<br>Whatever…</p>
<p>1143<br>01:15:12,000 –&gt; 01:15:14,000<br>Diaria…</p>
<p>1144<br>01:15:14,000 –&gt; 01:15:16,000<br>Anyway…</p>
<p>1145<br>01:15:16,000 –&gt; 01:15:18,000<br>So off the bat, you see that again…</p>
<p>1146<br>01:15:18,000 –&gt; 01:15:20,000<br>For this workload, they’re the fastest…</p>
<p>1147<br>01:15:20,000 –&gt; 01:15:22,000<br>But again…</p>
<p>1148<br>01:15:22,000 –&gt; 01:15:26,000<br>All these systems abstract away what the hardware actually is…</p>
<p>1149<br>01:15:26,000 –&gt; 01:15:28,000<br>You say, I want 10…</p>
<p>1150<br>01:15:28,000 –&gt; 01:15:30,000<br>You know, my data warehouse has 10 units…</p>
<p>1151<br>01:15:30,000 –&gt; 01:15:32,000<br>What does that mean?</p>
<p>1152<br>01:15:32,000 –&gt; 01:15:34,000<br>What is the disk?</p>
<p>1153<br>01:15:34,000 –&gt; 01:15:36,000<br>What is the CPU? What is the memory?</p>
<p>1154<br>01:15:36,000 –&gt; 01:15:38,000<br>So all that subtraction away…</p>
<p>1155<br>01:15:38,000 –&gt; 01:15:40,000<br>So you can say, what’s the cost to actually run this per hour…</p>
<p>1156<br>01:15:40,000 –&gt; 01:15:42,000<br>For these queries…</p>
<p>1157<br>01:15:42,000 –&gt; 01:15:44,000<br>And then you divide that time…</p>
<p>1158<br>01:15:44,000 –&gt; 01:15:46,000<br>It takes them on the workload by the cost per hour…</p>
<p>1159<br>01:15:46,000 –&gt; 01:15:48,000<br>And you can get what…</p>
<p>1160<br>01:15:48,000 –&gt; 01:15:50,000<br>What is actually the cost of actually running this thing?</p>
<p>1161<br>01:15:50,000 –&gt; 01:15:54,000<br>The memory of the network is significantly cheaper than the others…</p>
<p>1162<br>01:15:54,000 –&gt; 01:15:56,000<br>And yeah, we’re talking like $2…</p>
<p>1163<br>01:15:56,000 –&gt; 01:16:00,000<br>But like, this is like for a workload that took 900 seconds…</p>
<p>1164<br>01:16:00,000 –&gt; 01:16:02,000<br>Think of like in a real enterprise system…</p>
<p>1165<br>01:16:02,000 –&gt; 01:16:04,000<br>These queries are running all the time…</p>
<p>1166<br>01:16:04,000 –&gt; 01:16:06,000<br>Your thing is running all the time…</p>
<p>1167<br>01:16:06,000 –&gt; 01:16:10,000<br>And you know, this was certainly start to add up…</p>
<p>1168<br>01:16:10,000 –&gt; 01:16:12,000<br>And like I said, SkeelFactor 1 is not that big of a wearer…</p>
<p>1169<br>01:16:12,000 –&gt; 01:16:14,000<br>Big of a size we do…</p>
<p>1170<br>01:16:14,000 –&gt; 01:16:16,000<br>Alright, so do we trust these numbers…</p>
<p>1171<br>01:16:16,000 –&gt; 01:16:18,000<br>As much as we like looking at them…</p>
<p>1172<br>01:16:18,000 –&gt; 01:16:22,000<br>No, maybe…</p>
<p>1173<br>01:16:22,000 –&gt; 01:16:24,000<br>Maybe? Why maybe?</p>
<p>1174<br>01:16:24,000 –&gt; 01:16:26,000<br>What’s TPCD?</p>
<p>1175<br>01:16:26,000 –&gt; 01:16:28,000<br>Is it TPCDS?</p>
<p>1176<br>01:16:28,000 –&gt; 01:16:30,000<br>I know, well, what would it look like?</p>
<p>1177<br>01:16:30,000 –&gt; 01:16:32,000<br>Oh, with TPCH numbers?</p>
<p>1178<br>01:16:32,000 –&gt; 01:16:34,000<br>Similar? That’s not the thing he is that maybe…</p>
<p>1179<br>01:16:34,000 –&gt; 01:16:36,000<br>TPCDS is more complicated.</p>
<p>1180<br>01:16:36,000 –&gt; 01:16:38,000<br>Exactly, right? So…</p>
<p>1181<br>01:16:38,000 –&gt; 01:16:40,000<br>It’s simpler…</p>
<p>1182<br>01:16:40,000 –&gt; 01:16:46,000<br>Yeah, so I was thinking maybe they chose the TPCDS strategically to really display…</p>
<p>1183<br>01:16:46,000 –&gt; 01:16:48,000<br>What there is can be better…</p>
<p>1184<br>01:16:48,000 –&gt; 01:16:50,000<br>But when it comes to maybe like…</p>
<p>1185<br>01:16:50,000 –&gt; 01:16:54,000<br>Every day and a load of queries that are probably simpler than TPCDS…</p>
<p>1186<br>01:16:54,000 –&gt; 01:16:58,000<br>Perhaps it’s across the internet.</p>
<p>1187<br>01:16:58,000 –&gt; 01:17:00,000<br>Okay.</p>
<p>1188<br>01:17:02,000 –&gt; 01:17:04,000<br>Yes.</p>
<p>1189<br>01:17:04,000 –&gt; 01:17:06,000<br>Maybe they didn’t account for uploading the data.</p>
<p>1190<br>01:17:06,000 –&gt; 01:17:08,000<br>I said maybe they didn’t account for loading the data…</p>
<p>1191<br>01:17:08,000 –&gt; 01:17:10,000<br>Look, we saw with Snowflake.</p>
<p>1192<br>01:17:10,000 –&gt; 01:17:12,000<br>That one I’m less worried about…</p>
<p>1193<br>01:17:12,000 –&gt; 01:17:14,000<br>I don’t think they would be that…</p>
<p>1194<br>01:17:14,000 –&gt; 01:17:16,000<br>About…</p>
<p>1195<br>01:17:16,000 –&gt; 01:17:18,000<br>What did I mean by?</p>
<p>1196<br>01:17:18,000 –&gt; 01:17:22,000<br>I don’t think they would be that naive.</p>
<p>1197<br>01:17:22,000 –&gt; 01:17:26,000<br>So, like…</p>
<p>1198<br>01:17:26,000 –&gt; 01:17:30,000<br>Yeah, this is TPCDS for a different workload…</p>
<p>1199<br>01:17:30,000 –&gt; 01:17:32,000<br>But we just said they’re a query optimizer…</p>
<p>1200<br>01:17:32,000 –&gt; 01:17:34,000<br>You know, they inject rules…</p>
<p>1201<br>01:17:34,000 –&gt; 01:17:36,000<br>And make sure they generate the right query plan, right?</p>
<p>1202<br>01:17:36,000 –&gt; 01:17:38,000<br>Okay, I’m not saying they play tricks…</p>
<p>1203<br>01:17:38,000 –&gt; 01:17:40,000<br>But like…</p>
<p>1204<br>01:17:40,000 –&gt; 01:17:42,000<br>You don’t know what the query plan looks like…</p>
<p>1205<br>01:17:42,000 –&gt; 01:17:44,000<br>Maybe for whatever reason…</p>
<p>1206<br>01:17:44,000 –&gt; 01:17:46,000<br>Redshift or Synapse…</p>
<p>1207<br>01:17:46,000 –&gt; 01:17:48,000<br>Just picked a crappy query plan, right?</p>
<p>1208<br>01:17:48,000 –&gt; 01:17:50,000<br>Or in this case here…</p>
<p>1209<br>01:17:50,000 –&gt; 01:17:52,000<br>Synapse is $6 an hour…</p>
<p>1210<br>01:17:52,000 –&gt; 01:17:54,000<br>Maybe this is…</p>
<p>1211<br>01:17:54,000 –&gt; 01:17:56,000<br>You know, it’s just running on fewer machines…</p>
<p>1212<br>01:17:56,000 –&gt; 01:17:58,000<br>Because maybe the next one…</p>
<p>1213<br>01:17:58,000 –&gt; 01:18:00,000<br>Up won’t $12 an hour…</p>
<p>1214<br>01:18:00,000 –&gt; 01:18:02,000<br>But maybe that will cut the time down to what Ellibor can do.</p>
<p>1215<br>01:18:02,000 –&gt; 01:18:04,000<br>So, the main takeaway from all of this is that…</p>
<p>1216<br>01:18:04,000 –&gt; 01:18:06,000<br>There’s so many different factors in databases…</p>
<p>1217<br>01:18:06,000 –&gt; 01:18:08,000<br>That it’s really hard to be able to say like…</p>
<p>1218<br>01:18:08,000 –&gt; 01:18:10,000<br>You know, from one workload…</p>
<p>1219<br>01:18:10,000 –&gt; 01:18:12,000<br>And the thing that I made fun of before…</p>
<p>1220<br>01:18:12,000 –&gt; 01:18:14,000<br>That solves this problem…</p>
<p>1221<br>01:18:14,000 –&gt; 01:18:16,000<br>Because that can guarantee you…</p>
<p>1222<br>01:18:16,000 –&gt; 01:18:18,000<br>Have a…</p>
<p>1223<br>01:18:18,000 –&gt; 01:18:20,000<br>A level playing field…</p>
<p>1224<br>01:18:20,000 –&gt; 01:18:22,000<br>But that accounts for just like…</p>
<p>1225<br>01:18:22,000 –&gt; 01:18:24,000<br>Are you running the process correctly…</p>
<p>1226<br>01:18:24,000 –&gt; 01:18:26,000<br>It doesn’t explain why things are actually better than another…</p>
<p>1227<br>01:18:26,000 –&gt; 01:18:28,000<br>Right? Because again, yellow brick is going to do…</p>
<p>1228<br>01:18:28,000 –&gt; 01:18:30,000<br>High-cue style,</p>
<p>1229<br>01:18:30,000 –&gt; 01:18:32,000<br>Seaple-Slow compilation of query plans…</p>
<p>1230<br>01:18:32,000 –&gt; 01:18:34,000<br>Well, guess what? Redshift does the same thing…</p>
<p>1231<br>01:18:34,000 –&gt; 01:18:36,000<br>Right?</p>
<p>1232<br>01:18:36,000 –&gt; 01:18:38,000<br>Snowflake and BigQuery…</p>
<p>1233<br>01:18:38,000 –&gt; 01:18:40,000<br>And you’re running Spark SQL…</p>
<p>1234<br>01:18:40,000 –&gt; 01:18:42,000<br>Or how do you know that this was running photon…</p>
<p>1235<br>01:18:42,000 –&gt; 01:18:44,000<br>And not…</p>
<p>1236<br>01:18:44,000 –&gt; 01:18:46,000<br>Not regular Spark SQL.</p>
<p>1237<br>01:18:46,000 –&gt; 01:18:48,000<br>So, again, it’s nice to look at it…</p>
<p>1238<br>01:18:48,000 –&gt; 01:18:50,000<br>It doesn’t necessarily tell the whole story…</p>
<p>1239<br>01:18:50,000 –&gt; 01:18:52,000<br>And obviously, the end of the day…</p>
<p>1240<br>01:18:52,000 –&gt; 01:18:54,000<br>You really only care about not TPCDS numbers…</p>
<p>1241<br>01:18:54,000 –&gt; 01:18:56,000<br>Like what your workload actually can do.</p>
<p>1242<br>01:18:56,000 –&gt; 01:19:00,000<br>And so, what leads me to my…</p>
<p>1243<br>01:19:00,000 –&gt; 01:19:02,000<br>Finishing comment…</p>
<p>1244<br>01:19:02,000 –&gt; 01:19:04,000<br>Like, the…</p>
<p>1245<br>01:19:04,000 –&gt; 01:19:06,000<br>It’s very impressive what they did…</p>
<p>1246<br>01:19:06,000 –&gt; 01:19:07,000<br>Right? And insane.</p>
<p>1247<br>01:19:07,000 –&gt; 01:19:09,000<br>And so, you know, it’s a system that does all these optimizations…</p>
<p>1248<br>01:19:09,000 –&gt; 01:19:11,000<br>That the way they have.</p>
<p>1249<br>01:19:11,000 –&gt; 01:19:13,000<br>And if you were like a brand new startup today…</p>
<p>1250<br>01:19:13,000 –&gt; 01:19:15,000<br>And say, hey, I’m going to build a brand new system…</p>
<p>1251<br>01:19:15,000 –&gt; 01:19:17,000<br>And you would…</p>
<p>1252<br>01:19:17,000 –&gt; 01:19:19,000<br>You know, I would not go the path that they went down…</p>
<p>1253<br>01:19:19,000 –&gt; 01:19:21,000<br>To like…</p>
<p>1254<br>01:19:21,000 –&gt; 01:19:23,000<br>You know, implement all this low-level hardware stuff that they did.</p>
<p>1255<br>01:19:23,000 –&gt; 01:19:25,000<br>But again, it’s fascinating.</p>
<p>1256<br>01:19:25,000 –&gt; 01:19:27,000<br>Some things you wouldn’t do anymore…</p>
<p>1257<br>01:19:27,000 –&gt; 01:19:29,000<br>I don’t think you would want to use DBDK…</p>
<p>1258<br>01:19:29,000 –&gt; 01:19:31,000<br>I think you’d want to use BPF…</p>
<p>1259<br>01:19:31,000 –&gt; 01:19:33,000<br>And that could give you most of the same benefit…</p>
<p>1260<br>01:19:33,000 –&gt; 01:19:35,000<br>Either you ring my help in other situations as well…</p>
<p>1261<br>01:19:35,000 –&gt; 01:19:37,000<br>And I don’t think when the gatekeepers talked to us…</p>
<p>1262<br>01:19:37,000 –&gt; 01:19:39,000<br>They were doing that.</p>
<p>1263<br>01:19:39,000 –&gt; 01:19:41,000<br>But the key thing that matters…</p>
<p>1264<br>01:19:41,000 –&gt; 01:19:43,000<br>And related to the last slide is…</p>
<p>1265<br>01:19:43,000 –&gt; 01:19:45,000<br>All of this…</p>
<p>1266<br>01:19:45,000 –&gt; 01:19:47,000<br>All these optimizations…</p>
<p>1267<br>01:19:47,000 –&gt; 01:19:49,000<br>These OS-level stuff they’re doing…</p>
<p>1268<br>01:19:49,000 –&gt; 01:19:51,000<br>Does not matter if you pick crappy quarter plants.</p>
<p>1269<br>01:19:51,000 –&gt; 01:19:53,000<br>If your join orders are just wrong…</p>
<p>1270<br>01:19:53,000 –&gt; 01:19:55,000<br>Then who cares that you’re doing kernel bypass…</p>
<p>1271<br>01:19:55,000 –&gt; 01:19:57,000<br>With custom PCIe drivers.</p>
<p>1272<br>01:19:57,000 –&gt; 01:19:59,000<br>Because your join…</p>
<p>1273<br>01:19:59,000 –&gt; 01:20:01,000<br>Your query plant…</p>
<p>1274<br>01:20:01,000 –&gt; 01:20:03,000<br>Is going to be absolutely terrible.</p>
<p>1275<br>01:20:03,000 –&gt; 01:20:05,000<br>Okay?</p>
<p>1276<br>01:20:05,000 –&gt; 01:20:07,000<br>Alright, so next class is the last lecture…</p>
<p>1277<br>01:20:07,000 –&gt; 01:20:09,000<br>We’ll do Amazon Redshift…</p>
<p>1278<br>01:20:09,000 –&gt; 01:20:13,000<br>And then, like I said, I’ll give out the final exam…</p>
<p>1279<br>01:20:13,000 –&gt; 01:20:15,000<br>For you guys to take home…</p>
<p>1280<br>01:20:15,000 –&gt; 01:20:17,000<br>And that’ll be due next week…</p>
<p>1281<br>01:20:17,000 –&gt; 01:20:19,000<br>And then I have office hours today…</p>
<p>1282<br>01:20:19,000 –&gt; 01:20:21,000<br>But not on Wednesday…</p>
<p>1283<br>01:20:21,000 –&gt; 01:20:23,000<br>Because I got a flight for Seattle for legal stuff.</p>
<p>1284<br>01:20:23,000 –&gt; 01:20:25,000<br>Okay? Yeah, I can’t talk about it.</p>
<p>1285<br>01:20:25,000 –&gt; 01:20:29,000<br>You know, I’m glad you had a belt to get the 40M bar.</p>
<p>1286<br>01:20:29,000 –&gt; 01:20:31,000<br>Get a grip, take a sip, and you’ll be picking up bottles.</p>
<p>1287<br>01:20:31,000 –&gt; 01:20:33,000<br>Ain’t ain’t no puzzle, I’ll go through some more man.</p>
<p>1288<br>01:20:33,000 –&gt; 01:20:35,000<br>I’m telling the 40M I saw these glass floor cans.</p>
<p>1289<br>01:20:35,000 –&gt; 01:20:37,000<br>Slaps and sticks, and packs on a table.</p>
<p>1290<br>01:20:37,000 –&gt; 01:20:39,000<br>And I’m able to see St. I was on a label.</p>
<p>1291<br>01:20:39,000 –&gt; 01:20:41,000<br>No sure, put the fuck you know what got them.</p>
<p>1292<br>01:20:41,000 –&gt; 01:20:43,000<br>I take off the cap, my first attack on the bottle.</p>
<p>1293<br>01:20:43,000 –&gt; 01:20:45,000<br>Throw my green and freezer, throw my utility.</p>
<p>1294<br>01:20:45,000 –&gt; 01:20:47,000<br>Careful with the bottle, baby, you can still spill it.</p>
<p>1295<br>01:20:47,000 –&gt; 01:20:49,000<br>Cause St. I was insane, the paint last wet.</p>
<p>1296<br>01:20:49,000 –&gt; 01:20:51,000<br>You drink it down with the guys, little bottle head.</p>
<p>1297<br>01:20:51,000 –&gt; 01:20:53,000<br>Take back the pack of drugs.</p>
<p>1298<br>01:20:53,000 –&gt; 01:20:55,000<br>You gon’ get your suicide now, so drink it till it floods.</p>
<p>1299<br>01:20:55,000 –&gt; 01:20:57,000<br>Billy Dan’s the utility teacher, tell me good week guys.</p>
<p>1300<br>01:20:57,000 –&gt; 01:20:59,000<br>Be a man and get a can of faith, huh?</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CMU15721 P21S202421 YellowbrickDataWarehouseSystemCMUAdvancedDatabaseSystems</div>
      <div>http://example.com/2025/10/24/CMU15721 P21S202421-YellowbrickDataWarehouseSystemCMUAdvancedDatabaseSystems/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月24日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/24/CMU15721%20P22S202422-AmazonRedshiftDataWarehouseSystemCMUAdvancedDatabaseSystems/" title="CMU15721 P22S202422 AmazonRedshiftDataWarehouseSystemCMUAdvancedDatabaseSystems">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CMU15721 P22S202422 AmazonRedshiftDataWarehouseSystemCMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/24/CMU15721%20P2S202401-ModernOLAPDatabaseSystemsCMUAdvancedDatabaseSystems/" title="CMU15721 P2S202401 ModernOLAPDatabaseSystemsCMUAdvancedDatabaseSystems">
                        <span class="hidden-mobile">CMU15721 P2S202401 ModernOLAPDatabaseSystemsCMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
