

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="100:00:00,000 –&gt; 00:00:06,000Carnegie Mellon University’s advanced database systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio audience. 300:00:13,000 –&gt; 00:00:16,0">
<meta property="og:type" content="article">
<meta property="og:title" content="CMU15721 P2S202401 ModernOLAPDatabaseSystemsCMUAdvancedDatabaseSystems">
<meta property="og:url" content="http://example.com/2025/10/24/CMU15721%20P2S202401-ModernOLAPDatabaseSystemsCMUAdvancedDatabaseSystems/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="100:00:00,000 –&gt; 00:00:06,000Carnegie Mellon University’s advanced database systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio audience. 300:00:13,000 –&gt; 00:00:16,0">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-24T11:57:41.748Z">
<meta property="article:modified_time" content="2025-10-24T12:06:28.548Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>CMU15721 P2S202401 ModernOLAPDatabaseSystemsCMUAdvancedDatabaseSystems - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CMU15721 P2S202401 ModernOLAPDatabaseSystemsCMUAdvancedDatabaseSystems"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-24 19:57" pubdate>
          2025年10月24日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          8.2k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          68 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CMU15721 P2S202401 ModernOLAPDatabaseSystemsCMUAdvancedDatabaseSystems</h1>
            
            
              <div class="markdown-body">
                
                <p>1<br>00:00:00,000 –&gt; 00:00:06,000<br>Carnegie Mellon University’s advanced database systems courses</p>
<p>2<br>00:00:06,000 –&gt; 00:00:09,000<br>filming front of the live studio audience.</p>
<p>3<br>00:00:13,000 –&gt; 00:00:16,000<br>The first class is the beginning of the discussion of the lecture</p>
<p>4<br>00:00:16,000 –&gt; 00:00:18,000<br>material throughout the semester.</p>
<p>5<br>00:00:18,000 –&gt; 00:00:21,000<br>And it’s still sort of a high level overview of,</p>
<p>6<br>00:00:21,000 –&gt; 00:00:26,000<br>of, of, of, of, of the idea here is that we will set the foundation</p>
<p>7<br>00:00:26,000 –&gt; 00:00:31,000<br>for all the various parts of the systems and the papers we’re going to talk about,</p>
<p>8<br>00:00:31,000 –&gt; 00:00:34,000<br>like how to build these individual components in a, in a modern system.</p>
<p>9<br>00:00:34,000 –&gt; 00:00:37,000<br>And then obviously this is highly related to the projects</p>
<p>10<br>00:00:37,000 –&gt; 00:00:40,000<br>everybody working on which we’ll discuss at the end.</p>
<p>11<br>00:00:40,000 –&gt; 00:00:44,000<br>But this, this sets the ground, this is sort of sets the context in which we will build</p>
<p>12<br>00:00:44,000 –&gt; 00:00:47,000<br>all the things that we’ll talk about throughout the semester.</p>
<p>13<br>00:00:47,000 –&gt; 00:00:51,000<br>So, um, and we’re going to first talk about the, again, the basic background</p>
<p>14<br>00:00:51,000 –&gt; 00:00:56,000<br>of the, what we ended up with the, so what is the, the prevailing architecture</p>
<p>15<br>00:00:56,000 –&gt; 00:01:01,000<br>for a modern OLAP system, we talked about some high level choices and issues in this space.</p>
<p>16<br>00:01:01,000 –&gt; 00:01:04,000<br>So, I started early, sorry.</p>
<p>17<br>00:01:04,000 –&gt; 00:01:06,000<br>Yeah.</p>
<p>18<br>00:01:06,000 –&gt; 00:01:08,000<br>Super-ever.</p>
<p>19<br>00:01:08,000 –&gt; 00:01:09,000<br>Yeah.</p>
<p>20<br>00:01:09,000 –&gt; 00:01:10,000<br>Right, yeah.</p>
<p>21<br>00:01:10,000 –&gt; 00:01:15,000<br>So the idea, again, the idea is that we want to sort of talk about what some of the historical systems</p>
<p>22<br>00:01:15,000 –&gt; 00:01:19,000<br>look like, how we end up with what the, what people are, how people build these systems</p>
<p>23<br>00:01:19,000 –&gt; 00:01:23,000<br>and then we’ll talk about the high level issues you have in building one of these systems.</p>
<p>24<br>00:01:23,000 –&gt; 00:01:27,000<br>And then we’ll finish off just sort of a quick overview of what a query goes through,</p>
<p>25<br>00:01:27,000 –&gt; 00:01:31,000<br>what happens when you actually execute a query in one of these systems.</p>
<p>26<br>00:01:31,000 –&gt; 00:01:32,000<br>Okay?</p>
<p>27<br>00:01:32,000 –&gt; 00:01:37,000<br>And again, this is a grad level class, stop me and ask questions as we go along.</p>
<p>28<br>00:01:37,000 –&gt; 00:01:39,000<br>Okay?</p>
<p>29<br>00:01:39,000 –&gt; 00:01:44,000<br>All right, so if you recall from the intro class, we made this distinction between</p>
<p>30<br>00:01:44,000 –&gt; 00:01:48,000<br>these operational databases, or front-end databases, or OLTP databases,</p>
<p>31<br>00:01:48,000 –&gt; 00:01:54,000<br>and these analytical database systems, or OLAP, online analytical processing systems.</p>
<p>32<br>00:01:54,000 –&gt; 00:02:00,000<br>And again, the distinction was, and an OLTP system, that’s really the thing that faces the outside world,</p>
<p>33<br>00:02:00,000 –&gt; 00:02:04,000<br>either humans or computers, like a little web interface or a restaurant interface,</p>
<p>34<br>00:02:04,000 –&gt; 00:02:06,000<br>that’s ingesting new information.</p>
<p>35<br>00:02:06,000 –&gt; 00:02:11,000<br>Like you’re getting new state, you’re getting new changes, and you want to store that as quickly as possible.</p>
<p>36<br>00:02:11,000 –&gt; 00:02:16,000<br>And then once you sort of accumulated a bunch of this data, now you want to start extracting new information from it.</p>
<p>37<br>00:02:16,000 –&gt; 00:02:21,000<br>You want to extrapolate new knowledge, allows you to make decisions, or decide how to do certain things,</p>
<p>38<br>00:02:21,000 –&gt; 00:02:26,000<br>or provide justifications for whatever it is that you want to achieve in your business,</p>
<p>39<br>00:02:26,000 –&gt; 00:02:29,000<br>your institution, your organization, or whatever.</p>
<p>40<br>00:02:29,000 –&gt; 00:02:31,000<br>So that’s the goal we’re trying to do this semester.</p>
<p>41<br>00:02:31,000 –&gt; 00:02:35,000<br>We’re trying to take a bunch of data we’ve accumulated, and then run queries,</p>
<p>42<br>00:02:35,000 –&gt; 00:02:37,000<br>or run something on it to pull out new data.</p>
<p>43<br>00:02:37,000 –&gt; 00:02:42,000<br>That informs us about what our database actually contains.</p>
<p>44<br>00:02:42,000 –&gt; 00:02:46,000<br>And ideally find trends that we didn’t think of as humans easily.</p>
<p>45<br>00:02:46,000 –&gt; 00:02:53,000<br>So in the old days, people would run these sort of analytical workloads on what I call a monolithic database system,</p>
<p>46<br>00:02:53,000 –&gt; 00:03:01,000<br>meaning a system that had all the components and all the subsystems to actually execute queries and store data,</p>
<p>47<br>00:03:01,000 –&gt; 00:03:04,000<br>was all built inside this one piece of software.</p>
<p>48<br>00:03:05,000 –&gt; 00:03:12,000<br>If you ever run SQLite, or like DuckDB, or mySQL Postgres, that’s considered a monolithic system.</p>
<p>49<br>00:03:12,000 –&gt; 00:03:15,000<br>For embedded databases, less so because they don’t have threadings and so forth.</p>
<p>50<br>00:03:15,000 –&gt; 00:03:19,000<br>They’re like Postgres. You install Postgres, you put them on your laptop, put them on your server,</p>
<p>51<br>00:03:19,000 –&gt; 00:03:21,000<br>start creating tables on it.</p>
<p>52<br>00:03:21,000 –&gt; 00:03:26,000<br>Everything you need to do actually queries and store that data is inside of Postgres.</p>
<p>53<br>00:03:27,000 –&gt; 00:03:34,000<br>So you sort of need these monolithic database systems, was how people were storing the data in the old days.</p>
<p>54<br>00:03:34,000 –&gt; 00:03:38,000<br>And we’ll talk about what this means by centralized storage, but like a managed storage,</p>
<p>55<br>00:03:38,000 –&gt; 00:03:43,000<br>basically the database system is completely in charge of what the bits are getting written down to disk,</p>
<p>56<br>00:03:43,000 –&gt; 00:03:46,000<br>where they’re going, and how to pull them back in.</p>
<p>57<br>00:03:46,000 –&gt; 00:03:52,000<br>So the first sort of work, I shouldn’t say first, because there was, I mean,</p>
<p>58<br>00:03:52,000 –&gt; 00:03:56,000<br>the data from the 1970s, sort of built in the space.</p>
<p>59<br>00:03:56,000 –&gt; 00:04:03,000<br>But people started really paying attention to analytical workloads in the early 90s, maybe late 80s.</p>
<p>60<br>00:04:03,000 –&gt; 00:04:11,000<br>But the prevailing architecture at the time for how people built database systems was getting the classic database system architecture,</p>
<p>61<br>00:04:11,000 –&gt; 00:04:12,000<br>we talked about the intro class.</p>
<p>62<br>00:04:12,000 –&gt; 00:04:17,000<br>Roastore, pages on disk, it’s a buffer pool, fetching things in.</p>
<p>63<br>00:04:17,000 –&gt; 00:04:20,000<br>Because that’s what they were building for operational workloads.</p>
<p>64<br>00:04:20,000 –&gt; 00:04:26,000<br>Roastore is exactly what you want with you want to adjust data very quickly in transaction manner.</p>
<p>65<br>00:04:26,000 –&gt; 00:04:29,000<br>But obviously if you want analytics, that’s going to suck.</p>
<p>66<br>00:04:29,000 –&gt; 00:04:34,000<br>Because now if you’re doing OLAP queries, we only read a subset of the data, you’re fetching in the entire page,</p>
<p>67<br>00:04:34,000 –&gt; 00:04:38,000<br>you’re fetching in the entire row, and there’s going to be a bunch of data you don’t actually need.</p>
<p>68<br>00:04:38,000 –&gt; 00:04:44,000<br>So people realized that these were kind of slow, and so they started building what we call data cubes.</p>
<p>69<br>00:04:45,000 –&gt; 00:04:51,000<br>And you started to think of these as like materialize you are pre-computer aggregation query,</p>
<p>70<br>00:04:51,000 –&gt; 00:04:55,000<br>like a root grind, group I and so forth, across a bunch of different dimensions,</p>
<p>71<br>00:04:55,000 –&gt; 00:05:00,000<br>and you would generate this array more or less, store that in your database,</p>
<p>72<br>00:05:00,000 –&gt; 00:05:05,000<br>and then any analytical query that came along, you would then try to target that data cube,</p>
<p>73<br>00:05:05,000 –&gt; 00:05:08,000<br>because it’s already done a bunch of computation for you.</p>
<p>74<br>00:05:09,000 –&gt; 00:05:14,000<br>You could store these things in an array manner that was better than a Roastore.</p>
<p>75<br>00:05:15,000 –&gt; 00:05:19,000<br>So these things were not automatic, and administrator had a specify,</p>
<p>76<br>00:05:19,000 –&gt; 00:05:24,000<br>I want these pre-computer cubes, again, just like materialize views or regular views.</p>
<p>77<br>00:05:24,000 –&gt; 00:05:29,000<br>And then because materialize views are trying to, certainly at the time before,</p>
<p>78<br>00:05:29,000 –&gt; 00:05:33,000<br>the time of the 90s is definitely still now, they’re difficult to do incremental updates on.</p>
<p>79<br>00:05:34,000 –&gt; 00:05:40,000<br>You had to have a human say manually refresh, like a SQL command to populate the data cube.</p>
<p>80<br>00:05:40,000 –&gt; 00:05:45,000<br>So you would do something like a cron job at night, run the refresh to build the data cube.</p>
<p>81<br>00:05:45,000 –&gt; 00:05:52,000<br>So these were introduced, and I said in operational databases as a way to handle faster analytic queries</p>
<p>82<br>00:05:52,000 –&gt; 00:05:55,000<br>than what you would do over Ro-oriented systems.</p>
<p>83<br>00:05:55,000 –&gt; 00:06:01,000<br>So with the exception of Terra data, and this is the logo for S-base with Oracle bought in 2003 or so,</p>
<p>84<br>00:06:02,000 –&gt; 00:06:08,000<br>like SQL Server, DB2, S-base, Oracle, and Formix, all these guys had their own sort of variation of data cubes.</p>
<p>85<br>00:06:08,000 –&gt; 00:06:13,000<br>Terra data did as well, but Terra data was primarily a, an ellipse of some back in the day,</p>
<p>86<br>00:06:13,000 –&gt; 00:06:16,000<br>one of the first ones, actually the first one.</p>
<p>87<br>00:06:16,000 –&gt; 00:06:19,000<br>So basic idea is this, you have your O2B databases, your operational workloads,</p>
<p>88<br>00:06:19,000 –&gt; 00:06:24,000<br>this is where you’re getting your data, and then somebody wants to run some query like this,</p>
<p>89<br>00:06:24,000 –&gt; 00:06:28,000<br>where they have a cube function, and the group by clause, and then all you’re just going to do,</p>
<p>90<br>00:06:28,000 –&gt; 00:06:32,000<br>again, just do a sequential scan on each node, populate the cube for this,</p>
<p>91<br>00:06:32,000 –&gt; 00:06:37,000<br>and then when this query shows up, if you define this as a view, you would do the query on the cube.</p>
<p>92<br>00:06:37,000 –&gt; 00:06:42,000<br>Right? Just again, think of like pre-computer aggregations, that’s all it really was.</p>
<p>93<br>00:06:42,000 –&gt; 00:06:47,000<br>So what really changed, and got us on the path towards what we’re at today,</p>
<p>94<br>00:06:47,000 –&gt; 00:06:54,000<br>was in the mid-early-ish 2000s, where people started building these specialized database systems</p>
<p>95<br>00:06:54,000 –&gt; 00:07:00,000<br>called data warehouses, that were specifically designed for analytical workloads.</p>
<p>96<br>00:07:00,000 –&gt; 00:07:05,000<br>So even though a lot of these start off as forks of Postgres, we can go through that a little bit,</p>
<p>97<br>00:07:05,000 –&gt; 00:07:12,000<br>but even though they were mostly derived from RhoStort systems, they ripped out a lot of the storage internals,</p>
<p>98<br>00:07:12,000 –&gt; 00:07:18,000<br>they were ripped out of the execution engine, and replaced it with something that was targeting column-oriented data.</p>
<p>99<br>00:07:19,000 –&gt; 00:07:24,000<br>So all of these, except for data-legro and Monet, are forks of Postgres.</p>
<p>100<br>00:07:24,000 –&gt; 00:07:29,000<br>Park Cell is what as Reshift is based on, so we’ll cover the Reshift paper later on,</p>
<p>101<br>00:07:29,000 –&gt; 00:07:35,000<br>basically Amazon bought a license with the source code, and then hacked it up a lot, and that became Reshift.</p>
<p>102<br>00:07:35,000 –&gt; 00:07:42,000<br>Monet DB was written from scratch out of CWI. DuckDB is originally derived from Monet DB.</p>
<p>103<br>00:07:42,000 –&gt; 00:07:47,000<br>There was a version of DuckDB before DuckDB called Monet DB Light, and then they threw all that all away,</p>
<p>104<br>00:07:47,000 –&gt; 00:07:54,000<br>and then they wrote it as DuckDB. Vertica was started by Sturmbreaker and others back at MIT in Brown.</p>
<p>105<br>00:07:54,000 –&gt; 00:08:01,000<br>That’s the fork of Postgres. Data-legro was a hacked up, was it’s mid-aware in front of Ingress.</p>
<p>106<br>00:08:01,000 –&gt; 00:08:05,000<br>Microsoft bought it for like, I think a couple hundred million, and then immediately threw away,</p>
<p>107<br>00:08:05,000 –&gt; 00:08:08,000<br>because apparently it was garbage. And then it teased it was the early one.</p>
<p>108<br>00:08:08,000 –&gt; 00:08:15,000<br>This was actually pretty cool. This was a version of Postgres that had an FPGA accelerator to do the accelerator sequential scans.</p>
<p>109<br>00:08:15,000 –&gt; 00:08:19,000<br>Greenplum still around today, pretty widely used, that’s a fork version of Postgres.</p>
<p>110<br>00:08:19,000 –&gt; 00:08:27,000<br>These were all these monolithic systems where they were designed now to run little workloads in a control,</p>
<p>111<br>00:08:27,000 –&gt; 00:08:34,000<br>the complete storage layer of the system, and they sort of used their own proprietary formats.</p>
<p>112<br>00:08:34,000 –&gt; 00:08:43,000<br>That’ll make more sense, especially in the next class, but basically, again, they were in charge of what the bits look like on disk for the pages they were storing for the data.</p>
<p>113<br>00:08:43,000 –&gt; 00:08:48,000<br>The other thing to point out is that all of these were all shared nothing systems.</p>
<p>114<br>00:08:48,000 –&gt; 00:08:56,000<br>Again, we’ll cover that in a second, but again, they were assuming that every compute node in your database cluster had disk, memory, and CPU,</p>
<p>115<br>00:08:56,000 –&gt; 00:09:02,000<br>and each node was responsible for sorting some portion of the entire database.</p>
<p>116<br>00:09:02,000 –&gt; 00:09:07,000<br>So the way you would use it like this is, again, you have your O2B databases, now you have your giant data warehouse,</p>
<p>117<br>00:09:07,000 –&gt; 00:09:13,000<br>and the idea is that you wouldn’t get all your operational databases, all the data from here, back into your single data warehouse,</p>
<p>118<br>00:09:13,000 –&gt; 00:09:19,000<br>because now you have a single view or complete view of all the data across all your databases.</p>
<p>119<br>00:09:19,000 –&gt; 00:09:25,000<br>And so the way you would do this is using tools called extractions from a loader, ETL tools,</p>
<p>120<br>00:09:25,000 –&gt; 00:09:30,000<br>and you just sort of get the change data capture, or periodically getting updates from the O2B databases,</p>
<p>121<br>00:09:30,000 –&gt; 00:09:40,000<br>doing some amount of changes to them to clean up the data, like entity resolution, like if it’s A, PaVLo, and AMP, you could figure out that they refer to the same person,</p>
<p>122<br>00:09:40,000 –&gt; 00:09:45,000<br>all that sort of happens here, and then you load this now into your data warehouse.</p>
<p>123<br>00:09:45,000 –&gt; 00:09:53,000<br>But again, for this, because the data warehouse wants to have complete control of everything that’s storing, you’ve got to set up the schema at a time,</p>
<p>124<br>00:09:53,000 –&gt; 00:09:59,000<br>you’ve got to provision the hardware at a time, everything has to be sort of set up before you start putting data into it.</p>
<p>125<br>00:09:59,000 –&gt; 00:10:05,000<br>And it was a shared nothing system, so if you want to scale the capacity of the system, you have to add more nodes, and now you’ve got to start moving data around.</p>
<p>126<br>00:10:05,000 –&gt; 00:10:10,000<br>And that’s going to be one of the limitations we’ll see throughout this semester.</p>
<p>127<br>00:10:10,000 –&gt; 00:10:16,000<br>So then we hit the 2010s, late 2000s, early 2010s was one of these things took off.</p>
<p>128<br>00:10:16,000 –&gt; 00:10:20,000<br>We entered this new era that we’re sort of in today of these shared disk engines.</p>
<p>129<br>00:10:21,000 –&gt; 00:10:32,000<br>And the idea here is that instead of having the database system manage its own storage layer, we’re going to offload that to some other piece of software, some other service.</p>
<p>130<br>00:10:32,000 –&gt; 00:10:36,000<br>And in the cloud setting, it’s going to be an object store like S3.</p>
<p>131<br>00:10:36,000 –&gt; 00:10:46,000<br>And the idea here is that because we know long going to be responsible for managing the storage of data, we can optimize the computer there as much as possible.</p>
<p>132<br>00:10:46,000 –&gt; 00:10:54,000<br>We’re still going to have proprietary data formats, meaning if you’re using the regular snowflake, it was sort of the first one in this space,</p>
<p>133<br>00:10:54,000 –&gt; 00:11:03,000<br>snowflake when you store data into it, it’s going to store it in the snowflake format, that only snowflake understands, but it’s going to store it in NS3.</p>
<p>134<br>00:11:03,000 –&gt; 00:11:06,000<br>So that was the first generation of these systems, they’re going to manage all the files themselves.</p>
<p>135<br>00:11:06,000 –&gt; 00:11:13,000<br>The newer generation, and what was in the paper that you guys read, that sort of fall under this branding label or moniker of Lakehouse systems,</p>
<p>136<br>00:11:14,000 –&gt; 00:11:26,000<br>the idea is that it looks just like a shared disk system before, but now instead of having always using a proprietary storage format and only allowing data to be added to the database by going through the database system,</p>
<p>137<br>00:11:26,000 –&gt; 00:11:36,000<br>you allow anybody to write bunch of files out on S3, tell the database system, the Lakehouse system, hey, here’s my files, here’s what’s in them, and then now you can run queries on top of that data.</p>
<p>138<br>00:11:36,000 –&gt; 00:11:37,000<br>Yes.</p>
<p>139<br>00:11:37,000 –&gt; 00:11:57,000<br>So, the question is, I’m showing much of these logos at the bottom here, why are there so many, what are they competing on?</p>
<p>140<br>00:11:57,000 –&gt; 00:12:06,000<br>There’s so many of them, because there’s so much money in databases, that’s why there’s a lot of this, there’s Linux, and then what else?</p>
<p>141<br>00:12:06,000 –&gt; 00:12:10,000<br>There’s a lot of databases.</p>
<p>142<br>00:12:10,000 –&gt; 00:12:23,000<br>So, the key difference is going to be in, I think, some of these will be hosted services that you can only get through a certain cloud provider, like an Amazon Redshift, you can only get Amazon Redshift on Amazon.</p>
<p>143<br>00:12:23,000 –&gt; 00:12:32,000<br>With BigQuery, I think they have called Omni something or other, you can now run BigQuery stuff on AWS and Azure and so forth.</p>
<p>144<br>00:12:32,000 –&gt; 00:12:37,000<br>But for whatever reason, people start building the AWS systems because they think they can do a better job than what already exists.</p>
<p>145<br>00:12:37,000 –&gt; 00:12:50,000<br>And oftentimes, some of these projects actually spin out of larger tech companies that decided, oh, we want to build this stuff in-house, and then turns out to be useful, and then they converted to an open source project and get people to use it outside of it.</p>
<p>146<br>00:12:50,000 –&gt; 00:13:00,000<br>So, Presto started that Facebook, Pino started that at LinkedIn, Trino’s and Forkopesto, Drew, I forget, this came out of something as well.</p>
<p>147<br>00:13:00,000 –&gt; 00:13:04,000<br>But for whatever reason, people start building these various systems.</p>
<p>148<br>00:13:04,000 –&gt; 00:13:14,000<br>And I would say, at a high level, for the most part, as we’ll see throughout the semester, the high level, they’re all going to be basically the same.</p>
<p>149<br>00:13:14,000 –&gt; 00:13:22,000<br>The real difference is going to be the things that actually really matter in some cases is going to be the top layer, the front end, like what the user experience looks like, how good the query optimizer is.</p>
<p>150<br>00:13:22,000 –&gt; 00:13:24,000<br>In my opinion, that’s the part that really matters.</p>
<p>151<br>00:13:24,000 –&gt; 00:13:31,000<br>All the stuff that we’re talking about throughout the semester, it becomes almost commoditized or comes table stakes, everyone has it.</p>
<p>152<br>00:13:31,000 –&gt; 00:13:38,000<br>We still want to learn how to build it and why they do certain things and why they perform the way they perform.</p>
<p>153<br>00:13:38,000 –&gt; 00:13:42,000<br>Like that part is still super important, especially if you want to work on the internals of these systems.</p>
<p>154<br>00:13:42,000 –&gt; 00:13:47,000<br>But to the average user, it’s really the top part that really matters.</p>
<p>155<br>00:13:47,000 –&gt; 00:13:50,000<br>So, in other ways, are there too many databases?</p>
<p>156<br>00:13:50,000 –&gt; 00:13:52,000<br>Maybe.</p>
<p>157<br>00:13:52,000 –&gt; 00:14:01,000<br>But, like I said, there’s still a lot of money in the marketplace for this kind of stuff and people, these things don’t die.</p>
<p>158<br>00:14:01,000 –&gt; 00:14:05,000<br>But, in Pala, I didn’t really take off as much as Snuff Lake did.</p>
<p>159<br>00:14:05,000 –&gt; 00:14:07,000<br>They’re still maintaining in Pala, still people using Pala.</p>
<p>160<br>00:14:07,000 –&gt; 00:14:10,000<br>Would I recommend anybody starting off using in Pala? No.</p>
<p>161<br>00:14:10,000 –&gt; 00:14:14,000<br>But it’s still there.</p>
<p>162<br>00:14:14,000 –&gt; 00:14:15,000<br>Yes.</p>
<p>163<br>00:14:15,000 –&gt; 00:14:18,000<br>Are these the same or different from data lakes?</p>
<p>164<br>00:14:18,000 –&gt; 00:14:22,000<br>What I read about data lakes, they have a total of Pala.</p>
<p>165<br>00:14:22,000 –&gt; 00:14:25,000<br>Yes, the question is, what I’m describing here, the same as a data lake.</p>
<p>166<br>00:14:25,000 –&gt; 00:14:31,000<br>The term data lake basically meant is that, okay, here’s S3, anybody can store data in there.</p>
<p>167<br>00:14:31,000 –&gt; 00:14:35,000<br>But then the lake house architecture, you know, we’ll see in a second.</p>
<p>168<br>00:14:35,000 –&gt; 00:14:40,000<br>It does have the ability to ingest data through this lake house and keep track of things.</p>
<p>169<br>00:14:40,000 –&gt; 00:14:44,000<br>But also provides additional schema control and metadata stuff as well.</p>
<p>170<br>00:14:44,000 –&gt; 00:14:51,000<br>So, the data lake, the idea was like, okay, W3 files in S3 and then whoever did that is also responsible for telling the catalog.</p>
<p>171<br>00:14:52,000 –&gt; 00:14:53,000<br>Here’s my files.</p>
<p>172<br>00:14:53,000 –&gt; 00:14:59,000<br>With the lake house architecture, it’s supposed to be like a unified front and interface to develop and say, okay, here’s my new data.</p>
<p>173<br>00:14:59,000 –&gt; 00:15:02,000<br>And then the lake house can put it where it wants it.</p>
<p>174<br>00:15:02,000 –&gt; 00:15:05,000<br>You can tell it here’s where it is and sort of keep track of these things.</p>
<p>175<br>00:15:05,000 –&gt; 00:15:09,000<br>So it’s not to say you couldn’t build, ignoring the increment updates.</p>
<p>176<br>00:15:09,000 –&gt; 00:15:10,000<br>We’ll talk about in a second.</p>
<p>177<br>00:15:10,000 –&gt; 00:15:15,000<br>Now that you couldn’t do what a lake house does on a data lake, they’re essentially the same thing.</p>
<p>178<br>00:15:15,000 –&gt; 00:15:19,000<br>There’s more services to help you keep track of what’s going on.</p>
<p>179<br>00:15:19,000 –&gt; 00:15:23,000<br>Or, wondering, when we said shared disk, you could be in data lake or something.</p>
<p>180<br>00:15:23,000 –&gt; 00:15:27,000<br>So, the data lake would just be like, his question is, when I say shared disk, I mean data lake.</p>
<p>181<br>00:15:27,000 –&gt; 00:15:30,000<br>I mean, shared disk would be the distinction between shared nothing.</p>
<p>182<br>00:15:30,000 –&gt; 00:15:35,000<br>Right? That you have the separate computing storage and you rely on the object store to store your data.</p>
<p>183<br>00:15:35,000 –&gt; 00:15:42,000<br>So, you could do that in some systems before like, data bricks is the lake house stuff, right?</p>
<p>184<br>00:15:42,000 –&gt; 00:15:45,000<br>But the idea is like, there’s a much more manual stuff you have to do.</p>
<p>185<br>00:15:45,000 –&gt; 00:15:51,000<br>Like, if you just dump up files on s3, but nobody knows about them, then like, you can’t run queries on them.</p>
<p>186<br>00:15:51,000 –&gt; 00:15:52,000<br>But so, let me do it.</p>
<p>187<br>00:15:52,000 –&gt; 00:15:58,000<br>You would have to update the files on s3 and then tell some catalog service, here’s what my data is, so they can then run queries on them.</p>
<p>188<br>00:15:58,000 –&gt; 00:16:00,000<br>Like, to be like a manual process.</p>
<p>189<br>00:16:00,000 –&gt; 00:16:05,000<br>The lake house is trying to do all this sort of automatically for you.</p>
<p>190<br>00:16:05,000 –&gt; 00:16:08,000<br>It’s a marketing term, something that’s right.</p>
<p>191<br>00:16:08,000 –&gt; 00:16:11,000<br>The data lake just means, just like, okay, here’s s3.</p>
<p>192<br>00:16:11,000 –&gt; 00:16:13,000<br>Put my files there.</p>
<p>193<br>00:16:13,000 –&gt; 00:16:22,000<br>Right? But the important thing about this is though, like, what this allows you to do because it’s going to be a day lake or an object store,</p>
<p>194<br>00:16:22,000 –&gt; 00:16:30,000<br>like, I don’t have to go get approval from the DBA to say, I want to store this data, I think I spend time setting up the schema and figuring out what’s going to be in there and provision hardware.</p>
<p>195<br>00:16:30,000 –&gt; 00:16:32,000<br>You just start uploading files.</p>
<p>196<br>00:16:32,000 –&gt; 00:16:40,000<br>And then, for better or worse, like, that makes it easier to put data in there, but then now someone’s got to figure out what’s actually in there.</p>
<p>197<br>00:16:40,000 –&gt; 00:16:48,000<br>Right? So you’re sort of pushing the burden of figuring out how to interpret the data in the contents later down the pipeline.</p>
<p>198<br>00:16:48,000 –&gt; 00:16:55,000<br>In some cases, that’s a good idea. Sometimes it’s a bad idea.</p>
<p>199<br>00:16:55,000 –&gt; 00:16:57,000<br>That’s something that’s something like a philosophical discussion.</p>
<p>200<br>00:16:57,000 –&gt; 00:17:02,000<br>But the key thing here is that we’re separating compute and storage by using a shared disk architecture.</p>
<p>201<br>00:17:02,000 –&gt; 00:17:05,000<br>So we go back to our third diagram before.</p>
<p>202<br>00:17:06,000 –&gt; 00:17:17,000<br>Now, we’re all TB databases. They’re going to send all their data to an object store and maybe there’s an ETL thing or some kind of middleware here that’s going to do some transformation before it puts it in there.</p>
<p>203<br>00:17:17,000 –&gt; 00:17:25,000<br>And then we would maybe tell the catalog, here’s the files that I put in there and here’s their contents or here’s what, here’s the schema that’s in there.</p>
<p>204<br>00:17:25,000 –&gt; 00:17:30,000<br>And then now, if I want to run queries, the query engine on the side doesn’t, is not responsible for the storage anymore.</p>
<p>205<br>00:17:30,000 –&gt; 00:17:35,000<br>So it has to go to the catalog and say, what data do I actually have? Where are the files located in S3?</p>
<p>206<br>00:17:35,000 –&gt; 00:17:40,000<br>And then once I have that information, then I can run my queries on the object store.</p>
<p>207<br>00:17:40,000 –&gt; 00:17:47,000<br>So in this semester, this box here is what we care about. This is the thing that we’re going to, we’re actually going to design.</p>
<p>208<br>00:17:47,000 –&gt; 00:17:54,000<br>This is what we call an OLAP system, whether it’s a lake house system that comes with additional stuff that data bricks wants to sell you.</p>
<p>209<br>00:17:54,000 –&gt; 00:17:59,000<br>But this is the thing that we’re going to describe conceptually and how to build.</p>
<p>210<br>00:17:59,000 –&gt; 00:18:06,000<br>And this is the way people have been really building these systems for about 10 years, about 15 years maybe now.</p>
<p>211<br>00:18:06,000 –&gt; 00:18:11,000<br>That started off with like Dremel at Google and then Snowflake was the one that really commercialized it.</p>
<p>212<br>00:18:11,000 –&gt; 00:18:21,000<br>Like this is the sort of, I don’t call it a classic architecture. And I realized that you guys are in the 20s and I’m saying this 15 years ago, and the script of data bases, that’s actually not a lot of time.</p>
<p>213<br>00:18:21,000 –&gt; 00:18:22,000<br>Yes.</p>
<p>214<br>00:18:22,000 –&gt; 00:18:25,000<br>We are just wondering about the transformation of the OLAP system.</p>
<p>215<br>00:18:25,000 –&gt; 00:18:29,000<br>Data bases to like the OLAP databases, what is a time stand for computer stuff?</p>
<p>216<br>00:18:29,000 –&gt; 00:18:34,000<br>The answer is, we were going to talk about like this, this, it’s called changing the capture.</p>
<p>217<br>00:18:34,000 –&gt; 00:18:41,000<br>Like how do we get the updates from this into this? We’re not going to talk about that specifically, like what the process of do that.</p>
<p>218<br>00:18:41,000 –&gt; 00:18:45,000<br>I can point you to some previous lectures that we’ve had guest speakers talk about this.</p>
<p>219<br>00:18:45,000 –&gt; 00:18:49,000<br>We are going to care about like what the data is going to look like when it goes in there.</p>
<p>220<br>00:18:49,000 –&gt; 00:18:52,000<br>And that’s the next class, like Parkane or Files.</p>
<p>221<br>00:18:52,000 –&gt; 00:18:53,000<br>Right?</p>
<p>222<br>00:18:53,000 –&gt; 00:18:54,000<br>Yes.</p>
<p>223<br>00:18:54,000 –&gt; 00:18:58,000<br>So, it’s a part of the question.</p>
<p>224<br>00:18:58,000 –&gt; 00:19:02,000<br>The question is, what does that change the value of the whole architecture?</p>
<p>225<br>00:19:02,000 –&gt; 00:19:09,000<br>Like the physical needs basically then, and that it is a data that can complete the office code?</p>
<p>226<br>00:19:09,000 –&gt; 00:19:10,000<br>Yes.</p>
<p>227<br>00:19:10,000 –&gt; 00:19:11,000<br>Next class.</p>
<p>228<br>00:19:11,000 –&gt; 00:19:21,000<br>So, or save it is like, oh, in the lake house paper from the database guys, they talk about, oh, it’s a big problem with these data lake systems is that you get stale data.</p>
<p>229<br>00:19:21,000 –&gt; 00:19:22,000<br>Right?</p>
<p>230<br>00:19:22,000 –&gt; 00:19:25,000<br>Because again, we’re getting continuous updates from the operational side of things.</p>
<p>231<br>00:19:25,000 –&gt; 00:19:32,000<br>How do we integrate that into our database so that we’re always trying to look at the freshest data?</p>
<p>232<br>00:19:32,000 –&gt; 00:19:39,000<br>And so, what these data lake house systems provide also is the ability to do transactional updates,</p>
<p>233<br>00:19:39,000 –&gt; 00:19:44,000<br>you know, creation, deletion, updates, insertions into those database.</p>
<p>234<br>00:19:44,000 –&gt; 00:19:48,000<br>And the way you basically do this is that it’s the fracture mirror stuff we talked about last semester.</p>
<p>235<br>00:19:48,000 –&gt; 00:19:50,000<br>You’re just doing a log append to a file.</p>
<p>236<br>00:19:50,000 –&gt; 00:19:52,000<br>Here’s a little bit of latest changes.</p>
<p>237<br>00:19:52,000 –&gt; 00:19:56,000<br>And then there’s a background job that periodically takes that code lessons that combines that removes out stale data.</p>
<p>238<br>00:19:56,000 –&gt; 00:20:05,000<br>And then stores it now into Parkane file example for, in the case of data lake, or in the Delta Lake thing from Databricks.</p>
<p>239<br>00:20:05,000 –&gt; 00:20:10,000<br>They’ll take that thing Parkane file and then store that now into your object store.</p>
<p>240<br>00:20:10,000 –&gt; 00:20:13,000<br>And then update the catalogs, say here’s the latest version of it.</p>
<p>241<br>00:20:13,000 –&gt; 00:20:17,000<br>They also can keep track of, if you make schema changes, they keep track of those things for you.</p>
<p>242<br>00:20:17,000 –&gt; 00:20:27,000<br>Right? It’s providing more, as I said, more infrastructure to make sure that the object store is, can keep track of what’s actually in it.</p>
<p>243<br>00:20:27,000 –&gt; 00:20:30,000<br>So we’re not going to talk about any of this this semester.</p>
<p>244<br>00:20:31,000 –&gt; 00:20:33,000<br>Delta Lake is one example system.</p>
<p>245<br>00:20:33,000 –&gt; 00:20:36,000<br>I don’t know whether the paper mentions hoody and iceberg.</p>
<p>246<br>00:20:36,000 –&gt; 00:20:37,000<br>Hoody came out of Uber.</p>
<p>247<br>00:20:37,000 –&gt; 00:20:40,000<br>Iceberg came out of Netflix.</p>
<p>248<br>00:20:40,000 –&gt; 00:20:44,000<br>Snowflake has their own sort of thing, I think they call hybrid tables.</p>
<p>249<br>00:20:44,000 –&gt; 00:20:45,000<br>They can implement updates.</p>
<p>250<br>00:20:45,000 –&gt; 00:20:47,000<br>And they support iceberg as well.</p>
<p>251<br>00:20:47,000 –&gt; 00:20:51,000<br>This logo without a name is actually Google Napa.</p>
<p>252<br>00:20:51,000 –&gt; 00:20:53,000<br>They have a paper on this.</p>
<p>253<br>00:20:53,000 –&gt; 00:20:55,000<br>Google doesn’t really put names next to logos.</p>
<p>254<br>00:20:55,000 –&gt; 00:20:56,000<br>Amazon too.</p>
<p>255<br>00:20:56,000 –&gt; 00:20:58,000<br>How’s that making it know what this is?</p>
<p>256<br>00:20:59,000 –&gt; 00:21:02,000<br>So in their defense, that’s actually the internal system.</p>
<p>257<br>00:21:02,000 –&gt; 00:21:04,000<br>They’re publicly talking about it now.</p>
<p>258<br>00:21:04,000 –&gt; 00:21:08,000<br>We’re not going to talk about these things this semester, because we really want to focus on how do we run,</p>
<p>259<br>00:21:08,000 –&gt; 00:21:10,000<br>oh, that query is as fast as possible.</p>
<p>260<br>00:21:10,000 –&gt; 00:21:16,000<br>And once we have that, then we can go beyond that and build this Delta update stuff.</p>
<p>261<br>00:21:19,000 –&gt; 00:21:20,000<br>Okay.</p>
<p>262<br>00:21:21,000 –&gt; 00:21:24,000<br>So the paper you guys read, they make a bunch of different observations.</p>
<p>263<br>00:21:25,000 –&gt; 00:21:27,000<br>She mentioned the one about the still data.</p>
<p>264<br>00:21:27,000 –&gt; 00:21:30,000<br>But I want to point out the sort of the three things that when we keep in the back of our minds,</p>
<p>265<br>00:21:30,000 –&gt; 00:21:38,000<br>as we go throughout the semester, to understand and guide us on how we make decisions on building a system.</p>
<p>266<br>00:21:38,000 –&gt; 00:21:42,000<br>And again, even though beyond the semester, you may not go off and build data system internals,</p>
<p>267<br>00:21:42,000 –&gt; 00:21:46,000<br>but these are the things you should think about when we’re choosing maybe an OLAP system</p>
<p>268<br>00:21:46,000 –&gt; 00:21:51,000<br>in whatever your next project is at a startup or wherever you go next after you graduate here.</p>
<p>269<br>00:21:52,000 –&gt; 00:21:57,000<br>So the first thing to point out is that in a modern setting, in modern organizations,</p>
<p>270<br>00:21:57,000 –&gt; 00:21:59,000<br>people want to execute more than their SQL queries.</p>
<p>271<br>00:21:59,000 –&gt; 00:22:03,000<br>Now, I realize as someone who’s like a SQL maximalist where I think everything should be SQL,</p>
<p>272<br>00:22:03,000 –&gt; 00:22:05,000<br>this seems like heresy to me, right?</p>
<p>273<br>00:22:05,000 –&gt; 00:22:06,000<br>But I’m not naive.</p>
<p>274<br>00:22:06,000 –&gt; 00:22:09,000<br>I know that people, you know, want to run PyTorch, Intensive Flow,</p>
<p>275<br>00:22:09,000 –&gt; 00:22:16,000<br>and all these other ML workloads that aren’t, you know, that you can’t easily express in SQL.</p>
<p>276<br>00:22:16,000 –&gt; 00:22:21,000<br>Now, there’s projects like PostgreSQL that gives you UDFs, and they call us into PyTorch,</p>
<p>277<br>00:22:21,000 –&gt; 00:22:23,000<br>but most of you aren’t writing that.</p>
<p>278<br>00:22:23,000 –&gt; 00:22:26,000<br>Most of you are, you know, her data scientists that are writing stuff in notebooks.</p>
<p>279<br>00:22:26,000 –&gt; 00:22:31,000<br>And so the access patterns for ML workloads, for example,</p>
<p>280<br>00:22:31,000 –&gt; 00:22:36,000<br>are going to look a lot different than OLAP queries or SQL queries.</p>
<p>281<br>00:22:36,000 –&gt; 00:22:42,000<br>And we’ll see this later in the semester when we talk about the networking protocols for database systems,</p>
<p>282<br>00:22:42,000 –&gt; 00:22:48,000<br>that sometimes you want to do a bulk export of data without having a, you know,</p>
<p>283<br>00:22:48,000 –&gt; 00:22:52,000<br>it was retreating exactly as it exists in memory from the data system,</p>
<p>284<br>00:22:52,000 –&gt; 00:22:58,000<br>rather than having converted to a result format that you can then read as if you’re going through JDBC or ODBC.</p>
<p>285<br>00:22:58,000 –&gt; 00:23:01,000<br>Right? And so maybe you want to get things out of the patchy arrow format,</p>
<p>286<br>00:23:01,000 –&gt; 00:23:04,000<br>which will cover throughout the semester.</p>
<p>287<br>00:23:04,000 –&gt; 00:23:06,000<br>And that’s relevant to the projects.</p>
<p>288<br>00:23:06,000 –&gt; 00:23:11,000<br>So we’ll design our system, the most of the parts of the system,</p>
<p>289<br>00:23:11,000 –&gt; 00:23:18,000<br>at the sort of the planner level down, ML workloads are going to look a lot like Python workloads,</p>
<p>290<br>00:23:18,000 –&gt; 00:23:20,000<br>a lot like SQL workloads. All of that’s going to be the same.</p>
<p>291<br>00:23:20,000 –&gt; 00:23:24,000<br>It’s the front end part that I was saying before that we will need to expose different APIs</p>
<p>292<br>00:23:24,000 –&gt; 00:23:28,000<br>for how we want to get data and run queries.</p>
<p>293<br>00:23:28,000 –&gt; 00:23:33,000<br>The other point thing is that, the, as I said already, that like because of these shared disk architectures,</p>
<p>294<br>00:23:33,000 –&gt; 00:23:40,000<br>it’s no longer having the data system, having rigid control of exactly what data is going into the database</p>
<p>295<br>00:23:40,000 –&gt; 00:23:42,000<br>and how people can get data out of it.</p>
<p>296<br>00:23:42,000 –&gt; 00:23:48,000<br>Because now it’s just files in S3 and ignoring any governance or any security permissions of how people can get to those files.</p>
<p>297<br>00:23:48,000 –&gt; 00:23:54,000<br>If it’s just files in S3, then we don’t always have to go through the front end of the database system</p>
<p>298<br>00:23:54,000 –&gt; 00:23:57,000<br>to do anything with our data. Right?</p>
<p>299<br>00:23:57,000 –&gt; 00:24:03,000<br>That doesn’t mean we still don’t want to track schemas and versions of those schemas and what files actually exist.</p>
<p>300<br>00:24:03,000 –&gt; 00:24:07,000<br>And the catalog is a pivotal thing that makes us all work.</p>
<p>301<br>00:24:07,000 –&gt; 00:24:15,000<br>But because now anybody can put things in S3 in theory, you don’t have to go through that full bureaucracy that I mentioned before.</p>
<p>302<br>00:24:15,000 –&gt; 00:24:19,000<br>And the last one is that as they point out, and just think of your own behavior on the internet,</p>
<p>303<br>00:24:19,000 –&gt; 00:24:22,000<br>most data is unstructured or semi-structured.</p>
<p>304<br>00:24:22,000 –&gt; 00:24:25,000<br>So unstructured would be like an image or a video file.</p>
<p>305<br>00:24:25,000 –&gt; 00:24:31,000<br>I think most of the traffic on the internet is from YouTube or video files.</p>
<p>306<br>00:24:31,000 –&gt; 00:24:35,000<br>And then a lot of it is also unstructured or semi-structured.</p>
<p>307<br>00:24:35,000 –&gt; 00:24:41,000<br>So this would be something like JSON files or a combination of structured data like a tuple,</p>
<p>308<br>00:24:41,000 –&gt; 00:24:47,000<br>but then there’s some JSON portion or maybe raw text from a log file.</p>
<p>309<br>00:24:47,000 –&gt; 00:24:50,000<br>A lot of the data is going to come in this format.</p>
<p>310<br>00:24:50,000 –&gt; 00:24:54,000<br>For unstructured data, we’re not going to talk about anything about this this semester.</p>
<p>311<br>00:24:54,000 –&gt; 00:25:01,000<br>Like that’s taking like a transformer or some ML framework that then extracts information about what’s in an image or an urn of video file.</p>
<p>312<br>00:25:01,000 –&gt; 00:25:05,000<br>But with anything it spits out after you do that transformation is going to be structured.</p>
<p>313<br>00:25:05,000 –&gt; 00:25:10,000<br>So for SQL purposes, it’s not much that we can do for this.</p>
<p>314<br>00:25:10,000 –&gt; 00:25:14,000<br>For semi-structured, this is going to be a big issue that we have to care about.</p>
<p>315<br>00:25:14,000 –&gt; 00:25:17,000<br>Because people are going to have a dumb up bunch of JSON files in S3.</p>
<p>316<br>00:25:17,000 –&gt; 00:25:23,000<br>Even if it’s in a structured file format like a parquet or org file, which we read about next class,</p>
<p>317<br>00:25:23,000 –&gt; 00:25:26,000<br>then we still have to be able to make sense of it.</p>
<p>318<br>00:25:26,000 –&gt; 00:25:33,000<br>And this is a good example where we’ll see some systems, the different systems will do different tricks to make it work efficiently for this.</p>
<p>319<br>00:25:33,000 –&gt; 00:25:37,000<br>Snowflake, I think they assume that the data is always going to be there.</p>
<p>320<br>00:25:37,000 –&gt; 00:25:39,000<br>They generate columns for this.</p>
<p>321<br>00:25:39,000 –&gt; 00:25:41,000<br>Databricks, I think they do parsing on the fly.</p>
<p>322<br>00:25:41,000 –&gt; 00:25:43,000<br>There’s a bunch of different ways to handle that.</p>
<p>323<br>00:25:43,000 –&gt; 00:25:46,000<br>And we’ll see how we do that throughout the semester.</p>
<p>324<br>00:25:46,000 –&gt; 00:25:51,000<br>So again, we want to design our system keeping these things in the back of our mind.</p>
<p>325<br>00:25:51,000 –&gt; 00:25:55,000<br>And we’ll see throughout the semester how we do each of these.</p>
<p>326<br>00:25:55,000 –&gt; 00:26:05,000<br>The other interesting trend that has come out in the last decade is that we’ve gotten away from these monolithic database systems,</p>
<p>327<br>00:26:05,000 –&gt; 00:26:11,000<br>where now people are building services or individual components that are separate from the full system,</p>
<p>328<br>00:26:12,000 –&gt; 00:26:15,000<br>and it’s basically how they have sort of laid out the project this semester,</p>
<p>329<br>00:26:15,000 –&gt; 00:26:19,000<br>that in theory, these different services can be developed independently,</p>
<p>330<br>00:26:19,000 –&gt; 00:26:27,000<br>along with they expose and maintain an API that the other services can understand and use and is stable,</p>
<p>331<br>00:26:27,000 –&gt; 00:26:32,000<br>then you can start swapping these things in and out, or not have to build the entire system from scratch,</p>
<p>332<br>00:26:32,000 –&gt; 00:26:37,000<br>you could use some all-the-shelf tools to build a full-flyers-data-based system.</p>
<p>333<br>00:26:38,000 –&gt; 00:26:42,000<br>Again, everything in Postgres or everything in DuckDB is SQLite,</p>
<p>334<br>00:26:42,000 –&gt; 00:26:46,000<br>is written by those developers.</p>
<p>335<br>00:26:46,000 –&gt; 00:26:51,000<br>Ignoring third-party libraries for SSL and things like that, that’s obviously not what I’m talking about.</p>
<p>336<br>00:26:51,000 –&gt; 00:26:57,000<br>But the query optimizers of the catalog, the parser, all of that is built by the system developers.</p>
<p>337<br>00:26:57,000 –&gt; 00:27:02,000<br>But now what we can do instead is, in theory, use some, again, all-the-shelf tools,</p>
<p>338<br>00:27:02,000 –&gt; 00:27:07,000<br>and cobble them together and still make a full-fledged, full-feature database system.</p>
<p>339<br>00:27:07,000 –&gt; 00:27:11,000<br>The challenge is going to be, though, obviously, you know,</p>
<p>340<br>00:27:11,000 –&gt; 00:27:14,000<br>basic software engineering principle, the more abstraction layers you put in place,</p>
<p>341<br>00:27:14,000 –&gt; 00:27:17,000<br>the more inefficient the software will become.</p>
<p>342<br>00:27:17,000 –&gt; 00:27:24,000<br>Right? Just because there’s intimate knowledge about what the system wants to do at any given level,</p>
<p>343<br>00:27:24,000 –&gt; 00:27:30,000<br>and if you don’t expose that information up and down the layers of the software stack,</p>
<p>344<br>00:27:30,000 –&gt; 00:27:33,000<br>you end up doing the lowest common denominator.</p>
<p>345<br>00:27:33,000 –&gt; 00:27:34,000<br>Yes?</p>
<p>346<br>00:27:34,000 –&gt; 00:27:39,000<br>Do we know how costly it is to add these separation layers between different formats?</p>
<p>347<br>00:27:39,000 –&gt; 00:27:45,000<br>This question is, do we know how costly it is to start using these different services</p>
<p>348<br>00:27:45,000 –&gt; 00:27:47,000<br>versus having everything written scratch?</p>
<p>349<br>00:27:47,000 –&gt; 00:27:50,000<br>No. I mean, hard to study, though, right?</p>
<p>350<br>00:27:50,000 –&gt; 00:27:58,000<br>Like, hey, have two teams built a multi-million dollar project just to see whether one’s better at the end.</p>
<p>351<br>00:27:58,000 –&gt; 00:28:02,000<br>I mean, some parts are super hard, too.</p>
<p>352<br>00:28:02,000 –&gt; 00:28:04,000<br>Like, the query optimizer is the hardest part of the data system.</p>
<p>353<br>00:28:04,000 –&gt; 00:28:07,000<br>Most people can’t build that, and they do end up building it.</p>
<p>354<br>00:28:07,000 –&gt; 00:28:12,000<br>The first version is usually a bunch of heuristics, if-and-else statements, is garbage.</p>
<p>355<br>00:28:12,000 –&gt; 00:28:17,000<br>And so, in the case of query optimizer, I know, again, we have a project going in this class,</p>
<p>356<br>00:28:17,000 –&gt; 00:28:21,000<br>there’s been two other attempts to build standalone services in query optimizers.</p>
<p>357<br>00:28:21,000 –&gt; 00:28:26,000<br>There’s CalSight from LucidDB, and that’s probably the most common one,</p>
<p>358<br>00:28:26,000 –&gt; 00:28:30,000<br>and there’s Orca from the Green Plum, the Mware people.</p>
<p>359<br>00:28:30,000 –&gt; 00:28:37,000<br>But, like, there is a paper about the effort it took to get Orca to work in my SQL,</p>
<p>360<br>00:28:37,000 –&gt; 00:28:39,000<br>apparently, it was a huge pain.</p>
<p>361<br>00:28:39,000 –&gt; 00:28:42,000<br>Because there’s assumptions about these things, I mean, this is my last comment here,</p>
<p>362<br>00:28:42,000 –&gt; 00:28:46,000<br>there’s a bunch of challenges in calling these things together.</p>
<p>363<br>00:28:46,000 –&gt; 00:28:50,000<br>It’s not just like, you know, here’s the HTTP protocol, and everyone,</p>
<p>364<br>00:28:50,000 –&gt; 00:28:53,000<br>every web server speaks it, and it’s easy to combine these things together.</p>
<p>365<br>00:28:53,000 –&gt; 00:28:57,000<br>Did you ever use, like, at a REST API from any service, they want to data this way,</p>
<p>366<br>00:28:57,000 –&gt; 00:29:02,000<br>but there’s another one that’s another way that becomes a train wreck real quickly.</p>
<p>367<br>00:29:02,000 –&gt; 00:29:09,000<br>And so, making these things actually talk to each other, and actually making it fast, is non-trivial.</p>
<p>368<br>00:29:09,000 –&gt; 00:29:13,000<br>So, we’ll see how it goes for the project semester.</p>
<p>369<br>00:29:13,000 –&gt; 00:29:17,000<br>Another important thing that we’re going to cover, too, is what the intermediate representation,</p>
<p>370<br>00:29:17,000 –&gt; 00:29:23,000<br>the IR looks like for the various parts of these systems that are talking to each other.</p>
<p>371<br>00:29:23,000 –&gt; 00:29:30,000<br>And so, I’ll show in the slide what I mean, but, like, I’ve already mentioned this in the project right up.</p>
<p>372<br>00:29:30,000 –&gt; 00:29:36,000<br>They’re like, okay, there’s the query optimizer, and it’s going to generate a query that hands off to the scheduler.</p>
<p>373<br>00:29:36,000 –&gt; 00:29:38,000<br>Okay, well, what is actually handing?</p>
<p>374<br>00:29:38,000 –&gt; 00:29:43,000<br>The scheduler needs to know what’s actually inside the queries in order to make sense of what goes where.</p>
<p>375<br>00:29:43,000 –&gt; 00:29:46,000<br>So, how do you actually represent that?</p>
<p>376<br>00:29:46,000 –&gt; 00:29:49,000<br>And then now, how do they actually represent data?</p>
<p>377<br>00:29:49,000 –&gt; 00:29:52,000<br>What are the data types across these things need to be synchronized?</p>
<p>378<br>00:29:52,000 –&gt; 00:29:56,000<br>But again, if I’m using off-the-shelf components for my different projects and different teams,</p>
<p>379<br>00:29:56,000 –&gt; 00:30:00,000<br>they might have 32-bit ins one way, and then all int might be 64-bit ins another one.</p>
<p>380<br>00:30:00,000 –&gt; 00:30:02,000<br>Fixpoint Desmos is another challenge.</p>
<p>381<br>00:30:02,000 –&gt; 00:30:08,000<br>How they actually store the data itself can become jumbled up and difficult.</p>
<p>382<br>00:30:08,000 –&gt; 00:30:14,000<br>We’ll talk about file formats like Nesclass, and again, we’ll talk about execution engines and execution,</p>
<p>383<br>00:30:14,000 –&gt; 00:30:18,000<br>kind of a little execution fabric, so think of like, you know, a patchy ray, like that kind of stuff.</p>
<p>384<br>00:30:18,000 –&gt; 00:30:21,000<br>We’ll talk about that in the semester as well.</p>
<p>385<br>00:30:21,000 –&gt; 00:30:26,000<br>But there’s a bunch of these stuff, and people have talked about, hey, there’s all these existing things,</p>
<p>386<br>00:30:26,000 –&gt; 00:30:30,000<br>because I just copied together a Java database using all this Java stuff.</p>
<p>387<br>00:30:30,000 –&gt; 00:30:32,000<br>None of them have really taken off.</p>
<p>388<br>00:30:32,000 –&gt; 00:30:36,000<br>I think they’ve only been, like, toy exercises.</p>
<p>389<br>00:30:36,000 –&gt; 00:30:39,000<br>And this paper here that I’m referring to is optional.</p>
<p>390<br>00:30:39,000 –&gt; 00:30:46,000<br>This is from the Facebook guys, the Voltron people, basically, they’re arguing that this is how people should be building</p>
<p>391<br>00:30:46,000 –&gt; 00:30:51,000<br>gave you systems today, they’ve had these standalone components that interrupt.</p>
<p>392<br>00:30:51,000 –&gt; 00:30:57,000<br>So here’s a high level overview of what the internal is one of these OLAP systems look like,</p>
<p>393<br>00:30:57,000 –&gt; 00:31:01,000<br>given the context I just described, and essentially it’s going to mirror how we’re envisioning the projects</p>
<p>394<br>00:31:01,000 –&gt; 00:31:03,000<br>are going to work out the semester.</p>
<p>395<br>00:31:03,000 –&gt; 00:31:10,000<br>So at the very top, you have some user, they’re going to send a query, assuming it’s SQL to a front end part of the system.</p>
<p>396<br>00:31:10,000 –&gt; 00:31:16,000<br>And this will have like a language parker, a SQL parser that’s going to convert the SQL query into a bunch of tokens,</p>
<p>397<br>00:31:16,000 –&gt; 00:31:18,000<br>especially by its form.</p>
<p>398<br>00:31:18,000 –&gt; 00:31:24,000<br>And then now we’re going to send this, this, this, an intermediate representation of the SQL query to some planner.</p>
<p>399<br>00:31:24,000 –&gt; 00:31:27,000<br>And the planner’s going to have a bunch of different parts.</p>
<p>400<br>00:31:27,000 –&gt; 00:31:30,000<br>I have the binder, that’s going to respond to what we’re figuring out, like, you know, refers to it.</p>
<p>401<br>00:31:30,000 –&gt; 00:31:34,000<br>So there’s a token refers to a table name, did that table exist in my catalog?</p>
<p>402<br>00:31:34,000 –&gt; 00:31:40,000<br>Can I do some, some, some rewriting of the query to put it into a better canonical form?</p>
<p>403<br>00:31:40,000 –&gt; 00:31:48,000<br>And then I’ll have an optimizer that could do a cost-based search using cost models that are derived from the data itself to help figure out what’s the most optimal plan.</p>
<p>404<br>00:31:48,000 –&gt; 00:31:51,000<br>And so for this part of the planner, we got to call it to the catalog.</p>
<p>405<br>00:31:51,000 –&gt; 00:31:56,000<br>Right? Because we got to say, okay, again, I have this token, it’s table-foo, is this really a table?</p>
<p>406<br>00:31:56,000 –&gt; 00:31:59,000<br>What columns does it have? What’s the data type? Where’s this data actually being stored?</p>
<p>407<br>00:31:59,000 –&gt; 00:32:02,000<br>Do I have any statistics about that data?</p>
<p>408<br>00:32:02,000 –&gt; 00:32:05,000<br>That I can then feed into my cost models?</p>
<p>409<br>00:32:05,000 –&gt; 00:32:12,000<br>Then now I have, I now have a physical plan that I can actually execute in my system.</p>
<p>410<br>00:32:12,000 –&gt; 00:32:16,000<br>I got to hand that off to a scheduler, again, represent it in some intermediate form.</p>
<p>411<br>00:32:16,000 –&gt; 00:32:22,000<br>And the scheduler can look at that and say, okay, well, you want to run this, this plan for this data.</p>
<p>412<br>00:32:22,000 –&gt; 00:32:30,000<br>Let me go to the catalog and figure out where the data is actually located physically, or who’s responsible in my cluster for actually executing that data.</p>
<p>413<br>00:32:30,000 –&gt; 00:32:42,000<br>And then I now dispatch it to an execution engine, ignoring how I distribute the query out, who’s responsible for making sure that the compute nodes are always running, all that we can ignore.</p>
<p>414<br>00:32:42,000 –&gt; 00:32:49,000<br>And then as I’m actually my query plans, my operators, I may have requests that go get data from storage.</p>
<p>415<br>00:32:49,000 –&gt; 00:32:58,000<br>So I’d have an I.O. service that I would make requests to, and then that I.O. service is responsible for going out to storage, not saying or defining what it is.</p>
<p>416<br>00:32:58,000 –&gt; 00:33:04,000<br>Okay, assuming S3, assuming some distributed file system, could be local disk, doesn’t matter.</p>
<p>417<br>00:33:04,000 –&gt; 00:33:11,000<br>And then it’s going to go fetch this box and then hand it back up to the execution engine to then compute whatever it is that it wants to compute.</p>
<p>418<br>00:33:11,000 –&gt; 00:33:18,000<br>And then when I produce my final answer, then it goes all the way back up the stack to the end user.</p>
<p>419<br>00:33:18,000 –&gt; 00:33:22,000<br>So I have two things that are happening at the same time. Again, the catalog is super important.</p>
<p>420<br>00:33:22,000 –&gt; 00:33:31,000<br>The execution engine as it’s scanning data, someone asked on Slack, should my query planner actually be responsible for looking at the files and figuring out what’s actually in them?</p>
<p>421<br>00:33:31,000 –&gt; 00:33:37,000<br>No, right? Because then you have to have duplicate code, redundant code, have the ability to scan data.</p>
<p>422<br>00:33:37,000 –&gt; 00:33:42,000<br>The execution engine can just do this, because an analyzed command in SQL is just a sequential scan.</p>
<p>423<br>00:33:42,000 –&gt; 00:33:52,000<br>And then it can update the result to the catalog. Now whether or not it goes to the scheduler or whatever the coordinator sends this back over, doesn’t matter.</p>
<p>424<br>00:33:52,000 –&gt; 00:34:00,000<br>But I’m just showing that the execution can derive new information that isn’t just for the query, it’s actually for the contents of the files and go to the catalog.</p>
<p>425<br>00:34:00,000 –&gt; 00:34:05,000<br>I actually think that the utility automatically knows what the execution engine goes.</p>
<p>426<br>00:34:05,000 –&gt; 00:34:06,000<br>Sure, yes.</p>
<p>427<br>00:34:06,000 –&gt; 00:34:12,000<br>But that’s also the right method to execute.</p>
<p>428<br>00:34:12,000 –&gt; 00:34:18,000<br>I mean, it’s a command, right? Let’s say I have some files, I don’t know what’s in them.</p>
<p>429<br>00:34:18,000 –&gt; 00:34:23,000<br>It’s a schedule, go tell someone to do it for me. It’s another query.</p>
<p>430<br>00:34:23,000 –&gt; 00:34:27,000<br>But the query is coming from this, not from somebody other desk.</p>
<p>431<br>00:34:28,000 –&gt; 00:34:30,000<br>Yes.</p>
<p>432<br>00:34:30,000 –&gt; 00:34:36,000<br>So what I think about is putting this step forward, step forward, and then the path.</p>
<p>433<br>00:34:36,000 –&gt; 00:34:42,000<br>Okay, let’s cover that after class. Yeah. Okay. Other questions?</p>
<p>434<br>00:34:42,000 –&gt; 00:34:48,000<br>Okay, so again, same thing, IO service. How are we actually going to find out what files are in there?</p>
<p>435<br>00:34:48,000 –&gt; 00:34:56,000<br>Again, you could have a command that go through the front end, tell the catalog. Maybe IO services could see some stuff and can send it over.</p>
<p>436<br>00:34:56,000 –&gt; 00:35:01,000<br>It depends on the implementation. But I was again, the comment I said before.</p>
<p>437<br>00:35:01,000 –&gt; 00:35:08,000<br>The thing that’s responsible, we’re actually knowing what’s inside of the blocks that I’m getting from from from disk or my object store is going to be the execution.</p>
<p>438<br>00:35:08,000 –&gt; 00:35:15,000<br>Because otherwise you have a bunch of redundant code, you know, people running some stuff to go, you know, a bunch of threats up here, sort of reading data.</p>
<p>439<br>00:35:15,000 –&gt; 00:35:21,000<br>And that can may interfere with what’s going on down here.</p>
<p>440<br>00:35:22,000 –&gt; 00:35:29,000<br>I don’t think we’re reading any paper or statistics about the catalog. We’ll see how, as we go through this master, I’m going to implement this.</p>
<p>441<br>00:35:29,000 –&gt; 00:35:35,000<br>Like, Snowflake uses a whole another database system. They use FoundationDB to do this. This is basically a whole another data system.</p>
<p>442<br>00:35:35,000 –&gt; 00:35:41,000<br>And you want to be transactional. You want to be failsafe. You want to be high performance. This is a whole another problem in itself.</p>
<p>443<br>00:35:41,000 –&gt; 00:35:49,000<br>And I want to talk about how we do this first. Again, before we do the lake house stuff to do updates on top of that.</p>
<p>444<br>00:35:49,000 –&gt; 00:35:56,000<br>Because once you build that, this thing, then you can build that that additional incremental update part.</p>
<p>445<br>00:35:56,000 –&gt; 00:36:03,000<br>Okay. So let’s talk about a high low. Okay. That’s the context of the concept of what one of these systems look like.</p>
<p>446<br>00:36:03,000 –&gt; 00:36:16,000<br>Well, what actually happens when I actually one of these queries? So for this semester, although we’re going to be just, you know, the high level context of what the system were describing, it’s going to soon be distributed.</p>
<p>447<br>00:36:16,000 –&gt; 00:36:22,000<br>Snowflake distributed, redshift distributed, all these systems that we show them, throw scale out distributed database systems.</p>
<p>448<br>00:36:22,000 –&gt; 00:36:29,000<br>But we want to walk before we run. So most of the papers, unless all papers are going to read, are really about single note execution.</p>
<p>449<br>00:36:29,000 –&gt; 00:36:35,000<br>Because at a high level, distributed query execution is the same thing as you would do in a single note.</p>
<p>450<br>00:36:35,000 –&gt; 00:36:43,000<br>Modern CPUs have a bunch of cores. Sometimes you have multiple sockets. And you have to care about numerous regions where the actual memory is being located for each socket.</p>
<p>451<br>00:36:44,000 –&gt; 00:36:53,000<br>So all that is still going to be the same. Which is when you go distributed, there’s a bit more extra work to say, okay, well now I need to send data from this node to another node.</p>
<p>452<br>00:36:53,000 –&gt; 00:37:03,000<br>Well, that’s no different than sending from, you know, this CPU core to this CPU core. Right. It’s obviously potentially slower than going over the network.</p>
<p>453<br>00:37:04,000 –&gt; 00:37:13,000<br>But at a high level, the key concepts that we’re describing throughout this semester are the same on a single note as distributed note. Single note versus multiple notes.</p>
<p>454<br>00:37:13,000 –&gt; 00:37:25,000<br>So the query plan is going to be ideally a DAG of physical operators. So some systems and data fusion is one of them. It’s actually not a DAG, it’s a tree.</p>
<p>455<br>00:37:26,000 –&gt; 00:37:30,000<br>And we’ll see you later in the semester, well that’s going to cause problems when we need to do nested queries, the subqueries.</p>
<p>456<br>00:37:30,000 –&gt; 00:37:36,000<br>Because you want to be able to rewrite or maybe reuse computation from one part of the query for another query. But if it’s a tree, you can’t do that.</p>
<p>457<br>00:37:36,000 –&gt; 00:37:40,000<br>So ideally, you want things to be a DAG and not all systems actually do that.</p>
<p>458<br>00:37:40,000 –&gt; 00:37:50,000<br>And then the data system is going to look at the query plan, figure out what data needs to access, where it’s coming into an operator, and where it needs to go next.</p>
<p>459<br>00:37:50,000 –&gt; 00:37:59,000<br>And so we’re going to figure that out all ahead of time so that when you run the query, it knows exactly how to orchestrate and schedule things and where to send stuff.</p>
<p>460<br>00:37:59,000 –&gt; 00:38:11,000<br>We’ll see a little bit how we sprinkle some ad activity in this process where we can make changes on the fly to the query plan and how we move data or maybe scale things up and down based on what we see in the data.</p>
<p>461<br>00:38:11,000 –&gt; 00:38:23,000<br>Because that’s going to be a big theme throughout the semester, again in this data lake or lake house world, the object store world, you may not actually know what’s actually in the files because you haven’t done the scan on it yet.</p>
<p>462<br>00:38:23,000 –&gt; 00:38:30,000<br>So your estimations might be wrong, so maybe you underestimated overestimated different parts and you want your system to adapt a little bit.</p>
<p>463<br>00:38:30,000 –&gt; 00:38:31,000<br>Yes.</p>
<p>464<br>00:38:31,000 –&gt; 00:38:35,000<br>If a query time can be a DAG, what is the tree?</p>
<p>465<br>00:38:35,000 –&gt; 00:38:36,000<br>Tree.</p>
<p>466<br>00:38:38,000 –&gt; 00:38:39,000<br>Post goes as a tree.</p>
<p>467<br>00:38:39,000 –&gt; 00:38:40,000<br>Yes.</p>
<p>468<br>00:38:40,000 –&gt; 00:38:42,000<br>What does the tree look like?</p>
<p>469<br>00:38:42,000 –&gt; 00:38:44,000<br>Like where is the dog?</p>
<p>470<br>00:38:44,000 –&gt; 00:38:47,000<br>Like a DAG, you could have one part of it, another one.</p>
<p>471<br>00:38:47,000 –&gt; 00:38:49,000<br>Yeah, a tree where you only have one parent.</p>
<p>472<br>00:38:49,000 –&gt; 00:38:56,000<br>Or a DAG, you can do some computation here for a nested query and then send it to two different parts of the tree.</p>
<p>473<br>00:38:56,000 –&gt; 00:38:58,000<br>Or the query plan.</p>
<p>474<br>00:38:58,000 –&gt; 00:39:01,000<br>Okay.</p>
<p>475<br>00:39:01,000 –&gt; 00:39:12,000<br>So this again, this will be a high level overview of what is going to actually happen now in the execution engine with the IOS service when we execute a query.</p>
<p>476<br>00:39:12,000 –&gt; 00:39:15,000<br>So again, these are our worker nodes.</p>
<p>477<br>00:39:15,000 –&gt; 00:39:18,000<br>They’ll have local CPU, local memory, local disk.</p>
<p>478<br>00:39:18,000 –&gt; 00:39:25,000<br>And then they’re going to retrieve when the query starts, you know, think of the leaf nodes of the query plan like squirrel scans and so forth.</p>
<p>479<br>00:39:25,000 –&gt; 00:39:28,000<br>That’s going to go at what we’re going to call persistent data.</p>
<p>480<br>00:39:28,000 –&gt; 00:39:33,000<br>And this is the underlying tuples that are in our tables in our database.</p>
<p>481<br>00:39:33,000 –&gt; 00:39:40,000<br>So again, whether this comes from the IOS service through local disk or from the app store, at this point, it doesn’t matter.</p>
<p>482<br>00:39:40,000 –&gt; 00:39:46,000<br>So all now the worker nodes are going to do some computation for our query plan and they’re going to produce intermediate results.</p>
<p>483<br>00:39:46,000 –&gt; 00:39:53,000<br>So these intermediate results are again, are the artifacts that the operator generates that needs to go to the next stage of the query plan.</p>
<p>484<br>00:39:53,000 –&gt; 00:39:56,000<br>And again, we’ll talk about this throughout the semester.</p>
<p>485<br>00:39:56,000 –&gt; 00:40:02,000<br>Like we’ll assume that the unit of work for our worker nodes when we execute query is going to be a pipeline.</p>
<p>486<br>00:40:02,000 –&gt; 00:40:08,000<br>And then we have to obviously stop at a pipeline breaker and then potentially distribute data around as needed.</p>
<p>487<br>00:40:08,000 –&gt; 00:40:13,000<br>So the way we would distribute data around is one way to do is it’s through shuffle notes.</p>
<p>488<br>00:40:13,000 –&gt; 00:40:24,000<br>And the idea here is that you just, you hash whatever some partitioning key is on the data that you’re scanning, that you’re producing it in your results, and then you distribute it across them to these shuffle nodes.</p>
<p>489<br>00:40:24,000 –&gt; 00:40:28,000<br>And then this is sort of again, this is the, think of this as the pipeline breaker.</p>
<p>490<br>00:40:28,000 –&gt; 00:40:33,000<br>And then now these shuffle nodes are responsible for distributing this data to the next stage of the query plan, the worker nodes.</p>
<p>491<br>00:40:33,000 –&gt; 00:40:41,000<br>So there’s really no computation being done here. It’s just basically in and out, like storing things as a key value pair in memory and then sending out to the worker nodes.</p>
<p>492<br>00:40:41,000 –&gt; 00:40:46,000<br>I’m saying this is optional because most, not all of the lab data systems do this.</p>
<p>493<br>00:40:46,000 –&gt; 00:40:49,000<br>BigQuery and Dremel is probably the most famous one that does this.</p>
<p>494<br>00:40:49,000 –&gt; 00:40:55,000<br>And Google doesn’t say anything stop. They actually have specialized hardware on these things to keep everything in memory and runs as fast as possible.</p>
<p>495<br>00:40:55,000 –&gt; 00:41:02,000<br>And it allows them to do a bunch of tricks for scaling things in and out because now you have this pipeline breaker.</p>
<p>496<br>00:41:02,000 –&gt; 00:41:07,000<br>You can go look, I thought the data was going to be the order this size or this amount, but actually have this amount.</p>
<p>497<br>00:41:07,000 –&gt; 00:41:11,000<br>Do I need more nodes or less nodes? Should I change anything up in the query plan?</p>
<p>498<br>00:41:11,000 –&gt; 00:41:12,000<br>Yes.</p>
<p>499<br>00:41:12,000 –&gt; 00:41:16,000<br>So this is like a, as a part of a regular place where you can insert that ability into a query plan.</p>
<p>500<br>00:41:16,000 –&gt; 00:41:22,000<br>Yes, so statement is this, as a pipeline breaker is this a good, I’m saying stop point.</p>
<p>501<br>00:41:22,000 –&gt; 00:41:27,000<br>It’s a point in the query plan. You can say, okay, let me reassess what is coming into me.</p>
<p>502<br>00:41:27,000 –&gt; 00:41:31,000<br>And do I want to change anything upstream? So Google does that here.</p>
<p>503<br>00:41:31,000 –&gt; 00:41:36,000<br>Yes. So this idea, some of this idea comes from the map reduced world.</p>
<p>504<br>00:41:36,000 –&gt; 00:41:41,000<br>They would have an explicit shuffle phase. But the difference is in the map reduced world, if you’re familiar with things like Hadoop,</p>
<p>505<br>00:41:41,000 –&gt; 00:41:44,000<br>which you don’t recommend using, you don’t want to use that anymore.</p>
<p>506<br>00:41:44,000 –&gt; 00:41:52,000<br>But in that world, it was all batch base, meaning you had to accumulate all the intermediate results of the shuffle phase before you can start the next phase.</p>
<p>507<br>00:41:52,000 –&gt; 00:41:55,000<br>In a modern overlap system, you can use an streaming manner.</p>
<p>508<br>00:41:55,000 –&gt; 00:42:01,000<br>As the data arrives, you can start pushing it to an streaming fashion up to the next worker nodes.</p>
<p>509<br>00:42:01,000 –&gt; 00:42:05,000<br>Start executing right away. So having it in this sort of long pause.</p>
<p>510<br>00:42:05,000 –&gt; 00:42:11,000<br>The other thing point out too, for some publicity I’ve shown this in PowerPoint, I have the same number of worker nodes as shuffle nodes.</p>
<p>511<br>00:42:11,000 –&gt; 00:42:15,000<br>You don’t need to do that. You can scale things up and down accordingly.</p>
<p>512<br>00:42:15,000 –&gt; 00:42:20,000<br>Because if sometimes the intermediate data could be larger than your persistent data.</p>
<p>513<br>00:42:20,000 –&gt; 00:42:23,000<br>We’ll see this later in the semester when you do a worst case optimal joins.</p>
<p>514<br>00:42:23,000 –&gt; 00:42:28,000<br>That the intermediate and balloons to be much, much larger than the persistent data.</p>
<p>515<br>00:42:28,000 –&gt; 00:42:34,000<br>Even the final result will be much smaller, but this thing can get quite large.</p>
<p>516<br>00:42:34,000 –&gt; 00:42:38,000<br>And then now again, we do the next phase. These worker nodes produce more intermediate results.</p>
<p>517<br>00:42:38,000 –&gt; 00:42:47,000<br>And then we send this now to some final node to do some final call-lessing or aggregation to produce the final result that we send back to the user.</p>
<p>518<br>00:42:47,000 –&gt; 00:43:03,000<br>So I’m not showing this here, but the thing that’s above all of this, keep track of what’s going on, what workers are still alive, what stage they are in execution, how much data they’re generating, that’s the schedule in the coordinator, all above this.</p>
<p>519<br>00:43:03,000 –&gt; 00:43:10,000<br>It’s different than the orchestrator in Kubernetes. Because Kubernetes is just like seeing is the pod still up.</p>
<p>520<br>00:43:10,000 –&gt; 00:43:17,000<br>Doesn’t actually know what’s going on inside of it. You have to build that ourselves in our data systems, keep track of what, okay, what are you actually doing?</p>
<p>521<br>00:43:17,000 –&gt; 00:43:24,000<br>Because Kubernetes again, that doesn’t can’t see inside your query.</p>
<p>522<br>00:43:24,000 –&gt; 00:43:28,000<br>So I’ve already started that this, but the distinction is between persistent data and meter data.</p>
<p>523<br>00:43:28,000 –&gt; 00:43:34,000<br>Persistent data is the source of record for our database. Again, could be files in S3.</p>
<p>524<br>00:43:34,000 –&gt; 00:43:39,000<br>Could be proprietary storage that the data managed itself.</p>
<p>525<br>00:43:39,000 –&gt; 00:43:46,000<br>One key thing, though, is that all these modern systems, because you’re going to assume you’re going to run on something like S3.</p>
<p>526<br>00:43:46,000 –&gt; 00:43:53,000<br>An S3 is immutable. I can’t store file or object in S3 and then go back and make in place updates to particular bytes.</p>
<p>527<br>00:43:53,000 –&gt; 00:43:57,000<br>If I want to do that, I’ve got to overwrite the entire thing.</p>
<p>528<br>00:43:57,000 –&gt; 00:44:05,000<br>And so that means they’re all going to be using sort of pen-only architectures for how they designed the data encoded in the storage formats.</p>
<p>529<br>00:44:05,000 –&gt; 00:44:09,000<br>That’s sort of the thing I mentioned in the lake house before. Like, just log structure, a bunch of changes.</p>
<p>530<br>00:44:09,000 –&gt; 00:44:16,000<br>I think I might distort this JSON. And then they batch it together in the storage park, because it’s one right out to the outer store.</p>
<p>531<br>00:44:16,000 –&gt; 00:44:23,000<br>For the in-mater data, again, these are short-lived artifacts that we’re going to generate as we execute the query.</p>
<p>532<br>00:44:23,000 –&gt; 00:44:33,000<br>And because they are only really useful for the lifetime of the query itself, we don’t have to worry about durability and fault-tons in the same way we would with persistent data.</p>
<p>533<br>00:44:34,000 –&gt; 00:44:41,000<br>Now with persistent data, because we’re, again, assuming we’re running on an object store, they handle all that fault-tons and resiliency for us.</p>
<p>534<br>00:44:41,000 –&gt; 00:44:45,000<br>Again, one less thing we have to worry about as we bought a database system.</p>
<p>535<br>00:44:45,000 –&gt; 00:44:53,000<br>But for the in-mater data, wherever it’s possible for maintaining it, ideally, we don’t want it to store it on S3 because that’s slow and cost money.</p>
<p>536<br>00:44:53,000 –&gt; 00:44:57,000<br>So we’ll try to keep this in local caches, either in memory or in disk.</p>
<p>537<br>00:44:58,000 –&gt; 00:45:04,000<br>But again, but if like a node goes down, we can handle that.</p>
<p>538<br>00:45:04,000 –&gt; 00:45:11,000<br>We don’t have to make sure we store a good Julian copies of in-mater data, because who cares when the query is over, we throw it away.</p>
<p>539<br>00:45:11,000 –&gt; 00:45:17,000<br>There has been some research in maybe reusing in-mater results from one query to the next.</p>
<p>540<br>00:45:17,000 –&gt; 00:45:22,000<br>No system, as I know, actually supports that, because if you want to do that, then you just define a materialized view.</p>
<p>541<br>00:45:22,000 –&gt; 00:45:24,000<br>Because it’s basically the same thing.</p>
<p>542<br>00:45:24,000 –&gt; 00:45:25,000<br>Yes?</p>
<p>543<br>00:45:25,000 –&gt; 00:45:29,000<br>Do you say those node correlation, the meta-placic persisting in the operator?</p>
<p>544<br>00:45:29,000 –&gt; 00:45:35,000<br>Is there not a string of the meta-placing object entering from the data that you’ve given from the persisting data?</p>
<p>545<br>00:45:35,000 –&gt; 00:45:44,000<br>This question is, having got to be honest, there’s a comment here that says that there’s no correlation between the amount of innovative data that the query generates.</p>
<p>546<br>00:45:44,000 –&gt; 00:45:49,000<br>There’s no correlation between the size of the persistent data that they’re reading in or the execution time.</p>
<p>547<br>00:45:49,000 –&gt; 00:45:56,000<br>This result comes from the snowflake paper from two or three years ago, or last year.</p>
<p>548<br>00:45:56,000 –&gt; 00:46:01,000<br>They looked at all the queries that they actually actually did in snowflake, and they saw that this wasn’t the case at all.</p>
<p>549<br>00:46:01,000 –&gt; 00:46:08,000<br>Your statement is, could there just be an upper bound where I know the max limit of the amount of data I could generate?</p>
<p>550<br>00:46:08,000 –&gt; 00:46:10,000<br>No, because my query can do anything.</p>
<p>551<br>00:46:10,000 –&gt; 00:46:16,000<br>I can just do a billion times and generate a bunch of random stuff.</p>
<p>552<br>00:46:16,000 –&gt; 00:46:22,000<br>It costs you money. You can do it, but you can do it.</p>
<p>553<br>00:46:22,000 –&gt; 00:46:27,000<br>The challenge for query optimization is to know when this is going to happen.</p>
<p>554<br>00:46:27,000 –&gt; 00:46:33,000<br>When you have an operator that’s going to generate more innovative results than the data going into it.</p>
<p>555<br>00:46:33,000 –&gt; 00:46:38,000<br>You want to use that to figure out what joint algorithm should I use, worst case optimal, or hashed on.</p>
<p>556<br>00:46:38,000 –&gt; 00:46:42,000<br>Again, I keep saying this. We’ll cover it later this semester.</p>
<p>557<br>00:46:42,000 –&gt; 00:46:49,000<br>Hopefully, when I was taking classes back in the day, the professor would say a bunch of stuff in the beginning,</p>
<p>558<br>00:46:49,000 –&gt; 00:46:54,000<br>like, what the hell is he talking about? Later on, you learn all the stuff and it clicks.</p>
<p>559<br>00:46:54,000 –&gt; 00:46:58,000<br>I’m bringing this stuff up now because when we hit those lectures, that’s what he meant by this.</p>
<p>560<br>00:46:58,000 –&gt; 00:47:00,000<br>Worst case optimal joints. Now I know what that is.</p>
<p>561<br>00:47:00,000 –&gt; 00:47:06,000<br>It’s a prelude for what’s the common. Hopefully, you can see I’m getting excited.</p>
<p>562<br>00:47:06,000 –&gt; 00:47:21,000<br>The other thing that I consider now to in our system architecture is the way in which we’re going to transfer data between the different operators and the nodes.</p>
<p>563<br>00:47:21,000 –&gt; 00:47:26,000<br>It’s going to come down to where that persistent data is actually going to be stored.</p>
<p>564<br>00:47:26,000 –&gt; 00:47:34,000<br>Again, the highlight detail is if it was a shared nothing system, which I covered in a second,</p>
<p>565<br>00:47:34,000 –&gt; 00:47:38,000<br>you primarily push query to the data in a shared disk system.</p>
<p>566<br>00:47:38,000 –&gt; 00:47:45,000<br>You would think it primarily would be by textbook definition pulling data to the query, but we’ll see in a modern setting, these lines get blurred very quickly.</p>
<p>567<br>00:47:45,000 –&gt; 00:47:53,000<br>Because for the automated results, you push the query to the data sometimes, and in other cases, you want to, in some object storage,</p>
<p>568<br>00:47:53,000 –&gt; 00:47:58,000<br>you can actually push the query to the data down to the actual object store, other cases start moving things around.</p>
<p>569<br>00:47:58,000 –&gt; 00:48:08,000<br>It can get jumbled, so there’s not really a clean divide between the different models, but it’s going to understand them in the context again of the system that we’re trying to build conceptually in our minds.</p>
<p>570<br>00:48:09,000 –&gt; 00:48:15,000<br>The push approach, again, is that the idea is that the query itself,</p>
<p>571<br>00:48:15,000 –&gt; 00:48:21,000<br>you’re in the SQL string or the intermediate representation of the query plan, is going to be much smaller than the data,</p>
<p>572<br>00:48:21,000 –&gt; 00:48:29,000<br>so why transfer a bunch of data over to the node just to execute it, why not to send the query, which is much smaller over to where the data is actually being stored,</p>
<p>573<br>00:48:29,000 –&gt; 00:48:37,000<br>and I can do the processing there, and then send back the intermediate results, which ideally should be smaller than the persistent data.</p>
<p>574<br>00:48:38,000 –&gt; 00:48:49,000<br>This made a lot of sense in the old days when disks were super slow and networking was super slow. Usually, the network is always considered much slower than the disk.</p>
<p>575<br>00:48:49,000 –&gt; 00:48:53,000<br>That’s not true anymore. The hardware’s gotten really good.</p>
<p>576<br>00:48:54,000 –&gt; 00:49:05,000<br>The challenge in this space though is that you may not have the computational capabilities on where the data is actually being stored to do any processing on that side.</p>
<p>577<br>00:49:07,000 –&gt; 00:49:16,000<br>Again, you think of using S3, if you ignore the select operator, which is talking about the second, it’s just the API’s get, put, and delete.</p>
<p>578<br>00:49:17,000 –&gt; 00:49:23,000<br>You can’t say, oh, by the way, also execute this part of the query plan for me. You can, we’ll talk about in a second, that’s usually what the API is expecting.</p>
<p>579<br>00:49:23,000 –&gt; 00:49:29,000<br>Actually, in Google, that’s all they expose to you. You can’t do any computation there, so you can’t push the query to the data.</p>
<p>580<br>00:49:29,000 –&gt; 00:49:39,000<br>You instead have to pull the query to the data. You bring the data that you need, do the processing there, generate the intermediate results, and then send it to the next stage.</p>
<p>581<br>00:49:40,000 –&gt; 00:49:45,000<br>Again, the idea is, again, the size of the query relative to the size of the data that you’re processing is going to be much smaller.</p>
<p>582<br>00:49:45,000 –&gt; 00:49:52,000<br>The largest query I’ve ever heard of in the pure SQL string itself is 10 megabytes from Google. That’s a huge task query.</p>
<p>583<br>00:49:52,000 –&gt; 00:49:59,000<br>It’s big, but the processing terabytes of data. It’s not even close. Or the magnitude difference.</p>
<p>584<br>00:50:00,000 –&gt; 00:50:05,000<br>In the old days, this was considered the primary way to do it, especially in a shared nothing architecture.</p>
<p>585<br>00:50:05,000 –&gt; 00:50:12,000<br>But in a shared disk, you actually, if you just get an ignoring the extra features you can get from object stores, you would do this.</p>
<p>586<br>00:50:12,000 –&gt; 00:50:24,000<br>The extra features I’m talking about are in things like S3, they have a select operator, where now you can basically send what looks like a SQL query down to S3 when you make the get request.</p>
<p>587<br>00:50:25,000 –&gt; 00:50:32,000<br>And you can say, here’s run this filter on this data. And S3 actually knows the contents of what your objects actually look like.</p>
<p>588<br>00:50:32,000 –&gt; 00:50:38,000<br>So it’s not a dumb, key value story. Say, give me this bucket. And I don’t just give me the byte streamer, I don’t care what’s in it.</p>
<p>589<br>00:50:38,000 –&gt; 00:50:47,000<br>Like, as they say here, they know that it’s a CSV, a JSON, of its parquet. And they actually can process that natively where the data is actually being stored.</p>
<p>590<br>00:50:48,000 –&gt; 00:50:56,000<br>I don’t know how they charge you for this, whether it’s just the fetch command, or like they charge you the runtime, or because it runs as a Lambda function, I actually have no idea.</p>
<p>591<br>00:50:56,000 –&gt; 00:51:06,000<br>But again, this allows us to do a predicate push down in a shared disk architecture, which we, in according to the text, well, you would not be able to do.</p>
<p>592<br>00:51:07,000 –&gt; 00:51:15,000<br>Microsoft has their own thing. I don’t know whether you get SQL, but you can kind of see like the, you know, you pass in some kind of query there.</p>
<p>593<br>00:51:15,000 –&gt; 00:51:20,000<br>And again, as far as I know, Google doesn’t have this. I didn’t look this year, but the last year they didn’t have this as well.</p>
<p>594<br>00:51:20,000 –&gt; 00:51:29,000<br>So again, this is what I’m saying. The lines get blurred because you can do some predicate push down and other things. And projection push down as well.</p>
<p>595<br>00:51:30,000 –&gt; 00:51:31,000<br>All right, again, yes.</p>
<p>596<br>00:51:34,000 –&gt; 00:51:45,000<br>This question is, do you always want to do predicate push down at the option is available? No, because like it may be the case that the block of data you need is used over and over again, but bunch of queries that have different predicates.</p>
<p>597<br>00:51:45,000 –&gt; 00:51:56,000<br>So therefore, I’m making now multiple requests to s3 to get different portions of that same file, where it would have been cheaper just to go get it once, cache it, then do all my filtering locally.</p>
<p>598<br>00:51:57,000 –&gt; 00:52:05,000<br>Right. But how to figure that out? It’s hard. Right. And that’s why people pay a lot of money for databases.</p>
<p>599<br>00:52:05,000 –&gt; 00:52:18,000<br>Because they’ll they can do that all for you. I actually don’t know how many systems actually do that. This trick though. I think redshift does it because they obviously built it, but I don’t know what the snowflake does it.</p>
<p>600<br>00:52:19,000 –&gt; 00:52:30,000<br>All right. So again, share nothing architecture. This is what we covered in the interclass. You know, classic textbook definition actually comes from the stone breaker. This is something that he coined in the in 1980s.</p>
<p>601<br>00:52:30,000 –&gt; 00:52:40,000<br>And this was the prevailing architecture for for just a bit of databases, but for all of the P and OLAB systems for, you know, for 30 years until the 2010s.</p>
<p>602<br>00:52:41,000 –&gt; 00:52:57,000<br>Again, each each node itself is going to have its own local CPU locally attached disk and memory and anytime you want to send information to get data from another node, you got to go over the network and send PCP or your UDP requests.</p>
<p>603<br>00:52:57,000 –&gt; 00:53:07,000<br>Right. So we’ll call, you know, each of these things as a single data system node. So I think like EC2, you know, you get an instance that we’re talking about that.</p>
<p>604<br>00:53:08,000 –&gt; 00:53:20,000<br>The database is going to be partitioned into disjoint subsets across the nodes, right, this can picking like a partition key, you can either range partitioning or hash partitioning to divide them all up evenly across the different nodes.</p>
<p>605<br>00:53:20,000 –&gt; 00:53:36,000<br>And then now since the data is being stored by the data, and there’s local, then I can just use a positive API. You know, I can use cyscall, go get you know, FRED or FOPEN and get the file, the data that actually need because everything can, it’s just files on on disk that I control my file system.</p>
<p>606<br>00:53:38,000 –&gt; 00:53:39,000<br>Yes.</p>
<p>607<br>00:53:45,000 –&gt; 00:53:50,000<br>Yes, so his question is, if I add a new node, do I have to move data immediately or do I move data as it comes in?</p>
<p>608<br>00:53:50,000 –&gt; 00:54:00,000<br>So this is going to be one of the big problems I’m going to face and share nothing is that if we want to increase capacity here, then I have to add a new node, but then that node when it gets added doesn’t have any data in it.</p>
<p>609<br>00:54:01,000 –&gt; 00:54:10,000<br>So there’s this Genshius NSF, NSF melts, but then, wait, the NFS has to be like a central storage, right?</p>
<p>610<br>00:54:10,000 –&gt; 00:54:14,000<br>It can’t be, you can’t do like a peer-to-peer file system.</p>
<p>611<br>00:54:16,000 –&gt; 00:54:29,000<br>AFS, same thing, right, it’s a central storage. So that’s a shared discocker tagger. The difference is going to be though is that in something like AFS or NFS, the location or the distribution of the data is going to be a little bit more.</p>
<p>612<br>00:54:29,000 –&gt; 00:54:43,000<br>The distribution of the data physically is transparent to the database system because it’s a POSIX API. You just call F-open and F-read, you don’t actually know, I mean, the cover is where that data is actually being stored, right?</p>
<p>613<br>00:54:43,000 –&gt; 00:54:55,000<br>So unless you now somehow explicitly tell NFS to like partition things a certain way, and so that when you read this range versus that range, you know that only certain pieces of data that get it locally.</p>
<p>614<br>00:54:55,000 –&gt; 00:54:58,000<br>But again, it’s as opaque storage as that you don’t understand.</p>
<p>615<br>00:55:02,000 –&gt; 00:55:06,000<br>But through NFS? NFS doesn’t expose you that, it shows that too.</p>
<p>616<br>00:55:10,000 –&gt; 00:55:12,000<br>But then you got to get that information out of NFS.</p>
<p>617<br>00:55:14,000 –&gt; 00:55:20,000<br>Now you’re basically building a database system, like why bother you? We don’t, like no, you don’t want to run your database all NFS, right?</p>
<p>618<br>00:55:21,000 –&gt; 00:55:27,000<br>These are the scale like this. People do that, people run the sands all the time for distributed file systems.</p>
<p>619<br>00:55:29,000 –&gt; 00:55:37,000<br>The data is going to do better if it knows exactly where the data is actually. Maybe not physically stored because if it’s S3, all that’s abstracted away from you too as well.</p>
<p>620<br>00:55:37,000 –&gt; 00:55:52,000<br>But like it’s, how does it? The object store versus NFS would give you roughly the same interface except that you would have better control of, what can you control?</p>
<p>621<br>00:55:53,000 –&gt; 00:55:58,000<br>You have control of things like georeplicator not and NFS hides that from you. You get more metadata out of object stores.</p>
<p>622<br>00:55:58,000 –&gt; 00:56:05,000<br>I’m aware of objectures where the user and the customer is like a metadata over here. That’s the first figure of where this would be in the United States.</p>
<p>623<br>00:56:06,000 –&gt; 00:56:15,000<br>That’s the catalog. You’re talking about the catalog. So, see, some systems use Cassandra as the metadata layer, the key track of where the data is actually located. That’s the catalog.</p>
<p>624<br>00:56:16,000 –&gt; 00:56:24,000<br>Again, I mentioned snowflake, the used foundation DB. I can’t think of anybody offhand that uses Cassandra for this in databases.</p>
<p>625<br>00:56:24,000 –&gt; 00:56:31,000<br>But yeah, it’s the same idea. But that’s the separate databases to keep track of the metadata. That’s not this.</p>
<p>626<br>00:56:33,000 –&gt; 00:56:41,000<br>Okay. So, in the shared system, again, we have the separation between computing storage, we have the compute layer.</p>
<p>627<br>00:56:42,000 –&gt; 00:56:43,000<br>There’s still locally attached disk that, yes.</p>
<p>628<br>00:56:43,000 –&gt; 00:56:50,000<br>Very different questions about things. If one of the nodes goes down, you do all access to it.</p>
<p>629<br>00:56:50,000 –&gt; 00:56:56,000<br>Oh, yeah. Sorry. I got a side track. Sorry. First, David, if one of the nodes goes down, do you lose access to that data? Yes.</p>
<p>630<br>00:56:57,000 –&gt; 00:57:04,000<br>So, therefore, you have to replicate it. And again, this was going back to the same before. It’s now managed storage that the data systems controls and they’re in charge of replication.</p>
<p>631<br>00:57:05,000 –&gt; 00:57:11,000<br>So, they have to handle all that for you. In an S3 object store, Amazon handles that. Google handles that.</p>
<p>632<br>00:57:12,000 –&gt; 00:57:18,000<br>I don’t know how to do it. They guarantee certain amount of reliability. And, you know, for our purposes, it’s good enough.</p>
<p>633<br>00:57:19,000 –&gt; 00:57:26,000<br>So, like, in this world, you have to manage it. And then the question was, if I add another new node, or take a node away, do I have to reshuffle data? Yes.</p>
<p>634<br>00:57:27,000 –&gt; 00:57:34,000<br>And the data system has to do that. And you want to do this in a transactional manner on your catalog so that you avoid false negatives.</p>
<p>635<br>00:57:41,000 –&gt; 00:57:50,000<br>So, in the shared disk system, again, we have the compute layer and we have the storage layer here. And we just access this through a, you know, some kind of API.</p>
<p>636<br>00:57:51,000 –&gt; 00:58:02,000<br>And in the cloud world, instead of using positive API, because there’s not, you don’t want to use like a fused file system to talk to S3, you instead use whatever the user space API that the cloud provider provides for you.</p>
<p>637<br>00:58:03,000 –&gt; 00:58:10,000<br>Like, Amazon gives you a bunch of libraries to talk to S3. Some data systems go to extremes and throw that all away and rewrite it themselves.</p>
<p>638<br>00:58:11,000 –&gt; 00:58:20,000<br>So, we’ll see one in a second. But, like, that’s how we’re going to react to these things. And so, now, the way to think about this is the compute nodes are stateless.</p>
<p>639<br>00:58:21,000 –&gt; 00:58:32,000<br>Like, in my shared, in my shared nothing system going back, I actually pointed out, like, I partitioned my database and now each node ignoring replication is responsible for that partition of the data.</p>
<p>640<br>00:58:33,000 –&gt; 00:58:39,000<br>And so, if I want to take a node away, well, I got to copy whatever’s in it and send it to all the other nodes to redistribute it.</p>
<p>641<br>00:58:39,000 –&gt; 00:58:52,000<br>But, in a shared system, if I want to take away one of these compute nodes, well, okay, that’s fine, because the data is down here. So, I can theory kill this thing and then not lose any data.</p>
<p>642<br>00:58:53,000 –&gt; 00:59:06,000<br>Or, I could turn them, turn them all off, not pay for the compute costs, and then, you know, all my data is still retained. Whereas, in a shared nothing system, I got to keep the CPUs running because if they go away, then the data goes away.</p>
<p>643<br>00:59:07,000 –&gt; 00:59:10,000<br>Yeah, you can check Monte EBS and all that, but, like, yes.</p>
<p>644<br>00:59:19,000 –&gt; 00:59:30,000<br>So, his statement is, his statement is, and he’s cracked, like, if we shot, if we shot, in the shared nothing, a shared desk, if I’m literally running Quirry at the time, then yes, there’s a femoral state for that query that I would lose.</p>
<p>645<br>00:59:30,000 –&gt; 00:59:34,000<br>Yes, and we can talk about how to handle fault tolerance later on for that.</p>
<p>646<br>00:59:35,000 –&gt; 00:59:51,000<br>But, typically, again, if you say, ignoring unexpected failures, when you do a shutdown, you basically, you announce all the nodes, hey, I’m going to shut down, finish whatever jobs you’re doing, and then once the last queries, or plan fragments, lead the queue, then you can shut it down.</p>
<p>647<br>00:59:52,000 –&gt; 00:59:57,000<br>There’s a process of doing that. It’s not nothing fancy, it’s not hard.</p>
<p>648<br>00:59:58,000 –&gt; 01:00:20,000<br>Okay. Again, we’ll see this later in the semester, but there’ll be a catalog service that keeps track of, you know, this data actually here still can be partitioned, even though we see, you know, this thing could just be going to files on S3, and I could keep track of my catalog, which in my compute notes is responsible for those files.</p>
<p>649<br>01:00:21,000 –&gt; 01:00:29,000<br>And then if I increase or decrease my capacity, I’d run the men’s, and kind of update to my catalog and say, okay, these new nodes are now responsible for this other technique.</p>
<p>650<br>01:00:30,000 –&gt; 01:00:37,000<br>And then for that one, in case of like snowflake, I’ll just use consistent hashing to avoid having to reshuffle everything, right, that all the standard techniques still apply here.</p>
<p>651<br>01:00:38,000 –&gt; 01:01:01,000<br>Alright, so this to finish up with the distinction, again, in the shared nothing, it’s hard to scale capacity, but potentially it is faster, because now everything’s sort of local, but the engineering benefits and the operational benefits you get from something like a shared disc architecture is why basically every OLAP system built in the last 10 years uses this technique.</p>
<p>652<br>01:01:02,000 –&gt; 01:01:11,000<br>And even systems that started off using shared nothing like yellow brick have converted to this, because the benefits are so significant.</p>
<p>653<br>01:01:11,000 –&gt; 01:01:21,000<br>And again, like, Amazon’s approving S3 all the time, so as they make those changes, your data rise along the wave and gets all those updates and new features as well.</p>
<p>654<br>01:01:22,000 –&gt; 01:01:29,000<br>Right, because when S3 first came out, it didn’t have that select command now it does, so that’s something you know they’ve added that you can then take the benefit of without having to do any engineering for yourself.</p>
<p>655<br>01:01:30,000 –&gt; 01:01:35,000<br>And S3 is pretty cheap, that’s actually really cheap compared to EBS and other things.</p>
<p>656<br>01:01:36,000 –&gt; 01:01:42,000<br>It’s not the fastest, but then that’s okay, we’re database, we know how to do caching, it’s basically a buffer pool manager, right.</p>
<p>657<br>01:01:42,000 –&gt; 01:01:52,000<br>You’re not new caching, avoid that there’s long latency of doing this access, so we can do all of that to hide the slow rate around trip times from S3.</p>
<p>658<br>01:01:53,000 –&gt; 01:01:56,000<br>So again, this semester we’re going to focus on this implementation.</p>
<p>659<br>01:01:59,000 –&gt; 01:02:09,000<br>So this is not a new idea, again I should have showed that these things became vogue in the 2010s, but actually goes back to the 1980s, and traditionally these things were terrible, these shared disk data systems.</p>
<p>660<br>01:02:10,000 –&gt; 01:02:26,000<br>But because of the cloud stores, because of all these, every vendor has their own version, there’s local things like SEF and other stuff or cluster FS, like there’s local object stores you could use, these things are so prevalent, but again every system is based on this.</p>
<p>661<br>01:02:27,000 –&gt; 01:02:38,000<br>So just give an example of what a non cloud version of a shared disk architecture would look like, that’s old, this is Oracle Exadata, so when you buy Oracle Exadata it’s getting millions of dollars, they ship you a rack or a bunch of racks,</p>
<p>662<br>01:02:39,000 –&gt; 01:02:46,000<br>and you can buy a shared disk architecture, that’s going over like, infinity and n or fiber channel from the compute nodes to the storage nodes.</p>
<p>663<br>01:02:47,000 –&gt; 01:02:50,000<br>I think they can do predicate push down on the storage size as well.</p>
<p>664<br>01:02:50,000 –&gt; 01:03:02,000<br>And again, this is all running in the same rack, instead of like on the object store over the public network in Amazon, but again just showing that these ideas have been around for a long time.</p>
<p>665<br>01:03:02,000 –&gt; 01:03:05,000<br>So let’s talk about the object stores, from the portion that we care about.</p>
<p>666<br>01:03:06,000 –&gt; 01:03:22,000<br>So again, from the Davies’ perspective, it’s disk, and instead of going, again, we’re not using POSX API, we’re not using the libc calls, we’re using whatever the API, the cloud vendors providing us, but we’re going to be responsible for what we’re actually storing in it.</p>
<p>667<br>01:03:23,000 –&gt; 01:03:30,000<br>And then whether it’s going to be in a proprietary format that’s custom to the database system, or an open source format, we’ll cover next class.</p>
<p>668<br>01:03:31,000 –&gt; 01:03:33,000<br>Again, it doesn’t matter.</p>
<p>669<br>01:03:34,000 –&gt; 01:03:38,000<br>So most of the, you know, in most of these systems, they’re going to be storing in the Pax format.</p>
<p>670<br>01:03:39,000 –&gt; 01:03:52,000<br>Again, think of that as like, it’s a columnar format, but the tables me divided up into row groups or blocks, big blocks of data, and then within that block, all the data for a single two-bo is going to be located in it, but it’s just going to be stored in a columnar format.</p>
<p>671<br>01:03:53,000 –&gt; 01:04:02,000<br>And that’s different than like the really early column store systems, like in Vertica, for example, I think they stored the entire column as a separate, continuous file, and every column was its own file.</p>
<p>672<br>01:04:03,000 –&gt; 01:04:11,000<br>In the Pax world, again, you combine things together so that all the two-bo is spatially close to each other within the file, even though they’re stored in a columnar format.</p>
<p>673<br>01:04:12,000 –&gt; 01:04:21,000<br>Again, and we’ll cover this next class, but there’ll be some got a header of footer for all these files that’s contained how to get to these offsets, because everything has to be fixed length, to get to the different two-bo’s that you need.</p>
<p>674<br>01:04:22,000 –&gt; 01:04:34,000<br>How things are being compressed, any additional sketches or indexes or metadata, where we want to store, if I want the data is, but again, we scan through the execution engine and then feed that into the catalog so that the planner can use it.</p>
<p>675<br>01:04:34,000 –&gt; 01:04:37,000<br>Again, all this will cover and the next class.</p>
<p>676<br>01:04:37,000 –&gt; 01:04:45,000<br>And again, basically what happens is you would retrieve either the header, or in case of Park K and Ork, it’s always the footer, because it’s an appendly of storage.</p>
<p>677<br>01:04:45,000 –&gt; 01:04:50,000<br>So you start writing up the file and then you realize, okay, here’s all the data I just stored, and you put that in the footer.</p>
<p>678<br>01:04:50,000 –&gt; 01:04:56,000<br>So you can use your object store to go just retrieve the footer of the file, and then figure out what’s actually inside of it.</p>
<p>679<br>01:04:57,000 –&gt; 01:05:00,000<br>And again, they all have their own version of Put Get and Delete.</p>
<p>680<br>01:05:01,000 –&gt; 01:05:07,000<br>So this is the one system we’re going to cover later in the semester, but I bring it up now because it’s wild what they did.</p>
<p>681<br>01:05:07,000 –&gt; 01:05:14,000<br>So the yellow brick was originally an on-prem database system that was shared nothing. It was an appliance.</p>
<p>682<br>01:05:14,000 –&gt; 01:05:20,000<br>You would buy these custom hardware that they would put together that was tuned for the database system, and you run this on-prem.</p>
<p>683<br>01:05:20,000 –&gt; 01:05:25,000<br>They switch converted to a cloud-based database system, like a snowflake or like a redshift and others.</p>
<p>684<br>01:05:25,000 –&gt; 01:05:36,000<br>But they found that when they converted over to the run in the public cloud, the object store was so much slower than they were used to in their on-prem version of Put Get.</p>
<p>685<br>01:05:36,000 –&gt; 01:05:43,000<br>So they ended up rewriting a lot of things that Amazon provides you, or like, the Office doesn’t provide you, and everything’s all custom.</p>
<p>686<br>01:05:43,000 –&gt; 01:05:50,000<br>So for example, they threw away the Amazon libraries for their own libraries to call S3 using Intel DVDK.</p>
<p>687<br>01:05:50,000 –&gt; 01:06:02,000<br>So it’s doing kernel bypass, which will cover the latest semester, basically to do fast lookups to S3, get the contents, or get the data you need, and not make a copy in the kernel, just immediately pass it up to user space.</p>
<p>688<br>01:06:03,000 –&gt; 01:06:14,000<br>DVDK is a nightmare. We’ll cover that in the later semester. Or like, instead of running over TCP-IP, they wrote their own network protocol over UDP, because there’s just so much faster than them.</p>
<p>689<br>01:06:14,000 –&gt; 01:06:20,000<br>So they rewrote a ton of stuff and say, they write their own PCIe drivers. Like, who does that? Data is people, right? It’s awesome.</p>
<p>690<br>01:06:21,000 –&gt; 01:06:34,000<br>So, you know, there is, like, even though I said S3 is slow, or, you know, relative to local disk, there’s ways to make it faster. And again, caching is also going to help us hide long latency as well.</p>
<p>691<br>01:06:34,000 –&gt; 01:06:37,000<br>We’re relying on the local attached disk on the compute nodes.</p>
<p>692<br>01:06:38,000 –&gt; 01:06:51,000<br>Okay, so to finish up, today again, it was a vomiting a bunch of database stuff at you, as a preview for where we’re going to go this semester.</p>
<p>693<br>01:06:51,000 –&gt; 01:06:59,000<br>And we’re basically going to start from the bottom layer. We’re not going to talk about how S3 works, because that part we don’t really care about.</p>
<p>694<br>01:07:00,000 –&gt; 01:07:09,000<br>But we’re going to talk about what the data is actually going to look like in that we’re going to put in S3. And then we’ll start building layers on top of that, or, to be able to execute queries.</p>
<p>695<br>01:07:09,000 –&gt; 01:07:14,000<br>So the opposite direction will be what I showed in the beginning, going top down, we’re going to go bottom up.</p>
<p>696<br>01:07:14,000 –&gt; 01:07:24,000<br>And the idea really is about how to, what are the state of our implementation, the state of our methods, techniques, and algorithms, to do all the things that we laid out at the time.</p>
<p>697<br>01:07:24,000 –&gt; 01:07:31,000<br>And that’s the papers that I picked for you guys are really designed to expose you to, here’s a certain way of how people approach these problems.</p>
<p>698<br>01:07:31,000 –&gt; 01:07:35,000<br>But then we’ll also cover other papers that are related to the other techniques.</p>
<p>699<br>01:07:35,000 –&gt; 01:07:37,000<br>Okay?</p>
<p>700<br>01:07:37,000 –&gt; 01:07:45,000<br>All right, so next class, the paper guys have reading is actually something that I wrote with my former PGA student, basically it’s a survey of the internals of Parkay and Ork.</p>
<p>701<br>01:07:45,000 –&gt; 01:07:48,000<br>It’s going to talk a little bit about GPUs at the end.</p>
<p>702<br>01:07:49,000 –&gt; 01:07:53,000<br>We’re not going to cover GPUs or FGAs the semester, a whole new line of work.</p>
<p>703<br>01:07:53,000 –&gt; 01:07:57,000<br>We’re only going to be focused on how we do X2 queries on CPUs for now.</p>
<p>704<br>01:07:57,000 –&gt; 01:07:58,000<br>Okay?</p>
<p>705<br>01:07:58,000 –&gt; 01:08:01,000<br>And then, so next class will be Parkay and Ork.</p>
<p>706<br>01:08:01,000 –&gt; 01:08:05,000<br>It’s the most widely used, open source file formats.</p>
<p>707<br>01:08:05,000 –&gt; 01:08:15,000<br>And then the following class next Monday will be, here’s the, here’s new variations, new implementations of a file format that, that supposedly are better than Parkay and Ork.</p>
<p>708<br>01:08:15,000 –&gt; 01:08:18,000<br>So it’s like, sort of the next generation file formats that are coming out.</p>
<p>709<br>01:08:18,000 –&gt; 01:08:19,000<br>Okay?</p>
<p>710<br>01:08:19,000 –&gt; 01:08:21,000<br>Any other questions?</p>
<p>711<br>01:08:45,000 –&gt; 01:08:47,000<br>Okay?</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CMU15721 P2S202401 ModernOLAPDatabaseSystemsCMUAdvancedDatabaseSystems</div>
      <div>http://example.com/2025/10/24/CMU15721 P2S202401-ModernOLAPDatabaseSystemsCMUAdvancedDatabaseSystems/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月24日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/24/CMU15721%20P21S202421-YellowbrickDataWarehouseSystemCMUAdvancedDatabaseSystems/" title="CMU15721 P21S202421 YellowbrickDataWarehouseSystemCMUAdvancedDatabaseSystems">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CMU15721 P21S202421 YellowbrickDataWarehouseSystemCMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/24/CMU15721%20P1S202400-CourseOverviewLogisticsCMUAdvancedDatabaseSystems/" title="CMU15721 P1S202400 CourseOverviewLogisticsCMUAdvancedDatabaseSystems">
                        <span class="hidden-mobile">CMU15721 P1S202400 CourseOverviewLogisticsCMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
