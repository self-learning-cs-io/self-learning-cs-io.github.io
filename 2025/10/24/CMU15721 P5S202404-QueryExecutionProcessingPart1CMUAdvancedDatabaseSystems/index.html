

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="100:00:00,000 –&gt; 00:00:06,000Carnegie Mellon University’s advanced database systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio board. 300:00:09,000 –&gt; 00:00:16,000T">
<meta property="og:type" content="article">
<meta property="og:title" content="CMU15721 P5S202404 QueryExecutionProcessingPart1CMUAdvancedDatabaseSystems">
<meta property="og:url" content="http://example.com/2025/10/24/CMU15721%20P5S202404-QueryExecutionProcessingPart1CMUAdvancedDatabaseSystems/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="100:00:00,000 –&gt; 00:00:06,000Carnegie Mellon University’s advanced database systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio board. 300:00:09,000 –&gt; 00:00:16,000T">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-24T11:57:41.749Z">
<meta property="article:modified_time" content="2025-10-24T12:06:28.549Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>CMU15721 P5S202404 QueryExecutionProcessingPart1CMUAdvancedDatabaseSystems - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CMU15721 P5S202404 QueryExecutionProcessingPart1CMUAdvancedDatabaseSystems"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-24 19:57" pubdate>
          2025年10月24日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          8.9k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          74 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CMU15721 P5S202404 QueryExecutionProcessingPart1CMUAdvancedDatabaseSystems</h1>
            
            
              <div class="markdown-body">
                
                <p>1<br>00:00:00,000 –&gt; 00:00:06,000<br>Carnegie Mellon University’s advanced database systems courses</p>
<p>2<br>00:00:06,000 –&gt; 00:00:09,000<br>filming front of the live studio board.</p>
<p>3<br>00:00:09,000 –&gt; 00:00:16,000<br>Today we’re not just talking about</p>
<p>4<br>00:00:16,000 –&gt; 00:00:18,000<br>actually exiting queries.</p>
<p>5<br>00:00:18,000 –&gt; 00:00:21,000<br>So the last two classes,</p>
<p>6<br>00:00:21,000 –&gt; 00:00:25,000<br>the last two classes were focusing on</p>
<p>7<br>00:00:25,000 –&gt; 00:00:27,000<br>what the data is actually going to look like.</p>
<p>8<br>00:00:27,000 –&gt; 00:00:31,000<br>And we were designing our encoding schemes in such a way that when we actually</p>
<p>9<br>00:00:31,000 –&gt; 00:00:35,000<br>start running the queries, we would minimize the amount of data we have to fetch</p>
<p>10<br>00:00:35,000 –&gt; 00:00:38,000<br>from disk or remote object store and bring that into memory.</p>
<p>11<br>00:00:38,000 –&gt; 00:00:43,000<br>And we can be clever about encoding our data in certain ways that we can</p>
<p>12<br>00:00:43,000 –&gt; 00:00:49,000<br>ideally do our processing on it in its encoded form or in its compressed form.</p>
<p>13<br>00:00:49,000 –&gt; 00:00:53,000<br>RLE’s and obviously one, but there’s other ways we should have to do this as well.</p>
<p>14<br>00:00:54,000 –&gt; 00:00:57,000<br>And so for the next couple of weeks, we’re going to talk about how we actually</p>
<p>15<br>00:00:57,000 –&gt; 00:00:59,000<br>are going to execute queries.</p>
<p>16<br>00:00:59,000 –&gt; 00:01:03,000<br>And again, in the O-Lat world, it’s all about sequential scans.</p>
<p>17<br>00:01:03,000 –&gt; 00:01:05,000<br>We’re not going to do index lookups.</p>
<p>18<br>00:01:05,000 –&gt; 00:01:07,000<br>We’re ignoring bitmap indexes and other things.</p>
<p>19<br>00:01:07,000 –&gt; 00:01:10,000<br>There is going to be a B-Best tree to go find us single</p>
<p>20<br>00:01:10,000 –&gt; 00:01:11,000<br>things, single records.</p>
<p>21<br>00:01:11,000 –&gt; 00:01:14,000<br>We’re going to have to scan through large chunks of data.</p>
<p>22<br>00:01:14,000 –&gt; 00:01:18,000<br>So this is that again, that list of what I showed a couple of weeks ago.</p>
<p>23<br>00:01:18,000 –&gt; 00:01:22,000<br>Here’s the bag of techniques or tricks we can do to make</p>
<p>24<br>00:01:22,000 –&gt; 00:01:23,000<br>sequential scans run faster.</p>
<p>25<br>00:01:23,000 –&gt; 00:01:28,000<br>As I said, we’re not going to discuss material that is view and clustering</p>
<p>26<br>00:01:28,000 –&gt; 00:01:29,000<br>sorting in this semester.</p>
<p>27<br>00:01:29,000 –&gt; 00:01:32,000<br>But we’ve already just data encoding compression.</p>
<p>28<br>00:01:32,000 –&gt; 00:01:34,000<br>We’ve already discussed a little bit about data skipping.</p>
<p>29<br>00:01:34,000 –&gt; 00:01:38,000<br>Like how do you use zone maps to say, here’s the minmax of this giant block of data</p>
<p>30<br>00:01:38,000 –&gt; 00:01:44,000<br>and check to see whether the tuple you’re looking for or any tuple that could be in that block</p>
<p>31<br>00:01:44,000 –&gt; 00:01:45,000<br>based on that unzone map.</p>
<p>32<br>00:01:45,000 –&gt; 00:01:48,000<br>So for the rest of the semester, we’re going to go through all of these.</p>
<p>33<br>00:01:48,000 –&gt; 00:01:51,000<br>And it’s not going to be like, here’s a lecture on test parallelization.</p>
<p>34<br>00:01:52,000 –&gt; 00:01:54,000<br>Here’s a lecture on query parallelization.</p>
<p>35<br>00:01:54,000 –&gt; 00:01:59,000<br>It’s going to come up in different points throughout the entire semester.</p>
<p>36<br>00:01:59,000 –&gt; 00:02:04,000<br>Sometimes we’ll delay discussing certain things, and then we’ll have to bring in</p>
<p>37<br>00:02:04,000 –&gt; 00:02:08,000<br>this bag of tricks for discuss how to do joins efficiently or how to do code</p>
<p>38<br>00:02:08,000 –&gt; 00:02:09,000<br>specialization, other things.</p>
<p>39<br>00:02:09,000 –&gt; 00:02:15,000<br>So again, we’ll go through these throughout the semester.</p>
<p>40<br>00:02:15,000 –&gt; 00:02:20,000<br>So at its core, what this class is somewhat really about is how to build a database</p>
<p>41<br>00:02:20,000 –&gt; 00:02:25,000<br>system to run efficiently on your data for given set of queries.</p>
<p>42<br>00:02:25,000 –&gt; 00:02:30,000<br>And the idea here is that we want to make the full use of the hardware that’s available to us.</p>
<p>43<br>00:02:30,000 –&gt; 00:02:37,000<br>So again, we can run queries fast and at a lower cost than otherwise doing something stupid.</p>
<p>44<br>00:02:37,000 –&gt; 00:02:42,000<br>And so all of the things I showed on the last slide, there’s not one I can point to</p>
<p>45<br>00:02:42,000 –&gt; 00:02:46,000<br>and say, hey, if you’re building a brand new system, here’s the one thing you want to do above</p>
<p>46<br>00:02:46,000 –&gt; 00:02:47,000<br>everything else.</p>
<p>47<br>00:02:48,000 –&gt; 00:02:52,000<br>Just briefly going back to it, all these things matter, all these things are going to be</p>
<p>48<br>00:02:52,000 –&gt; 00:02:56,000<br>accumulative or multiplicative where we can add them on top of each other and get better results</p>
<p>49<br>00:02:56,000 –&gt; 00:02:58,000<br>and make things run faster.</p>
<p>50<br>00:02:58,000 –&gt; 00:03:03,000<br>And so it’s really about understanding from engineering perspective what are the trade</p>
<p>51<br>00:03:03,000 –&gt; 00:03:08,000<br>offs that one of these techniques could make, both in terms of the performance costs and</p>
<p>52<br>00:03:08,000 –&gt; 00:03:10,000<br>actually the engineering costs.</p>
<p>53<br>00:03:10,000 –&gt; 00:03:13,000<br>How much time is it going to make for some actually to build this and maintain it as</p>
<p>54<br>00:03:13,000 –&gt; 00:03:15,000<br>another big problem as well.</p>
<p>55<br>00:03:16,000 –&gt; 00:03:20,000<br>So the spoiler is going to be just in time query compilation, amazing results,</p>
<p>56<br>00:03:20,000 –&gt; 00:03:25,000<br>you’ll get really fast, but it’s a big pain to maintain and build.</p>
<p>57<br>00:03:25,000 –&gt; 00:03:30,000<br>And so therefore most systems are going to use precompiled primitives or operations.</p>
<p>58<br>00:03:30,000 –&gt; 00:03:32,000<br>This is actually with a vector wise paper.</p>
<p>59<br>00:03:32,000 –&gt; 00:03:35,000<br>I don’t think they didn’t mention in that paper, but it’s one of the techniques that they used.</p>
<p>60<br>00:03:35,000 –&gt; 00:03:42,000<br>So this is not a scientific list of what I think the top three optimizations from the previous list.</p>
<p>61<br>00:03:43,000 –&gt; 00:03:45,000<br>These aren’t the ones that I think they’re going to matter most.</p>
<p>62<br>00:03:45,000 –&gt; 00:03:49,000<br>In the context of query execution, these are going to matter, obviously data skipping,</p>
<p>63<br>00:03:49,000 –&gt; 00:03:54,000<br>like if you’re looking for a thing that’s not in any possible block in your zone map can help you</p>
<p>64<br>00:03:54,000 –&gt; 00:03:58,000<br>help you avoid reading any data, like nothing you go faster than that and reading nothing.</p>
<p>65<br>00:03:58,000 –&gt; 00:04:02,000<br>But if you actually do have to run queries, these are the ones I think are going to matter most.</p>
<p>66<br>00:04:02,000 –&gt; 00:04:07,000<br>So we’ll spend a little bit time talking about vectorization, not actually the vectorized algorithms</p>
<p>67<br>00:04:07,000 –&gt; 00:04:10,000<br>to do our query operators that will come in a week.</p>
<p>68<br>00:04:11,000 –&gt; 00:04:18,000<br>Today is really setting up the query processing model so that we can then feed data in such a way that we can vectorize them.</p>
<p>69<br>00:04:18,000 –&gt; 00:04:23,000<br>Task parallelization, we’ll talk about today is basically how to take a query, break it up to disjoint tasks,</p>
<p>70<br>00:04:23,000 –&gt; 00:04:28,000<br>and run them in parallel, with different cores, different threads, different nodes.</p>
<p>71<br>00:04:28,000 –&gt; 00:04:32,000<br>And then code specialization, again, well this will be a big thing starting next week.</p>
<p>72<br>00:04:32,000 –&gt; 00:04:37,000<br>Basically how can we avoid giant switch statements and indirection in our database system</p>
<p>73<br>00:04:38,000 –&gt; 00:04:45,000<br>by having exactly what the query wants to use or the instructions of the query wants to use process the data.</p>
<p>74<br>00:04:45,000 –&gt; 00:04:48,000<br>Again, we’ll see the two ways to do this.</p>
<p>75<br>00:04:50,000 –&gt; 00:04:54,000<br>So at a high level optimization goals are going to be the following three things.</p>
<p>76<br>00:04:54,000 –&gt; 00:05:01,000<br>So in order to get queries run fast, the most obvious thing we can do is just reduce the number of instructions we have to use to execute it.</p>
<p>77<br>00:05:02,000 –&gt; 00:05:07,000<br>We want to use fewer instructions on the CPU to run the query in the same manner work.</p>
<p>78<br>00:05:07,000 –&gt; 00:05:10,000<br>The compiler can help a little bit.</p>
<p>79<br>00:05:10,000 –&gt; 00:05:16,000<br>If you pass in the O2 flag, I don’t know what the equivalent is in Rust, but you pass over to, and GCC and Clang,</p>
<p>80<br>00:05:16,000 –&gt; 00:05:21,000<br>it’ll be more aggressive in trying to optimize things so that you’ll get fewer instructions.</p>
<p>81<br>00:05:21,000 –&gt; 00:05:28,000<br>As far as I know, people particularly don’t ship production databases with O3 enabled, O3 compilations,</p>
<p>82<br>00:05:29,000 –&gt; 00:05:34,000<br>because things can get kind of hairy, but can reorder things in a way that will be incorrect.</p>
<p>83<br>00:05:34,000 –&gt; 00:05:42,000<br>So instead, what we’re going to do is try to design our database system, design our execution engine, just to use fewer instructions.</p>
<p>84<br>00:05:42,000 –&gt; 00:05:48,000<br>If you don’t take my word, you don’t want to ship O3, this is from the Linux mailing list two or three years ago,</p>
<p>85<br>00:05:48,000 –&gt; 00:05:53,000<br>and basically Linus is saying here he thinks O3 is generally unsafe.</p>
<p>86<br>00:05:54,000 –&gt; 00:05:59,000<br>So again, as far as the database systems will ship with O2, enable compilations.</p>
<p>87<br>00:05:59,000 –&gt; 00:06:04,000<br>So after we’ve reduced the instructions that we want to use to execute queries,</p>
<p>88<br>00:06:04,000 –&gt; 00:06:08,000<br>the next thing we can do is try to reduce the number of cycles per instruction.</p>
<p>89<br>00:06:08,000 –&gt; 00:06:13,000<br>The idea here is that when we actually have to execute instructions to run a query,</p>
<p>90<br>00:06:13,000 –&gt; 00:06:20,000<br>we want the data that it’s going to need to operate on to process to be in L1, L2 cache,</p>
<p>91<br>00:06:20,000 –&gt; 00:06:23,000<br>even better would be CPU registers.</p>
<p>92<br>00:06:23,000 –&gt; 00:06:29,000<br>And that means we want to reduce cache misses due to memory lows and stores.</p>
<p>93<br>00:06:29,000 –&gt; 00:06:35,000<br>We want to maximize the locality of the data that we’re going to process on in our operators in our query plan,</p>
<p>94<br>00:06:35,000 –&gt; 00:06:40,000<br>so that they’re going to sit in the CPU caches, and we’ll see how to do this through pipelining,</p>
<p>95<br>00:06:40,000 –&gt; 00:06:46,000<br>and more aggressively with operator fusion in push-based query execution.</p>
<p>96<br>00:06:47,000 –&gt; 00:06:54,000<br>So the weird thing about this, not weird, but the thing why this one’s going to be tricky,</p>
<p>97<br>00:06:54,000 –&gt; 00:06:56,000<br>everyone can reason out this, the first one.</p>
<p>98<br>00:06:56,000 –&gt; 00:06:58,000<br>Run few instructions, don’t do stupid things.</p>
<p>99<br>00:06:58,000 –&gt; 00:07:02,000<br>Don’t make library calls, start computing pived, unnecessarily running a query.</p>
<p>100<br>00:07:02,000 –&gt; 00:07:04,000<br>That’s obviously stupid, but things like that.</p>
<p>101<br>00:07:04,000 –&gt; 00:07:06,000<br>This one’s a bit more tricky.</p>
<p>102<br>00:07:06,000 –&gt; 00:07:10,000<br>And the reason why this can be tricky is because we as humans,</p>
<p>103<br>00:07:10,000 –&gt; 00:07:13,000<br>the way we naturally write system code or code,</p>
<p>104<br>00:07:14,000 –&gt; 00:07:18,000<br>is not going to be always the best way that the CPU actually wants that code,</p>
<p>105<br>00:07:18,000 –&gt; 00:07:20,000<br>or wants to run instructions.</p>
<p>106<br>00:07:20,000 –&gt; 00:07:24,000<br>Because the out-of-order, super-scaler, CPU, which we’ll cover in a second,</p>
<p>107<br>00:07:24,000 –&gt; 00:07:28,000<br>what is ideal for humans for us to reason about and maintain software,</p>
<p>108<br>00:07:28,000 –&gt; 00:07:30,000<br>may actually be the worst thing for the CPU.</p>
<p>109<br>00:07:30,000 –&gt; 00:07:34,000<br>So we’ll have to look at what the algorithms we’re going to use when we run out queries or build our system</p>
<p>110<br>00:07:34,000 –&gt; 00:07:38,000<br>to make sure that we account for what the CPU expects or wants</p>
<p>111<br>00:07:38,000 –&gt; 00:07:41,000<br>and try to design the code in such a way that it generates that for us.</p>
<p>112<br>00:07:42,000 –&gt; 00:07:44,000<br>Because the compiler isn’t always going to magically do that for us.</p>
<p>113<br>00:07:46,000 –&gt; 00:07:48,000<br>And the last one is obvious as well.</p>
<p>114<br>00:07:48,000 –&gt; 00:07:52,000<br>We want to parallelize execution, more as law, it is more or less ending.</p>
<p>115<br>00:07:52,000 –&gt; 00:07:54,000<br>And we’re not getting faster clock speeds,</p>
<p>116<br>00:07:54,000 –&gt; 00:07:57,000<br>although Intel is more recently rationing that up.</p>
<p>117<br>00:07:57,000 –&gt; 00:07:59,000<br>But we’re going to get a lot more courts.</p>
<p>118<br>00:07:59,000 –&gt; 00:08:02,000<br>And the cool thing about this is that on newer CPUs,</p>
<p>119<br>00:08:02,000 –&gt; 00:08:06,000<br>there’s a mix of the high performance cores and the efficiency cores.</p>
<p>120<br>00:08:06,000 –&gt; 00:08:10,000<br>So now we can, in theory, start scheduling things based on one core versus another.</p>
<p>121<br>00:08:11,000 –&gt; 00:08:15,000<br>Then you throw in GPUs and those things have tens of thousands for cores, which is insane.</p>
<p>122<br>00:08:15,000 –&gt; 00:08:20,000<br>So we’re going to cover all of these throughout the lecture.</p>
<p>123<br>00:08:20,000 –&gt; 00:08:24,000<br>Today we’ll talk a little bit about how to do this one, the second one.</p>
<p>124<br>00:08:24,000 –&gt; 00:08:27,000<br>And this last one a little bit.</p>
<p>125<br>00:08:27,000 –&gt; 00:08:30,000<br>The first one we’ll see this when we talk about code specialization,</p>
<p>126<br>00:08:30,000 –&gt; 00:08:33,000<br>query compilation, and precompiled and jitted.</p>
<p>127<br>00:08:35,000 –&gt; 00:08:39,000<br>So just make sure we’re all using the same terminology of vernacular</p>
<p>128<br>00:08:39,000 –&gt; 00:08:43,000<br>when we’re describing queries and what we’re actually going to be executing.</p>
<p>129<br>00:08:43,000 –&gt; 00:08:46,000<br>You can think of a query plan as a DAGO operators.</p>
<p>130<br>00:08:46,000 –&gt; 00:08:50,000<br>So we have a SQL query here, and we converted it into a physical plan.</p>
<p>131<br>00:08:50,000 –&gt; 00:08:54,000<br>So we have scans in the bottom, then our projections feeding to a join,</p>
<p>132<br>00:08:54,000 –&gt; 00:09:01,000<br>followed by a, that’s the projection type, the filters, join, and then projection.</p>
<p>133<br>00:09:01,000 –&gt; 00:09:04,000<br>So these are the operators that we’re going to have in a query plan.</p>
<p>134<br>00:09:05,000 –&gt; 00:09:10,000<br>And then the database system is going to convert them into operator instances</p>
<p>135<br>00:09:10,000 –&gt; 00:09:13,000<br>that are going to be invocations of that operator.</p>
<p>136<br>00:09:13,000 –&gt; 00:09:16,000<br>And the reason why we have the distinguished operator instance and operators</p>
<p>137<br>00:09:16,000 –&gt; 00:09:20,000<br>is because we could have an operator run in parallel.</p>
<p>138<br>00:09:20,000 –&gt; 00:09:23,000<br>This table is the billion tuples.</p>
<p>139<br>00:09:23,000 –&gt; 00:09:27,000<br>I could divide it up this scan operator into 10 operator instances</p>
<p>140<br>00:09:27,000 –&gt; 00:09:32,000<br>that are each going to scan different row groups, different files, and S3.</p>
<p>141<br>00:09:34,000 –&gt; 00:09:38,000<br>And task is going to be a sequence of one or more operator instances.</p>
<p>142<br>00:09:38,000 –&gt; 00:09:41,000<br>And this will basically be the same thing as pipelines, but not always.</p>
<p>143<br>00:09:41,000 –&gt; 00:09:45,000<br>It’s basically, you’re going to recognize that, oh, as soon as I do the scan,</p>
<p>144<br>00:09:45,000 –&gt; 00:09:46,000<br>I immediately want to do the filter.</p>
<p>145<br>00:09:46,000 –&gt; 00:09:49,000<br>So I can combine these two operator instances together in a single task.</p>
<p>146<br>00:09:49,000 –&gt; 00:09:53,000<br>And that’s what’s getting scheduled by the system to run.</p>
<p>147<br>00:09:53,000 –&gt; 00:09:57,000<br>And then a task set will be just a collection of these extracurital tasks</p>
<p>148<br>00:09:57,000 –&gt; 00:10:02,000<br>we could have for this pipeline that we could then ship out to the different course.</p>
<p>149<br>00:10:03,000 –&gt; 00:10:07,000<br>So the pipelines are going to be important part of what we talk about today and go in forward.</p>
<p>150<br>00:10:07,000 –&gt; 00:10:11,000<br>And so again, the pipeline is the boundary in our query plan that specifies</p>
<p>151<br>00:10:11,000 –&gt; 00:10:16,000<br>how much we can process a single tuple or batch of tuples or set of tuples</p>
<p>152<br>00:10:16,000 –&gt; 00:10:21,000<br>up through the query plan to at some point we reach an operator where we need to see</p>
<p>153<br>00:10:21,000 –&gt; 00:10:27,000<br>all the other tuples within our pipeline before we can proceed up into the query plan.</p>
<p>154<br>00:10:28,000 –&gt; 00:10:31,000<br>So in this side here, we’re doing the scan on A, then the filter,</p>
<p>155<br>00:10:31,000 –&gt; 00:10:36,000<br>and assume the build side of the hash join, the build side here is part of this pipeline.</p>
<p>156<br>00:10:36,000 –&gt; 00:10:41,000<br>Like I can’t send any tuple up beyond the join until I see what comes on the other side.</p>
<p>157<br>00:10:41,000 –&gt; 00:10:47,000<br>So assuming I execute pipeline one, again, whether it’s a single task or multiple task running parallel,</p>
<p>158<br>00:10:47,000 –&gt; 00:10:49,000<br>it doesn’t matter at this point.</p>
<p>159<br>00:10:49,000 –&gt; 00:10:51,000<br>And then once that’s complete, I can then run pipeline two.</p>
<p>160<br>00:10:51,000 –&gt; 00:10:53,000<br>And now pipeline two could do the filter.</p>
<p>161<br>00:10:53,000 –&gt; 00:10:56,000<br>So I do the scan on B filter and then do the probe in the hash join.</p>
<p>162<br>00:10:57,000 –&gt; 00:11:03,000<br>And now we know that any tuple that matches in the join can then be pushed up to the projection operator as part of the output.</p>
<p>163<br>00:11:03,000 –&gt; 00:11:12,000<br>I don’t want to start running the join, I start probing the join on this query until I’ve populated everything on the A side</p>
<p>164<br>00:11:12,000 –&gt; 00:11:14,000<br>because otherwise I could have false negatives.</p>
<p>165<br>00:11:14,000 –&gt; 00:11:18,000<br>Now, I’m showing this pipeline all the way up on one side.</p>
<p>166<br>00:11:18,000 –&gt; 00:11:23,000<br>That’s ideally what we’re going to want to do to maximize the reuse of data,</p>
<p>167<br>00:11:23,000 –&gt; 00:11:27,000<br>like to minimize the number of cycles for tuple as we go along.</p>
<p>168<br>00:11:27,000 –&gt; 00:11:34,000<br>But I could have just done the scan on A and then filter it, you know, materialize the output, scan on B, filter it,</p>
<p>169<br>00:11:34,000 –&gt; 00:11:37,000<br>materialize the output, and then have these be two separate pipelines.</p>
<p>170<br>00:11:37,000 –&gt; 00:11:40,000<br>And then a third pipeline could then be, okay, let me actually do the join.</p>
<p>171<br>00:11:40,000 –&gt; 00:11:42,000<br>That actually would be slow.</p>
<p>172<br>00:11:42,000 –&gt; 00:11:49,000<br>Basically, running much of data between these two pipelines, where it’s better off to do a pipeline that tries to get all the way to the top.</p>
<p>173<br>00:11:50,000 –&gt; 00:11:57,000<br>Again, we’ll see why this matters when we start doing operator fusion and other techniques.</p>
<p>174<br>00:11:57,000 –&gt; 00:12:05,000<br>We’re going to allow you to potentially start executing the join on the thread or another CPU core while you’re still executing to filter.</p>
<p>175<br>00:12:05,000 –&gt; 00:12:12,000<br>So you’re saying that couldn’t you start running the join at the same time or are you still scanning A and B?</p>
<p>176<br>00:12:12,000 –&gt; 00:12:20,000<br>Yeah, where there would be more bandwidth, certainly, but like maybe that the whole thing finished faster because we’re taking better advantage of parallel threads.</p>
<p>177<br>00:12:20,000 –&gt; 00:12:26,000<br>So let’s say that the very last tuple, ignore parallel threads, so ignore multiple operating instances.</p>
<p>178<br>00:12:26,000 –&gt; 00:12:32,000<br>One, this thing’s running myself. The very last tuple that you see in A is the only tuple that’s going to match in B.</p>
<p>179<br>00:12:32,000 –&gt; 00:12:39,000<br>But if I start probing into this hash table before I finish scanning A, I could have a false negative because I didn’t put that tuple last tuple in.</p>
<p>180<br>00:12:40,000 –&gt; 00:12:43,000<br>Yeah, you can’t do that. What’s that?</p>
<p>181<br>00:12:45,000 –&gt; 00:12:49,000<br>You’re still building A but you can’t check to see whether something exists until it’s populated.</p>
<p>182<br>00:12:53,000 –&gt; 00:12:56,000<br>That’s his question. Can you start pipeline two before you start pipeline one?</p>
<p>183<br>00:12:56,000 –&gt; 00:13:04,000<br>That would be, again, the problem I said earlier, if I start scanning B, start filtering it, what do I do with that output?</p>
<p>184<br>00:13:05,000 –&gt; 00:13:13,000<br>It’s got to go somewhere. I’m just writing to the disk memory and that’s the pressure for the overall system.</p>
<p>185<br>00:13:13,000 –&gt; 00:13:20,000<br>It’s a better idea to maybe run this in parallel, populate the hash table, then run B.</p>
<p>186<br>00:13:21,000 –&gt; 00:13:31,000<br>Okay. So, lots of stuff today, but I want to first start talking about the paper you guys read.</p>
<p>187<br>00:13:31,000 –&gt; 00:13:48,000<br>It’s an older paper, but it’s very important. It’s very seminal about why the designs of database systems at the time when the paper is written in 2005 are insufficient for, if you want to run OLAP queries,</p>
<p>188<br>00:13:49,000 –&gt; 00:13:55,000<br>high performance OLAP queries. Why am I making you read a paper that is almost old as some of you guys here?</p>
<p>189<br>00:13:55,000 –&gt; 00:14:06,000<br>19 years old now. Because that paper is seminal, meaning every OLAP system that’s around today, for the most part, followed the design guidelines that was laid up by that paper.</p>
<p>190<br>00:14:06,000 –&gt; 00:14:14,000<br>And everything with some exception about the IT name stuff, which you can talk about in a second, the core ideas still matter a lot.</p>
<p>191<br>00:14:15,000 –&gt; 00:14:23,000<br>Then we’ll talk about processing models, the plan processing directions, whether it’s in bottoms up or top down or pushed over to the pool.</p>
<p>192<br>00:14:23,000 –&gt; 00:14:27,000<br>Filter reposition, we’ll talk about that a little bit this, we’ll talk about that when we talk about vectorization next week.</p>
<p>193<br>00:14:27,000 –&gt; 00:14:35,000<br>But basically, when I start applying predicates and I start matching tuples and tuples don’t match, what do I actually store when I go from the operative to the next?</p>
<p>194<br>00:14:35,000 –&gt; 00:14:39,000<br>And then we have time we’ll finish up the different modes of parallel execution.</p>
<p>195<br>00:14:40,000 –&gt; 00:14:48,000<br>The idea is, again, we’re going to talk about how do you architect the system so that you can run these tasks or the operative instances in parallel.</p>
<p>196<br>00:14:48,000 –&gt; 00:15:00,000<br>And then in a few weeks we’ll then cover how do you implement the algorithms that then the implementation itself to do parallel execution, for joins and sorting.</p>
<p>197<br>00:15:01,000 –&gt; 00:15:13,000<br>So again, the the the the the melody BX 100 paper is from 2005 and it’s essentially a low level analysis of the of for in memory for in memory workloads.</p>
<p>198<br>00:15:13,000 –&gt; 00:15:27,000<br>What what are the bottom next you’re going to face when you want to run OLAP queries, right? And the idea the big idea of the thing you break through was they looked at all these existing systems at the time and showed how if you want to run OLAP queries, right,</p>
<p>199<br>00:15:27,000 –&gt; 00:15:40,000<br>running large scans over and doing joins and so forth that there are the existing systems at the time are certainly not well designed for the modern out of order super scalar CPU architectures that Intel was putting out at the time.</p>
<p>200<br>00:15:40,000 –&gt; 00:15:56,000<br>Right. And the idea is that if you can redesign your database, that’s the better target for what the CPU wants from you, what what kind of you know if you design the system itself, how data flows through it, what instructions you’re calling and when, then you get much, much better performance because</p>
<p>201<br>00:15:56,000 –&gt; 00:16:10,000<br>your designing system such a way that the CPU is is happier, right? Instead of you as a programmer trying to make things easier for yourself and it was hard for the CPU, you make life slightly harder for yourself and the CPU can run much, much more efficiently.</p>
<p>202<br>00:16:11,000 –&gt; 00:16:24,000<br>So what happened was the the back on the story is that there was this project out of C2Y where this paper came from for call, Moneady to be the days in the 90s and basically they were doing some experiments on it, they realized that</p>
<p>203<br>00:16:24,000 –&gt; 00:16:44,000<br>oh, the way, you know, the way they’re going to do the processing model and send the entire columns from one operative next, that’s terrible for the CPU and there’s all this in direction things much slower. So they built this X100 prototype, they then spun it out as a startup called vector wise that was acquired by acting in 2010.</p>
<p>204<br>00:16:44,000 –&gt; 00:16:53,000<br>They got then rebranded as vector, which I think it’s a terrible name for a database because you search vector database, you’re not going to get this thing, you’re going to get re-vying, pine cone, all these other ones.</p>
<p>205<br>00:16:54,000 –&gt; 00:17:12,000<br>Then the cloud version of vector wise is now called Adelante. Then acting got bought by, I think Indian holding company HCO two years ago, wish, right? And so it’s still there. Acting is also the original is what ingress became so ingress got bought and sold over the various years.</p>
<p>206<br>00:17:12,000 –&gt; 00:17:26,000<br>And then at some point, it got rebranded to act in and they kept sort of had these older databases. And then vector wise was the sort of the high performance column, it’s column store engine for ingress. And then it got rebranded.</p>
<p>207<br>00:17:27,000 –&gt; 00:17:41,000<br>But anyway, so again, the reason why I can, I had you guys read this paper even though it’s from 2005 is because this is, you know, this is how you want to design a system even today. Now there’s all this other stuff about itanium, which I’m assuming that’s a CPU architecture that no one actually who here is her titanium one.</p>
<p>208<br>00:17:42,000 –&gt; 00:18:09,000<br>It’s basically, it was like, it was another super scaler CPU from Intel in collaboration with HP like the 2000s. It was meant to replace x86, right? But it had like this massive like pipeline. It did things a little bit different than how we, you know, zion’s work today. But it didn’t go anywhere. They coat it off. And so we’ll say this maybe some other papers to this other Intel hardware that people sort of target that doesn’t exist anymore. We don’t care about. But the high level idea is actually, actually matter.</p>
<p>209<br>00:18:09,000 –&gt; 00:18:19,000<br>And again, just to show you that this paper, even though it’s from 2005, it’s still timely. Earlier in this year, at CITER, this paper won the test of time award.</p>
<p>210<br>00:18:19,000 –&gt; 00:18:29,000<br>So because the research community recognized how important this paper was and how this has massive influence in this in the database marketplace for OLEC queries.</p>
<p>211<br>00:18:29,000 –&gt; 00:18:42,000<br>Right, so that’s right there that’s Peter Bonds. That’s the, it’s the guy that, you know, it did the early work of an ADB did the work early working vector wise. Now he’s a technically intern at mother duck, but he did early work on duck DB.</p>
<p>212<br>00:18:42,000 –&gt; 00:18:57,000<br>Nails is the, I think it’s the CEO of the money to be company. That’s Marsins, the Kowski. So after vector wise got bought by acting, he then went and formed snowflake. A lot of the ideas that are in this paper is what snowflakes based on. Right.</p>
<p>213<br>00:18:57,000 –&gt; 00:19:12,000<br>We’ll cover him later. That’s magna pat Helen. She’s a do you know, but he’s at Salesforce. That’s the guy invented a poker. He met a patchy flink. We’re not gonna cover flink, but anyway. Again, this paper is super, super influential.</p>
<p>214<br>00:19:13,000 –&gt; 00:19:26,000<br>All right, so this is sort of a crash course in what CPUs look like and just for like what it matters for us as database people, right. So is there everything you need to know in like two slides at a high level.</p>
<p>215<br>00:19:26,000 –&gt; 00:19:38,000<br>So the, the CPU is basically an order line organized the execution instructions to these pipeline stages and the, the CPU is basic goal is to try to keep this pipeline busy at all times.</p>
<p>216<br>00:19:38,000 –&gt; 00:19:52,000<br>There’s always something to do, right. So that means that if there are instructions that you can’t complete in the single cycle, it’s going to try to keep executing things in the pipeline because it’s, you know, there’s a catch miss. It’s got to fetch the memory. So it’s always going to try to keep executing things.</p>
<p>217<br>00:19:53,000 –&gt; 00:20:08,000<br>And in a super scale CPU architecture, there’s been multiple pipelines running at the same time in parallel. And so they’re going to run slightly out of order, meaning like the instructions may execute differently than the order that they appear in the code.</p>
<p>218<br>00:20:08,000 –&gt; 00:20:21,000<br>But then the CPU, at least in case of Zions, are going to a bunch of extra work to make sure that once you get all the data that you needed, you then check to see whether the output of these adult instructions would have matched the same as it was.</p>
<p>219<br>00:20:22,000 –&gt; 00:20:32,000<br>So that’s the reason why it ran in order, right. AMD is doing such a thing, same thing, like all the super scale CPUs are doing, essentially the same thing. The GPU cores are not.</p>
<p>220<br>00:20:32,000 –&gt; 00:20:41,000<br>Okay, precisely what does it do is super scale, multiple cores, multiple pipelines. So within one core, you’ll have multiple pipelines.</p>
<p>221<br>00:20:41,000 –&gt; 00:20:46,000<br>That’s okay. Yeah. One core. Yes. But then what?</p>
<p>222<br>00:20:46,000 –&gt; 00:20:49,000<br>What CPUs are not.</p>
<p>223<br>00:20:49,000 –&gt; 00:20:58,000<br>So where did you say that? The 90s. Yeah, the 90s. But GPU cores, I don’t think do this as well. Right.</p>
<p>224<br>00:20:58,000 –&gt; 00:21:10,000<br>Right. Because this is actually really complex to do. You’re like basically, it’s the same thing as like optimistic or commercial for transactions. You’re assuming everything’s going to be okay. You let things run sort of specularly.</p>
<p>225<br>00:21:10,000 –&gt; 00:21:18,000<br>And then you just to check at the end did it actually match out. Right. So this is fantastic. Right. Everything works great when when you get it right.</p>
<p>226<br>00:21:18,000 –&gt; 00:21:35,000<br>And if the CPU recognizes that there’s a dependency, like I need to know the output of one instruction, but more to do the next instruction. Or if I do a misprediction. Right. Meaning like there’s an if clause. It sees that and says, you know, a branch construction.</p>
<p>227<br>00:21:35,000 –&gt; 00:21:44,000<br>It sees that and tries to predict which path down the branch you’re going to go. Like if then else, it tries to pick which one you’re going to go and then specically executes whatever things the path that you’re going to take.</p>
<p>228<br>00:21:44,000 –&gt; 00:21:52,000<br>And then if you get it wrong, you have to flush the pipeline and roll everything back and restart. And that’s really expensive.</p>
<p>229<br>00:21:52,000 –&gt; 00:22:02,000<br>So again, these stairs can occur in the two ways I just said about. So one is dependencies, right. If one instruction depends on the output of another instruction, then you can’t.</p>
<p>230<br>00:22:02,000 –&gt; 00:22:07,000<br>You can’t immediately put that in the same pipeline. Right. You get the stall and wait. Yes.</p>
<p>231<br>00:22:07,000 –&gt; 00:22:14,000<br>So one pipeline corresponds to one or because you think cycle, you take a one or four items from each pipeline.</p>
<p>232<br>00:22:14,000 –&gt; 00:22:21,000<br>There’s one pipeline card that’s going to each one core. No, every quarter, multiple pipelines.</p>
<p>233<br>00:22:21,000 –&gt; 00:22:24,000<br>The pipelines are short. I think like.</p>
<p>234<br>00:22:24,000 –&gt; 00:22:31,000<br>I think the latest young are like 20 instructions on a pipeline. We’re not like the.</p>
<p>235<br>00:22:31,000 –&gt; 00:22:37,000<br>And the X100 paper, they talk about how like I think the penny and four is one of them have like 31 instructions.</p>
<p>236<br>00:22:37,000 –&gt; 00:22:42,000<br>It was insane. They’re now more reasonable. But again, if you still have one of these.</p>
<p>237<br>00:22:42,000 –&gt; 00:22:52,000<br>If it predict something wrong, you got to flush the pipeline unduced off and install until you bring back instructions that you should have executed.</p>
<p>238<br>00:22:52,000 –&gt; 00:23:08,000<br>And so in the case of this first one, dependencies, this could occur when there is, you know, if we’re scanning a tuple and we need to store the data in an output buffer before we can execute anything else, like we need to know what the result of that computation was before we can go on to do the next.</p>
<p>239<br>00:23:08,000 –&gt; 00:23:10,000<br>Yes.</p>
<p>240<br>00:23:10,000 –&gt; 00:23:21,000<br>So, uh, it says that the first one says, then it cannot be pushed immediately into the same pipeline. No, because you can’t like.</p>
<p>241<br>00:23:21,000 –&gt; 00:23:28,000<br>You basically have to wait until he’s figured out what’s going to happen. Then you put it in. Yeah, but I actually I don’t know whether you could have like.</p>
<p>242<br>00:23:28,000 –&gt; 00:23:30,000<br>Here’s the pipeline. I’m really running. But here’s the pipeline.</p>
<p>243<br>00:23:30,000 –&gt; 00:23:37,000<br>As soon as I find out with the first one, then I can run the other one. I don’t know if it does that. I don’t like it does.</p>
<p>244<br>00:23:37,000 –&gt; 00:23:49,000<br>The second one is for branch prediction. And this is basically means that like if, as I said, there’s an if clause or some kind of conditional statement, it’s going to try to predict what you’re going to do.</p>
<p>245<br>00:23:49,000 –&gt; 00:23:58,000<br>And this part is super sophisticated in CPUs, AMD and Intel. Like this is like the secret sauce of the CPUs and they don’t share what exactly the branch predictor is actually doing.</p>
<p>246<br>00:23:58,000 –&gt; 00:24:04,000<br>Right, because this entails those, you know, it’s very sophisticated. Actually, they all are.</p>
<p>247<br>00:24:04,000 –&gt; 00:24:15,000<br>So again, the idea is that if we build our system in such a way that we have a lot of conditionals, we may end up making things worse for us because.</p>
<p>248<br>00:24:15,000 –&gt; 00:24:24,000<br>So if you think of like your scanning data, you don’t actually know what path you’re going to take because it’s going to depend on what the query is, like the conditionals or the the wear clause, depend on what your data looks like.</p>
<p>249<br>00:24:24,000 –&gt; 00:24:33,000<br>So there’s no way that easy way to really predict for every single query what path you’re going to take down for different conditions.</p>
<p>250<br>00:24:33,000 –&gt; 00:24:41,000<br>So for this last one here, we’ll talk a little bit about how to how to fix it. So again, because we have these long pipelines, we’re trying to expect it to be branches.</p>
<p>251<br>00:24:41,000 –&gt; 00:24:53,000<br>So once the stall, the, sorry, what’s the hide these long stalls between spending instructions and going fetching things from, you know, L3 cash, real to cash into our registers, right.</p>
<p>252<br>00:24:53,000 –&gt; 00:25:00,000<br>The way the one spot of the data system that’s that’s this is going to come up a lot is just the basic filter operations when we do sequential scans.</p>
<p>253<br>00:25:00,000 –&gt; 00:25:09,000<br>As I said, because it’s going to depend on the, the filter basically conditional, like where something, you know, equals something, that’s a niff clause.</p>
<p>254<br>00:25:09,000 –&gt; 00:25:16,000<br>And whether or not that that predicate is going to be able to true depends on the data.</p>
<p>255<br>00:25:16,000 –&gt; 00:25:30,000<br>Right. So this is nearly impossible for, for, you know, even us as the database system, we’re actually running the code to predict let alone the CPU because the CPU doesn’t know anything about, you know, what a database system is or what a query is.</p>
<p>256<br>00:25:30,000 –&gt; 00:25:36,000<br>And I think it may know a compiler hint can potentially use to resolve this.</p>
<p>257<br>00:25:36,000 –&gt; 00:25:42,000<br>I don’t know what had to do with Ross, but in C++, there’s something in the standard.</p>
<p>258<br>00:25:42,000 –&gt; 00:25:55,000<br>You can call likely and unlikely. So they have these, these, these compiler directors where you can specify whether a, a conditional clause is going to be our code path is going to be likely or unlikely.</p>
<p>259<br>00:25:55,000 –&gt; 00:26:01,000<br>Right. C doesn’t have this. They, I think they avoided this. Again, I don’t know whether Russ has this.</p>
<p>260<br>00:26:01,000 –&gt; 00:26:11,000<br>And this, this is, I did some quick searching to see what systems actually supported click house has this. I know ductybeat does not because they’re trying to be portable postgres has this.</p>
<p>261<br>00:26:11,000 –&gt; 00:26:22,000<br>Right. But the, these are not hints actually to the CPU. You can’t tell the CPU, like the print certificate, hey, I’m lucky to go down this path. Right.</p>
<p>262<br>00:26:22,000 –&gt; 00:26:38,000<br>I think Intel had had some capabilities in the early days to do this, but 2006 they took it out. Right. So this is, this is just a hint to the compiler for it to potentially reorganize your code itself so that the likely path is maybe at the top of something. Right.</p>
<p>263<br>00:26:39,000 –&gt; 00:26:51,000<br>And then if you read this blog article from a, I think is a compiler engineer at Intel, he basically says don’t use this. It’s going to make some kinds of action make things worse and it’s not often going to actually make things better for you. Right.</p>
<p>264<br>00:26:52,000 –&gt; 00:26:59,000<br>So I think the interesting thing we take click house, take all these systems that do use this. Like because they’re all just pound of fines. So you just hide it.</p>
<p>265<br>00:26:59,000 –&gt; 00:27:06,000<br>You see whether that can make a difference. I have no idea what it is. Okay. It’s unstable. Okay. Yes.</p>
<p>266<br>00:27:06,000 –&gt; 00:27:08,000<br>I know it’s easy.</p>
<p>267<br>00:27:08,000 –&gt; 00:27:12,000<br>Yes.</p>
<p>268<br>00:27:12,000 –&gt; 00:27:17,000<br>But it’s not, but again, it doesn’t help the CPU.</p>
<p>269<br>00:27:17,000 –&gt; 00:27:21,000<br>So it’s not a good thing to do.</p>
<p>270<br>00:27:21,000 –&gt; 00:27:33,000<br>I mean for a six or is it like for like embedded? It’s for what’s that? It’s for a six.</p>
<p>271<br>00:27:33,000 –&gt; 00:27:38,000<br>So that’s like specialized hardware. I have no idea. I’m for Zion’s. This doesn’t do it.</p>
<p>272<br>00:27:38,000 –&gt; 00:27:50,000<br>I’m sorry for the CPU. I mentioned FPGA. So you’re saying for the network hardware stuff or for the network boxes. They take heavy advantage of this.</p>
<p>273<br>00:27:50,000 –&gt; 00:27:56,000<br>But again, I’m telling you can’t you can’t tell the CPU that. Yeah, so I don’t know what they’re doing.</p>
<p>274<br>00:27:56,000 –&gt; 00:28:02,000<br>No, it’s a compiler director. It’s up to the couple. Oh, sure.</p>
<p>275<br>00:28:02,000 –&gt; 00:28:17,000<br>It’s just changing the order of the assembly so that the likely pass are closer to the top when you go into the conditional.</p>
<p>276<br>00:28:17,000 –&gt; 00:28:22,000<br>Again, I don’t know what this is custom hardware or not. Like, yeah.</p>
<p>277<br>00:28:22,000 –&gt; 00:28:26,000<br>I don’t know what this matters for data systems. Not every system needs to use that.</p>
<p>278<br>00:28:26,000 –&gt; 00:28:30,000<br>Post-cars, look how stars and a few others do. Yes.</p>
<p>279<br>00:28:30,000 –&gt; 00:28:36,000<br>Are there ever any architectures for you? But like give it into the branch of the machine?</p>
<p>280<br>00:28:36,000 –&gt; 00:28:45,000<br>Yeah. They’re used to be an opcode that you could you could tell the CPU. But that was like 2005 and earlier.</p>
<p>281<br>00:28:45,000 –&gt; 00:28:55,000<br>Intel supported it back in the day, but not now.</p>
<p>282<br>00:28:55,000 –&gt; 00:28:58,000<br>Because people just like your stupid with it too.</p>
<p>283<br>00:28:58,000 –&gt; 00:29:06,000<br>Also, it’s a little bit more like how you actually want to see if you want to see if you can set the line to know that that sequence all that I’ve been talking about.</p>
<p>284<br>00:29:06,000 –&gt; 00:29:09,000<br>That’s that’s been designed for 20 30 years.</p>
<p>285<br>00:29:09,000 –&gt; 00:29:12,000<br>It really we are figuring out which one is the case.</p>
<p>286<br>00:29:12,000 –&gt; 00:29:15,000<br>So when you do this, you kind of mess with that.</p>
<p>287<br>00:29:15,000 –&gt; 00:29:21,000<br>But let’s look at a case where like, like, even if you had it, it wouldn’t help.</p>
<p>288<br>00:29:21,000 –&gt; 00:29:27,000<br>So let’s have a simple query here. Select start from table or key is greater than low value and key less than high value.</p>
<p>289<br>00:29:27,000 –&gt; 00:29:33,000<br>So this is how you probably write this code. I don’t want to show code in slides, but this is simple enough. I think you guys can get it.</p>
<p>290<br>00:29:33,000 –&gt; 00:29:37,000<br>This is how you write this code in bus top or a basic implementation.</p>
<p>291<br>00:29:37,000 –&gt; 00:29:46,000<br>You have a four-lip iterates over to every two-pool in the table. You go grab the key, then you have the if clause, if key, greater than low and key less than high, then you copy it in the output buffer.</p>
<p>292<br>00:29:46,000 –&gt; 00:29:50,000<br>Then you iterate whatever the buffer offset by one.</p>
<p>293<br>00:29:50,000 –&gt; 00:29:56,000<br>So what’s the problem for the CPU in this code?</p>
<p>294<br>00:29:56,000 –&gt; 00:29:59,000<br>The if clause.</p>
<p>295<br>00:29:59,000 –&gt; 00:30:03,000<br>So you can rewrite this to not do any branching.</p>
<p>296<br>00:30:03,000 –&gt; 00:30:09,000<br>The if clause is going to find a branch and the CPU is going to try to predict, am I going to go down this path or not?</p>
<p>297<br>00:30:09,000 –&gt; 00:30:18,000<br>So you can write it like this as a branchless version, where the very first thing you do, as you scan the table, you immediately copy the two-pool in the output buffer.</p>
<p>298<br>00:30:18,000 –&gt; 00:30:23,000<br>You don’t check to see whether it’s going to satisfy the conditional clause.</p>
<p>299<br>00:30:23,000 –&gt; 00:30:37,000<br>Then you have this clause here where you then check the low and high, but these are returnary operations where it’s going to return 1 or 0, you end the bits together, and then that tells you whether the delta is 0 or 1.</p>
<p>300<br>00:30:37,000 –&gt; 00:30:44,000<br>And then in that case, if it’s 0, then you would loop back around and just overwrite the last thing you copied.</p>
<p>301<br>00:30:44,000 –&gt; 00:30:47,000<br>And again, we said all our columns are fixed length.</p>
<p>302<br>00:30:47,000 –&gt; 00:30:53,000<br>So I don’t have to worry about, you know, am I going to overflow the buffer or underflow it based on, you know, 1 to 1 of the next.</p>
<p>303<br>00:30:53,000 –&gt; 00:30:57,000<br>I just take the bits, plop it down, and it overwrite the previous one.</p>
<p>304<br>00:30:57,000 –&gt; 00:31:04,000<br>Now I’m missing a little piece at the end where it says, okay, if I jump out of it, when it come out of the for loop, is the last thing was the last delta 0, 1.</p>
<p>305<br>00:31:04,000 –&gt; 00:31:09,000<br>And make sure I don’t include that as the output, right?</p>
<p>306<br>00:31:09,000 –&gt; 00:31:13,000<br>So this seems bizarre, right? As humans, we were like, this seems super wasteful.</p>
<p>307<br>00:31:13,000 –&gt; 00:31:17,000<br>You’re copying every single time. It’s surely that’s more expensive than the, than the if clause, right?</p>
<p>308<br>00:31:17,000 –&gt; 00:31:22,000<br>But it’s not because the CPU knows how to, you know, this is just deterministic, straight line code.</p>
<p>309<br>00:31:22,000 –&gt; 00:31:26,000<br>It can rip through that way, way faster than the branched-miss prediction.</p>
<p>310<br>00:31:26,000 –&gt; 00:31:27,000<br>Yes.</p>
<p>311<br>00:31:27,000 –&gt; 00:31:40,000<br>So is it question is, oh, is there any advantage of using turneries versus like, I mean, the compiler might just convert it into, to ones and zeros anyway.</p>
<p>312<br>00:31:40,000 –&gt; 00:31:41,000<br>If you use bollions.</p>
<p>313<br>00:31:41,000 –&gt; 00:31:42,000<br>Right.</p>
<p>314<br>00:31:42,000 –&gt; 00:31:45,000<br>I’m saying white is the turner.</p>
<p>315<br>00:31:45,000 –&gt; 00:31:49,000<br>Is there any reason you’re on this?</p>
<p>316<br>00:31:49,000 –&gt; 00:31:53,000<br>Like, like, just move everything between greater than low.</p>
<p>317<br>00:31:53,000 –&gt; 00:32:00,000<br>If I did a key greater than, and greater than low and key less than high, 1 or 0.</p>
<p>318<br>00:32:00,000 –&gt; 00:32:08,000<br>Yeah, just, yeah, you could, you could, the compiler, this part I’m not worried about, the compiler, the compiler to fix that.</p>
<p>319<br>00:32:08,000 –&gt; 00:32:10,000<br>Okay, I was going to think that, no.</p>
<p>320<br>00:32:10,000 –&gt; 00:32:14,000<br>The thing I, again, I’m trying to iterate, it’s like, you’re always copying. That matters.</p>
<p>321<br>00:32:14,000 –&gt; 00:32:17,000<br>Right.</p>
<p>322<br>00:32:17,000 –&gt; 00:32:22,000<br>So the, again, you think there’s be, there’s be terrible because the CPU is blindly copying.</p>
<p>323<br>00:32:22,000 –&gt; 00:32:24,000<br>But it actually helps.</p>
<p>324<br>00:32:24,000 –&gt; 00:32:31,000<br>So this is a, this is from the vector wise people, but this is a few years later in, I think, 2013 or 2012.</p>
<p>325<br>00:32:31,000 –&gt; 00:32:41,000<br>This is showing you, for the two different purchases I showed, like, here’s performance you give and you get as you vary the selectivity of that, of that wear clause.</p>
<p>326<br>00:32:41,000 –&gt; 00:32:50,000<br>So if, if no two pulls are matching on the, on the side here, up to roughly about, you know, 5% selectivity, right.</p>
<p>327<br>00:32:50,000 –&gt; 00:32:56,000<br>The branching case is actually faster because the CPU is going to say, oh, not going to match, not going to match over and over again, right.</p>
<p>328<br>00:32:56,000 –&gt; 00:33:01,000<br>And the, avoiding that extra copy cost every single time is way faster.</p>
<p>329<br>00:33:01,000 –&gt; 00:33:08,000<br>The red line essentially flat because you’re just doing the same work every single time, no matter whether the two pulls going to satisfy the product or not.</p>
<p>330<br>00:33:08,000 –&gt; 00:33:16,000<br>But then you see this, this nice little, this arch here, where, you know, the height, sorry, the top is roughly roughly about 50%.</p>
<p>331<br>00:33:16,000 –&gt; 00:33:24,000<br>And at this point, it’s a flip at the coin every single time the CPU is predicting just getting it wrong over and over again.</p>
<p>332<br>00:33:24,000 –&gt; 00:33:28,000<br>And then, again, it becomes more selective the CPU can, can figure it out better.</p>
<p>333<br>00:33:28,000 –&gt; 00:33:33,000<br>So I had students reproduce this graph. I don’t have it in the slides.</p>
<p>334<br>00:33:33,000 –&gt; 00:33:41,000<br>Basically, six years ago, five, five, six years ago, we basically saw the same thing on newer CPUs, right.</p>
<p>335<br>00:33:41,000 –&gt; 00:33:50,000<br>So again, the, the, this is just showing you again, which seems like bizarre thing or weight well thing for humans to do is actually going to be better for the CPU.</p>
<p>336<br>00:33:50,000 –&gt; 00:33:58,000<br>In the case here, we’re counting CPU cycles for two pull, right. It’s not, not exactly runtime.</p>
<p>337<br>00:33:58,000 –&gt; 00:34:03,000<br>So in terms of how to produce instructions, and again, we’ll talk more about this about the semester.</p>
<p>338<br>00:34:03,000 –&gt; 00:34:17,000<br>But the idea here is that we want to specialize our database system, data systems code so that when we operate on data, we know exactly the data type, the size, and then whatever it is, the operation that we want to do on it.</p>
<p>339<br>00:34:17,000 –&gt; 00:34:28,000<br>And so we don’t have to have these giant switch statements as says, if my data type is in 32 versus in 64 or float, whatever, then you know, here’s my instructions to do addition or subtraction, whatever it is.</p>
<p>340<br>00:34:28,000 –&gt; 00:34:40,000<br>Likewise, I don’t want to have to traverse the expression tree when I have my wear clause, predicate to say, you know, is it greater than or less than, again, which is usually implicate implemented as giant switch statements.</p>
<p>341<br>00:34:40,000 –&gt; 00:34:47,000<br>So we want to avoid all of that as much as possible and just have exactly the code we want to run during our query.</p>
<p>342<br>00:34:47,000 –&gt; 00:34:55,000<br>Because again, now there’s no conditionals, there’s no branching, we’re just giving the CPU, we’re just feeding it, here’s the exact instructions we want you to execute over and over again.</p>
<p>343<br>00:34:55,000 –&gt; 00:35:05,000<br>So an example of doing this wrong, or not wrong, but like, well, wrong in the context of the worst way to do it for a modern CPU is you can look at Postgres numeric type.</p>
<p>344<br>00:35:06,000 –&gt; 00:35:15,000<br>And so it’s this function to add two numerics together. And so what do we see? We see a bunch of these if clauses to check to see whether it’s a positive number or a negative number, whether it’s not a number.</p>
<p>345<br>00:35:15,000 –&gt; 00:35:22,000<br>And then we have this giant switch statement here that you have to deal with all the different variations of how to do the addition.</p>
<p>346<br>00:35:22,000 –&gt; 00:35:30,000<br>Again, this is just adding two numerics together. If I have a billion numbers in my column and try to add it to another billion numbers, then I’m going to execute the instructions over and over again.</p>
<p>347<br>00:35:31,000 –&gt; 00:35:33,000<br>This will be terrible for a modern CPU.</p>
<p>348<br>00:35:33,000 –&gt; 00:35:36,000<br>Is it the same thing as the matching of the Rust?</p>
<p>349<br>00:35:36,000 –&gt; 00:35:40,000<br>The same thing as the matching of the Rust. It’s not like a pilot time thing, right?</p>
<p>350<br>00:35:50,000 –&gt; 00:35:52,000<br>It doesn’t matter, it’s a jump table, yes, but…</p>
<p>351<br>00:35:53,000 –&gt; 00:35:54,000<br>It jumps.</p>
<p>352<br>00:35:54,000 –&gt; 00:35:57,000<br>Jumps, that’s bad. Jumps are bad. Yeah.</p>
<p>353<br>00:35:58,000 –&gt; 00:36:01,000<br>Function calls are bad, jumps are bad, right?</p>
<p>354<br>00:36:01,000 –&gt; 00:36:06,000<br>But we do need to verify the different types. If you don’t have Roger, I’m not sure how you read it.</p>
<p>355<br>00:36:06,000 –&gt; 00:36:09,000<br>His question is, like, you do need to get the types. Yes.</p>
<p>356<br>00:36:09,000 –&gt; 00:36:13,000<br>So, if you had read it without this switch, I’m not sure how you do that.</p>
<p>357<br>00:36:13,000 –&gt; 00:36:16,000<br>Because you need to do this stuff that they want to prepare.</p>
<p>358<br>00:36:17,000 –&gt; 00:36:22,000<br>Ignore numerics, right? Because we know how to optimize this.</p>
<p>359<br>00:36:22,000 –&gt; 00:36:29,000<br>But just think of, like, if you had a number plus a number, so, like, is it in 32, is it in 64,</p>
<p>360<br>00:36:29,000 –&gt; 00:36:32,000<br>and you would have to have different branches for all those.</p>
<p>361<br>00:36:32,000 –&gt; 00:36:35,000<br>But because we’re at SQL to clear the language, we have a catalog.</p>
<p>362<br>00:36:35,000 –&gt; 00:36:42,000<br>We know exactly the data types. If we set up the system in such a way that we know exactly the instructions we want to execute,</p>
<p>363<br>00:36:42,000 –&gt; 00:36:48,000<br>we can design things ahead of time and be wave-hastic.</p>
<p>364<br>00:36:48,000 –&gt; 00:36:53,000<br>And whether or not we pre-compile the primitives we want to use to operate on different data types,</p>
<p>365<br>00:36:53,000 –&gt; 00:36:56,000<br>or we just in time compile it, we’ll cover that later.</p>
<p>366<br>00:37:00,000 –&gt; 00:37:08,000<br>All right. So, now with all that in mind, now we want to talk about how we want to design the execution in this processing model</p>
<p>367<br>00:37:09,000 –&gt; 00:37:14,000<br>to then lead us to the path of enlightenment or whatever you want to call it, of being able to achieve those three goals.</p>
<p>368<br>00:37:15,000 –&gt; 00:37:23,000<br>So, this would be a somewhat of a review from the intro class, but I’m going to go a bit more detail of how the system is actually going to operate</p>
<p>369<br>00:37:23,000 –&gt; 00:37:32,000<br>beyond what we covered in the intro class. And then that’ll segue into discussing the direction of how we move data between different operators.</p>
<p>370<br>00:37:33,000 –&gt; 00:37:39,000<br>So, the processing model is going to find essentially how the data system is going to execute a query plan,</p>
<p>371<br>00:37:39,000 –&gt; 00:37:51,000<br>meaning how it’s going to tell what operator to run next, and then where that operator is going to send data to, or where that operator is going to get data from, so to speak.</p>
<p>372<br>00:37:52,000 –&gt; 00:37:57,000<br>And there’s a different trade-offs we have for OTP systems, O-Lap systems, O-Lap systems, O-Lap workloads, and O-Lap workloads.</p>
<p>373<br>00:37:58,000 –&gt; 00:38:08,000<br>And we’ll see how, in the case of the volcano model, the iterator model, that’s the default choice of most data systems like the row stores, and that’s great for OTP, but it’s not going to be so great for O-Lap.</p>
<p>374<br>00:38:10,000 –&gt; 00:38:18,000<br>So, every processing model is going to be defined in terms of execution paths. And the two types of execution paths we can have are the control flow and the data flow.</p>
<p>375<br>00:38:18,000 –&gt; 00:38:24,000<br>So, the control flow is going to be how the data system is going to tell an operator or an operator instance, okay, now it’s your turn to run.</p>
<p>376<br>00:38:25,000 –&gt; 00:38:32,000<br>And then the data flow is going to specify for each operator instance, where does it send data to, and where is it getting data from.</p>
<p>377<br>00:38:33,000 –&gt; 00:38:42,000<br>And so, the output of these operators can either be whole tuples, in the case of the row store, or a subset of columns, and in our case, well, we’re going to care about in the O-Lap world is going to be NSMs.</p>
<p>378<br>00:38:43,000 –&gt; 00:38:50,000<br>And what we can ignore late materialization, whether or not it’s the all of the columns or subset of columns, we can worry about that later.</p>
<p>379<br>00:38:51,000 –&gt; 00:38:58,000<br>So, the three processing models that we’re going to care about are iterators, materialization, or the Clomenar one from the X100 paper.</p>
<p>380<br>00:38:59,000 –&gt; 00:39:08,000<br>And then that’ll lead to, again, the vectorized model. And this will always be this last one here as well, every O-Lap system, except for a few exceptions, or can implement this approach.</p>
<p>381<br>00:39:08,000 –&gt; 00:39:12,000<br>Because it’s sort of the best of both worlds of the iterator model and materialization model.</p>
<p>382<br>00:39:13,000 –&gt; 00:39:33,000<br>So, the iterator model, or call us a called the volcano model, or the pipeline model, I’ll often probably just say volcano model, is, again, is basically how every database system up until the monidv paper or the vectorized paper you guys read, this is how pretty much everyone implemented their query processing model.</p>
<p>383<br>00:39:34,000 –&gt; 00:39:41,000<br>So, in your source code and your system, you’re going to have all your operator implementations, and each one is going to provide this next function.</p>
<p>384<br>00:39:42,000 –&gt; 00:40:01,000<br>And so, what’s going to happen is every time you want to get a tuple from an operator on the control flow path, you would then invoke the next function on that operator, who then we respond for producing either a single tuple or some kind of end of file or null marker to say, I have no more tuples, never ask me for more data.</p>
<p>385<br>00:40:03,000 –&gt; 00:40:12,000<br>And you can sort of think of, like, within that operator, it’s just going to be a for loop that’s going to retrieve all the tuples that it needs from a, from its child operators.</p>
<p>386<br>00:40:12,000 –&gt; 00:40:26,000<br>And depending on whether it’s a pipeline breaker or not, it, either gets all the data from its children, if it is a pipeline breaker, or it can just get one tuple, and it longs it satisfies whatever that operator wants to do in it, and can boost the output, then it’s done.</p>
<p>387<br>00:40:26,000 –&gt; 00:40:40,000<br>So, you sort of think of, like, in the way you actually implement this, you would have these open and closed functions on the operator, is like, constructors or deconstructors for your operators, and then, you open it, call next next on it, get all the output you want, and then when it says I’m done, then you call closed, and that cleans everything up.</p>
<p>388<br>00:40:41,000 –&gt; 00:40:51,000<br>So, this is a high level example, so we want to join R and S, and we have a join conditional on RID, SID, and then we have additional wear clause, where S value is greater than 100.</p>
<p>389<br>00:40:52,000 –&gt; 00:41:08,000<br>So, you could sort of think of, like, each of these operator can have these, these implementations, as I said, they’re going to be basically just four loops that’s going to make calls to its children operators to pull data up or move data up, and then produce output if when it’s available.</p>
<p>390<br>00:41:08,000 –&gt; 00:41:13,000<br>So, you can think of these, these blocks, blocks a code here, these are all the next functions.</p>
<p>391<br>00:41:14,000 –&gt; 00:41:30,000<br>So, we’re going to start off, the database says, okay, I want to run this query, assume we’re going from the top to the bottom, we’ll be called next on the first operator here, and then immediately inside that, we have this for loop that says call next on my child operator.</p>
<p>392<br>00:41:31,000 –&gt; 00:41:44,000<br>So, that is the blocking call, so the control flow would move from the top operator to the second operator here, for the do the hash join, where immediately inside that one, we have a for loop that says call next on the child because it wants to build the hash table.</p>
<p>393<br>00:41:44,000 –&gt; 00:41:57,000<br>So, then the control flow takes us down here, and now we’re just iterating over the table R, and we’re calling a mid, which is the return control or sending one to pull back up to calling function.</p>
<p>394<br>00:41:57,000 –&gt; 00:42:06,000<br>And so, there’s some state inside of these operators that keeps track of the last time you called next, here’s where I was, here’s where my cursor was when I was scanning the table.</p>
<p>395<br>00:42:06,000 –&gt; 00:42:15,000<br>So, we’re going to keep calling next on the bottom operator here until we get all the data until we get into file, and at this point here we know our hash table has all the two pulls we need.</p>
<p>396<br>00:42:15,000 –&gt; 00:42:17,000<br>So, again, we won’t want how many false negatives.</p>
<p>397<br>00:42:17,000 –&gt; 00:42:34,000<br>So, then I go down to the next block in my operator, and I’m calling next on my child, on the right child, so I come down this side, same things, scan over R, pass it up to the next operator, it applies the predicate, if that gets, that gets, that gets, a saddest predicate, then it sends it up to do the probe.</p>
<p>398<br>00:42:34,000 –&gt; 00:42:38,000<br>So, you sort of, dayd you chain these things up, up like this.</p>
<p>399<br>00:42:38,000 –&gt; 00:42:52,000<br>So, again, going back to the notion of pipelines, sort of think of this block three, and the first half of block two, that’s pipeline, pipeline one, and then this, five, four, the bottom two, and one, that’s pipeline two.</p>
<p>400<br>00:42:52,000 –&gt; 00:43:03,000<br>So, I XU pipeline one, ideally, and then, sorry, in this case here, there’s pipeline boundaries, but you’re not doing any optimizations, right?</p>
<p>401<br>00:43:03,000 –&gt; 00:43:10,000<br>Because implicitly, the code is set up, so you know that you can’t run the second pipeline until the first part is actually done.</p>
<p>402<br>00:43:10,000 –&gt; 00:43:25,000<br>So, again, this will differ when we see the operator fusion technique or the push-based approach, because they’re actually going to try to combine these within a single operative instance, and not have these, you know, calling next within themselves.</p>
<p>403<br>00:43:25,000 –&gt; 00:43:32,000<br>So, that’s it, this is pretty much what everybody implements in the first database systems that they build, or a Rostrord systems.</p>
<p>404<br>00:43:32,000 –&gt; 00:43:34,000<br>Bust-Tob is based on this, yes.</p>
<p>405<br>00:43:34,000 –&gt; 00:43:40,000<br>And that’s the biggest light, wouldn’t you want to build a hash table on the table that has the filter?</p>
<p>406<br>00:43:40,000 –&gt; 00:43:44,000<br>The next question is, should you want to build a hash table, or the one that has the filter?</p>
<p>407<br>00:43:44,000 –&gt; 00:43:54,000<br>You don’t know, right? First of all, this is a logical diagram, it’s a pipeline slide. I don’t have stats here, but what if this thing is like one-tubal?</p>
<p>408<br>00:43:54,000 –&gt; 00:44:01,000<br>So, anyway.</p>
<p>409<br>00:44:01,000 –&gt; 00:44:06,000<br>So, the iterative model is very important to the lens.</p>
<p>410<br>00:44:06,000 –&gt; 00:44:14,000<br>Alpo control is really easy for this, because if you know you’ve got enough tuples that’s the output, you just stop calling next, and you finish.</p>
<p>411<br>00:44:14,000 –&gt; 00:44:21,000<br>The downside though is that we’re basically mixing control flow with data flow, and going back quickly.</p>
<p>412<br>00:44:21,000 –&gt; 00:44:35,000<br>There’s no way to say, okay, I don’t want to execute this thing anymore, because I’ve got enough data, or stop executing certain parts, because I’m calling next to get things up.</p>
<p>413<br>00:44:35,000 –&gt; 00:44:43,000<br>If I call next on this, implicitly, because I call next on its children, because that’s sort of how the query plan has been set up.</p>
<p>414<br>00:44:43,000 –&gt; 00:44:49,000<br>That’s how the iterative model allows you to control the behavior of the execution.</p>
<p>415<br>00:44:49,000 –&gt; 00:45:04,000<br>It’s going to also do pipelining. Again, the idea here is that we want to have, for every single tuple that we get from a child operator, we want to do as much processing as we can up the query plan, until we get a pipeline breaker or a producer.</p>
<p>416<br>00:45:04,000 –&gt; 00:45:16,000<br>The idea is there, we can achieve good cache locality, because we bring a tuple in, and we do as much work as we can, whilst in memory, before we go off to the next tuple.</p>
<p>417<br>00:45:16,000 –&gt; 00:45:24,000<br>Until we hit a pipeline breaker, which again, we know we can’t have an operator complete until we get all its children to emit their tuples.</p>
<p>418<br>00:45:24,000 –&gt; 00:45:33,000<br>The build side of joins, we have this problem, subquaries depending on whether to get rewritten or not, and to join to have this problem in order by sorting, obviously, has this problem.</p>
<p>419<br>00:45:33,000 –&gt; 00:45:38,000<br>Some aggregations, min and max, right? Same thing.</p>
<p>420<br>00:45:38,000 –&gt; 00:45:53,000<br>The downside, though, is that you’re basically calling next for every single tuple. If I have a billion tuples, calling a billion function calls, it’s called next, times the number of whatever operators that I have, or number of tuples that are being sent up.</p>
<p>421<br>00:45:53,000 –&gt; 00:46:08,000<br>So, an alternative approach that was pioneered by MoNetB in the late 90s, early 2000s, was to do what is called the materialization model, where every operator produces all its output, all at once.</p>
<p>422<br>00:46:08,000 –&gt; 00:46:16,000<br>Any time you call it next on it, it generates all the output, and then hands that off to the next operator. So once you call it next, you never go back and ask for more data for it.</p>
<p>423<br>00:46:16,000 –&gt; 00:46:24,000<br>The idea is to call materialization model, because you materialize each operator, materializing all its output as a single result.</p>
<p>424<br>00:46:24,000 –&gt; 00:46:30,000<br>And again, the output could either be a single row, a single column, or the entire table. Yes.</p>
<p>425<br>00:46:30,000 –&gt; 00:46:35,000<br>Why is my break a little basically materialization? Like, because the end of materializing the entire result?</p>
<p>426<br>00:46:35,000 –&gt; 00:46:42,000<br>The statement is, pipeline breakers are essentially the same thing as pipeline breakers, because you materialize the entire result.</p>
<p>427<br>00:46:42,000 –&gt; 00:46:52,000<br>Not necessarily, because I could call it next on a pipeline breaker.</p>
<p>428<br>00:46:52,000 –&gt; 00:47:01,000<br>Well, you’re like, yes, I could produce all the output, and then something’s going to feed into that pipeline breaker result.</p>
<p>429<br>00:47:01,000 –&gt; 00:47:10,000<br>I could just get a single tuple. I wouldn’t pass, because it’s still like one to about a time called next. I wouldn’t have this giant output get shoved all the way up, all of a sudden switched the processing model.</p>
<p>430<br>00:47:10,000 –&gt; 00:47:15,000<br>But to your point, like the pipeline breaker, it has like, you’re materializing all the results at that point there. Yes.</p>
<p>431<br>00:47:15,000 –&gt; 00:47:22,000<br>Yes. But in the case of materialization model, every next call moves the entire result always up.</p>
<p>432<br>00:47:22,000 –&gt; 00:47:30,000<br>So it’s not that simple as saying that the majority of the components are saying, every operator has a pipeline break. It’s not as simple as just saying that.</p>
<p>433<br>00:47:30,000 –&gt; 00:47:37,000<br>It’s not as simple as saying in materialized model that every operator is a pipeline breaker.</p>
<p>434<br>00:47:37,000 –&gt; 00:47:46,000<br>That’s okay. Yeah, that’s fine. Yeah, it makes sense. Yeah. You wouldn’t describe it that way, though, but it makes sense. Yes.</p>
<p>435<br>00:47:46,000 –&gt; 00:47:51,000<br>So let’s see how to go back to our query head before.</p>
<p>436<br>00:47:51,000 –&gt; 00:47:57,000<br>And again, now in our operator implementations, now we see that like again, we have this output buffer.</p>
<p>437<br>00:47:57,000 –&gt; 00:48:03,000<br>And we just keep adding tuples and then there’s a return clause where we can all the output goes up to the next guy.</p>
<p>438<br>00:48:03,000 –&gt; 00:48:14,000<br>So just like before, we started the top call, the root operator calls child.out output, calls this guy, who then has to build a hash table, calls down to the scan on R.</p>
<p>439<br>00:48:14,000 –&gt; 00:48:20,000<br>Again, we populate the entire output buffer and then show the result up.</p>
<p>440<br>00:48:20,000 –&gt; 00:48:29,000<br>And again, if I have a billion tuples, even if I want one column for a billion tuples, my output buffer is going to be a billion tuples in this approach.</p>
<p>441<br>00:48:29,000 –&gt; 00:48:39,000<br>And then again, same thing. I go down the up, sorry, all the tuples, I go down the other side. I call the filter, which then calls the scan on S.</p>
<p>442<br>00:48:39,000 –&gt; 00:48:45,000<br>And then same thing, the data gets moved up like this.</p>
<p>443<br>00:48:45,000 –&gt; 00:48:58,000<br>So an obvious optimization here is that for this side of the query plan, I’m scanning S and then materialize the result and then neatly handed off to a filter operator, who then basically throws through and starts to step away.</p>
<p>444<br>00:48:58,000 –&gt; 00:49:06,000<br>So an obvious optimization is to inline these two, or fuse them together so that as you scan S, then you validate the predicate.</p>
<p>445<br>00:49:06,000 –&gt; 00:49:10,000<br>And then if that evites true, then you put them in the output.</p>
<p>446<br>00:49:10,000 –&gt; 00:49:20,000<br>Again, you could do that branchless, taking we saw before. I’m showing you the if calls, but you could do that thing with that optimization to solve.</p>
<p>447<br>00:49:20,000 –&gt; 00:49:30,000<br>So this is great for OTP, because in that world, the queries are accessing the single tuple. So even though you’re materializing the entire result, it’s going to be one tuple.</p>
<p>448<br>00:49:30,000 –&gt; 00:49:38,000<br>And then it’s just one less next call to go before you get the end of file. You know, you got everything you would ever need for an operator and you can move one.</p>
<p>449<br>00:49:38,000 –&gt; 00:49:44,000<br>You have to do that inline and you make sure you’re not passing up more data than you actually need, but it works great.</p>
<p>450<br>00:49:44,000 –&gt; 00:49:49,000<br>And then when we built HStore that became voltage B, this is, we use this approach.</p>
<p>451<br>00:49:49,000 –&gt; 00:50:04,000<br>But I would argue, and the paper you read argues that this is bad for OLAB, because you know, you may be coalescing or turning on a bunch of data on higher parts of the query plan, but you’re moving these large, large columns from one operator to the next.</p>
<p>452<br>00:50:04,000 –&gt; 00:50:11,000<br>So it’s great that again, you have fewer next calls, but you’re moving data more data than you potentially actually need.</p>
<p>453<br>00:50:12,000 –&gt; 00:50:22,000<br>So the vectorization model is an obvious optimization, or it’s obvious now, but at the time it wasn’t, that’s sort of getting the best of both worlds, right?</p>
<p>454<br>00:50:22,000 –&gt; 00:50:33,000<br>You’re still going to have this next call that’s going to move tuples up or move tuples from one operator to the next, but instead of moving a single tuple as you would an iterator model, you’re going to move a batch of tuples or a vector tuples.</p>
<p>455<br>00:50:34,000 –&gt; 00:50:40,000<br>Again, the naming is bad, because you say like, oh, it’s the vectorized query processing.</p>
<p>456<br>00:50:40,000 –&gt; 00:50:47,000<br>Now with vector databases that people may think you’re doing, like sending embeddings or something like that, which we’re not doing.</p>
<p>457<br>00:50:47,000 –&gt; 00:50:50,000<br>So we’re going to make a batch of tuples instead of a single tuple.</p>
<p>458<br>00:50:50,000 –&gt; 00:50:59,000<br>And then we’re going to have our operator, like the loops themselves, be designed to operate on these batches of tuples at a time.</p>
<p>459<br>00:51:00,000 –&gt; 00:51:06,000<br>And the size of the batch can vary depending on what the data looks like, what the query actually wants to do, or what the harbor looks like.</p>
<p>460<br>00:51:06,000 –&gt; 00:51:09,000<br>I think of the paper, they were talking about 1024.</p>
<p>461<br>00:51:09,000 –&gt; 00:51:13,000<br>That’s usually roughly what I think most systems are using.</p>
<p>462<br>00:51:13,000 –&gt; 00:51:18,000<br>Some of them might be a bit smaller. We’ll see examples of that later on.</p>
<p>463<br>00:51:18,000 –&gt; 00:51:28,000<br>And again, the batches are either going to be one column or subset of columns based on whether or not you’re doing latent materialization or not, or about, you’ve already done projections on it.</p>
<p>464<br>00:51:29,000 –&gt; 00:51:38,000<br>So if we go back to our query one more time, now within our implementations, we still have an output buffer.</p>
<p>465<br>00:51:38,000 –&gt; 00:51:48,000<br>We’re going to add things to it, but now we’re going to have this conditional clause as when we’ve accumulated enough tuples for the size that’s expected for our vectors, then we can emit it up or send it up.</p>
<p>466<br>00:51:48,000 –&gt; 00:51:56,000<br>So same thing we called that before, fill up our vector output buffer, and then we use that to send it up.</p>
<p>467<br>00:51:56,000 –&gt; 00:52:00,000<br>Send a tuple batch, and then same thing down on the other side here. Yes.</p>
<p>468<br>00:52:06,000 –&gt; 00:52:15,000<br>His question is, or statement is, if I’m at the end of R, and if the size of the output buffer is less than N, but if I’m done, then yeah, you send it up.</p>
<p>469<br>00:52:15,000 –&gt; 00:52:18,000<br>Yes, so you would have a little thing outside the full cost. Yes.</p>
<p>470<br>00:52:18,000 –&gt; 00:52:27,000<br>And then you just, let’s talk about this in a second, but you basically keep track of like, okay, here’s the rows that are actually active, and whether you use that bitmaps or offsets, we’ll see that in a second.</p>
<p>471<br>00:52:33,000 –&gt; 00:52:43,000<br>So, this is the vectorized preprocessing model is what every modern OLAP system is going to use today.</p>
<p>472<br>00:52:44,000 –&gt; 00:52:58,000<br>And it’s because it’s greatly reduced the number of next calls we have to have per operator, and it’s going to allow a out of order CPU to be efficiently execute our operators over batch of tuples, assuming we’ve designed our system in a way to operate on these vectors.</p>
<p>473<br>00:52:59,000 –&gt; 00:53:07,000<br>So again, in the authors of the paper talked about they could have called it the array processing model, but that’s essentially what it comes down to what you’re doing.</p>
<p>474<br>00:53:08,000 –&gt; 00:53:18,000<br>These batches of tuples are just arrays, and then within each operator, as you get the input vector from your child, now you have a for loop overgoing over arrays.</p>
<p>475<br>00:53:18,000 –&gt; 00:53:25,000<br>And that’s the ideal scenario for out of order super scalars CPUs. They law processing arrays, right?</p>
<p>476<br>00:53:25,000 –&gt; 00:53:31,000<br>So you can do all of the specs, the execution tricks we talked about before.</p>
<p>477<br>00:53:31,000 –&gt; 00:53:34,000<br>You can do vectorization with SIMD, which we’ll see more about next week, right?</p>
<p>478<br>00:53:34,000 –&gt; 00:53:46,000<br>All these things we can take advantage of because we know we’re doing the same operation opener again within a tight kernel on the data that’s going to the same type, same length for the most part, ignoring strings.</p>
<p>479<br>00:53:46,000 –&gt; 00:53:49,000<br>But like, we can crush that, yes.</p>
<p>480<br>00:53:49,000 –&gt; 00:54:08,000<br>I’ll try to understand the distinction between materialized vectors, because in both cases it seems like you are sending a bunch of data to see what’s the fundamental difference between materialization model and vectorized vectors.</p>
<p>481<br>00:54:08,000 –&gt; 00:54:25,000<br>That like, the size of the output isn’t everything, so that we can take advantage of pipelining for our vectors because we’re taking batches of tuples in sort of digestible bytes.</p>
<p>482<br>00:54:25,000 –&gt; 00:54:31,000<br>Right? It’s having this whole thing, a process that process it for the entire operator, then move into the next operator.</p>
<p>483<br>00:54:31,000 –&gt; 00:54:42,000<br>I can have this like pipeline execution where I’m just taking some vector tuples and ripping through it and only going back to the next vector either when I reach my final output or all of the tuples got thrown away or something like that.</p>
<p>484<br>00:54:42,000 –&gt; 00:54:49,000<br>So I can materialize your savings to limit to say I don’t even want so many tuples, but you’re understanding what’s the important sort of?</p>
<p>485<br>00:54:49,000 –&gt; 00:55:00,000<br>So in case of materialization model, you can use limit to say I don’t want certain tuples, but like oftentimes that limit clause is applied near the root of the query plant.</p>
<p>486<br>00:55:00,000 –&gt; 00:55:06,000<br>Like give me the top 10 accounts based on some number, some column.</p>
<p>487<br>00:55:06,000 –&gt; 00:55:09,000<br>I got to sort them before I can get the top 10.</p>
<p>488<br>00:55:09,000 –&gt; 00:55:15,000<br>So that means that even though I’m going to throw away most of the data in materialization model, I got to pump a lot of that data up.</p>
<p>489<br>00:55:15,000 –&gt; 00:55:27,000<br>You say okay, vectorization model, you still have to do that, right? But like the implementations of the operators themselves can deal with these chunks of data that can fit in your L3, L2 caches.</p>
<p>490<br>00:55:27,000 –&gt; 00:55:32,000<br>You’re not dealing with this giant blob of data that’s going to have a bunch of cache cache misses.</p>
<p>491<br>00:55:32,000 –&gt; 00:55:33,000<br>Yes?</p>
<p>492<br>00:55:33,000 –&gt; 00:55:44,000<br>Can you verify what the different type of control flow is?</p>
<p>493<br>00:55:44,000 –&gt; 00:55:50,000<br>So this question is what’s the difference between control flow and data flow?</p>
<p>494<br>00:55:50,000 –&gt; 00:56:08,000<br>So control flow again is how the part of the data system that says okay time to execute this query, how it tells an operator to say start running.</p>
<p>495<br>00:56:08,000 –&gt; 00:56:17,000<br>And in the case of, in all the purchases I’m showing here, we’re using top the bottom. So we’re calling next and that’s the control flow.</p>
<p>496<br>00:56:17,000 –&gt; 00:56:22,000<br>So we call next on the top operator, it then calls next on this child operator, that’s the control flow.</p>
<p>497<br>00:56:22,000 –&gt; 00:56:26,000<br>And then the data flow is where the data is moving back.</p>
<p>498<br>00:56:26,000 –&gt; 00:56:40,000<br>We’ll see a better distinction when we talk about pushers as pull. In the pushers as pull, the execution of one operator versus the next operator is not embedded in the execution of another operator.</p>
<p>499<br>00:56:40,000 –&gt; 00:56:46,000<br>That we can then say okay now it’s time to execute this pipeline and that’s, there’s an asset that makes that call.</p>
<p>500<br>00:56:46,000 –&gt; 00:56:50,000<br>And then that operator then does not, or pipeline does not call the another pipeline.</p>
<p>501<br>00:56:50,000 –&gt; 00:56:57,000<br>There’s something else that centralize, that’s managing all that. Where is it? Go ahead, yes.</p>
<p>502<br>00:56:57,000 –&gt; 00:57:01,000<br>So the two different options for control flow is like a plug and pull.</p>
<p>503<br>00:57:01,000 –&gt; 00:57:14,000<br>Yeah, yes, but like you wouldn’t say it that way, right? That you’d have a different approach to doing control flow if you’re doing a push versus approach versus a pull.</p>
<p>504<br>00:57:14,000 –&gt; 00:57:16,000<br>Okay, we’ll come to that in a second.</p>
<p>505<br>00:57:16,000 –&gt; 00:57:20,000<br>And what are the different options for data for the two bull versus bull?</p>
<p>506<br>00:57:20,000 –&gt; 00:57:24,000<br>Yeah, so to point its questions, what’s the different option for data flow?</p>
<p>507<br>00:57:24,000 –&gt; 00:57:28,000<br>You can think of the pricing processing model as defining that as well.</p>
<p>508<br>00:57:28,000 –&gt; 00:57:41,000<br>The pushers as pull is part of that as well. But like, yeah, the data flow is at a single two bull, all the two bulls or a batch of two bulls.</p>
<p>509<br>00:57:41,000 –&gt; 00:57:51,000<br>So another great thing to also do because again, we have these tight kernels that are processing these batches of two bulls.</p>
<p>510<br>00:57:51,000 –&gt; 00:58:00,000<br>All the instructions that were going to execute for that, which is inside that kernel, every single iteration is going to be in our instruction cache.</p>
<p>511<br>00:58:00,000 –&gt; 00:58:02,000<br>That’s going to be super fast.</p>
<p>512<br>00:58:03,000 –&gt; 00:58:15,000<br>We’ll have very few data dependency control dependencies because we don’t need to see the output of another two bull within our batch to determine what the next thing we need to execute.</p>
<p>513<br>00:58:15,000 –&gt; 00:58:22,000<br>It’s not always entirely true, but like you can, you know, you, you, in most cases, this will be the case.</p>
<p>514<br>00:58:22,000 –&gt; 00:58:28,000<br>Whether or not a predicate, a two bull, predicate doesn’t matter whether the last two bulls said it had that predicate or not.</p>
<p>515<br>00:58:28,000 –&gt; 00:58:33,000<br>Again, not always true when the functions complicate things, but we can ignore that.</p>
<p>516<br>00:58:33,000 –&gt; 00:58:40,000<br>Again, we’ll see this next week. The great thing about having these type four loops is over arrays, that’s what the CPU wants.</p>
<p>517<br>00:58:40,000 –&gt; 00:58:43,000<br>That’s what the compiler wants to be able to vectorize this using SIMD.</p>
<p>518<br>00:58:43,000 –&gt; 00:58:47,000<br>We’ll see how to explicitly do that in next week.</p>
<p>519<br>00:58:48,000 –&gt; 00:58:54,000<br>So this is from the, from Peter Bonso’s slide from the, when he won the, the test of time award for this paper.</p>
<p>520<br>00:58:54,000 –&gt; 00:59:07,000<br>But in the discussion of why, why they saw the vectorized model be so much faster than the iterator of the volcano model or the materialization model that was used in the NDB.</p>
<p>521<br>00:59:08,000 –&gt; 00:59:12,000<br>In case the volcano model, the interpretation overhead will cover later on, that’s precognitive.</p>
<p>522<br>00:59:12,000 –&gt; 00:59:17,000<br>But now you don’t have this, you know, per two bull navigation of them calling next and next or every single two bull.</p>
<p>523<br>00:59:17,000 –&gt; 00:59:27,000<br>It’s now, you know, if I’m, if I’m back, you know, 1024 size batches, I’m just reducing the number of calls by 1024. It’s pretty significant.</p>
<p>524<br>00:59:27,000 –&gt; 00:59:33,000<br>In the case of MnDB, we’ll see more later on, but the query plans we want much simpler because it’s just like it was an iterator model,</p>
<p>525<br>00:59:34,000 –&gt; 00:59:41,000<br>except now you’re passing batches of two bulls. Whereas in MnDB, they were sort of keeping track of implicitly, here’s all the columns I’m passing around.</p>
<p>526<br>00:59:41,000 –&gt; 00:59:53,000<br>And it was, it was way more complicated. And then all the optimizations you get from the compiler or SIMD, that’s just, you know, it’s, it’s, it’s, it’s, it’s in addition to all the other things just by designing this in itself,</p>
<p>527<br>00:59:53,000 –&gt; 01:00:00,000<br>to pass around batches of data, things run faster, but then oh by the way, the compiler can also rip through it much, much better as well.</p>
<p>528<br>01:00:04,000 –&gt; 01:00:19,000<br>So leading to his question about push versus pull, but in all the examples that it showed, there was this next function, you know, whether it’s vectorized, materialized or, or iterator, there’s this next call that I’m making on the per operator, right?</p>
<p>529<br>01:00:19,000 –&gt; 01:00:24,000<br>And I’m always starting at the top, calling the root and going down and, and bringing things up.</p>
<p>530<br>01:00:24,000 –&gt; 01:00:35,000<br>And again, this is how most systems are going to implement X to change, but it isn’t the only way. And this gets into the distinction of, of, of top to bottom, the pole based approach to the bottom top.</p>
<p>531<br>01:00:35,000 –&gt; 01:00:50,000<br>So again, the top to bottom is what I just showed, you want to start exceeding the query, you call next on the root, and then that would then call next on its children, propagate down to the pooling data from the bottom of the query plan, up to the top to the root, and that produces the final output.</p>
<p>532<br>01:00:51,000 –&gt; 01:01:06,000<br>And the, you’re always going to be calling, you know, next to get the next tuple, unless it’s a pipeline breaker, because that’ll stage data in a, in a sort of, in a result that you then go access, but you’re always passing tuples by calling, passing data by calling next.</p>
<p>533<br>01:01:06,000 –&gt; 01:01:12,000<br>And that, that’s going to be a function call, that’s a jump instruction, and again, that’s bad for a super scaler CPU.</p>
<p>534<br>01:01:13,000 –&gt; 01:01:35,000<br>Alternate approach is the push based model, where you start with the leaf notes in the query plan, and you have some outside controller or a scheduler, you know, initiate the invocation of that, of that, that operator, the pipeline, and then it’s going to take the data that it generates and push it to the next operator.</p>
<p>535<br>01:01:36,000 –&gt; 01:01:37,000<br>Yes.</p>
<p>536<br>01:01:38,000 –&gt; 01:01:41,000<br>How long do I mean doing a few? How does that, we’ll get that in a second. Yes.</p>
<p>537<br>01:01:42,000 –&gt; 01:01:51,000<br>So, this is rare, this is probably more common now, but again, when this paper, the paper you guys are in 2005, this approach didn’t exist.</p>
<p>538<br>01:01:51,000 –&gt; 01:01:58,000<br>This shows up in a paper that you’re starting to read in a few weeks from the Germans, and this is called Hyper, the dude’s insane.</p>
<p>539<br>01:01:58,000 –&gt; 01:02:09,000<br>It’s a one person author in paper, he invented, or didn’t invent, but he showed how to query, just kind of query compilation with the LLVM in Hyper, plus also he invents the push based model in the paper as well.</p>
<p>540<br>01:02:09,000 –&gt; 01:02:15,000<br>And he’s got three cleats, he teaches two classes a semester, and he doesn’t do drugs, it’s insane.</p>
<p>541<br>01:02:15,000 –&gt; 01:02:16,000<br>Right?</p>
<p>542<br>01:02:16,000 –&gt; 01:02:20,000<br>He’s the exact opposite, he’s very straight list.</p>
<p>543<br>01:02:20,000 –&gt; 01:02:22,000<br>Anyway, so let’s see how to do this.</p>
<p>544<br>01:02:22,000 –&gt; 01:02:37,000<br>Okay, so here’s our same query I have before, and now here’s our two pipelines, but now instead of a bunch of different operators that will have to implement the individual, blocks of code that implement the individual operators, now we’re just going to have two for loops.</p>
<p>545<br>01:02:37,000 –&gt; 01:02:44,000<br>Right? And so for the first first pipeline, we’re going to scan R and then populate the hash table.</p>
<p>546<br>01:02:44,000 –&gt; 01:02:54,000<br>But in the second pipeline, we’re going to scan S, and now you can see where that we’re going to try to do is ride every single two pull all the way up to the top of the query plan.</p>
<p>547<br>01:02:54,000 –&gt; 01:03:01,000<br>Before we go back and look at the next two pull or the batch of two pulls, even though I’m showing this opportunity single two pull, you could do this on a batch as well.</p>
<p>548<br>01:03:01,000 –&gt; 01:03:11,000<br>Right? So for every two pull and S, then you’ve got to the predicate, if that matches, then you probably hash table, then if that matches, then you put it as the output.</p>
<p>549<br>01:03:11,000 –&gt; 01:03:18,000<br>Yes. This is fusion right here. How did that even end up happening? What do you have to happen?</p>
<p>550<br>01:03:18,000 –&gt; 01:03:28,000<br>In the sense that you want your faculty to be an operator, so would you be hard code every single combination of all pages?</p>
<p>551<br>01:03:28,000 –&gt; 01:03:38,000<br>Okay, so his question is basically, how do you do this? Would you have to hard code every single possible combination of query plans?</p>
<p>552<br>01:03:38,000 –&gt; 01:03:45,000<br>I’m going to execute this. No. So how would you have that? Two weeks.</p>
<p>553<br>01:03:45,000 –&gt; 01:03:57,000<br>The answer is going to be, you know, just in time compile this, you really generate the code on the fly for the query plan that fuses this together, then compile it with the LLVM or GCC or Clang, then run that.</p>
<p>554<br>01:03:57,000 –&gt; 01:04:06,000<br>That’s approach number one. Approach number two is that you recognize, I only have so many data types in my database system, and there’s only so many things I could do to them.</p>
<p>555<br>01:04:06,000 –&gt; 01:04:21,000<br>So each of these are just functions, right? And I just put them in an array, and I execute one by one. That’s what vector wise does. That’s code specialization. Give me two weeks.</p>
<p>556<br>01:04:21,000 –&gt; 01:04:30,000<br>Okay, what do that? Right? Yeah. Your mind looks blown, in fact you can compile this on the fly. Yeah, this is what they do. They’re German, right?</p>
<p>557<br>01:04:30,000 –&gt; 01:04:46,000<br>This is good too. This is good too. So this is hard, right? Like, I don’t get to it. It’s even crazier than that. The new version, like in this version, they will, in hyper, they would generate the LLVM IR than compile that.</p>
<p>558<br>01:04:46,000 –&gt; 01:04:56,000<br>Single-store will generate C code, then compile that, at least the version. In the latest version, the new system, they’re being called Umbra, he doesn’t generate IR, he generates literally x86 assembly.</p>
<p>559<br>01:04:56,000 –&gt; 01:05:07,000<br>Like C++ macros, then he runs that through the assembler. Then on the background, he’s running the LLVM, compiles the assembly code into a shared object, and when that’s done, he then links it in.</p>
<p>560<br>01:05:07,000 –&gt; 01:05:25,000<br>Yeah, German, yes. So when you’re telling the language of the child, in each of those, like, the fixed number of functions, you have compile and compile an array, but these are function pointers, and when you, like, when you want to use them together, like, finding this function a, function p, and then you, like, rearrange everything in your array, so you can actually get that word.</p>
<p>561<br>01:05:25,000 –&gt; 01:05:37,000<br>Yes, the question is, the way you want to do this, if there are a bunch of, function pointers, would it be a bunch of arrays where I’m putting in, I need to do this, the file like this, file like this, and then would you just, invoke this function pointer, as you go along? Yes.</p>
<p>562<br>01:05:37,000 –&gt; 01:05:45,000<br>And that, that would suck if you’re doing it on a per-tubal basis. But if you do batches of tuples in the vectorized model, then that amortizes the jump call.</p>
<p>563<br>01:05:45,000 –&gt; 01:05:50,000<br>And then now you don’t have giant switch statements of, like, what branch should I go down? Yes.</p>
<p>564<br>01:05:50,000 –&gt; 01:06:02,000<br>So, I can’t you do this in a pool-based model? Good question. I mean, totally the fusion part.</p>
<p>565<br>01:06:02,000 –&gt; 01:06:18,000<br>I mean, at a high level, is this the same, at a high level, is it the same, more or less, yes, right?</p>
<p>566<br>01:06:18,000 –&gt; 01:06:26,000<br>Because this, like, you couldn’t say, okay, do the scan, then, you know, the next call does this, the next call does that, right?</p>
<p>567<br>01:06:26,000 –&gt; 01:06:34,000<br>But again, in the pool-based approach, the way the software is actually engineered and designed, the abstraction is through these next functions.</p>
<p>568<br>01:06:34,000 –&gt; 01:06:42,000<br>So, if could you take a pool-based model and then co-genit to turn it into this? Yes.</p>
<p>569<br>01:06:42,000 –&gt; 01:06:47,000<br>You said this is the main reason for you to buy a new pool, or other other factors that I’m not seeing.</p>
<p>570<br>01:06:47,000 –&gt; 01:06:50,000<br>So, the control flow matters too, right?</p>
<p>571<br>01:06:50,000 –&gt; 01:06:58,000<br>So, like, when I actually get x-tute this, there’s some outside scan, which says, okay, run this, when this populates the hash table, right?</p>
<p>572<br>01:06:58,000 –&gt; 01:07:03,000<br>And then you can then also specify where the output’s going and say some output buffer, and then that this thing’s going to know about.</p>
<p>573<br>01:07:03,000 –&gt; 01:07:06,000<br>When that’s done, then I schedule the next one, and it produces the output, right?</p>
<p>574<br>01:07:06,000 –&gt; 01:07:11,000<br>So, you have a complete control over everything. Yes.</p>
<p>575<br>01:07:11,000 –&gt; 01:07:12,000<br>Yes.</p>
<p>576<br>01:07:12,000 –&gt; 01:07:17,000<br>And also, you can get that great with using, like, some sort of pushing, but they didn’t go, frankly.</p>
<p>577<br>01:07:17,000 –&gt; 01:07:20,000<br>How does the schedule specify where it’s going to be?</p>
<p>578<br>01:07:20,000 –&gt; 01:07:27,000<br>Like, you need to admit, like, those they are, like, physically, feeds into two different pipelines, so these are the two different ways.</p>
<p>579<br>01:07:27,000 –&gt; 01:07:32,000<br>So, the question is, how, if Arnysa go to two different places, how do we handle that?</p>
<p>580<br>01:07:32,000 –&gt; 01:07:36,000<br>Yeah, Arnysa, is the schedule worth all of us? Or is it just the reason it’s free to use the…</p>
<p>581<br>01:07:36,000 –&gt; 01:07:44,000<br>Like, so, the way would be, the schedule, something before we start executing would specify where this output’s going to go.</p>
<p>582<br>01:07:44,000 –&gt; 01:07:48,000<br>So, it’s got to go to locations, you tell it, by the way, send it to locations.</p>
<p>583<br>01:07:48,000 –&gt; 01:07:57,000<br>And it can either be, like, the operator itself, the execution, could be responsible for sending it exactly to the location needs to go to,</p>
<p>584<br>01:07:57,000 –&gt; 01:08:04,000<br>or you could have, like, a shuffle service, which we’ll cover later, this says, like, okay, well, I don’t exactly where to get it to where it needs to go,</p>
<p>585<br>01:08:04,000 –&gt; 01:08:11,000<br>but if I know a sentence to this other service, it will then distribute it for me.</p>
<p>586<br>01:08:11,000 –&gt; 01:08:15,000<br>Okay, I don’t want to get too far ahead of the… Like…</p>
<p>587<br>01:08:15,000 –&gt; 01:08:17,000<br>Yeah, yeah.</p>
<p>588<br>01:08:17,000 –&gt; 01:08:22,000<br>Yeah. Stuff is really cool. But not everyone does exactly the…</p>
<p>589<br>01:08:22,000 –&gt; 01:08:29,000<br>Again, as I said before, the co-genic this on the fly is going to be hard to maintain.</p>
<p>590<br>01:08:30,000 –&gt; 01:08:32,000<br>The Germans can do it for you others can.</p>
<p>591<br>01:08:32,000 –&gt; 01:08:33,000<br>And they’re…</p>
<p>592<br>01:08:33,000 –&gt; 01:08:34,000<br>You also make it easy to do?</p>
<p>593<br>01:08:34,000 –&gt; 01:08:35,000<br>No. No.</p>
<p>594<br>01:08:35,000 –&gt; 01:08:36,000<br>No.</p>
<p>595<br>01:08:36,000 –&gt; 01:08:37,000<br>No.</p>
<p>596<br>01:08:37,000 –&gt; 01:08:42,000<br>Okay, I’ll leave a little mystery, right?</p>
<p>597<br>01:08:42,000 –&gt; 01:08:43,000<br>We’ll see why.</p>
<p>598<br>01:08:43,000 –&gt; 01:08:46,000<br>And then, beyond, we implemented this here, right?</p>
<p>599<br>01:08:46,000 –&gt; 01:08:52,000<br>We implemented it twice, and even the second time was meant to be easier for other people to use, and it was still…</p>
<p>600<br>01:08:52,000 –&gt; 01:08:55,000<br>No, it was not impossible, it’s harder.</p>
<p>601<br>01:08:56,000 –&gt; 01:08:59,000<br>All right, so again, these are just reiterating what I’ve already said.</p>
<p>602<br>01:08:59,000 –&gt; 01:09:01,000<br>Top to bottom, we have complete control.</p>
<p>603<br>01:09:01,000 –&gt; 01:09:04,000<br>We control the output via limit.</p>
<p>604<br>01:09:04,000 –&gt; 01:09:10,000<br>And basically, the child operator has a block until the parent operator blocks until the child comes back with the results.</p>
<p>605<br>01:09:10,000 –&gt; 01:09:12,000<br>And the…</p>
<p>606<br>01:09:12,000 –&gt; 01:09:18,000<br>Again, the next functions aren’t cheap because there’s tension going to be virtual function in low-cups in C++.</p>
<p>607<br>01:09:18,000 –&gt; 01:09:22,000<br>Because I’m stitching together this query plan with pointers.</p>
<p>608<br>01:09:22,000 –&gt; 01:09:28,000<br>And then, at runtime, I have to do the virtual function to look up and say, okay, what is the actual location of the function?</p>
<p>609<br>01:09:28,000 –&gt; 01:09:31,000<br>I went execute for that given…</p>
<p>610<br>01:09:31,000 –&gt; 01:09:33,000<br>For that given child operator.</p>
<p>611<br>01:09:33,000 –&gt; 01:09:38,000<br>And then, of course, these next calls are going to be jumps, and that’s going to suck for us in the CPU.</p>
<p>612<br>01:09:38,000 –&gt; 01:09:44,000<br>In the case of the bottom of the top, you can have tighter control of the caches and registers in the pipelines.</p>
<p>613<br>01:09:44,000 –&gt; 01:09:51,000<br>In the case of the hyperpaper, again, not only are they going to keep data in L1 cache, they’re going to keep in CPU registers.</p>
<p>614<br>01:09:51,000 –&gt; 01:09:54,000<br>They can’t go faster than that.</p>
<p>615<br>01:09:54,000 –&gt; 01:10:03,000<br>So, the only challenge is that, in some cases, you may not have complete control of limiting the size of an output buffer.</p>
<p>616<br>01:10:03,000 –&gt; 01:10:08,000<br>Because you have no way to…</p>
<p>617<br>01:10:08,000 –&gt; 01:10:15,000<br>In the case of the next call, if I got enough data at the top of my query plan, then I don’t call next anymore.</p>
<p>618<br>01:10:15,000 –&gt; 01:10:26,000<br>But in the case of the push-based model, even though I may be still sending the entire batches, instead of all the output, I may get more data in a batch than I actually want.</p>
<p>619<br>01:10:26,000 –&gt; 01:10:35,000<br>Yes?</p>
<p>620<br>01:10:35,000 –&gt; 01:10:44,000<br>So, the only benefit you get from having Apple control at the top is through limits.</p>
<p>621<br>01:10:44,000 –&gt; 01:10:47,000<br>I think that’s true, yes.</p>
<p>622<br>01:10:47,000 –&gt; 01:10:51,000<br>When no functions…</p>
<p>623<br>01:10:51,000 –&gt; 01:10:55,000<br>Superficially, yes, but I met you wrong.</p>
<p>624<br>01:10:55,000 –&gt; 01:10:59,000<br>In the case of push-based, it’s actually tricky to do also sort merge.</p>
<p>625<br>01:10:59,000 –&gt; 01:11:02,000<br>You need two iterators at the same time, and you have to keep the extra statement.</p>
<p>626<br>01:11:02,000 –&gt; 01:11:06,000<br>Not impossible, you can do it. It’s a little bit more tricky.</p>
<p>627<br>01:11:06,000 –&gt; 01:11:12,000<br>Because it’s not that nested for loops ripping through a single two-board within a batch.</p>
<p>628<br>01:11:12,000 –&gt; 01:11:13,000<br>Yes?</p>
<p>629<br>01:11:13,000 –&gt; 01:11:19,000<br>If we’re doing push-based, is there still a distinction between iterators and the parallelization?</p>
<p>630<br>01:11:19,000 –&gt; 01:11:26,000<br>If we’re doing push-based, is there still a distinction between two-board-a-time, entire output versus the entire…</p>
<p>631<br>01:11:26,000 –&gt; 01:11:28,000<br>Is this a vector?</p>
<p>632<br>01:11:28,000 –&gt; 01:11:29,000<br>Absolutely, yes.</p>
<p>633<br>01:11:29,000 –&gt; 01:11:34,000<br>So, my example here, I’m iterating over a single two-pole, and I call it eValPredicate.</p>
<p>634<br>01:11:34,000 –&gt; 01:11:40,000<br>And again, assuming that they’re function pointers, I’m jumping every single time to eValPredicate for one two-pole.</p>
<p>635<br>01:11:40,000 –&gt; 01:11:49,000<br>What you could do is pass a batch of tuples, call this vectorized version of eValPredicate, and then it gets a batch of processes of those.</p>
<p>636<br>01:11:49,000 –&gt; 01:11:52,000<br>So, they’re compatible.</p>
<p>637<br>01:11:54,000 –&gt; 01:11:55,000<br>Okay.</p>
<p>638<br>01:11:55,000 –&gt; 01:11:59,000<br>I don’t think we’re going to get through parallel execution, but let’s finish up with…</p>
<p>639<br>01:11:59,000 –&gt; 01:12:00,000<br>We’ll cover that next class.</p>
<p>640<br>01:12:00,000 –&gt; 01:12:05,000<br>But let’s finish up with how to represent filter data.</p>
<p>641<br>01:12:06,000 –&gt; 01:12:12,000<br>So, in the iterator model, because we’re operating…</p>
<p>642<br>01:12:12,000 –&gt; 01:12:23,000<br>Every operator is going to process one two-pole at a time, if something doesn’t match like a predicate, something’s not meant to be produced as an output, then we don’t send it up to the parent operator.</p>
<p>643<br>01:12:23,000 –&gt; 01:12:32,000<br>Either call next and get the next two-pole from our child or whatever, you know, the local thing we’re processing, or we return back into file, meaning we have no more.</p>
<p>644<br>01:12:32,000 –&gt; 01:12:44,000<br>So, that means that at no point in the query plan, what we send up data that we know has been disqualified or thrown out, because we wouldn’t have emitted it up.</p>
<p>645<br>01:12:44,000 –&gt; 01:12:46,000<br>Right?</p>
<p>646<br>01:12:46,000 –&gt; 01:13:00,000<br>But in the vectorized model, you can’t do that, because you’re operating on batches of tuples, so you may have, based on what your predicate is, or whatever the operation is trying to do, you may end up filtering out or throwing away some tuples inside the vector,</p>
<p>647<br>01:13:00,000 –&gt; 01:13:04,000<br>while other tuples still need to be passed up.</p>
<p>648<br>01:13:04,000 –&gt; 01:13:12,000<br>So, now the question is, how do we handle that? Basically, we have a vector that’s going to have things half the data is we want to keep, half the data we know we want to throw away.</p>
<p>649<br>01:13:12,000 –&gt; 01:13:14,000<br>I’m sorry.</p>
<p>650<br>01:13:14,000 –&gt; 01:13:23,000<br>So, let’s say that we’re a query like this, where the where clause is where column 0 is null, and column 1, or column 1 is like, and then being a wildcard.</p>
<p>651<br>01:13:23,000 –&gt; 01:13:36,000<br>So, say this is my data, I’ve called it 0, column 1, and so if I now do the filtering on this data, say this is coming in as a batch, this is the output I really want.</p>
<p>652<br>01:13:36,000 –&gt; 01:13:49,000<br>Right? This is because this is the logically, this is the correct result. But how do I get there? Because I don’t want to have to copy everything out and then put it back into another buffer, that’s going to be slow.</p>
<p>653<br>01:13:49,000 –&gt; 01:14:02,000<br>So, I need a way to represent logically that these are the tuples that have been filtered out, even though physically, I may be still be passing on dead tuples or tuples that I don’t need.</p>
<p>654<br>01:14:02,000 –&gt; 01:14:10,000<br>So, there’s two approaches to do this. The first is used what is called a selection vector. Sometimes I’ll also call it a position list.</p>
<p>655<br>01:14:10,000 –&gt; 01:14:25,000<br>And the idea here, it’s just going to be a densely packed array of the offsets of the tuples within my vector that I’m passing from one upper to the next, that are still valid, are still alive, are still active.</p>
<p>656<br>01:14:25,000 –&gt; 01:14:36,000<br>So, again, going back to my example here, my selected vector would just be a list of offsets, 1, 3, 4, because they correspond to the tuples that satisfy the predicate.</p>
<p>657<br>01:14:36,000 –&gt; 01:14:44,000<br>Now, this is what gets passed on as the output of next, or if I’m pushing it along in the push-based model. This is what the next operator is going to process on.</p>
<p>658<br>01:14:44,000 –&gt; 01:14:56,000<br>So, now, when I start doing whatever it is, I get this batch of data, I have this selection vector, I said then account for some of the data may have been discarded, some of the data is still active.</p>
<p>659<br>01:14:56,000 –&gt; 01:15:16,000<br>Yes? Why do this? This must have x-symd, the question is why do this? Because the answer is going to be yes for s-symd, and for other operations, it’s actually going to be faster for us just to pass along garbage.</p>
<p>660<br>01:15:16,000 –&gt; 01:15:24,000<br>And then, if the selection vector goes zero, then I know everything is discarded, and I just throw the entire thing away, and jump out.</p>
<p>661<br>01:15:24,000 –&gt; 01:15:35,000<br>But it’s going to be faster for us to not have to say, everything will step, saying in C-ser-visualizes in the fusion model, or the fusion approach.</p>
<p>662<br>01:15:35,000 –&gt; 01:15:48,000<br>It’s easier from going from one line to the next line, within my nest before the, to not have to allocate memory resizing stuff.</p>
<p>663<br>01:15:48,000 –&gt; 01:16:05,000<br>The alternative approach is to do bitmaps, and this is just going to be a bitmap that has the same length as the number of tuples in my vector that I’m passing along, and it’s just a zero one that specifies whether the, the tuple at the given offset is valid or not.</p>
<p>664<br>01:16:05,000 –&gt; 01:16:24,000<br>And again, as you brought out, this is going to matter, this is going to make our life easier in some cases, because some simd instructions in AVX512 will actually take this as input as a mask, and you can use it to tell it, hey, don’t process the data in these lanes, because I don’t care, I don’t care for the output.</p>
<p>665<br>01:16:25,000 –&gt; 01:16:42,000<br>So again, we’ll, we’ll, we’ll see more this later on, actually, you know, how to design, you know, simd optimize operations, or algorithms for, or query plans or X data system that can use all these things.</p>
<p>666<br>01:16:42,000 –&gt; 01:16:53,000<br>The current research literature actually says the top one is the faster way to do this. So the photon paper from, from data brefs will read later on says this, and then our research paper.</p>
<p>667<br>01:16:54,000 –&gt; 01:17:14,000<br>Why would this even be a little bit doing the field system, because if you’re doing fields thing, it’s just one or whatever next layer form, because if you don’t find line of all papers, that means that the pipeline we can end up with, we don’t need to use that kind of data flow, if we just are dealing in that abstraction.</p>
<p>668<br>01:17:15,000 –&gt; 01:17:20,000<br>So your statement is if we’re doing this, why don’t we even need that?</p>
<p>669<br>01:17:29,000 –&gt; 01:17:43,000<br>So like literally thing like, say, say to T2, it’s being a simple to say it’s a batch, a vector. I call this evalpredicate, right? It’s going to then populate either the position list, the selection vector, or the output, the bitmap, right?</p>
<p>670<br>01:17:44,000 –&gt; 01:17:53,000<br>If it’s the bitmap, I do pop count, tell me how many zeros I have, or how many ones I have. If it’s, if I have at least one, one, then I know I want to do this.</p>
<p>671<br>01:17:54,000 –&gt; 01:18:04,000<br>If you don’t do that, then I got to go copy, go iterate over every single tuple within my batch, populate a new output buffer, then feed that into my probe hash table.</p>
<p>672<br>01:18:05,000 –&gt; 01:18:14,000<br>It’s just faster to do zero copy, operate on the data as it exists, but I’m just upending bits or a position list to say what’s tuple’s actually matter.</p>
<p>673<br>01:18:14,000 –&gt; 01:18:24,000<br>And all these implementations of these operators, which we’ll cover later, take as input, whether the selection vector or the bitmask, and know whether or not to even consider a tuple at an offset.</p>
<p>674<br>01:18:25,000 –&gt; 01:18:33,000<br>What if we had s was the batch and there was an out of loop that was actually looping through a bunch of s’s, but you see how that could be much better.</p>
<p>675<br>01:18:33,000 –&gt; 01:18:38,000<br>Yeah, but don’t do that. We can do code, and we can do all of this stuff, we can do that.</p>
<p>676<br>01:18:38,000 –&gt; 01:18:43,000<br>Yeah, but like that’s, there’s going to be, you know, scalar instruction, sissy. We can vectorize all of this.</p>
<p>677<br>01:18:43,000 –&gt; 01:18:45,000<br>We can vector as evalpred in the program.</p>
<p>678<br>01:18:45,000 –&gt; 01:18:46,000<br>Absolutely, yeah.</p>
<p>679<br>01:18:46,000 –&gt; 01:18:47,000<br>Yeah.</p>
<p>680<br>01:18:48,000 –&gt; 01:18:53,000<br>That’s why this course exists and why you pay you a lot of money to do this stuff, right? It’s hard.</p>
<p>681<br>01:18:53,000 –&gt; 01:18:56,000<br>All right. Yes.</p>
<p>682<br>01:18:56,000 –&gt; 01:19:05,000<br>So, stay with us.</p>
<p>683<br>01:19:06,000 –&gt; 01:19:14,000<br>So, could you do in a pull-based approach?</p>
<p>684<br>01:19:14,000 –&gt; 01:19:20,000<br>Like this way for the vector is for the company, they have only the advantage.</p>
<p>685<br>01:19:20,000 –&gt; 01:19:27,000<br>But so, so, so, but you put a batch of tuples, a vector tuples, or a single tuple.</p>
<p>686<br>01:19:28,000 –&gt; 01:19:30,000<br>So, so, so, so, so, so, so, so, so, so, you stay with us.</p>
<p>687<br>01:19:30,000 –&gt; 01:19:37,000<br>Can’t, if, if I have, in my example here, so I have three out of, three out of what, three out of five match.</p>
<p>688<br>01:19:37,000 –&gt; 01:19:47,000<br>So, I have two empty slots. Can I just call it next below me, get the next vector, then, you know, at least find two matches that I’ve been putting this vector here.</p>
<p>689<br>01:19:47,000 –&gt; 01:19:52,000<br>But that’s a more bookkeeping, keep track of like how many spaces do I have.</p>
<p>690<br>01:19:52,000 –&gt; 01:19:57,000<br>Then I still got to maintain the, the, the, the in the end result of the child thing I call before me.</p>
<p>691<br>01:19:57,000 –&gt; 01:19:59,000<br>It’s not worth it.</p>
<p>692<br>01:19:59,000 –&gt; 01:20:02,000<br>Has his way more in direction, way more branching, way more conditionals.</p>
<p>693<br>01:20:02,000 –&gt; 01:20:06,000<br>If I just blindly just keep track of it, here’s what it doesn’t match. This is wavehazard.</p>
<p>694<br>01:20:06,000 –&gt; 01:20:12,000<br>Again, as humans, this goes back to the, the branchless, you know, conditional I showed before or this branchless scan.</p>
<p>695<br>01:20:12,000 –&gt; 01:20:15,000<br>As, you’re all coming up with examples like, hey, this seems kind of wasteful.</p>
<p>696<br>01:20:15,000 –&gt; 01:20:20,000<br>You’re passing along tuples like, you know, what, what if I have 1024, my vector size is 1024,</p>
<p>697<br>01:20:20,000 –&gt; 01:20:27,000<br>all but one of them are thrown away. Well, again, it’s just faster to pass along the other 10,</p>
<p>698<br>01:20:27,000 –&gt; 01:20:35,000<br>1023 useless tuples than rather than having to do what he’s proposing, I’m going getting any results and filling in the empty space.</p>
<p>699<br>01:20:35,000 –&gt; 01:20:42,000<br>Right. It’s not worth it. Just do exactly, if you do the same thing that’s very straightforward.</p>
<p>700<br>01:20:42,000 –&gt; 01:20:48,000<br>And even though you may end up executing more instructions, but you end up using fewer cycles because again,</p>
<p>701<br>01:20:48,000 –&gt; 01:20:55,000<br>it’s set up in a way that the CPU wants. Yes.</p>
<p>702<br>01:20:55,000 –&gt; 01:21:01,000<br>So, the direction vector is exactly like getting, or the right for it.</p>
<p>703<br>01:21:01,000 –&gt; 01:21:07,000<br>The statement is, with a selection vector, getting shorter and shorter, it depends on the query, tens of the data, right.</p>
<p>704<br>01:21:07,000 –&gt; 01:21:13,000<br>But you would, you would typically size it for the exact size of the vector you’re passing along.</p>
<p>705<br>01:21:13,000 –&gt; 01:21:20,000<br>So, you could say 1024 and then just have a length to say where the offset is. Where the end actually is.</p>
<p>706<br>01:21:20,000 –&gt; 01:21:30,000<br>And again, these aren’t, have to be 64 bit IDs. If you only have 1024 possible values, you could store that in 16 bit numbers.</p>
<p>707<br>01:21:30,000 –&gt; 01:21:36,000<br>So, it’s not that big. Yes.</p>
<p>708<br>01:21:36,000 –&gt; 01:21:43,000<br>The size meaning like the allocated size or the actual contents. But why?</p>
<p>709<br>01:21:43,000 –&gt; 01:21:50,000<br>I don’t know. You definitely end up using the empty space.</p>
<p>710<br>01:21:50,000 –&gt; 01:21:55,000<br>But it’s not, I think what you’re saying is like, could you basically do a mislap allocation to say like,</p>
<p>711<br>01:21:55,000 –&gt; 01:22:01,000<br>here’s my 10 value vector and here’s my 30 value vector, whatever, like 32 value of 64.</p>
<p>712<br>01:22:01,000 –&gt; 01:22:08,000<br>Like, as I get full in one vector, then I go use the other one and therefore things hang around in L3, sorry L1 more.</p>
<p>713<br>01:22:08,000 –&gt; 01:22:12,000<br>But it’s not that big. 1024 times 16 bits is not that big.</p>
<p>714<br>01:22:12,000 –&gt; 01:22:18,000<br>Yeah. Okay, what you’re proposing is you could do, is that the CPU wants though.</p>
<p>715<br>01:22:18,000 –&gt; 01:22:23,000<br>No. Because the worst thing the worst thing to possibly do is call Malak while we’re doing any of this.</p>
<p>716<br>01:22:23,000 –&gt; 01:22:26,000<br>Because who are we talking to here? We call Malak.</p>
<p>717<br>01:22:26,000 –&gt; 01:22:30,000<br>The operating system, right? He’s going to screw us over.</p>
<p>718<br>01:22:30,000 –&gt; 01:22:33,000<br>So like, we don’t want to, like, we pre-allocate everything ahead of time.</p>
<p>719<br>01:22:33,000 –&gt; 01:22:40,000<br>Yes, there would be weights in space, but like, it’s better than having, again, this, this, it’s better than having code to figure out,</p>
<p>720<br>01:22:40,000 –&gt; 01:22:43,000<br>okay, try to be clever. Simple as better in this case.</p>
<p>721<br>01:22:43,000 –&gt; 01:22:47,000<br>Because again, it’s just ripping through two pulls as fast as possible.</p>
<p>722<br>01:22:47,000 –&gt; 01:22:56,000<br>Okay? All right, well over time, we’ll cover, we’ll cover pale execution of building class on Wednesday.</p>
<p>723<br>01:22:56,000 –&gt; 01:22:57,000<br>Okay?</p>
<p>724<br>01:22:57,000 –&gt; 01:23:00,000<br>All right guys, see ya.</p>
<p>725<br>01:23:27,000 –&gt; 01:23:29,000<br>All right.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CMU15721 P5S202404 QueryExecutionProcessingPart1CMUAdvancedDatabaseSystems</div>
      <div>http://example.com/2025/10/24/CMU15721 P5S202404-QueryExecutionProcessingPart1CMUAdvancedDatabaseSystems/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月24日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/24/CMU15721%20P4S202403-DataFormatsEncodingPart2CMUAdvancedDatabaseSystems/" title="CMU15721 P4S202403 DataFormatsEncodingPart2CMUAdvancedDatabaseSystems">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CMU15721 P4S202403 DataFormatsEncodingPart2CMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/24/CMU15721%20P22S202422-AmazonRedshiftDataWarehouseSystemCMUAdvancedDatabaseSystems/" title="CMU15721 P22S202422 AmazonRedshiftDataWarehouseSystemCMUAdvancedDatabaseSystems">
                        <span class="hidden-mobile">CMU15721 P22S202422 AmazonRedshiftDataWarehouseSystemCMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
