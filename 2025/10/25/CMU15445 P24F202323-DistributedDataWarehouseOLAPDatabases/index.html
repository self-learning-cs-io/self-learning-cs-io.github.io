

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="100:00:00,000 –&gt; 00:00:26,859The 200:00:26,859 –&gt; 00:00:28,699Random applause for TTGPL. 300:00:30,899 –&gt; 00:00:31,739Back. 400:00:31,739 –&gt; 00:00:32,420You feel better? 500:00:32,420 –&amp;gt">
<meta property="og:type" content="article">
<meta property="og:title" content="CMU15445 P24F202323 DistributedDataWarehouseOLAPDatabases">
<meta property="og:url" content="http://example.com/2025/10/25/CMU15445%20P24F202323-DistributedDataWarehouseOLAPDatabases/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="100:00:00,000 –&gt; 00:00:26,859The 200:00:26,859 –&gt; 00:00:28,699Random applause for TTGPL. 300:00:30,899 –&gt; 00:00:31,739Back. 400:00:31,739 –&gt; 00:00:32,420You feel better? 500:00:32,420 –&amp;gt">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-25T05:03:39.737Z">
<meta property="article:modified_time" content="2025-10-25T05:03:39.737Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>CMU15445 P24F202323 DistributedDataWarehouseOLAPDatabases - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CMU15445 P24F202323 DistributedDataWarehouseOLAPDatabases"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-25 13:03" pubdate>
          2025年10月25日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          13k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          112 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CMU15445 P24F202323 DistributedDataWarehouseOLAPDatabases</h1>
            
            
              <div class="markdown-body">
                
                <p>1<br>00:00:00,000 –&gt; 00:00:26,859<br>The</p>
<p>2<br>00:00:26,859 –&gt; 00:00:28,699<br>Random applause for TTGPL.</p>
<p>3<br>00:00:30,899 –&gt; 00:00:31,739<br>Back.</p>
<p>4<br>00:00:31,739 –&gt; 00:00:32,420<br>You feel better?</p>
<p>5<br>00:00:32,420 –&gt; 00:00:33,420<br>Yeah.</p>
<p>6<br>00:00:33,420 –&gt; 00:00:36,060<br>And apparently your mom saw the video.</p>
<p>7<br>00:00:36,060 –&gt; 00:00:37,820<br>It was concerned.</p>
<p>8<br>00:00:37,820 –&gt; 00:00:39,299<br>I’m doing all the other.</p>
<p>9<br>00:00:39,299 –&gt; 00:00:40,780<br>So your mom watches these videos?</p>
<p>10<br>00:00:40,780 –&gt; 00:00:41,299<br>Yeah.</p>
<p>11<br>00:00:41,299 –&gt; 00:00:41,780<br>Yeah.</p>
<p>12<br>00:00:41,780 –&gt; 00:00:43,780<br>She just watches them to just make sure that my life is good.</p>
<p>13<br>00:00:43,780 –&gt; 00:00:44,980<br>OK.</p>
<p>14<br>00:00:44,980 –&gt; 00:00:47,700<br>But I mean, does she know everything is going on with you?</p>
<p>15<br>00:00:47,700 –&gt; 00:00:50,980<br>I mean, I try to tell her the parts that she’s</p>
<p>16<br>00:00:50,980 –&gt; 00:00:52,859<br>I’m OK and she’s OK with me, no?</p>
<p>17<br>00:00:52,859 –&gt; 00:00:54,299<br>Does she know about the pregnancy scare?</p>
<p>18<br>00:00:54,299 –&gt; 00:00:56,299<br>I’m 12.</p>
<p>19<br>00:00:56,299 –&gt; 00:00:57,140<br>All right.</p>
<p>20<br>00:00:57,140 –&gt; 00:00:57,979<br>All right.</p>
<p>21<br>00:00:57,979 –&gt; 00:01:01,179<br>All right, guys.</p>
<p>22<br>00:01:01,179 –&gt; 00:01:02,019<br>Let’s get started.</p>
<p>23<br>00:01:02,019 –&gt; 00:01:02,859<br>All right.</p>
<p>24<br>00:01:02,859 –&gt; 00:01:04,739<br>Again, this is same as last time.</p>
<p>25<br>00:01:04,739 –&gt; 00:01:06,939<br>And homework 5 is due on Sunday.</p>
<p>26<br>00:01:06,939 –&gt; 00:01:07,859<br>homework project 4.</p>
<p>27<br>00:01:07,859 –&gt; 00:01:09,299<br>We do the week following that.</p>
<p>28<br>00:01:09,299 –&gt; 00:01:12,699<br>As a person piata on Monday’s class will not be here.</p>
<p>29<br>00:01:12,699 –&gt; 00:01:13,979<br>It’ll be on Zoom.</p>
<p>30<br>00:01:13,979 –&gt; 00:01:17,939<br>And that’s the guest speaker from Singlestore.</p>
<p>31<br>00:01:17,939 –&gt; 00:01:21,340<br>And then please go vote also to for the when</p>
<p>32<br>00:01:21,340 –&gt; 00:01:24,060<br>we do the speed run on Wednesday next week.</p>
<p>33<br>00:01:24,060 –&gt; 00:01:26,900<br>Some people are putting voting for Singlestore.</p>
<p>34<br>00:01:26,900 –&gt; 00:01:28,340<br>We have a whole lecture on Singlestore.</p>
<p>35<br>00:01:28,340 –&gt; 00:01:29,780<br>You don’t need to vote for it again.</p>
<p>36<br>00:01:29,780 –&gt; 00:01:30,980<br>We’re not going to cover it.</p>
<p>37<br>00:01:30,980 –&gt; 00:01:32,460<br>So put other systems.</p>
<p>38<br>00:01:32,460 –&gt; 00:01:35,980<br>And then the final exam will be on the 12th.</p>
<p>39<br>00:01:35,980 –&gt; 00:01:37,980<br>And then as always, we’ll post on pets as well.</p>
<p>40<br>00:01:37,980 –&gt; 00:01:40,460<br>We can sign up to be a TX.</p>
<p>41<br>00:01:40,460 –&gt; 00:01:43,460<br>And then I’ll also post for the faculty course evaluations.</p>
<p>42<br>00:01:43,460 –&gt; 00:01:46,060<br>And I’ll just say things to every year.</p>
<p>43<br>00:01:46,060 –&gt; 00:01:47,540<br>Please be brutally honest.</p>
<p>44<br>00:01:47,540 –&gt; 00:01:49,340<br>Tell us what sucks about the course, what you don’t like,</p>
<p>45<br>00:01:49,340 –&gt; 00:01:52,340<br>because we actually listen to it and incorporate your feedback</p>
<p>46<br>00:01:52,340 –&gt; 00:01:54,420<br>and future versions of the course.</p>
<p>47<br>00:01:54,420 –&gt; 00:01:57,140<br>So in previous semesters, Project 2 used to be like,</p>
<p>48<br>00:01:57,140 –&gt; 00:01:58,820<br>hey, let’s vote a concurrent B plus tree.</p>
<p>49<br>00:01:58,820 –&gt; 00:02:00,740<br>And the students complained that that project was super hard</p>
<p>50<br>00:02:00,740 –&gt; 00:02:02,820<br>in 3 and 4 for a competitive easier.</p>
<p>51<br>00:02:02,820 –&gt; 00:02:05,100<br>So we sort of spread out the load throughout the entire semester.</p>
<p>52<br>00:02:05,100 –&gt; 00:02:07,380<br>So that’s a good example of something we listen to.</p>
<p>53<br>00:02:07,380 –&gt; 00:02:09,099<br>One year, a student started to give me</p>
<p>54<br>00:02:09,099 –&gt; 00:02:12,259<br>a psychological evaluation on myself.</p>
<p>55<br>00:02:12,259 –&gt; 00:02:13,580<br>And that I did not listen to.</p>
<p>56<br>00:02:13,580 –&gt; 00:02:18,180<br>But other things you don’t like about me, please go for it.</p>
<p>57<br>00:02:18,180 –&gt; 00:02:20,180<br>Any questions about what’s remaining for you guys</p>
<p>58<br>00:02:20,180 –&gt; 00:02:22,020<br>or expect for you guys for the rest of the semester?</p>
<p>59<br>00:02:24,980 –&gt; 00:02:25,500<br>All right, awesome.</p>
<p>60<br>00:02:25,500 –&gt; 00:02:27,180<br>Let’s jump in this.</p>
<p>61<br>00:02:27,180 –&gt; 00:02:28,340<br>All right, so today’s class, we’re</p>
<p>62<br>00:02:28,340 –&gt; 00:02:30,819<br>going to talk about distributed database</p>
<p>63<br>00:02:30,819 –&gt; 00:02:33,460<br>or distributed analytical database systems.</p>
<p>64<br>00:02:33,460 –&gt; 00:02:34,819<br>So before we get into that, I want</p>
<p>65<br>00:02:34,819 –&gt; 00:02:37,460<br>to talk a little bit about how these analytical databases</p>
<p>66<br>00:02:37,460 –&gt; 00:02:41,659<br>are being used in modern application scenarios or setups.</p>
<p>67<br>00:02:41,659 –&gt; 00:02:45,260<br>And then that’ll then help motivate why we want to start using</p>
<p>68<br>00:02:45,260 –&gt; 00:02:46,780<br>why we made the distributed database.</p>
<p>69<br>00:02:46,780 –&gt; 00:02:47,300<br>Sorry, yes.</p>
<p>70<br>00:02:47,300 –&gt; 00:02:47,819<br>Sorry.</p>
<p>71<br>00:02:48,099 –&gt; 00:02:50,620<br>So finally, is that common sense of community?</p>
<p>72<br>00:02:50,620 –&gt; 00:02:55,259<br>A question is, for the final exam, is a comprehensive no?</p>
<p>73<br>00:02:55,259 –&gt; 00:02:57,419<br>But there’s things you need to be obviously</p>
<p>74<br>00:02:57,419 –&gt; 00:03:00,060<br>aware of in order to do is the questions.</p>
<p>75<br>00:03:00,060 –&gt; 00:03:01,739<br>Like if you’re brain dead and clearly forgot</p>
<p>76<br>00:03:01,739 –&gt; 00:03:04,259<br>SQL, then you’re in our problems, right?</p>
<p>77<br>00:03:04,259 –&gt; 00:03:05,819<br>Or if you don’t know what a buffer pool is used for,</p>
<p>78<br>00:03:05,819 –&gt; 00:03:06,579<br>you’re in our problems.</p>
<p>79<br>00:03:06,579 –&gt; 00:03:10,180<br>But the core questions will be from the, whatever,</p>
<p>80<br>00:03:10,180 –&gt; 00:03:12,900<br>the electrified midterm going forward.</p>
<p>81<br>00:03:12,900 –&gt; 00:03:15,539<br>And we’ll post a study guide app online.</p>
<p>82<br>00:03:17,900 –&gt; 00:03:18,659<br>Yes.</p>
<p>83<br>00:03:18,659 –&gt; 00:03:19,699<br>If there are any practices, it’s there.</p>
<p>84<br>00:03:19,699 –&gt; 00:03:21,060<br>There will be a practice of examples like the midterm.</p>
<p>85<br>00:03:21,060 –&gt; 00:03:22,340<br>Yes.</p>
<p>86<br>00:03:22,340 –&gt; 00:03:22,859<br>Yes.</p>
<p>87<br>00:03:22,859 –&gt; 00:03:24,019<br>Is it full three hours?</p>
<p>88<br>00:03:24,019 –&gt; 00:03:27,180<br>Question, is it a full three hours?</p>
<p>89<br>00:03:27,180 –&gt; 00:03:29,180<br>What do you mean by that?</p>
<p>90<br>00:03:29,180 –&gt; 00:03:31,539<br>It’s supposed to be 8 or 11, so is it</p>
<p>91<br>00:03:31,539 –&gt; 00:03:34,019<br>like a three hour exam or is it?</p>
<p>92<br>00:03:34,019 –&gt; 00:03:34,659<br>Oh, it’s a question.</p>
<p>93<br>00:03:34,659 –&gt; 00:03:38,060<br>Is the, will it be twice as long as the midterm?</p>
<p>94<br>00:03:38,060 –&gt; 00:03:38,819<br>No.</p>
<p>95<br>00:03:38,819 –&gt; 00:03:41,659<br>Also, it’s going to be an hour or a half.</p>
<p>96<br>00:03:41,659 –&gt; 00:03:47,699<br>It’ll be, so you can take up the three hours long as you need.</p>
<p>97<br>00:03:47,699 –&gt; 00:03:50,739<br>But it’s like, is it going to be double the number of questions</p>
<p>98<br>00:03:50,739 –&gt; 00:03:51,259<br>on the midterm?</p>
<p>99<br>00:03:51,259 –&gt; 00:03:52,060<br>No.</p>
<p>100<br>00:03:52,060 –&gt; 00:03:52,899<br>Yeah.</p>
<p>101<br>00:03:55,379 –&gt; 00:03:58,060<br>You know, we used to keep track of like the time</p>
<p>102<br>00:03:58,060 –&gt; 00:03:59,899<br>with people who were turned in the final exam</p>
<p>103<br>00:03:59,899 –&gt; 00:04:02,379<br>and their grade similar to the midterm.</p>
<p>104<br>00:04:02,379 –&gt; 00:04:04,620<br>And it used to be like for previous years,</p>
<p>105<br>00:04:04,620 –&gt; 00:04:07,539<br>like if you took the all three hours,</p>
<p>106<br>00:04:07,539 –&gt; 00:04:10,739<br>you typically were in the lower end of the distribution curve.</p>
<p>107<br>00:04:10,739 –&gt; 00:04:13,019<br>In recent years, people get like perfect scores.</p>
<p>108<br>00:04:13,019 –&gt; 00:04:14,659<br>They take the whole three hours.</p>
<p>109<br>00:04:14,659 –&gt; 00:04:20,060<br>So it’s not meant to be twice as hard as long as the midterm.</p>
<p>110<br>00:04:20,060 –&gt; 00:04:20,699<br>Other questions?</p>
<p>111<br>00:04:23,500 –&gt; 00:04:24,300<br>All right, cool.</p>
<p>112<br>00:04:24,300 –&gt; 00:04:24,500<br>All right.</p>
<p>113<br>00:04:24,500 –&gt; 00:04:26,219<br>So again, what I want to talk about first</p>
<p>114<br>00:04:26,219 –&gt; 00:04:29,100<br>is like what these OLAP systems sort of look like,</p>
<p>115<br>00:04:29,100 –&gt; 00:04:33,860<br>how they’re incorporated in your application environments.</p>
<p>116<br>00:04:33,860 –&gt; 00:04:36,579<br>And then that’ll then motivate why we want to be potentially</p>
<p>117<br>00:04:36,579 –&gt; 00:04:38,819<br>built or need it to be a database system that</p>
<p>118<br>00:04:38,819 –&gt; 00:04:41,259<br>do analytics on it.</p>
<p>119<br>00:04:41,259 –&gt; 00:04:44,300<br>And in most cases, I’ll say also to the,</p>
<p>120<br>00:04:44,300 –&gt; 00:04:46,060<br>and this is purely a conjecture of mine,</p>
<p>121<br>00:04:46,060 –&gt; 00:04:49,939<br>but like the most people are going to need</p>
<p>122<br>00:04:49,939 –&gt; 00:04:52,339<br>a distributed analytical system because you’re</p>
<p>123<br>00:04:52,339 –&gt; 00:04:54,180<br>going to have potentially have a lot of data enriched</p>
<p>124<br>00:04:54,180 –&gt; 00:04:55,579<br>with not just the data from your applications</p>
<p>125<br>00:04:55,579 –&gt; 00:04:57,620<br>but from outside sources.</p>
<p>126<br>00:04:57,620 –&gt; 00:05:01,740<br>And for those kind of things, a scaled architecture,</p>
<p>127<br>00:05:01,740 –&gt; 00:05:04,180<br>like for an OLAP system makes more sense</p>
<p>128<br>00:05:04,180 –&gt; 00:05:05,939<br>than maybe in an OLTP system.</p>
<p>129<br>00:05:05,939 –&gt; 00:05:10,139<br>But again, as always, in databases, it depends.</p>
<p>130<br>00:05:10,139 –&gt; 00:05:12,939<br>So this is a very common setup right here,</p>
<p>131<br>00:05:12,939 –&gt; 00:05:15,379<br>where you have this separation in your application</p>
<p>132<br>00:05:15,379 –&gt; 00:05:19,219<br>between the OLTP databases and the OLAP database.</p>
<p>133<br>00:05:19,219 –&gt; 00:05:21,060<br>And I’m using OLAP database in a singular form</p>
<p>134<br>00:05:21,060 –&gt; 00:05:24,300<br>because the term data warehouse is meant to be like,</p>
<p>135<br>00:05:24,300 –&gt; 00:05:27,060<br>here’s where all your data from your front end OLTP databases</p>
<p>136<br>00:05:27,060 –&gt; 00:05:30,779<br>go, and you’re going to run all your analytics on there.</p>
<p>137<br>00:05:30,779 –&gt; 00:05:33,539<br>And even though I’m showing that the database drum,</p>
<p>138<br>00:05:33,539 –&gt; 00:05:35,379<br>there’s multilater drums here, and then one giant</p>
<p>139<br>00:05:35,379 –&gt; 00:05:37,779<br>day drum for the OLAP system, again,</p>
<p>140<br>00:05:37,779 –&gt; 00:05:39,539<br>these are just logical views.</p>
<p>141<br>00:05:39,540 –&gt; 00:05:43,220<br>It could be spread across multiple nodes.</p>
<p>142<br>00:05:43,220 –&gt; 00:05:46,700<br>So a very common setup is that you have your application</p>
<p>143<br>00:05:46,700 –&gt; 00:05:48,980<br>running with these OLTP databases in the front end.</p>
<p>144<br>00:05:48,980 –&gt; 00:05:50,980<br>And think of these as individual silos.</p>
<p>145<br>00:05:50,980 –&gt; 00:05:53,780<br>Like there’s one application writing data in this one,</p>
<p>146<br>00:05:53,780 –&gt; 00:05:54,900<br>another application writing there,</p>
<p>147<br>00:05:54,900 –&gt; 00:05:56,860<br>and they don’t really talk to each other.</p>
<p>148<br>00:05:56,860 –&gt; 00:05:58,220<br>And then you want to combine them together</p>
<p>149<br>00:05:58,220 –&gt; 00:06:00,620<br>into your giant OLAP system, and you’ll</p>
<p>150<br>00:06:00,620 –&gt; 00:06:03,379<br>use a technique or a method called extract, transform,</p>
<p>151<br>00:06:03,379 –&gt; 00:06:05,020<br>and load, or ETL.</p>
<p>152<br>00:06:05,020 –&gt; 00:06:07,900<br>And the idea here is that you’re going to extract data</p>
<p>153<br>00:06:07,899 –&gt; 00:06:10,139<br>from your front end OLTP databases,</p>
<p>154<br>00:06:10,139 –&gt; 00:06:12,099<br>transform it in some way to clean things up,</p>
<p>155<br>00:06:12,099 –&gt; 00:06:14,339<br>to put it into a uniform schema, and then</p>
<p>156<br>00:06:14,339 –&gt; 00:06:17,259<br>load it into your data warehouse.</p>
<p>157<br>00:06:17,259 –&gt; 00:06:18,459<br>And the reason why you have to do this</p>
<p>158<br>00:06:18,459 –&gt; 00:06:21,219<br>is because these applications could have</p>
<p>159<br>00:06:21,219 –&gt; 00:06:23,579<br>been written by different people at different times</p>
<p>160<br>00:06:23,579 –&gt; 00:06:25,979<br>and using different naming conventions.</p>
<p>161<br>00:06:25,979 –&gt; 00:06:27,500<br>So maybe the one at the bottom, they</p>
<p>162<br>00:06:27,500 –&gt; 00:06:31,099<br>have a table of users, and they’re using somebody’s first name.</p>
<p>163<br>00:06:31,099 –&gt; 00:06:33,219<br>They’ll use F name, or F underscore name.</p>
<p>164<br>00:06:33,219 –&gt; 00:06:34,579<br>But then at another database, they’ll</p>
<p>165<br>00:06:34,579 –&gt; 00:06:36,620<br>use first underscore name.</p>
<p>166<br>00:06:36,620 –&gt; 00:06:38,620<br>And as we as humans, we know they’re</p>
<p>167<br>00:06:38,620 –&gt; 00:06:40,100<br>corresponding to the same thing.</p>
<p>168<br>00:06:40,100 –&gt; 00:06:43,220<br>But if you just look at the raw schema, they’re different.</p>
<p>169<br>00:06:43,220 –&gt; 00:06:44,300<br>So that’s the transform phase.</p>
<p>170<br>00:06:44,300 –&gt; 00:06:46,340<br>It’s sort of you sort of clean things up.</p>
<p>171<br>00:06:46,340 –&gt; 00:06:48,780<br>The example I always like to use is Zingga,</p>
<p>172<br>00:06:48,780 –&gt; 00:06:51,500<br>think of like Farm Bill, about the data reference.</p>
<p>173<br>00:06:51,500 –&gt; 00:06:55,740<br>But they would always buy these different game studios</p>
<p>174<br>00:06:55,740 –&gt; 00:07:00,300<br>and buy their online applications, their online games.</p>
<p>175<br>00:07:00,300 –&gt; 00:07:02,699<br>But then they would leave the front end database alone.</p>
<p>176<br>00:07:02,699 –&gt; 00:07:03,939<br>So whatever the front end database was</p>
<p>177<br>00:07:03,939 –&gt; 00:07:04,939<br>when they acquired these companies,</p>
<p>178<br>00:07:04,939 –&gt; 00:07:06,819<br>they just let them do whatever they were doing.</p>
<p>179<br>00:07:06,819 –&gt; 00:07:08,819<br>But then they obviously wanted to put it into a giant data</p>
<p>180<br>00:07:08,819 –&gt; 00:07:09,540<br>warehouse.</p>
<p>181<br>00:07:09,540 –&gt; 00:07:11,579<br>So they would do this ETL process to clean things up,</p>
<p>182<br>00:07:11,579 –&gt; 00:07:13,420<br>because everyone would use sort of different naming</p>
<p>183<br>00:07:13,420 –&gt; 00:07:14,779<br>conventions.</p>
<p>184<br>00:07:14,779 –&gt; 00:07:16,939<br>So there’s a bunch of tools that will help you do this.</p>
<p>185<br>00:07:16,939 –&gt; 00:07:22,060<br>Informatica is probably the most famous one of all these.</p>
<p>186<br>00:07:22,060 –&gt; 00:07:24,339<br>But then all the various database vendors,</p>
<p>187<br>00:07:24,339 –&gt; 00:07:25,779<br>like Oracle and SQL Server, they all</p>
<p>188<br>00:07:25,779 –&gt; 00:07:28,500<br>have their own various versions of this.</p>
<p>189<br>00:07:28,500 –&gt; 00:07:29,740<br>So this is how traditionally people did</p>
<p>190<br>00:07:29,740 –&gt; 00:07:33,220<br>data warehouses starting since maybe the 1990s.</p>
<p>191<br>00:07:33,220 –&gt; 00:07:35,580<br>The current modern trend is a variation</p>
<p>192<br>00:07:35,580 –&gt; 00:07:40,940<br>of this called, so the ETL, ELT, track load and transform.</p>
<p>193<br>00:07:40,940 –&gt; 00:07:42,420<br>And the idea here is that you’re still</p>
<p>194<br>00:07:42,420 –&gt; 00:07:44,500<br>going to pull out data for your OTB databases,</p>
<p>195<br>00:07:44,500 –&gt; 00:07:46,940<br>and then you’re still going to load it into your OLAP system,</p>
<p>196<br>00:07:46,940 –&gt; 00:07:49,500<br>but you’re not actually going to transform it before you load it.</p>
<p>197<br>00:07:49,500 –&gt; 00:07:52,620<br>You just load all the raw files into your data warehouse.</p>
<p>198<br>00:07:52,620 –&gt; 00:07:53,980<br>And then the transform process actually</p>
<p>199<br>00:07:53,980 –&gt; 00:07:56,780<br>really occurs inside the data warehouse itself,</p>
<p>200<br>00:07:56,780 –&gt; 00:07:58,820<br>because there’s going to be a bunch of SQL queries</p>
<p>201<br>00:07:58,820 –&gt; 00:08:02,220<br>to convert it and then load it back into as more data files.</p>
<p>202<br>00:08:03,180 –&gt; 00:08:05,820<br>DBT is probably the most famous one of all these.</p>
<p>203<br>00:08:05,820 –&gt; 00:08:06,500<br>There’s air bite.</p>
<p>204<br>00:08:06,500 –&gt; 00:08:09,420<br>There’s a bunch of other ones that can do this kind of stuff.</p>
<p>205<br>00:08:09,420 –&gt; 00:08:11,700<br>And again, you think, OK, is it actually really different?</p>
<p>206<br>00:08:11,700 –&gt; 00:08:14,860<br>If you do one versus the other, yeah.</p>
<p>207<br>00:08:14,860 –&gt; 00:08:17,820<br>It depends on how the architecture is set up.</p>
<p>208<br>00:08:17,820 –&gt; 00:08:21,020<br>But it’s not to say that the ones at the top can’t do ELT.</p>
<p>209<br>00:08:21,020 –&gt; 00:08:22,460<br>It’s just these ones of the bottom are still</p>
<p>210<br>00:08:22,460 –&gt; 00:08:26,260<br>the design for doing those transformations.</p>
<p>211<br>00:08:26,260 –&gt; 00:08:28,620<br>So again, now you can see why you could have a ton of database</p>
<p>212<br>00:08:28,620 –&gt; 00:08:30,060<br>in your data warehouse, because you’re</p>
<p>213<br>00:08:30,060 –&gt; 00:08:31,700<br>getting data from all these other things.</p>
<p>214<br>00:08:31,699 –&gt; 00:08:36,019<br>Or like things like 5Tran, and I think a bunch of the other ones,</p>
<p>215<br>00:08:36,019 –&gt; 00:08:38,819<br>they can pull data from outside sources,</p>
<p>216<br>00:08:38,819 –&gt; 00:08:40,659<br>like say getting weather information.</p>
<p>217<br>00:08:40,659 –&gt; 00:08:43,740<br>You can have a pipeline that pipes that data into your data</p>
<p>218<br>00:08:43,740 –&gt; 00:08:44,259<br>warehouse.</p>
<p>219<br>00:08:44,259 –&gt; 00:08:45,460<br>You can enrich the data that you do have.</p>
<p>220<br>00:08:48,419 –&gt; 00:08:51,580<br>So I’m going to use the term OLAP.</p>
<p>221<br>00:08:51,580 –&gt; 00:08:53,340<br>Sometimes you’ll see these kind of databases</p>
<p>222<br>00:08:53,340 –&gt; 00:08:57,139<br>refer to as BI or business intelligence databases.</p>
<p>223<br>00:08:57,139 –&gt; 00:08:59,340<br>Or sometimes you’ll see them called decision support systems,</p>
<p>224<br>00:08:59,340 –&gt; 00:09:01,220<br>DSS.</p>
<p>225<br>00:09:01,220 –&gt; 00:09:02,820<br>They all pretty much mean the same thing.</p>
<p>226<br>00:09:02,820 –&gt; 00:09:05,259<br>You’re trying to extract new knowledge</p>
<p>227<br>00:09:05,259 –&gt; 00:09:07,019<br>from the data you’ve already collected</p>
<p>228<br>00:09:07,019 –&gt; 00:09:08,780<br>from your front end database system.</p>
<p>229<br>00:09:08,780 –&gt; 00:09:12,379<br>You try to identify things like, here’s the most,</p>
<p>230<br>00:09:12,379 –&gt; 00:09:15,379<br>here’s the item that most you will buy in the city of Pittsburgh</p>
<p>231<br>00:09:15,379 –&gt; 00:09:19,379<br>in December when the weather goes below some temperature,</p>
<p>232<br>00:09:19,379 –&gt; 00:09:21,180<br>some degree.</p>
<p>233<br>00:09:21,180 –&gt; 00:09:25,139<br>Another famous example was Walmart, one of the figure out,</p>
<p>234<br>00:09:25,139 –&gt; 00:09:26,340<br>this is like early 2000s.</p>
<p>235<br>00:09:26,340 –&gt; 00:09:28,980<br>They wanted to figure out right before hurricane occurs</p>
<p>236<br>00:09:28,980 –&gt; 00:09:31,340<br>and right after hurricane hits in the south,</p>
<p>237<br>00:09:31,340 –&gt; 00:09:32,700<br>what does everyone go buy?</p>
<p>238<br>00:09:32,700 –&gt; 00:09:36,379<br>So that they see this hurricane coming,</p>
<p>239<br>00:09:36,379 –&gt; 00:09:38,539<br>they start stocking their warehouses</p>
<p>240<br>00:09:38,539 –&gt; 00:09:41,420<br>near the stores in the south, but obviously</p>
<p>241<br>00:09:41,420 –&gt; 00:09:44,100<br>outside of the danger zone of the hurricane.</p>
<p>242<br>00:09:44,100 –&gt; 00:09:46,180<br>So then immediately after the hurricane hits,</p>
<p>243<br>00:09:46,180 –&gt; 00:09:48,340<br>they then send in the trucks with all the supplies</p>
<p>244<br>00:09:48,340 –&gt; 00:09:50,259<br>that they know people are going to buy.</p>
<p>245<br>00:09:50,259 –&gt; 00:09:53,180<br>That’s the idea of what we’re trying to do here.</p>
<p>246<br>00:09:53,180 –&gt; 00:09:56,860<br>We have the information from all our front end database.</p>
<p>247<br>00:09:56,860 –&gt; 00:10:01,740<br>We’re trying to then process it and then help us make decisions.</p>
<p>248<br>00:10:01,740 –&gt; 00:10:07,300<br>So the way we’re going to model our database</p>
<p>249<br>00:10:07,300 –&gt; 00:10:09,340<br>is going to use potentially two different techniques</p>
<p>250<br>00:10:09,340 –&gt; 00:10:12,300<br>in our O-Lapse system called a star schema and snowflake schema.</p>
<p>251<br>00:10:12,300 –&gt; 00:10:15,060<br>And if everyone wants to know why snowflake is called snowflake,</p>
<p>252<br>00:10:15,060 –&gt; 00:10:18,019<br>because they support snowflake schemas, right?</p>
<p>253<br>00:10:18,019 –&gt; 00:10:22,860<br>Star schemas were bigger in the 90s.</p>
<p>254<br>00:10:22,860 –&gt; 00:10:26,420<br>Star schema is a subset of what you can do in a snowflake schema</p>
<p>255<br>00:10:26,419 –&gt; 00:10:28,500<br>is more general.</p>
<p>256<br>00:10:28,500 –&gt; 00:10:31,099<br>But traditionally, doing joins is really expensive</p>
<p>257<br>00:10:31,099 –&gt; 00:10:33,219<br>before column stores, before all these other acrylic</p>
<p>258<br>00:10:33,219 –&gt; 00:10:35,339<br>acceleration stuff you can do.</p>
<p>259<br>00:10:35,339 –&gt; 00:10:36,979<br>So you would say, I want to do star schemas.</p>
<p>260<br>00:10:36,979 –&gt; 00:10:38,979<br>But nowadays, pretty much everyone does</p>
<p>261<br>00:10:38,979 –&gt; 00:10:40,019<br>a snowflake schema.</p>
<p>262<br>00:10:40,019 –&gt; 00:10:41,860<br>So there used to be some systems where you couldn’t actually</p>
<p>263<br>00:10:41,860 –&gt; 00:10:46,019<br>load a database unless you put it into a star schema form.</p>
<p>264<br>00:10:46,019 –&gt; 00:10:48,620<br>Now, we don’t really talk about data modeling in this class,</p>
<p>265<br>00:10:48,620 –&gt; 00:10:50,219<br>because it’s really about how to build a system.</p>
<p>266<br>00:10:50,219 –&gt; 00:10:51,620<br>I think there’s courses in Heinz College</p>
<p>267<br>00:10:51,620 –&gt; 00:10:54,419<br>and the teachers of those things.</p>
<p>268<br>00:10:54,419 –&gt; 00:10:55,699<br>And we don’t teach about normal forms,</p>
<p>269<br>00:10:55,700 –&gt; 00:10:57,420<br>because that’s a waste of time, because nobody actually</p>
<p>270<br>00:10:57,420 –&gt; 00:10:58,420<br>uses them in the real world.</p>
<p>271<br>00:10:58,420 –&gt; 00:11:00,820<br>But I’m going to talk a little bit about what these two things</p>
<p>272<br>00:11:00,820 –&gt; 00:11:01,340<br>look like.</p>
<p>273<br>00:11:01,340 –&gt; 00:11:03,180<br>And then that’ll help us motivate why we</p>
<p>274<br>00:11:03,180 –&gt; 00:11:05,060<br>want to do efficient joins, or why we</p>
<p>275<br>00:11:05,060 –&gt; 00:11:07,259<br>can use some things with joins that may not, maybe not</p>
<p>276<br>00:11:07,259 –&gt; 00:11:09,820<br>others for certain tables.</p>
<p>277<br>00:11:09,820 –&gt; 00:11:11,900<br>So this is what a star schema looks like.</p>
<p>278<br>00:11:11,900 –&gt; 00:11:13,540<br>So the basic idea is that in the middle here,</p>
<p>279<br>00:11:13,540 –&gt; 00:11:15,140<br>you have what is called the fact table.</p>
<p>280<br>00:11:15,140 –&gt; 00:11:16,980<br>So this is modeling, say, again,</p>
<p>281<br>00:11:16,980 –&gt; 00:11:18,780<br>do you think Walmart is an example?</p>
<p>282<br>00:11:18,780 –&gt; 00:11:22,980<br>This fact table here is going to be the every item that anyone</p>
<p>283<br>00:11:22,980 –&gt; 00:11:25,779<br>ever has ever bought at a store at Walmart.</p>
<p>284<br>00:11:25,779 –&gt; 00:11:27,120<br>Like anything that was skinned along catch,</p>
<p>285<br>00:11:27,120 –&gt; 00:11:29,779<br>or anything Amazon, anytime anybody bought something online,</p>
<p>286<br>00:11:29,779 –&gt; 00:11:33,139<br>the fact table is going to have a single row for every single item.</p>
<p>287<br>00:11:33,139 –&gt; 00:11:35,139<br>And so you would keep track of the price,</p>
<p>288<br>00:11:35,139 –&gt; 00:11:37,740<br>and the quantity keeps those as attributes directly</p>
<p>289<br>00:11:37,740 –&gt; 00:11:39,580<br>into the fact table.</p>
<p>290<br>00:11:39,580 –&gt; 00:11:42,220<br>But then you’re going to have all these foreign key references</p>
<p>291<br>00:11:42,220 –&gt; 00:11:44,700<br>to what are called dimension tables that</p>
<p>292<br>00:11:44,700 –&gt; 00:11:48,139<br>are going to be around the outside of the fact table.</p>
<p>293<br>00:11:48,139 –&gt; 00:11:50,100<br>So this should be sort of obvious.</p>
<p>294<br>00:11:50,100 –&gt; 00:11:52,300<br>For every single item ever has ever bought,</p>
<p>295<br>00:11:52,299 –&gt; 00:11:54,779<br>I don’t want to store the name, the description of that item</p>
<p>296<br>00:11:54,779 –&gt; 00:11:55,899<br>over and over again.</p>
<p>297<br>00:11:55,899 –&gt; 00:11:58,259<br>I want to put it into a dimension table</p>
<p>298<br>00:11:58,259 –&gt; 00:12:00,219<br>so that anytime I need to get it,</p>
<p>299<br>00:12:00,219 –&gt; 00:12:03,859<br>I just have a foreign key look up to this.</p>
<p>300<br>00:12:03,859 –&gt; 00:12:06,019<br>And then now when you’re doing a bunch of joins,</p>
<p>301<br>00:12:06,019 –&gt; 00:12:09,500<br>you’re basically, do the calculation of the same before?</p>
<p>302<br>00:12:09,500 –&gt; 00:12:13,699<br>Find what item people bought right after Hurricane</p>
<p>303<br>00:12:13,699 –&gt; 00:12:15,859<br>in this geographical region.</p>
<p>304<br>00:12:15,859 –&gt; 00:12:18,259<br>I just rip through my entire fact table</p>
<p>305<br>00:12:18,259 –&gt; 00:12:21,419<br>and do additional filtering on whether or not</p>
<p>306<br>00:12:21,419 –&gt; 00:12:25,339<br>additional dimensions that I have.</p>
<p>307<br>00:12:25,339 –&gt; 00:12:27,059<br>But again, for an star schema, I can only</p>
<p>308<br>00:12:27,059 –&gt; 00:12:30,579<br>have one level of dimension tables going on the outside,</p>
<p>309<br>00:12:30,579 –&gt; 00:12:32,299<br>because you want to reduce the number of joins</p>
<p>310<br>00:12:32,299 –&gt; 00:12:35,059<br>you have to do.</p>
<p>311<br>00:12:35,059 –&gt; 00:12:40,259<br>And so snowflake schema, you can have any arbitrary amount</p>
<p>312<br>00:12:40,259 –&gt; 00:12:42,099<br>of nesting or leveling.</p>
<p>313<br>00:12:42,099 –&gt; 00:12:43,459<br>And this is a term we don’t want to talk about.</p>
<p>314<br>00:12:43,459 –&gt; 00:12:44,860<br>It’s called normalization.</p>
<p>315<br>00:12:44,860 –&gt; 00:12:46,819<br>So normalization is like splitting a table up</p>
<p>316<br>00:12:46,819 –&gt; 00:12:49,699<br>into the smallest of top-up units</p>
<p>317<br>00:12:49,980 –&gt; 00:12:51,340<br>to reduce the amount of redundant information</p>
<p>318<br>00:12:51,340 –&gt; 00:12:52,500<br>you’re keeping track of.</p>
<p>319<br>00:12:52,500 –&gt; 00:12:55,620<br>So like going back here for the product category,</p>
<p>320<br>00:12:55,620 –&gt; 00:12:57,940<br>I’m keeping track of within the name of the product</p>
<p>321<br>00:12:57,940 –&gt; 00:13:00,020<br>of the description, but also the category name</p>
<p>322<br>00:13:00,020 –&gt; 00:13:01,460<br>and the category description.</p>
<p>323<br>00:13:01,460 –&gt; 00:13:04,020<br>So I’ll have multiple entries in the same category.</p>
<p>324<br>00:13:04,020 –&gt; 00:13:06,300<br>I’ll just repeat that information over and over again.</p>
<p>325<br>00:13:06,300 –&gt; 00:13:08,300<br>The idea is there because you’re denormalizing it,</p>
<p>326<br>00:13:08,300 –&gt; 00:13:10,940<br>putting it into one table, I don’t have to do a join</p>
<p>327<br>00:13:10,940 –&gt; 00:13:12,340<br>to go get that.</p>
<p>328<br>00:13:12,340 –&gt; 00:13:14,020<br>But then the downside, of course, is that if anytime</p>
<p>329<br>00:13:14,020 –&gt; 00:13:15,580<br>I update the name of a category,</p>
<p>330<br>00:13:15,580 –&gt; 00:13:18,420<br>I got to make sure I update all the tuples.</p>
<p>331<br>00:13:18,419 –&gt; 00:13:19,539<br>That’s basically what normalization,</p>
<p>332<br>00:13:19,539 –&gt; 00:13:21,019<br>denormalization stuff.</p>
<p>333<br>00:13:21,019 –&gt; 00:13:22,459<br>We don’t teach you the things.</p>
<p>334<br>00:13:22,459 –&gt; 00:13:24,539<br>But you’ll see it in the real world,</p>
<p>335<br>00:13:24,539 –&gt; 00:13:26,860<br>but not described in terms of normal forms, which</p>
<p>336<br>00:13:26,860 –&gt; 00:13:29,699<br>is the whole other, it was the theory of how</p>
<p>337<br>00:13:29,699 –&gt; 00:13:32,019<br>to design our model database.</p>
<p>338<br>00:13:32,019 –&gt; 00:13:33,740<br>We can ignore that.</p>
<p>339<br>00:13:33,740 –&gt; 00:13:35,659<br>Nobody doesn’t real world.</p>
<p>340<br>00:13:35,659 –&gt; 00:13:36,939<br>So again, going back here.</p>
<p>341<br>00:13:36,939 –&gt; 00:13:39,740<br>So now if I do the snowflake schema,</p>
<p>342<br>00:13:39,740 –&gt; 00:13:43,500<br>I can normalize out the category information</p>
<p>343<br>00:13:43,500 –&gt; 00:13:46,099<br>from my products so that now I have a separate category</p>
<p>344<br>00:13:46,099 –&gt; 00:13:48,259<br>lookup table.</p>
<p>345<br>00:13:48,259 –&gt; 00:13:49,939<br>And now if I want to get the name of a category</p>
<p>346<br>00:13:49,939 –&gt; 00:13:52,460<br>for a given product, I just do a join against that.</p>
<p>347<br>00:13:52,460 –&gt; 00:13:54,500<br>All right?</p>
<p>348<br>00:13:54,500 –&gt; 00:13:58,620<br>So this is a lot of what I’ve already said before.</p>
<p>349<br>00:13:58,620 –&gt; 00:14:00,980<br>In a star schema, the advantage is going</p>
<p>350<br>00:14:00,980 –&gt; 00:14:02,580<br>to be it’s going to run a lot faster,</p>
<p>351<br>00:14:02,580 –&gt; 00:14:06,299<br>potentially because I don’t have to do a bunch of joins.</p>
<p>352<br>00:14:06,299 –&gt; 00:14:10,419<br>But I’m going to have this duplication of data</p>
<p>353<br>00:14:10,419 –&gt; 00:14:12,379<br>because I’ve sort of flattened my tables down</p>
<p>354<br>00:14:12,379 –&gt; 00:14:16,259<br>or combine tables into single tables.</p>
<p>355<br>00:14:16,259 –&gt; 00:14:17,639<br>The snowflake schemas are going to be</p>
<p>356<br>00:14:17,639 –&gt; 00:14:19,899<br>require more joins, and potentially the queries</p>
<p>357<br>00:14:19,899 –&gt; 00:14:25,179<br>will be more difficult to do a good planning on.</p>
<p>358<br>00:14:25,179 –&gt; 00:14:26,700<br>But again, I have the advantage that I</p>
<p>359<br>00:14:26,700 –&gt; 00:14:29,580<br>have that isolation of the data, or I’m</p>
<p>360<br>00:14:29,580 –&gt; 00:14:31,460<br>reducing the number of copies of it,</p>
<p>361<br>00:14:31,460 –&gt; 00:14:34,139<br>and it sort of makes it easier to read them out.</p>
<p>362<br>00:14:34,139 –&gt; 00:14:35,580<br>It’s going on.</p>
<p>363<br>00:14:35,580 –&gt; 00:14:37,379<br>So when you go in the real world for analytical systems,</p>
<p>364<br>00:14:37,379 –&gt; 00:14:38,939<br>you will probably see a snowflake schema.</p>
<p>365<br>00:14:38,939 –&gt; 00:14:42,659<br>Nobody really does star schemas anymore.</p>
<p>366<br>00:14:42,659 –&gt; 00:14:45,779<br>Again, this is why snowflake is called snowflake.</p>
<p>367<br>00:14:45,779 –&gt; 00:14:48,179<br>OK?</p>
<p>368<br>00:14:48,179 –&gt; 00:14:49,819<br>All right, so with that said, here’s the problem</p>
<p>369<br>00:14:49,819 –&gt; 00:14:51,899<br>we’re trying to solve today.</p>
<p>370<br>00:14:51,899 –&gt; 00:14:54,819<br>Applications ever comes along against our data warehouse</p>
<p>371<br>00:14:54,819 –&gt; 00:14:56,579<br>or analytical database system, and they</p>
<p>372<br>00:14:56,579 –&gt; 00:14:59,459<br>want to do a join on RNS.</p>
<p>373<br>00:14:59,459 –&gt; 00:15:04,419<br>And say R is the fact table when S is the dimension table,</p>
<p>374<br>00:15:04,419 –&gt; 00:15:06,419<br>but for now it doesn’t matter.</p>
<p>375<br>00:15:06,419 –&gt; 00:15:09,620<br>And I partitioned my database for these two tables</p>
<p>376<br>00:15:09,620 –&gt; 00:15:12,379<br>across four different partitions, or still</p>
<p>377<br>00:15:12,379 –&gt; 00:15:13,980<br>because four different partitions,</p>
<p>378<br>00:15:13,980 –&gt; 00:15:16,539<br>doesn’t matter whether it’s shared disk or shared</p>
<p>379<br>00:15:16,539 –&gt; 00:15:18,300<br>nothing at this point.</p>
<p>380<br>00:15:18,300 –&gt; 00:15:19,259<br>We still have the same problem.</p>
<p>381<br>00:15:19,259 –&gt; 00:15:20,940<br>We want to do a join.</p>
<p>382<br>00:15:20,940 –&gt; 00:15:24,420<br>So the dumbest way to do a join would be,</p>
<p>383<br>00:15:24,420 –&gt; 00:15:27,180<br>well, I know I need data at these four partitions.</p>
<p>384<br>00:15:27,180 –&gt; 00:15:30,580<br>So let me just go copy all of them back to a single node</p>
<p>385<br>00:15:30,580 –&gt; 00:15:32,779<br>so I can do a join.</p>
<p>386<br>00:15:32,779 –&gt; 00:15:34,259<br>But obviously that defeats the purpose</p>
<p>387<br>00:15:34,259 –&gt; 00:15:36,620<br>of having a distributed database, right?</p>
<p>388<br>00:15:36,620 –&gt; 00:15:40,139<br>Because if my database is 10 petabytes,</p>
<p>389<br>00:15:40,139 –&gt; 00:15:42,899<br>and I only have a terabyte of memory on that one partition</p>
<p>390<br>00:15:42,899 –&gt; 00:15:45,259<br>when I want one node, then I’m going to read everything</p>
<p>391<br>00:15:45,259 –&gt; 00:15:48,419<br>from disk from the other nodes, put it into memory,</p>
<p>392<br>00:15:48,419 –&gt; 00:15:51,139<br>and then now I’m basically a single node machine.</p>
<p>393<br>00:15:51,139 –&gt; 00:15:56,419<br>And I don’t get any benefit of having multiple resources.</p>
<p>394<br>00:15:56,419 –&gt; 00:15:58,659<br>So this sucks, and we want to avoid this.</p>
<p>395<br>00:15:58,659 –&gt; 00:16:00,500<br>And obviously, when you want to do this in such a way</p>
<p>396<br>00:16:00,500 –&gt; 00:16:04,500<br>that I distribute the query across these multiple nodes,</p>
<p>397<br>00:16:04,500 –&gt; 00:16:06,299<br>in such a way that we don’t have any false positives,</p>
<p>398<br>00:16:06,299 –&gt; 00:16:08,500<br>false negatives.</p>
<p>399<br>00:16:08,500 –&gt; 00:16:11,139<br>We want to peer as if we’re running on a single node,</p>
<p>400<br>00:16:11,139 –&gt; 00:16:13,939<br>even though we’re not.</p>
<p>401<br>00:16:13,939 –&gt; 00:16:17,779<br>So we’ve already gone through the Snowflake versus Star</p>
<p>402<br>00:16:17,779 –&gt; 00:16:19,539<br>schema stuff.</p>
<p>403<br>00:16:19,539 –&gt; 00:16:20,699<br>We’re going to talk about the execution models</p>
<p>404<br>00:16:20,699 –&gt; 00:16:22,659<br>you could have for a distributed database for doing</p>
<p>405<br>00:16:22,659 –&gt; 00:16:23,539<br>our left stuff.</p>
<p>406<br>00:16:23,539 –&gt; 00:16:25,379<br>Talk a little bit about how we want to do query planning,</p>
<p>407<br>00:16:25,379 –&gt; 00:16:28,340<br>and what changes now when we’re in an distributed environment.</p>
<p>408<br>00:16:28,340 –&gt; 00:16:31,419<br>And then in the TLDR, for that, it’s going to be everything’s</p>
<p>409<br>00:16:31,419 –&gt; 00:16:32,019<br>the same.</p>
<p>410<br>00:16:32,019 –&gt; 00:16:36,460<br>It’s just now we need to account for the location of data,</p>
<p>411<br>00:16:36,460 –&gt; 00:16:37,779<br>and maybe how it’s partitioned.</p>
<p>412<br>00:16:37,779 –&gt; 00:16:40,139<br>But all the self-taught stuff, like join ordering,</p>
<p>413<br>00:16:40,139 –&gt; 00:16:41,819<br>credit can push down, projection push down,</p>
<p>414<br>00:16:41,819 –&gt; 00:16:44,179<br>all that is still here.</p>
<p>415<br>00:16:44,179 –&gt; 00:16:46,419<br>Then we’ll talk about how we can actually execute joins.</p>
<p>416<br>00:16:46,419 –&gt; 00:16:48,139<br>And again, there isn’t, it’s still going</p>
<p>417<br>00:16:48,139 –&gt; 00:16:50,779<br>to be either sortmars join or hash join in distributed</p>
<p>418<br>00:16:50,779 –&gt; 00:16:51,860<br>environment.</p>
<p>419<br>00:16:51,860 –&gt; 00:16:55,299<br>Most distributed data says are going to have joins.</p>
<p>420<br>00:16:55,299 –&gt; 00:16:57,659<br>Because most of the time you’re going to be hash partitioned.</p>
<p>421<br>00:16:57,659 –&gt; 00:16:59,100<br>But we’ll get that in a second.</p>
<p>422<br>00:16:59,100 –&gt; 00:17:01,580<br>But again, we need to account for where the data is actually</p>
<p>423<br>00:17:01,580 –&gt; 00:17:03,340<br>located that we need.</p>
<p>424<br>00:17:03,340 –&gt; 00:17:05,339<br>And if it is partitioned, what key is it partitioned on?</p>
<p>425<br>00:17:05,339 –&gt; 00:17:07,819<br>Is it the thing we’re trying to join on or not?</p>
<p>426<br>00:17:07,819 –&gt; 00:17:09,500<br>And then we’ll finish off doing, again,</p>
<p>427<br>00:17:09,500 –&gt; 00:17:12,299<br>quick overview of what cloud database systems look like,</p>
<p>428<br>00:17:12,299 –&gt; 00:17:15,819<br>specifically in the context of OLAP systems.</p>
<p>429<br>00:17:15,819 –&gt; 00:17:18,539<br>And I’ll confess now that this is a precursor</p>
<p>430<br>00:17:18,539 –&gt; 00:17:21,299<br>to what 721 will be next semester.</p>
<p>431<br>00:17:21,299 –&gt; 00:17:23,299<br>OK?</p>
<p>432<br>00:17:23,299 –&gt; 00:17:25,299<br>All right.</p>
<p>433<br>00:17:25,299 –&gt; 00:17:27,220<br>All right, so the first thing we’ve got to do is execute queries.</p>
<p>434<br>00:17:27,220 –&gt; 00:17:29,019<br>Obviously.</p>
<p>435<br>00:17:29,019 –&gt; 00:17:30,740<br>And as I said already, it’s basically</p>
<p>436<br>00:17:30,740 –&gt; 00:17:33,740<br>going to be the same thing as we do in a single node database</p>
<p>437<br>00:17:33,740 –&gt; 00:17:34,539<br>system.</p>
<p>438<br>00:17:34,539 –&gt; 00:17:37,220<br>The query plan is going to be most likely a DAG</p>
<p>439<br>00:17:37,220 –&gt; 00:17:39,140<br>of these physical operators that are</p>
<p>440<br>00:17:39,140 –&gt; 00:17:42,420<br>going to be moving data from one operator to the next.</p>
<p>441<br>00:17:42,420 –&gt; 00:17:46,740<br>And whether we’re doing a push versus pull in a processing model,</p>
<p>442<br>00:17:46,740 –&gt; 00:17:48,740<br>it doesn’t matter.</p>
<p>443<br>00:17:48,740 –&gt; 00:17:50,779<br>And so now when in our database system,</p>
<p>444<br>00:17:50,779 –&gt; 00:17:52,140<br>since we know we’re distributed,</p>
<p>445<br>00:17:52,140 –&gt; 00:17:54,740<br>we know that data could be not physically</p>
<p>446<br>00:17:54,740 –&gt; 00:17:56,860<br>located where the operator is actually running,</p>
<p>447<br>00:17:56,860 –&gt; 00:17:59,700<br>we have to consider where is the data coming from,</p>
<p>448<br>00:17:59,700 –&gt; 00:18:01,740<br>and where does it need to go next.</p>
<p>449<br>00:18:01,740 –&gt; 00:18:04,259<br>And this is all stuff we do some bed inside of our query</p>
<p>450<br>00:18:04,259 –&gt; 00:18:07,339<br>plan.</p>
<p>451<br>00:18:07,339 –&gt; 00:18:10,420<br>So again, using table scans, we would know,</p>
<p>452<br>00:18:10,420 –&gt; 00:18:11,900<br>we need to access this table.</p>
<p>453<br>00:18:11,900 –&gt; 00:18:15,299<br>And if I’m partition or not, if yes, where’s the data</p>
<p>454<br>00:18:15,299 –&gt; 00:18:18,379<br>located, or if I’m shared disk, for sure, nothing,</p>
<p>455<br>00:18:18,379 –&gt; 00:18:21,379<br>is it from a central disk I can get the data from,</p>
<p>456<br>00:18:21,379 –&gt; 00:18:23,980<br>or do I need to go to a node that has it?</p>
<p>457<br>00:18:23,980 –&gt; 00:18:25,579<br>For joins, again, we’ll cover these in a second.</p>
<p>458<br>00:18:25,579 –&gt; 00:18:26,500<br>Aggregations are sorting.</p>
<p>459<br>00:18:26,500 –&gt; 00:18:28,420<br>All this is still the same.</p>
<p>460<br>00:18:29,140 –&gt; 00:18:30,140<br>Right?</p>
<p>461<br>00:18:32,140 –&gt; 00:18:38,060<br>So then the next question is, for a given query plan,</p>
<p>462<br>00:18:38,060 –&gt; 00:18:43,860<br>how are we going to get the data we need to execute these operators?</p>
<p>463<br>00:18:43,860 –&gt; 00:18:47,300<br>And we talked a little bit about this before in the OTB systems,</p>
<p>464<br>00:18:47,300 –&gt; 00:18:49,460<br>about this notion of, I’m not going to push the query</p>
<p>465<br>00:18:49,460 –&gt; 00:18:51,740<br>to where the data is actually physically located,</p>
<p>466<br>00:18:51,740 –&gt; 00:18:56,140<br>or should I pull the data to where I want to run my query?</p>
<p>467<br>00:18:56,140 –&gt; 00:19:02,220<br>And in the OTB world, the size of the query and the size of the data,</p>
<p>468<br>00:19:02,220 –&gt; 00:19:05,500<br>the data is not going to be that big.</p>
<p>469<br>00:19:05,500 –&gt; 00:19:09,259<br>But as I showed my early example, if my database is 10 petabytes,</p>
<p>470<br>00:19:09,259 –&gt; 00:19:12,540<br>then maybe I don’t want to pull the data to some node.</p>
<p>471<br>00:19:12,540 –&gt; 00:19:17,620<br>I may want to send my execution request over to where it’s located.</p>
<p>472<br>00:19:17,620 –&gt; 00:19:19,820<br>Then there’s also now a question of what</p>
<p>473<br>00:19:19,820 –&gt; 00:19:23,220<br>happens with the data, the in-bidant results that I generate.</p>
<p>474<br>00:19:23,220 –&gt; 00:19:26,900<br>Should I store them on my current node,</p>
<p>475<br>00:19:26,900 –&gt; 00:19:30,220<br>and then let someone come get it for me, or can I decide, OK,</p>
<p>476<br>00:19:30,220 –&gt; 00:19:31,380<br>I know where this needs to go.</p>
<p>477<br>00:19:31,380 –&gt; 00:19:33,259<br>Let me set sending it now.</p>
<p>478<br>00:19:33,259 –&gt; 00:19:35,940<br>Or should I write it out to a shared disk?</p>
<p>479<br>00:19:35,940 –&gt; 00:19:39,579<br>So that way, if I crash, and with my node crashes,</p>
<p>480<br>00:19:39,579 –&gt; 00:19:43,660<br>the other node can say, OK, well, I know they process the thing I asked it for.</p>
<p>481<br>00:19:43,660 –&gt; 00:19:46,579<br>But here’s, and here, let me go get it from a shared disk.</p>
<p>482<br>00:19:46,579 –&gt; 00:19:49,940<br>Instead of having to restart the entire query.</p>
<p>483<br>00:19:49,940 –&gt; 00:19:50,940<br>So let’s go through these.</p>
<p>484<br>00:19:50,940 –&gt; 00:19:52,700<br>One by one.</p>
<p>485<br>00:19:52,700 –&gt; 00:19:54,620<br>So again, pushing query to the data, again,</p>
<p>486<br>00:19:54,620 –&gt; 00:19:57,380<br>the idea is that we’re going to send the query or some portion of it,</p>
<p>487<br>00:19:57,380 –&gt; 00:20:01,100<br>like a query fragment, to the node that contains the data.</p>
<p>488<br>00:20:01,100 –&gt; 00:20:04,019<br>And again, this doesn’t matter where the shared disk or shared nothing,</p>
<p>489<br>00:20:04,019 –&gt; 00:20:07,580<br>because we have this notion of logical partitioning.</p>
<p>490<br>00:20:07,580 –&gt; 00:20:12,700<br>We know that if the data is at this node that it’s not actually located,</p>
<p>491<br>00:20:12,700 –&gt; 00:20:15,580<br>that it has not physically stored it yet, but it’s responsible for it,</p>
<p>492<br>00:20:15,580 –&gt; 00:20:19,299<br>it knows how to go to shared disk and pull it in.</p>
<p>493<br>00:20:19,299 –&gt; 00:20:21,259<br>And the advantage of this is that we want to be</p>
<p>494<br>00:20:21,259 –&gt; 00:20:26,379<br>do as much filtering and processing on the data where it resides</p>
<p>495<br>00:20:26,379 –&gt; 00:20:29,579<br>before we send it off to the next node.</p>
<p>496<br>00:20:29,579 –&gt; 00:20:34,740<br>Because traditionally, the network was always faster than disk.</p>
<p>497<br>00:20:34,740 –&gt; 00:20:37,819<br>That is changing a little bit now.</p>
<p>498<br>00:20:37,819 –&gt; 00:20:40,700<br>But the network’s in disk getting ready very fast.</p>
<p>499<br>00:20:40,700 –&gt; 00:20:45,180<br>But again, just think of like if you had a pay for the network traffic itself</p>
<p>500<br>00:20:45,180 –&gt; 00:20:47,299<br>in a data center, or to go outside of data center,</p>
<p>501<br>00:20:47,299 –&gt; 00:20:50,259<br>like Amazon charges you when you leave their data center.</p>
<p>502<br>00:20:50,259 –&gt; 00:20:54,299<br>I want to throw away as much useful data as I can before I send along.</p>
<p>503<br>00:20:54,299 –&gt; 00:20:56,619<br>Again, it’s the same thing as doing projection pushdown,</p>
<p>504<br>00:20:56,619 –&gt; 00:21:00,099<br>or predicate pushdown when we did query planning on a single node.</p>
<p>505<br>00:21:00,099 –&gt; 00:21:04,660<br>We want to filter things that are much as possible before we start sending it along.</p>
<p>506<br>00:21:04,660 –&gt; 00:21:09,339<br>So if we can send some portion of our query to where the data is located,</p>
<p>507<br>00:21:09,339 –&gt; 00:21:12,059<br>do much processing as we can there before it moves on to the next stage,</p>
<p>508<br>00:21:12,059 –&gt; 00:21:15,579<br>then we could get a big win there.</p>
<p>509<br>00:21:15,579 –&gt; 00:21:18,379<br>In some cases though, you actually may want to pull the data to the query.</p>
<p>510<br>00:21:18,380 –&gt; 00:21:21,340<br>Again, if it’s the shared disk, you may have to, right?</p>
<p>511<br>00:21:21,340 –&gt; 00:21:27,380<br>Because I can’t maybe run my query on the shared disk architecture on the object store.</p>
<p>512<br>00:21:27,380 –&gt; 00:21:28,260<br>Not entirely true.</p>
<p>513<br>00:21:28,260 –&gt; 00:21:30,580<br>We’ll see that in a second.</p>
<p>514<br>00:21:30,580 –&gt; 00:21:33,380<br>But if you can’t, there’s no compute resources.</p>
<p>515<br>00:21:33,380 –&gt; 00:21:36,300<br>There’s nothing to say, hey, execute some piece of code for me,</p>
<p>516<br>00:21:36,300 –&gt; 00:21:40,220<br>where the data is located, then you have to pull it in.</p>
<p>517<br>00:21:40,220 –&gt; 00:21:45,020<br>And so the reason why I’m saying the lines get blurred is because in modern cloud systems,</p>
<p>518<br>00:21:45,019 –&gt; 00:21:51,059<br>like the object stores, you can actually run what looks like queries.</p>
<p>519<br>00:21:51,059 –&gt; 00:21:58,819<br>So in S3 in their documentation, you can basically run SQL queries on S3.</p>
<p>520<br>00:21:58,819 –&gt; 00:22:02,460<br>And it’s not full SQL obviously, and it’s pretty basic.</p>
<p>521<br>00:22:02,460 –&gt; 00:22:07,059<br>But this basic, this basic, basic, predicate pushdown that we talked about before,</p>
<p>522<br>00:22:07,059 –&gt; 00:22:12,619<br>where I say, okay, I had this filter clause, this filter my where clause,</p>
<p>523<br>00:22:12,619 –&gt; 00:22:15,339<br>when I go request the data from the object store,</p>
<p>524<br>00:22:15,339 –&gt; 00:22:17,859<br>I also pass along that where clause and let it do some filtering.</p>
<p>525<br>00:22:17,859 –&gt; 00:22:20,459<br>So it only sends me back the data that I need.</p>
<p>526<br>00:22:20,459 –&gt; 00:22:24,099<br>Instead of copying the whole thing, figuring out later.</p>
<p>527<br>00:22:24,099 –&gt; 00:22:28,859<br>So it’s not just Amazon, Microsoft also has this for their blob storage,</p>
<p>528<br>00:22:28,859 –&gt; 00:22:31,619<br>that you can do something that looks like SQL.</p>
<p>529<br>00:22:31,619 –&gt; 00:22:33,659<br>And actually, what’s really cool about this stuff too,</p>
<p>530<br>00:22:33,659 –&gt; 00:22:36,539<br>because we’ll talk about file formats in a second,</p>
<p>531<br>00:22:36,539 –&gt; 00:22:42,700<br>is that these object stores have native support for CSVs and JSON files,</p>
<p>532<br>00:22:42,700 –&gt; 00:22:45,379<br>and parquet, which is a binary format, we’re covering in a second.</p>
<p>533<br>00:22:45,379 –&gt; 00:22:50,379<br>But it’s not just like, it’s not just like raw text files,</p>
<p>534<br>00:22:50,379 –&gt; 00:22:54,180<br>you can store things in an efficient, compressed binary format,</p>
<p>535<br>00:22:54,180 –&gt; 00:22:59,940<br>and they’ll know how to process it on the fly for you and run your queries.</p>
<p>536<br>00:22:59,940 –&gt; 00:23:02,859<br>I don’t know whether you can do aggregations, I know you can do filtering.</p>
<p>537<br>00:23:02,859 –&gt; 00:23:06,019<br>So it’s not a full database engine down there, you can do whatever you want.</p>
<p>538<br>00:23:06,019 –&gt; 00:23:09,740<br>But you can clean some things up before you send it over.</p>
<p>539<br>00:23:09,740 –&gt; 00:23:15,859<br>I haven’t looked this year, but GCP or Google didn’t support this when I looked at it.</p>
<p>540<br>00:23:15,859 –&gt; 00:23:19,779<br>And this is not actually not unique to these cloud vendors or object stores.</p>
<p>541<br>00:23:19,779 –&gt; 00:23:23,259<br>This is an old idea for shared systems before the cloud.</p>
<p>542<br>00:23:23,259 –&gt; 00:23:28,859<br>Oracle X-A data basically has an FPGA on their storage node.</p>
<p>543<br>00:23:28,859 –&gt; 00:23:30,859<br>So you can send the where clause to the FPGA,</p>
<p>544<br>00:23:30,859 –&gt; 00:23:35,099<br>and it filters the data as it’s coming over the wire to you, which is pretty cool.</p>
<p>545<br>00:23:35,099 –&gt; 00:23:37,299<br>And obviously, since Oracle controls the whole thing,</p>
<p>546<br>00:23:37,299 –&gt; 00:23:41,539<br>it’s an Oracle’s proprietary format.</p>
<p>547<br>00:23:41,539 –&gt; 00:23:46,579<br>So the lines get blurred when you say whether to share disk or share nothing.</p>
<p>548<br>00:23:46,579 –&gt; 00:23:51,099<br>The shared disk systems, especially in the cloud, are getting very good for this kind of stuff.</p>
<p>549<br>00:23:51,099 –&gt; 00:23:56,419<br>And I don’t know whether it’s snowflake and others taking advantages.</p>
<p>550<br>00:23:56,419 –&gt; 00:23:58,740<br>I know Reship does.</p>
<p>551<br>00:23:58,740 –&gt; 00:24:00,980<br>That’s public.</p>
<p>552<br>00:24:00,980 –&gt; 00:24:03,099<br>So again, this is just repeating what I’ve already said.</p>
<p>553<br>00:24:03,099 –&gt; 00:24:05,099<br>But we want to push the query to the data.</p>
<p>554<br>00:24:05,099 –&gt; 00:24:07,339<br>Query goes to this node here.</p>
<p>555<br>00:24:07,339 –&gt; 00:24:11,659<br>It recognizes that it wants to join RNS,</p>
<p>556<br>00:24:11,659 –&gt; 00:24:16,059<br>but RNS are partition based on the ID column, which was what we’re joining on.</p>
<p>557<br>00:24:16,059 –&gt; 00:24:18,819<br>So instead of the top node telling the bottom node,</p>
<p>558<br>00:24:18,819 –&gt; 00:24:21,659<br>send me whatever you all the data you have,</p>
<p>559<br>00:24:21,659 –&gt; 00:24:24,379<br>it sends down the query plan fragment.</p>
<p>560<br>00:24:24,379 –&gt; 00:24:26,459<br>Because again, there’s some metadata we’re keeping track of.</p>
<p>561<br>00:24:26,459 –&gt; 00:24:30,419<br>This says, for these tables, they’re partitioned on this column.</p>
<p>562<br>00:24:30,419 –&gt; 00:24:32,099<br>In this case, we’re doing range partitioning.</p>
<p>563<br>00:24:32,099 –&gt; 00:24:37,819<br>And I know what the range values of that ID that are located at different nodes,</p>
<p>564<br>00:24:37,819 –&gt; 00:24:39,299<br>are controlled by different nodes.</p>
<p>565<br>00:24:39,299 –&gt; 00:24:41,699<br>So I can send my query plan fragment down here to say, hey,</p>
<p>566<br>00:24:41,699 –&gt; 00:24:44,539<br>by the way, join RNS for this range.</p>
<p>567<br>00:24:44,539 –&gt; 00:24:47,579<br>And then the result gets sent back up to this node,</p>
<p>568<br>00:24:47,579 –&gt; 00:24:50,699<br>who then does a union of the local result it computed,</p>
<p>569<br>00:24:50,699 –&gt; 00:24:57,099<br>and the result that it got from the other node.</p>
<p>570<br>00:24:57,099 –&gt; 00:25:00,819<br>Share disk, same idea, for pulling the query to the data.</p>
<p>571<br>00:25:00,819 –&gt; 00:25:04,099<br>So in this case here, I want to get,</p>
<p>572<br>00:25:04,099 –&gt; 00:25:05,939<br>I query shows up at the top node.</p>
<p>573<br>00:25:05,939 –&gt; 00:25:08,700<br>It knows the bottom node here is responsible for this range.</p>
<p>574<br>00:25:08,700 –&gt; 00:25:12,700<br>So but they both had to go to share disk and get the pages that they need</p>
<p>575<br>00:25:12,700 –&gt; 00:25:14,179<br>to then do the processing.</p>
<p>576<br>00:25:14,179 –&gt; 00:25:19,700<br>And then the bottom guy sends his result up to the top node.</p>
<p>577<br>00:25:19,700 –&gt; 00:25:21,179<br>And again, it just unies the results.</p>
<p>578<br>00:25:21,820 –&gt; 00:25:23,820<br>Right?</p>
<p>579<br>00:25:23,820 –&gt; 00:25:26,740<br>OK.</p>
<p>580<br>00:25:26,740 –&gt; 00:25:33,660<br>So in the example that I’ve shown here,</p>
<p>581<br>00:25:33,660 –&gt; 00:25:36,140<br>when you get the result from the other node,</p>
<p>582<br>00:25:36,140 –&gt; 00:25:38,700<br>who processed some portion of the query for you,</p>
<p>583<br>00:25:38,700 –&gt; 00:25:42,539<br>that node is just going to store it in this buffer pool.</p>
<p>584<br>00:25:42,539 –&gt; 00:25:44,100<br>That way, if it runs out of memory,</p>
<p>585<br>00:25:44,100 –&gt; 00:25:45,940<br>because you’re getting too much data from the other guy,</p>
<p>586<br>00:25:45,940 –&gt; 00:25:50,860<br>it just spills a disk until you can combine the result and send it back.</p>
<p>587<br>00:25:50,859 –&gt; 00:25:52,459<br>Now, in some cases, for some queries,</p>
<p>588<br>00:25:52,459 –&gt; 00:25:55,819<br>you actually can just immediately start sending the data back out</p>
<p>589<br>00:25:55,819 –&gt; 00:25:58,179<br>to the client as it comes in.</p>
<p>590<br>00:25:58,179 –&gt; 00:26:00,579<br>But in some cases, if there’s another stage in the query plan,</p>
<p>591<br>00:26:00,579 –&gt; 00:26:03,899<br>you may need to store it locally and then send along the next stage.</p>
<p>592<br>00:26:03,899 –&gt; 00:26:06,899<br>You just can’t immediately send it out.</p>
<p>593<br>00:26:06,899 –&gt; 00:26:10,699<br>And then the last lecture, we made a big deal</p>
<p>594<br>00:26:10,699 –&gt; 00:26:16,179<br>about making sure that like, about crash recovery,</p>
<p>595<br>00:26:16,179 –&gt; 00:26:18,179<br>doing two-phase commit across the different nodes.</p>
<p>596<br>00:26:18,179 –&gt; 00:26:19,859<br>If we want to make a change, everyone’s</p>
<p>597<br>00:26:19,859 –&gt; 00:26:22,419<br>going to have a reason that this is going to happen.</p>
<p>598<br>00:26:22,419 –&gt; 00:26:25,339<br>But we don’t really worry about that in this world,</p>
<p>599<br>00:26:25,339 –&gt; 00:26:27,899<br>because we’re not making changes to the database.</p>
<p>600<br>00:26:27,899 –&gt; 00:26:30,339<br>We’re just doing some kind of read-only select query</p>
<p>601<br>00:26:30,339 –&gt; 00:26:33,379<br>that’s trying to get new data.</p>
<p>602<br>00:26:33,379 –&gt; 00:26:36,259<br>But then the challenge is now, what happens</p>
<p>603<br>00:26:36,259 –&gt; 00:26:39,419<br>if my query is going to run for a really long time?</p>
<p>604<br>00:26:39,419 –&gt; 00:26:41,699<br>Think hours?</p>
<p>605<br>00:26:41,699 –&gt; 00:26:47,299<br>Days less common now, but the old days, this was an issue.</p>
<p>606<br>00:26:47,299 –&gt; 00:26:48,819<br>If my query is going to run for five hours,</p>
<p>607<br>00:26:48,819 –&gt; 00:26:54,019<br>but then after hour three, one node goes down, what happens?</p>
<p>608<br>00:26:54,019 –&gt; 00:26:57,220<br>If I’m just storing things in an informal cache</p>
<p>609<br>00:26:57,220 –&gt; 00:26:59,980<br>in my buffer pool, and it’s written to my local disk</p>
<p>610<br>00:26:59,980 –&gt; 00:27:03,179<br>at the node, again, whether it’s shared disk or shared nothing,</p>
<p>611<br>00:27:03,179 –&gt; 00:27:04,379<br>then the whole query has to restart.</p>
<p>612<br>00:27:07,259 –&gt; 00:27:10,980<br>So there’s another notion of fault tolerance for databases.</p>
<p>613<br>00:27:10,980 –&gt; 00:27:13,019<br>But it’s really about query fault tolerance,</p>
<p>614<br>00:27:13,019 –&gt; 00:27:15,579<br>meaning if my query is going to spread across</p>
<p>615<br>00:27:15,579 –&gt; 00:27:18,259<br>multiple machines, all processing in parallel,</p>
<p>616<br>00:27:18,259 –&gt; 00:27:20,740<br>that I want to avoid the issue of one node going down,</p>
<p>617<br>00:27:20,740 –&gt; 00:27:24,180<br>taking the whole thing, and crashing the whole query</p>
<p>618<br>00:27:24,180 –&gt; 00:27:26,900<br>and having to restart.</p>
<p>619<br>00:27:26,900 –&gt; 00:27:29,660<br>And again, this is nothing to do with whether the data’s</p>
<p>620<br>00:27:29,660 –&gt; 00:27:32,099<br>replicate or not.</p>
<p>621<br>00:27:32,099 –&gt; 00:27:34,059<br>You wouldn’t really necessarily want to maybe replicate</p>
<p>622<br>00:27:34,059 –&gt; 00:27:36,460<br>the result at different nodes, or have two nodes compute</p>
<p>623<br>00:27:36,460 –&gt; 00:27:39,460<br>the same answer so that you can case one of them goes down,</p>
<p>624<br>00:27:39,460 –&gt; 00:27:41,099<br>because that would be really inefficient.</p>
<p>625<br>00:27:41,099 –&gt; 00:27:44,940<br>So we need a better way to record the enemy results</p>
<p>626<br>00:27:44,940 –&gt; 00:27:47,619<br>as our query runs so that if there is a crash,</p>
<p>627<br>00:27:47,619 –&gt; 00:27:50,299<br>one of the node does go down, then we don’t</p>
<p>628<br>00:27:50,299 –&gt; 00:27:53,899<br>have to restart the whole thing.</p>
<p>629<br>00:27:53,899 –&gt; 00:27:55,779<br>And so the idea is what we’re going to do here</p>
<p>630<br>00:27:55,779 –&gt; 00:27:58,739<br>is that we just need a place that where we can store data</p>
<p>631<br>00:27:58,739 –&gt; 00:28:00,459<br>for the query while it’s running.</p>
<p>632<br>00:28:00,459 –&gt; 00:28:04,899<br>So that if one node goes down, we can go pick up those results.</p>
<p>633<br>00:28:04,899 –&gt; 00:28:05,659<br>What can we use for that?</p>
<p>634<br>00:28:09,259 –&gt; 00:28:10,299<br>The shared disk, right?</p>
<p>635<br>00:28:10,299 –&gt; 00:28:11,699<br>Because that thing is not going to wear.</p>
<p>636<br>00:28:11,699 –&gt; 00:28:15,059<br>Amazon S3 going down would be, I mean, it does go down,</p>
<p>637<br>00:28:15,059 –&gt; 00:28:19,619<br>but it would be taking the entire internet down, right?</p>
<p>638<br>00:28:19,619 –&gt; 00:28:21,980<br>So we can just use a shared disk storage</p>
<p>639<br>00:28:21,980 –&gt; 00:28:25,700<br>as a way to keep almost like a checkpoint for our queries</p>
<p>640<br>00:28:25,700 –&gt; 00:28:27,579<br>while they’re running.</p>
<p>641<br>00:28:27,579 –&gt; 00:28:29,179<br>So again, same setup here.</p>
<p>642<br>00:28:29,179 –&gt; 00:28:31,220<br>I want to run this query.</p>
<p>643<br>00:28:31,220 –&gt; 00:28:34,259<br>I asked the bottom guy here to do the join.</p>
<p>644<br>00:28:34,259 –&gt; 00:28:36,659<br>And then instead of maybe sending the result immediately</p>
<p>645<br>00:28:36,659 –&gt; 00:28:39,220<br>back up to the node there, I’m going to write it</p>
<p>646<br>00:28:39,220 –&gt; 00:28:41,980<br>to my object store or write it to my shared disk.</p>
<p>647<br>00:28:41,980 –&gt; 00:28:46,180<br>And then tell the guy up there, or you would coordinate</p>
<p>648<br>00:28:46,180 –&gt; 00:28:48,299<br>it ahead of time, hey, by the way, you</p>
<p>649<br>00:28:48,299 –&gt; 00:28:50,779<br>have to run this query plan, query plan fragment.</p>
<p>650<br>00:28:50,779 –&gt; 00:28:52,660<br>Here’s the location on shared disk where you can go</p>
<p>651<br>00:28:52,660 –&gt; 00:28:53,660<br>to go to my result.</p>
<p>652<br>00:28:53,660 –&gt; 00:28:57,779<br>So now if this guy crashes, then the other node</p>
<p>653<br>00:28:57,779 –&gt; 00:29:01,500<br>can just retrieve that result and pick up where it left off.</p>
<p>654<br>00:29:01,500 –&gt; 00:29:04,420<br>Now there’s much coordination going on about like, OK,</p>
<p>655<br>00:29:04,420 –&gt; 00:29:05,700<br>this node went down.</p>
<p>656<br>00:29:05,700 –&gt; 00:29:07,299<br>Let me spin up another node in Kubernetes</p>
<p>657<br>00:29:07,299 –&gt; 00:29:10,539<br>or whatever you’re using to then replace it.</p>
<p>658<br>00:29:10,539 –&gt; 00:29:13,420<br>But I don’t need to re-compute everything that it actually did.</p>
<p>659<br>00:29:15,940 –&gt; 00:29:19,420<br>Who here has heard of a doop?</p>
<p>660<br>00:29:19,420 –&gt; 00:29:22,339<br>All right, less than half.</p>
<p>661<br>00:29:22,339 –&gt; 00:29:23,460<br>So our map reduced.</p>
<p>662<br>00:29:23,460 –&gt; 00:29:25,259<br>Who here has heard of Map reduced?</p>
<p>663<br>00:29:25,259 –&gt; 00:29:26,420<br>Same people.</p>
<p>664<br>00:29:26,420 –&gt; 00:29:30,259<br>So in the 2000s, Google came out this paper</p>
<p>665<br>00:29:30,259 –&gt; 00:29:33,339<br>of 2004ish for this technique called Map Reduce.</p>
<p>666<br>00:29:33,339 –&gt; 00:29:37,099<br>It’s basically a distributed programming paradigm</p>
<p>667<br>00:29:37,099 –&gt; 00:29:40,299<br>of framework where you write these specialized map</p>
<p>668<br>00:29:40,299 –&gt; 00:29:44,579<br>and reduce functions that allow you to do data processing.</p>
<p>669<br>00:29:44,579 –&gt; 00:29:47,019<br>It was basically arbitrary Java code,</p>
<p>670<br>00:29:47,019 –&gt; 00:29:48,740<br>or at least in who did it was.</p>
<p>671<br>00:29:48,740 –&gt; 00:29:51,339<br>And their invitation, because they</p>
<p>672<br>00:29:51,339 –&gt; 00:29:53,299<br>were assuming you’re running on cheap hardware,</p>
<p>673<br>00:29:53,299 –&gt; 00:29:57,779<br>thinking thousands of single unit servers,</p>
<p>674<br>00:29:57,779 –&gt; 00:30:01,460<br>they would do this check pointing after every single sort</p>
<p>675<br>00:30:01,460 –&gt; 00:30:03,859<br>of map reduced phase.</p>
<p>676<br>00:30:03,859 –&gt; 00:30:05,619<br>And they would duplicate it three or four times.</p>
<p>677<br>00:30:05,619 –&gt; 00:30:07,019<br>It would be super expensive.</p>
<p>678<br>00:30:07,019 –&gt; 00:30:09,339<br>So in my example here, I’m showing, like, oh, yeah,</p>
<p>679<br>00:30:09,339 –&gt; 00:30:11,659<br>I run this join, and then all the results</p>
<p>680<br>00:30:11,659 –&gt; 00:30:14,740<br>get sent back to the shared disk.</p>
<p>681<br>00:30:14,740 –&gt; 00:30:16,139<br>They would do that for everything.</p>
<p>682<br>00:30:16,139 –&gt; 00:30:17,779<br>But you can be actually a bit smarter about it,</p>
<p>683<br>00:30:17,779 –&gt; 00:30:19,659<br>and then recognize it, OK, well, maybe I’m</p>
<p>684<br>00:30:19,659 –&gt; 00:30:22,059<br>going to do this join, and then immediately do something</p>
<p>685<br>00:30:22,059 –&gt; 00:30:22,779<br>else right after.</p>
<p>686<br>00:30:22,779 –&gt; 00:30:24,339<br>But I can do that locally in my node.</p>
<p>687<br>00:30:24,339 –&gt; 00:30:28,939<br>So maybe I don’t need to send out the result to the shared disk.</p>
<p>688<br>00:30:28,939 –&gt; 00:30:34,179<br>So in modern systems, they’ll do this kind of check pointing</p>
<p>689<br>00:30:34,179 –&gt; 00:30:36,939<br>similar to what Hadoop was doing to avoid having</p>
<p>690<br>00:30:36,939 –&gt; 00:30:37,939<br>to restart entire query.</p>
<p>691<br>00:30:37,940 –&gt; 00:30:40,100<br>There’s a failure, but they’re not</p>
<p>692<br>00:30:40,100 –&gt; 00:30:43,220<br>like check pointing blindly for every single step</p>
<p>693<br>00:30:43,220 –&gt; 00:30:45,340<br>as you would do in map reduced.</p>
<p>694<br>00:30:49,500 –&gt; 00:30:53,380<br>I can go for a long time, but problems with map reduced.</p>
<p>695<br>00:30:53,380 –&gt; 00:30:54,580<br>We won’t hold on to the time.</p>
<p>696<br>00:30:54,580 –&gt; 00:30:57,180<br>So let’s skip it.</p>
<p>697<br>00:30:57,180 –&gt; 00:30:58,980<br>Basically, nobody runs map reduced now anyway.</p>
<p>698<br>00:30:58,980 –&gt; 00:30:59,620<br>Nobody runs to do.</p>
<p>699<br>00:30:59,620 –&gt; 00:31:00,820<br>That’s all been deprecated.</p>
<p>700<br>00:31:00,820 –&gt; 00:31:04,059<br>And even things like, oh, as I say, Hadoop,</p>
<p>701<br>00:31:04,059 –&gt; 00:31:09,659<br>it was you write raw Java functions for our Java code</p>
<p>702<br>00:31:09,659 –&gt; 00:31:11,980<br>to process data as if it was a query.</p>
<p>703<br>00:31:11,980 –&gt; 00:31:13,419<br>People realized that was a bad idea.</p>
<p>704<br>00:31:13,419 –&gt; 00:31:15,579<br>So then they put SQL on top of it with this thing called</p>
<p>705<br>00:31:15,579 –&gt; 00:31:17,859<br>Hive, which is invented by Facebook.</p>
<p>706<br>00:31:17,859 –&gt; 00:31:19,179<br>And then that’s a terrible idea too,</p>
<p>707<br>00:31:19,179 –&gt; 00:31:20,460<br>because you’re basically converting SQL queries</p>
<p>708<br>00:31:20,460 –&gt; 00:31:21,339<br>into map reduced jobs.</p>
<p>709<br>00:31:21,339 –&gt; 00:31:23,579<br>So all the crappy problems you have of map reduced,</p>
<p>710<br>00:31:23,579 –&gt; 00:31:26,379<br>like inefficient architecture, you inherit, even though now</p>
<p>711<br>00:31:26,379 –&gt; 00:31:27,940<br>you’re least writing in SQL.</p>
<p>712<br>00:31:27,940 –&gt; 00:31:30,460<br>And then they realized that was a bad idea.</p>
<p>713<br>00:31:30,460 –&gt; 00:31:32,539<br>So then there’s things like presto or Treno, which</p>
<p>714<br>00:31:32,539 –&gt; 00:31:36,139<br>are covered in a second that are more efficient replacements</p>
<p>715<br>00:31:36,139 –&gt; 00:31:39,159<br>for running analytical queries on top of shared</p>
<p>716<br>00:31:39,159 –&gt; 00:31:41,339<br>distortion.</p>
<p>717<br>00:31:41,339 –&gt; 00:31:44,259<br>But this query check pointing to the default tolerance,</p>
<p>718<br>00:31:44,259 –&gt; 00:31:46,259<br>that is one thing that came out of the map reduced world</p>
<p>719<br>00:31:46,259 –&gt; 00:31:49,099<br>that has permeated throughout indistributed</p>
<p>720<br>00:31:49,099 –&gt; 00:31:53,379<br>evidences and distribution of relational evidences.</p>
<p>721<br>00:31:53,379 –&gt; 00:31:55,019<br>For query planning, again, it’s all</p>
<p>722<br>00:31:55,019 –&gt; 00:31:56,299<br>the stuff we talked about for.</p>
<p>723<br>00:31:56,299 –&gt; 00:31:58,779<br>We still want to do projection pushdown, predicate pushdown.</p>
<p>724<br>00:31:58,779 –&gt; 00:32:01,099<br>We still need to figure out the optimal join ordering.</p>
<p>725<br>00:32:01,099 –&gt; 00:32:03,259<br>Again, all of that doesn’t go away.</p>
<p>726<br>00:32:03,259 –&gt; 00:32:05,659<br>But now what we need to do is consider, again,</p>
<p>727<br>00:32:05,659 –&gt; 00:32:08,859<br>where data is actually physically located</p>
<p>728<br>00:32:08,859 –&gt; 00:32:11,539<br>and also the network transfer calls.</p>
<p>729<br>00:32:11,539 –&gt; 00:32:15,619<br>Because again, as equivalent to reading something from disk,</p>
<p>730<br>00:32:15,619 –&gt; 00:32:18,779<br>I got to send things over the network.</p>
<p>731<br>00:32:18,779 –&gt; 00:32:19,740<br>That doesn’t come for free.</p>
<p>732<br>00:32:19,740 –&gt; 00:32:21,859<br>There’s a cost to that.</p>
<p>733<br>00:32:21,859 –&gt; 00:32:25,459<br>And so there are some systems like DB2, for example,</p>
<p>734<br>00:32:25,459 –&gt; 00:32:28,500<br>I know, when the dataism boots up,</p>
<p>735<br>00:32:28,500 –&gt; 00:32:31,460<br>and it knows that it’s in an attributed configuration,</p>
<p>736<br>00:32:31,460 –&gt; 00:32:33,660<br>it actually runs a bunch of micro benchmarks,</p>
<p>737<br>00:32:33,660 –&gt; 00:32:38,420<br>basically running like sending packets over the wire</p>
<p>738<br>00:32:38,420 –&gt; 00:32:40,980<br>to the different nodes and measuring the latency.</p>
<p>739<br>00:32:40,980 –&gt; 00:32:42,819<br>And then it uses those measurements</p>
<p>740<br>00:32:42,819 –&gt; 00:32:46,700<br>to as values in its cost model decide how expensive</p>
<p>741<br>00:32:46,700 –&gt; 00:32:48,700<br>is something versus reading from local from disk</p>
<p>742<br>00:32:48,700 –&gt; 00:32:51,259<br>or reading from memory.</p>
<p>743<br>00:32:51,259 –&gt; 00:32:54,700<br>So I would say that’s the right way to do it.</p>
<p>744<br>00:32:54,700 –&gt; 00:32:57,299<br>Most systems just have a hard-coded value to say,</p>
<p>745<br>00:32:57,299 –&gt; 00:32:59,180<br>here’s how much it costs to send a byte over the network,</p>
<p>746<br>00:32:59,180 –&gt; 00:33:01,779<br>or a megabyte over the network.</p>
<p>747<br>00:33:01,779 –&gt; 00:33:04,579<br>But of course, the network connections aren’t always</p>
<p>748<br>00:33:04,579 –&gt; 00:33:05,379<br>symmetrical.</p>
<p>749<br>00:33:05,379 –&gt; 00:33:07,339<br>So doing some kind of micro benchmarking</p>
<p>750<br>00:33:07,339 –&gt; 00:33:11,099<br>like DB2 does is the right way to go.</p>
<p>751<br>00:33:11,099 –&gt; 00:33:12,379<br>All right, so the question is now, what</p>
<p>752<br>00:33:12,379 –&gt; 00:33:14,139<br>are we actually going to send between the different nodes</p>
<p>753<br>00:33:14,139 –&gt; 00:33:17,659<br>to tell them to do work on behalf of our distributed query?</p>
<p>754<br>00:33:17,659 –&gt; 00:33:20,700<br>So the most common approach is to send physical operators.</p>
<p>755<br>00:33:20,700 –&gt; 00:33:22,980<br>And this is basically, again, the same thing</p>
<p>756<br>00:33:22,980 –&gt; 00:33:26,220<br>you would get in parallel execution on your single node</p>
<p>757<br>00:33:26,220 –&gt; 00:33:27,460<br>database system.</p>
<p>758<br>00:33:27,460 –&gt; 00:33:30,620<br>You would break up your query plan into plan fragments,</p>
<p>759<br>00:33:30,620 –&gt; 00:33:33,220<br>and then you distributed them amongst the nodes.</p>
<p>760<br>00:33:33,220 –&gt; 00:33:35,059<br>And you may annotate it with information</p>
<p>761<br>00:33:35,059 –&gt; 00:33:38,660<br>about where the data is coming from and where it needs to go next.</p>
<p>762<br>00:33:38,660 –&gt; 00:33:41,339<br>But what you’re sending to the different nodes</p>
<p>763<br>00:33:41,339 –&gt; 00:33:43,900<br>are these physical plans, because the physical operators,</p>
<p>764<br>00:33:43,900 –&gt; 00:33:46,380<br>because the plans already been decided</p>
<p>765<br>00:33:46,380 –&gt; 00:33:48,460<br>by some kind of centralized coordinator or centralized</p>
<p>766<br>00:33:48,460 –&gt; 00:33:50,460<br>optimizer.</p>
<p>767<br>00:33:50,460 –&gt; 00:33:54,539<br>And most systems are going to use this approach.</p>
<p>768<br>00:33:54,539 –&gt; 00:33:59,019<br>An alternative is that you take the output of a query optimizer</p>
<p>769<br>00:33:59,019 –&gt; 00:34:01,659<br>that’s going to centralize coordinator.</p>
<p>770<br>00:34:01,659 –&gt; 00:34:03,339<br>That’s going to be physical operators.</p>
<p>771<br>00:34:03,339 –&gt; 00:34:05,819<br>And then you break that up into the fragments</p>
<p>772<br>00:34:05,819 –&gt; 00:34:07,460<br>that you want to send in different nodes.</p>
<p>773<br>00:34:07,460 –&gt; 00:34:13,139<br>But then you reverse those physical operators back to SQL.</p>
<p>774<br>00:34:13,139 –&gt; 00:34:16,460<br>And then you send SQL to the different nodes.</p>
<p>775<br>00:34:16,460 –&gt; 00:34:19,059<br>And the idea here is that you’ve already</p>
<p>776<br>00:34:19,059 –&gt; 00:34:22,380<br>done so this global optimization of figuring out</p>
<p>777<br>00:34:22,380 –&gt; 00:34:24,460<br>what query plan fragments need to execute on what</p>
<p>778<br>00:34:24,460 –&gt; 00:34:25,539<br>nodes.</p>
<p>779<br>00:34:25,539 –&gt; 00:34:29,260<br>But then rather than deciding for that node,</p>
<p>780<br>00:34:29,260 –&gt; 00:34:31,659<br>here’s exactly the query plan I want you to run.</p>
<p>781<br>00:34:31,659 –&gt; 00:34:36,340<br>You get them SQL, which they can then now parse and optimize</p>
<p>782<br>00:34:36,340 –&gt; 00:34:39,740<br>locally, because they may make a better decision</p>
<p>783<br>00:34:39,740 –&gt; 00:34:41,940<br>on the node that they’re running on based on what they see</p>
<p>784<br>00:34:41,940 –&gt; 00:34:43,539<br>and the data that they do have.</p>
<p>785<br>00:34:43,539 –&gt; 00:34:46,019<br>The idea is, again, instead of having a global view,</p>
<p>786<br>00:34:46,019 –&gt; 00:34:49,300<br>or trying to maintain a global view of how to optimize the system,</p>
<p>787<br>00:34:49,300 –&gt; 00:34:52,460<br>you get far enough to say, OK, guys, here’s the work I want you to do.</p>
<p>788<br>00:34:52,460 –&gt; 00:34:55,740<br>But then they can each make their own local decision.</p>
<p>789<br>00:34:55,740 –&gt; 00:34:56,420<br>So this is rare.</p>
<p>790<br>00:34:56,420 –&gt; 00:34:58,500<br>Very few systems do this.</p>
<p>791<br>00:34:58,500 –&gt; 00:35:00,699<br>Single store, I think they still do this.</p>
<p>792<br>00:35:00,699 –&gt; 00:35:02,139<br>I don’t think they would have changed.</p>
<p>793<br>00:35:02,139 –&gt; 00:35:02,740<br>But they do this.</p>
<p>794<br>00:35:02,740 –&gt; 00:35:07,099<br>And then VTES is a, it’s not really an analytical database</p>
<p>795<br>00:35:07,099 –&gt; 00:35:10,179<br>system, but it is a distributed system.</p>
<p>796<br>00:35:10,179 –&gt; 00:35:13,980<br>It’s a sort of sharding middleware for my SQL that</p>
<p>797<br>00:35:13,980 –&gt; 00:35:16,059<br>was developed by YouTube.</p>
<p>798<br>00:35:16,059 –&gt; 00:35:18,940<br>I say YouTube runs off my SQL.</p>
<p>799<br>00:35:18,940 –&gt; 00:35:21,139<br>And because the nodes that they’re talking to,</p>
<p>800<br>00:35:21,139 –&gt; 00:35:26,259<br>or I’ll just my SQL nodes, my SQL can’t take in a physical plan.</p>
<p>801<br>00:35:26,259 –&gt; 00:35:29,980<br>They got to convert it back to SQL and then send it to the nodes.</p>
<p>802<br>00:35:29,980 –&gt; 00:35:33,980<br>And then that node can do all the parsing and optimizing locally.</p>
<p>803<br>00:35:33,980 –&gt; 00:35:36,019<br>So the idea is like this.</p>
<p>804<br>00:35:36,019 –&gt; 00:35:38,460<br>If you want to send SQL queries.</p>
<p>805<br>00:35:38,460 –&gt; 00:35:40,379<br>So this is the join we have before.</p>
<p>806<br>00:35:40,379 –&gt; 00:35:42,219<br>In our catalog, we keep track of, here’s</p>
<p>807<br>00:35:42,219 –&gt; 00:35:46,179<br>the range partition we have for our database, for our tables.</p>
<p>808<br>00:35:46,179 –&gt; 00:35:51,219<br>So we take the original query.</p>
<p>809<br>00:35:51,219 –&gt; 00:35:53,980<br>And then we figure out, OK, here’s the data I need to access.</p>
<p>810<br>00:35:53,980 –&gt; 00:35:59,099<br>And then I modify the query now to include the join clause for the data that’s</p>
<p>811<br>00:35:59,099 –&gt; 00:36:01,139<br>local to it.</p>
<p>812<br>00:36:01,139 –&gt; 00:36:04,419<br>And then this node will get that query, run it through the same optimization</p>
<p>813<br>00:36:04,419 –&gt; 00:36:07,819<br>path that I did before.</p>
<p>814<br>00:36:07,819 –&gt; 00:36:12,179<br>And the idea here is, again, maybe the centralized view doesn’t have the complete</p>
<p>815<br>00:36:12,179 –&gt; 00:36:15,940<br>view or up-to-date view of what the SS6R in the database.</p>
<p>816<br>00:36:16,940 –&gt; 00:36:19,099<br>Or at each node.</p>
<p>817<br>00:36:19,099 –&gt; 00:36:22,019<br>And then the, again, there’ll be some centralized coordinator that knows how to</p>
<p>818<br>00:36:22,019 –&gt; 00:36:24,579<br>union results and put things back together at the end.</p>
<p>819<br>00:36:27,539 –&gt; 00:36:31,539<br>I think it’s a clever idea.</p>
<p>820<br>00:36:31,539 –&gt; 00:36:35,379<br>Reversing from physical plan back to SQL is not trivial.</p>
<p>821<br>00:36:35,379 –&gt; 00:36:38,219<br>And most of it don’t do this.</p>
<p>822<br>00:36:38,219 –&gt; 00:36:43,579<br>And when you think about it too in an O-lap system, oftentimes the data is not</p>
<p>823<br>00:36:43,579 –&gt; 00:36:45,299<br>changing that often.</p>
<p>824<br>00:36:45,300 –&gt; 00:36:46,740<br>There’s ways to handle that.</p>
<p>825<br>00:36:46,740 –&gt; 00:36:48,700<br>We’ll be in cover later.</p>
<p>826<br>00:36:48,700 –&gt; 00:36:54,940<br>So the idea that every node may have a better view of what the data looks like versus</p>
<p>827<br>00:36:54,940 –&gt; 00:36:57,220<br>the global view, it doesn’t always hold up.</p>
<p>828<br>00:36:57,220 –&gt; 00:36:58,060<br>Yes.</p>
<p>829<br>00:36:58,060 –&gt; 00:36:59,740<br>Why do you have to move the code to SQL?</p>
<p>830<br>00:36:59,740 –&gt; 00:37:03,900<br>Why can’t you send it to the plan and then the local node again?</p>
<p>831<br>00:37:03,900 –&gt; 00:37:05,660<br>Do you rest the condition?</p>
<p>832<br>00:37:05,660 –&gt; 00:37:06,460<br>Yeah, to his point.</p>
<p>833<br>00:37:06,460 –&gt; 00:37:08,460<br>So why do you have to send SQL?</p>
<p>834<br>00:37:08,460 –&gt; 00:37:11,940<br>Why couldn’t you send, say, the physical plan or the logical plan?</p>
<p>835<br>00:37:11,940 –&gt; 00:37:17,139<br>Then just had their query optimizer just optimize that.</p>
<p>836<br>00:37:17,139 –&gt; 00:37:23,380<br>Because then you need to build a separate code now for the optimizer to take in that as input</p>
<p>837<br>00:37:23,380 –&gt; 00:37:30,220<br>and inject all the internal metadata about the search process.</p>
<p>838<br>00:37:30,220 –&gt; 00:37:33,260<br>It’s weird.</p>
<p>839<br>00:37:33,260 –&gt; 00:37:37,300<br>It’s not now people usually write their optimizers.</p>
<p>840<br>00:37:37,300 –&gt; 00:37:38,300<br>You should.</p>
<p>841<br>00:37:39,019 –&gt; 00:37:43,539<br>Ideally, you want your optimizer to be able to stop, dump out its state, and then load</p>
<p>842<br>00:37:43,539 –&gt; 00:37:44,539<br>it back in.</p>
<p>843<br>00:37:44,539 –&gt; 00:37:46,860<br>Very few systems can do that.</p>
<p>844<br>00:37:46,860 –&gt; 00:37:47,860<br>Right?</p>
<p>845<br>00:37:47,860 –&gt; 00:37:53,100<br>Think of it like it’s like rebooting the stack of a search tree.</p>
<p>846<br>00:37:53,100 –&gt; 00:37:55,820<br>You have to suck all that out and then inject it back in.</p>
<p>847<br>00:37:55,820 –&gt; 00:37:57,820<br>You can do it as an additional engineering.</p>
<p>848<br>00:37:57,820 –&gt; 00:37:58,820<br>Great.</p>
<p>849<br>00:37:58,820 –&gt; 00:38:00,820<br>All right.</p>
<p>850<br>00:38:00,820 –&gt; 00:38:08,019<br>So now we got to talk about how we actually want to execute our joins.</p>
<p>851<br>00:38:08,019 –&gt; 00:38:12,699<br>So again, as I said before, we’re still going to be either doing hash join or software’s</p>
<p>852<br>00:38:12,699 –&gt; 00:38:13,699<br>join.</p>
<p>853<br>00:38:13,699 –&gt; 00:38:17,539<br>Nestle loop join if you’re really unlucky.</p>
<p>854<br>00:38:17,539 –&gt; 00:38:24,780<br>But the trade-offs between those two approaches are still the same, even in a distributed environment.</p>
<p>855<br>00:38:24,780 –&gt; 00:38:28,619<br>And as I said before, we could just try to put all our data that’s called spread across</p>
<p>856<br>00:38:28,619 –&gt; 00:38:34,099<br>different nodes onto a single node and then do the join there that it would work and</p>
<p>857<br>00:38:34,099 –&gt; 00:38:38,860<br>wouldn’t have any false negatives because we’re guaranteed that the data, you know, we’re</p>
<p>858<br>00:38:38,860 –&gt; 00:38:41,139<br>trying to join will be located with each other.</p>
<p>859<br>00:38:41,139 –&gt; 00:38:43,339<br>But obviously that’s not realistic.</p>
<p>860<br>00:38:43,339 –&gt; 00:38:44,339<br>All right.</p>
<p>861<br>00:38:44,339 –&gt; 00:38:51,019<br>And you know, you’re not unlikely that a single node can handle all your data.</p>
<p>862<br>00:38:51,019 –&gt; 00:38:56,299<br>So the way we’re going to do this is that to join two tables R and S, we need to get the</p>
<p>863<br>00:38:56,300 –&gt; 00:39:03,100<br>ideas that get the data that we’re trying to join from the two tables on the same node</p>
<p>864<br>00:39:03,100 –&gt; 00:39:09,980<br>based on the join key regardless of how the data has been either replicated or partitioned.</p>
<p>865<br>00:39:09,980 –&gt; 00:39:14,140<br>And so we’re going to go through four scenarios going from like best to worst and we’re going</p>
<p>866<br>00:39:14,140 –&gt; 00:39:19,980<br>to see how the, you know, basically if the data isn’t in the, isn’t partitioned or placed</p>
<p>867<br>00:39:19,980 –&gt; 00:39:25,580<br>in the way that that you need for the query, the data is going to have to move it around.</p>
<p>868<br>00:39:25,579 –&gt; 00:39:29,539<br>And this is all transparent to you as the application developer, as the user writing the</p>
<p>869<br>00:39:29,539 –&gt; 00:39:33,500<br>query, you don’t know how the data is getting where it needs to go.</p>
<p>870<br>00:39:33,500 –&gt; 00:39:37,059<br>But obviously if we’re actually building a system, we need to care about these things.</p>
<p>871<br>00:39:37,059 –&gt; 00:39:41,420<br>And the key thing we need to avoid is false negatives because we don’t want to, you know,</p>
<p>872<br>00:39:41,420 –&gt; 00:39:45,739<br>join like R and S looking, you know, where there is a join or it is two tables, it should</p>
<p>873<br>00:39:45,739 –&gt; 00:39:47,980<br>match when I give them join key values.</p>
<p>874<br>00:39:47,980 –&gt; 00:39:52,019<br>But because they’re basically located on different nodes, you know, we’re going to get incorrect</p>
<p>875<br>00:39:52,019 –&gt; 00:39:53,019<br>results for this.</p>
<p>876<br>00:39:53,019 –&gt; 00:39:54,019<br>Okay.</p>
<p>877<br>00:39:54,420 –&gt; 00:39:57,059<br>So again, we’re going to go through four scenarios.</p>
<p>878<br>00:39:57,059 –&gt; 00:39:59,420<br>And I would say everything I’m going to talk about here is again, the same for whether</p>
<p>879<br>00:39:59,420 –&gt; 00:40:00,900<br>it’s shared disk or shared nothing.</p>
<p>880<br>00:40:00,900 –&gt; 00:40:03,940<br>It doesn’t, it doesn’t matter.</p>
<p>881<br>00:40:03,940 –&gt; 00:40:10,300<br>So that’s case scenario is when one of the tables we want to join on is replicated at</p>
<p>882<br>00:40:10,300 –&gt; 00:40:11,300<br>every node.</p>
<p>883<br>00:40:11,300 –&gt; 00:40:13,259<br>Again, you can have this in your data system.</p>
<p>884<br>00:40:13,259 –&gt; 00:40:17,019<br>You can have some tables to partitioned and some tables are replicated.</p>
<p>885<br>00:40:17,019 –&gt; 00:40:20,579<br>Going back to that snowflake schema stuff before, those dimension tables are usually pretty</p>
<p>886<br>00:40:20,579 –&gt; 00:40:22,219<br>small relative to the fact table.</p>
<p>887<br>00:40:22,219 –&gt; 00:40:23,219<br>The fact tables may be huge.</p>
<p>888<br>00:40:23,219 –&gt; 00:40:27,980<br>I think every item anyone, anyone’s ever bought from Amazon, it has to be billions of</p>
<p>889<br>00:40:27,980 –&gt; 00:40:29,779<br>not trains, right?</p>
<p>890<br>00:40:29,779 –&gt; 00:40:32,819<br>But your dimension tables think of things like zip code.</p>
<p>891<br>00:40:32,819 –&gt; 00:40:38,619<br>It’s what 40,000 zip codes in the US post office changes them four times a year.</p>
<p>892<br>00:40:38,619 –&gt; 00:40:43,539<br>So you can take that 40,000 table table and you can replicate that on every single node</p>
<p>893<br>00:40:43,539 –&gt; 00:40:44,539<br>as a dimension table.</p>
<p>894<br>00:40:44,539 –&gt; 00:40:49,419<br>So now when you’re doing a join against your fact table, the data is just right there to</p>
<p>895<br>00:40:49,419 –&gt; 00:40:52,179<br>do the join, right?</p>
<p>896<br>00:40:52,179 –&gt; 00:40:55,980<br>So all you need to do now in this scenario here, again, so if I want to join R and S on</p>
<p>897<br>00:40:55,980 –&gt; 00:40:58,419<br>ID, S is replicated everywhere.</p>
<p>898<br>00:40:58,419 –&gt; 00:41:00,940<br>We partition R based on ID.</p>
<p>899<br>00:41:00,940 –&gt; 00:41:06,980<br>So each node does its local join and then this node will send its result to this other node</p>
<p>900<br>00:41:06,980 –&gt; 00:41:08,739<br>who then does you use the results.</p>
<p>901<br>00:41:08,739 –&gt; 00:41:13,899<br>You don’t actually need to look at the results, for this example here, because you know that</p>
<p>902<br>00:41:13,899 –&gt; 00:41:19,500<br>it’s non-overlapping partitions for R. So I just literally can catenate the byte buffers</p>
<p>903<br>00:41:19,500 –&gt; 00:41:23,019<br>on top of each other and then send that back to the client.</p>
<p>904<br>00:41:23,019 –&gt; 00:41:25,019<br>Right?</p>
<p>905<br>00:41:25,019 –&gt; 00:41:28,980<br>So this is the best case scenario because I did no data transfer in order to compute the</p>
<p>906<br>00:41:28,980 –&gt; 00:41:34,420<br>join and then obviously I have to send the result, but like depends on this activity or</p>
<p>907<br>00:41:34,420 –&gt; 00:41:36,500<br>whatever whatever it is I’m trying to join on.</p>
<p>908<br>00:41:36,500 –&gt; 00:41:42,940<br>And this is the bare minimum I need to send over to process this query.</p>
<p>909<br>00:41:42,940 –&gt; 00:41:46,619<br>So if your tables aren’t replicated, then it’s basically the same thing.</p>
<p>910<br>00:41:46,619 –&gt; 00:41:52,380<br>But if the two tables are partitioned on the same attributes as you’re trying to join</p>
<p>911<br>00:41:52,380 –&gt; 00:41:54,299<br>on.</p>
<p>912<br>00:41:54,299 –&gt; 00:41:57,980<br>So in this case here, again, I want to join R and S on the ID field and then it just</p>
<p>913<br>00:41:57,980 –&gt; 00:42:01,699<br>so happens that the range partitions for R and S in each partitions are exactly the</p>
<p>914<br>00:42:01,699 –&gt; 00:42:03,380<br>same.</p>
<p>915<br>00:42:03,380 –&gt; 00:42:09,219<br>So now again, I have each node process its local join in parallel and then this other node</p>
<p>916<br>00:42:09,219 –&gt; 00:42:14,059<br>sends a result to the other guy, concatenate the results and send it back to the client.</p>
<p>917<br>00:42:14,059 –&gt; 00:42:17,699<br>Right?</p>
<p>918<br>00:42:17,699 –&gt; 00:42:22,659<br>So this is nice, but it doesn’t always happen.</p>
<p>919<br>00:42:22,659 –&gt; 00:42:28,460<br>It’s not always the case that you’re going to be exactly partitioned on the thing you want</p>
<p>920<br>00:42:28,460 –&gt; 00:42:29,460<br>to join on.</p>
<p>921<br>00:42:29,460 –&gt; 00:42:32,259<br>And we’re not going to talk about how you actually pick the partitioning key, but that’s a whole</p>
<p>922<br>00:42:32,259 –&gt; 00:42:33,259<br>another problem.</p>
<p>923<br>00:42:33,259 –&gt; 00:42:36,659<br>Like that’s been shown to be NP hard.</p>
<p>924<br>00:42:36,659 –&gt; 00:42:40,340<br>For an arbitrary set of queries and an arbitrary set of attributes you could partition on,</p>
<p>925<br>00:42:40,340 –&gt; 00:42:44,100<br>you can run out the ideal partitioning scheme for your table.</p>
<p>926<br>00:42:44,100 –&gt; 00:42:45,900<br>For your tables is non-trigger.</p>
<p>927<br>00:42:45,900 –&gt; 00:42:52,220<br>And of course, like some cases maybe you get for 99% of the queries, you get this nice</p>
<p>928<br>00:42:52,220 –&gt; 00:42:56,180<br>layout like this, but of course there’s some query that shows up where it isn’t joining</p>
<p>929<br>00:42:56,180 –&gt; 00:42:58,380<br>on the partition key.</p>
<p>930<br>00:42:58,380 –&gt; 00:43:00,820<br>So that’s the third scenario.</p>
<p>931<br>00:43:00,820 –&gt; 00:43:06,500<br>So in this case here, the R table is partitioned on ID, but now my S table is partitioned on</p>
<p>932<br>00:43:06,500 –&gt; 00:43:08,780<br>value, some other attribute.</p>
<p>933<br>00:43:08,780 –&gt; 00:43:10,380<br>Okay.</p>
<p>934<br>00:43:10,380 –&gt; 00:43:16,700<br>And so if I just do the join on the local data here, again, I can add up false negatives because</p>
<p>935<br>00:43:16,700 –&gt; 00:43:22,500<br>the thing, if there’s some ID equals one, it may be on this other node here.</p>
<p>936<br>00:43:22,500 –&gt; 00:43:23,500<br>I don’t know that.</p>
<p>937<br>00:43:23,500 –&gt; 00:43:24,500<br>Right?</p>
<p>938<br>00:43:24,500 –&gt; 00:43:29,540<br>Because I partitioned it on a different key or different attribute.</p>
<p>939<br>00:43:29,540 –&gt; 00:43:36,380<br>So when this occurs, you have to do what is called a broadcast where you have the, you’re</p>
<p>940<br>00:43:36,380 –&gt; 00:43:40,180<br>basically reorganizing the one of the tables.</p>
<p>941<br>00:43:40,180 –&gt; 00:43:46,140<br>And so you’re going to take all the values of S for some ID within some range and you can</p>
<p>942<br>00:43:46,140 –&gt; 00:43:49,180<br>send it over to that partition.</p>
<p>943<br>00:43:49,180 –&gt; 00:43:50,180<br>So I take it back.</p>
<p>944<br>00:43:50,180 –&gt; 00:43:54,619<br>You’re going to basically send whatever date you have here from S. You’re going to send</p>
<p>945<br>00:43:54,619 –&gt; 00:43:57,099<br>that to every other node that’s involved in the join.</p>
<p>946<br>00:43:57,099 –&gt; 00:44:02,220<br>So now basically S is going to be replicated just as it was in the first scenario at every</p>
<p>947<br>00:44:02,220 –&gt; 00:44:03,220<br>single node.</p>
<p>948<br>00:44:03,619 –&gt; 00:44:07,579<br>So again, the reason why it’s called a broadcast is like, you’re sending, hey guys, here’s</p>
<p>949<br>00:44:07,579 –&gt; 00:44:11,659<br>the values I have for this table and everyone gets a copy of it now.</p>
<p>950<br>00:44:11,659 –&gt; 00:44:14,379<br>And again, you can do this if it’s like a dimension table that’s going to be much smaller</p>
<p>951<br>00:44:14,379 –&gt; 00:44:18,099<br>than the fact table.</p>
<p>952<br>00:44:18,099 –&gt; 00:44:21,459<br>So sometimes you’ll see this in the literature or in like documentation, they’ll call this</p>
<p>953<br>00:44:21,459 –&gt; 00:44:23,419<br>a broadcast join.</p>
<p>954<br>00:44:23,419 –&gt; 00:44:28,099<br>And it really just means that they’re doing this step to send the data around.</p>
<p>955<br>00:44:28,099 –&gt; 00:44:32,339<br>And they usually say it’s a broadcast hash join, a broadcast sort of a join.</p>
<p>956<br>00:44:32,340 –&gt; 00:44:33,820<br>It’s just they had this extra step.</p>
<p>957<br>00:44:33,820 –&gt; 00:44:34,820<br>Yes.</p>
<p>958<br>00:44:34,820 –&gt; 00:44:37,780<br>If you’re going to do this anyway, why not go back to scenario one?</p>
<p>959<br>00:44:37,780 –&gt; 00:44:40,940<br>So actually, if you can do this anyway, why not go back to scenario one?</p>
<p>960<br>00:44:40,940 –&gt; 00:44:46,539<br>I mean, this will put you into scenario one.</p>
<p>961<br>00:44:46,539 –&gt; 00:44:51,140<br>It made me the case that there’s that someone picked, I want a partition on value because</p>
<p>962<br>00:44:51,140 –&gt; 00:44:54,860<br>I have most of my queries are going to want to do a join in value or do look up some values.</p>
<p>963<br>00:44:54,860 –&gt; 00:44:56,860<br>It just happened for this one query.</p>
<p>964<br>00:44:56,860 –&gt; 00:44:58,860<br>The data is not in the layout that I want.</p>
<p>965<br>00:44:58,860 –&gt; 00:45:00,860<br>So I got to move things around.</p>
<p>966<br>00:45:01,380 –&gt; 00:45:07,620<br>In other times, we’ll talk about data lakes in the second or lake houses.</p>
<p>967<br>00:45:07,620 –&gt; 00:45:13,380<br>In that world, you are just loading much of data files in and you’re not doing any re-agordization</p>
<p>968<br>00:45:13,380 –&gt; 00:45:15,140<br>to do any partitioning of that.</p>
<p>969<br>00:45:15,140 –&gt; 00:45:19,780<br>So you basically have to look through the file and then say, okay, well, here’s the data</p>
<p>970<br>00:45:19,780 –&gt; 00:45:21,180<br>that I’m seeing.</p>
<p>971<br>00:45:21,180 –&gt; 00:45:24,539<br>Do I want to partition it and send it around or do I want to broadcast it and send it</p>
<p>972<br>00:45:24,539 –&gt; 00:45:25,539<br>around?</p>
<p>973<br>00:45:25,539 –&gt; 00:45:26,539<br>Right?</p>
<p>974<br>00:45:26,539 –&gt; 00:45:27,539<br>Yeah.</p>
<p>975<br>00:45:27,539 –&gt; 00:45:30,820<br>I didn’t really say this ahead of time.</p>
<p>976<br>00:45:30,820 –&gt; 00:45:35,019<br>This is, we’re talking about here so far, what we call it, sort of managed storage, whereas</p>
<p>977<br>00:45:35,019 –&gt; 00:45:38,620<br>I, it’s a letter like me calling in certain queries into the database and the data says,</p>
<p>978<br>00:45:38,620 –&gt; 00:45:42,380<br>oh, I know what this data is, I know what table it says, and they can decide how to move</p>
<p>979<br>00:45:42,380 –&gt; 00:45:43,380<br>things around.</p>
<p>980<br>00:45:43,380 –&gt; 00:45:45,140<br>In the data lake world, you don’t have full control.</p>
<p>981<br>00:45:45,140 –&gt; 00:45:47,980<br>We’ll cover that in a second.</p>
<p>982<br>00:45:47,980 –&gt; 00:45:48,980<br>Yes.</p>
<p>983<br>00:45:48,980 –&gt; 00:45:55,780<br>So, Professor, if there are 10 nodes, how many connections do you show to how many?</p>
<p>984<br>00:45:55,780 –&gt; 00:45:56,780<br>Everyone.</p>
<p>985<br>00:45:56,780 –&gt; 00:46:00,380<br>So his question is, if you have N nodes, is every other node sending N minus 1 nodes?</p>
<p>986<br>00:46:00,380 –&gt; 00:46:01,380<br>Yes.</p>
<p>987<br>00:46:01,380 –&gt; 00:46:04,380<br>You’re broadcasting everything you have to everyone else.</p>
<p>988<br>00:46:04,380 –&gt; 00:46:05,380<br>Yes.</p>
<p>989<br>00:46:05,380 –&gt; 00:46:14,140<br>Why not just like, take a note and advocate every, every, every, every, in the path and then</p>
<p>990<br>00:46:14,140 –&gt; 00:46:19,180<br>just get it to the heavy N node so that you don’t need to do the n squared.</p>
<p>991<br>00:46:19,180 –&gt; 00:46:26,860<br>Yes, David is, instead of doing the n squared broadcast, why not just have all the nodes pick</p>
<p>992<br>00:46:26,860 –&gt; 00:46:30,460<br>this one guy, everyone sends S and then it sends it out.</p>
<p>993<br>00:46:30,460 –&gt; 00:46:31,860<br>You can do that.</p>
<p>994<br>00:46:31,860 –&gt; 00:46:32,860<br>Right?</p>
<p>995<br>00:46:32,860 –&gt; 00:46:36,900<br>The big idea is though, we’re basically replicating S after it wasn’t replicating.</p>
<p>996<br>00:46:36,900 –&gt; 00:46:38,140<br>That’s the broadcast phase.</p>
<p>997<br>00:46:38,140 –&gt; 00:46:43,140<br>How you actually do that depends on the presentation.</p>
<p>998<br>00:46:43,140 –&gt; 00:46:49,340<br>There’s mom’s calling, probably.</p>
<p>999<br>00:46:49,340 –&gt; 00:46:50,340<br>Anyway.</p>
<p>1000<br>00:46:50,340 –&gt; 00:46:51,340<br>Okay.</p>
<p>1001<br>00:46:51,340 –&gt; 00:46:52,340<br>And then we do, yes, sorry.</p>
<p>1002<br>00:46:52,340 –&gt; 00:47:01,340<br>And then share nothing and, um, what happens if, when you combine like tables and each</p>
<p>1003<br>00:47:01,340 –&gt; 00:47:05,140<br>of those nodes, like, run out, or start running out storage?</p>
<p>1004<br>00:47:05,140 –&gt; 00:47:06,140<br>I’m sorry.</p>
<p>1005<br>00:47:06,140 –&gt; 00:47:07,140<br>What?</p>
<p>1006<br>00:47:07,140 –&gt; 00:47:08,140<br>And it’s in a shared system.</p>
<p>1007<br>00:47:08,140 –&gt; 00:47:09,140<br>I’m not sure nothing.</p>
<p>1008<br>00:47:09,140 –&gt; 00:47:13,380<br>You can buy, like a bunch of stuff into a single node, but like, what do we need?</p>
<p>1009<br>00:47:13,380 –&gt; 00:47:14,380<br>Stuff.</p>
<p>1010<br>00:47:14,380 –&gt; 00:47:15,380<br>I mean, like, might S here?</p>
<p>1011<br>00:47:15,380 –&gt; 00:47:16,380<br>Yes, sure.</p>
<p>1012<br>00:47:16,380 –&gt; 00:47:17,980<br>You, you put a mine S and then you run out of memory?</p>
<p>1013<br>00:47:17,980 –&gt; 00:47:18,980<br>Yeah.</p>
<p>1014<br>00:47:18,980 –&gt; 00:47:19,980<br>Or, uh, storage.</p>
<p>1015<br>00:47:19,980 –&gt; 00:47:20,980<br>Okay.</p>
<p>1016<br>00:47:20,980 –&gt; 00:47:21,980<br>Okay.</p>
<p>1017<br>00:47:21,980 –&gt; 00:47:22,980<br>So, say for a memory.</p>
<p>1018<br>00:47:22,980 –&gt; 00:47:27,380<br>If you run out of memory, again, the A to B results, this is going to get staged in my</p>
<p>1019<br>00:47:27,380 –&gt; 00:47:28,380<br>buffer pool.</p>
<p>1020<br>00:47:28,380 –&gt; 00:47:29,980<br>It just gets written out the disk.</p>
<p>1021<br>00:47:29,980 –&gt; 00:47:32,980<br>And then I pay that cost of swapping it back in, right?</p>
<p>1022<br>00:47:32,980 –&gt; 00:47:33,980<br>Right.</p>
<p>1023<br>00:47:33,980 –&gt; 00:47:37,579<br>That’s, that’s, uh, that’s all the stuff we did before, right?</p>
<p>1024<br>00:47:37,579 –&gt; 00:47:38,980<br>The other one is like, what about running out of disk?</p>
<p>1025<br>00:47:38,980 –&gt; 00:47:39,980<br>Well, that’s the same thing as a single node.</p>
<p>1026<br>00:47:39,980 –&gt; 00:47:40,980<br>Right?</p>
<p>1027<br>00:47:40,980 –&gt; 00:47:41,980<br>I run out of disk.</p>
<p>1028<br>00:47:41,980 –&gt; 00:47:42,980<br>I crash.</p>
<p>1029<br>00:47:42,980 –&gt; 00:47:43,980<br>Nothing you can do.</p>
<p>1030<br>00:47:43,980 –&gt; 00:47:44,980<br>Right?</p>
<p>1031<br>00:47:44,980 –&gt; 00:47:49,460<br>You can’t, you can’t, like, is there not a way to like sort of like hop around or like</p>
<p>1032<br>00:47:49,460 –&gt; 00:47:55,460<br>have instead of art, like, the, um, art from one to 100, what sort of hop around the node</p>
<p>1033<br>00:47:55,460 –&gt; 00:48:01,460<br>and try and join with them and sort of aggregate the, aggregate the, the result is in some</p>
<p>1034<br>00:48:01,460 –&gt; 00:48:02,460<br>way.</p>
<p>1035<br>00:48:02,460 –&gt; 00:48:07,460<br>You’re, your statement is, could, is there a way or like, instead of sent like, uh, basically</p>
<p>1036<br>00:48:07,460 –&gt; 00:48:10,699<br>saying, like, if I recognize that if I put everything on a single node here, I’ll run</p>
<p>1037<br>00:48:10,699 –&gt; 00:48:12,900<br>out of memory or run out of disk.</p>
<p>1038<br>00:48:12,900 –&gt; 00:48:16,460<br>So let me try to just rebalance things on the fly.</p>
<p>1039<br>00:48:16,460 –&gt; 00:48:23,460<br>Uh, no, like, so instead, like, uh, actually I didn’t have a mind to do it in order to do any</p>
<p>1040<br>00:48:23,460 –&gt; 00:48:24,460<br>of that.</p>
<p>1041<br>00:48:24,460 –&gt; 00:48:25,460<br>Right.</p>
<p>1042<br>00:48:25,460 –&gt; 00:48:26,460<br>We can come back to you.</p>
<p>1043<br>00:48:26,460 –&gt; 00:48:27,460<br>Yes.</p>
<p>1044<br>00:48:27,460 –&gt; 00:48:28,460<br>I think, like, it’s kind of like some word of that.</p>
<p>1045<br>00:48:28,460 –&gt; 00:48:34,460<br>And like, when you, if you pull like S or in R, both are too big for an individual node</p>
<p>1046<br>00:48:34,460 –&gt; 00:48:35,460<br>disk, yes.</p>
<p>1047<br>00:48:35,460 –&gt; 00:48:39,460<br>Is there a way to break S in R, often, such a way that you can perform it at the way across the</p>
<p>1048<br>00:48:39,460 –&gt; 00:48:44,460<br>different nodes, but only loading a part of S in a part of our particular node?</p>
<p>1049<br>00:48:45,460 –&gt; 00:48:46,460<br>Uh, yeah.</p>
<p>1050<br>00:48:46,460 –&gt; 00:48:52,460<br>So could you, so the statement is, uh, if R and S are too big to, the, the partitions are</p>
<p>1051<br>00:48:52,460 –&gt; 00:48:54,460<br>in S are too big to put on a single node.</p>
<p>1052<br>00:48:54,460 –&gt; 00:48:57,460<br>Could you basically, like, I’m just like a streaming thing where you bring in some of it.</p>
<p>1053<br>00:48:57,460 –&gt; 00:49:00,460<br>And that way you can do a portion of the join that you have so far.</p>
<p>1054<br>00:49:00,460 –&gt; 00:49:05,460<br>So again, just think going back to our hash join example before I got to build the, the hash</p>
<p>1055<br>00:49:05,460 –&gt; 00:49:07,460<br>table on the, the build side of the join.</p>
<p>1056<br>00:49:07,460 –&gt; 00:49:09,460<br>So I need to build that first.</p>
<p>1057<br>00:49:10,460 –&gt; 00:49:14,460<br>If that, if I, that one I can stream, right?</p>
<p>1058<br>00:49:14,460 –&gt; 00:49:20,460<br>And then when I do my, uh, my probe, same thing, I can stream the data in and do that incrementally.</p>
<p>1059<br>00:49:20,460 –&gt; 00:49:21,460<br>So yeah, you can do that.</p>
<p>1060<br>00:49:21,460 –&gt; 00:49:26,460<br>So yeah, I’m showing this sort of a high level, like, okay, you’re going to move data around.</p>
<p>1061<br>00:49:26,460 –&gt; 00:49:31,460<br>Uh, it’s not like you do this step and then you can do the join, although I’m showing that in, in PowerPoint.</p>
<p>1062<br>00:49:31,460 –&gt; 00:49:37,460<br>But like, again, using hash join example, I could, I could build a hash table on, on, on R.</p>
<p>1063<br>00:49:37,460 –&gt; 00:49:49,460<br>And then as, uh, as I’m getting the, the, the, the two of us from S are coming over the network, then I do the probe in, you know, and then, and then send out the mean,</p>
<p>1064<br>00:49:49,460 –&gt; 00:49:50,460<br>in result somewhere.</p>
<p>1065<br>00:49:50,460 –&gt; 00:49:52,460<br>And then I go back and get more as I bring things in.</p>
<p>1066<br>00:49:52,460 –&gt; 00:49:53,460<br>Yes.</p>
<p>1067<br>00:49:54,460 –&gt; 00:49:56,460<br>The same idea is on a single node system.</p>
<p>1068<br>00:49:56,460 –&gt; 00:49:58,460<br>You do the same thing, right?</p>
<p>1069<br>00:49:58,460 –&gt; 00:50:01,460<br>You wouldn’t, going back to single node for a hash join.</p>
<p>1070<br>00:50:01,460 –&gt; 00:50:08,460<br>I wouldn’t, uh, on the, on the, on the probe side, I wouldn’t bring everything into memory, all those systems do that.</p>
<p>1071<br>00:50:08,460 –&gt; 00:50:10,460<br>Bring everything memory and then do the probe.</p>
<p>1072<br>00:50:10,460 –&gt; 00:50:15,460<br>I can do it in, you know, you know, get next and get, get a batch of things.</p>
<p>1073<br>00:50:15,460 –&gt; 00:50:18,460<br>Okay.</p>
<p>1074<br>00:50:18,460 –&gt; 00:50:20,460<br>All right.</p>
<p>1075<br>00:50:20,460 –&gt; 00:50:25,460<br>And then the worst case scenario is when the, those tables are not partitioned on the join key.</p>
<p>1076<br>00:50:25,460 –&gt; 00:50:34,460<br>And so now in this case here, I got to reorganize and basically sending out a complete copy of the database, uh, across, across nodes.</p>
<p>1077<br>00:50:34,460 –&gt; 00:50:40,460<br>Now in this case here, since we’re not going to replicate the two tables, uh, we don’t need to do the n, n squared broadcast.</p>
<p>1078<br>00:50:40,460 –&gt; 00:50:42,460<br>We know where, where data needs to actually go.</p>
<p>1079<br>00:50:42,460 –&gt; 00:50:48,460<br>It’s basically like dumping the table out and then loading it back in, but with this time with a different partitioning key.</p>
<p>1080<br>00:50:48,460 –&gt; 00:50:49,460<br>Right?</p>
<p>1081<br>00:50:49,460 –&gt; 00:50:54,460<br>So R is partition or n as a partition of value, but I need IDs.</p>
<p>1082<br>00:50:54,460 –&gt; 00:51:01,460<br>So I’m going to send all the data for some range of R ID over there, uh, same thing for, for R over here.</p>
<p>1083<br>00:51:01,460 –&gt; 00:51:04,460<br>And then I can send over s for both of them.</p>
<p>1084<br>00:51:04,460 –&gt; 00:51:07,460<br>And then now the data is in the form that I need, right?</p>
<p>1085<br>00:51:07,460 –&gt; 00:51:13,460<br>Where I’m guarantee there’s no false negatives because all, I do my join, it’s all, all of me local for the IDs.</p>
<p>1086<br>00:51:13,460 –&gt; 00:51:15,460<br>Everyone does their join locally.</p>
<p>1087<br>00:51:15,460 –&gt; 00:51:18,460<br>And then we ship back the result and produce a minor answer.</p>
<p>1088<br>00:51:18,460 –&gt; 00:51:19,460<br>Yes.</p>
<p>1089<br>00:51:19,460 –&gt; 00:51:21,460<br>What’s the key to your ask?</p>
<p>1090<br>00:51:21,460 –&gt; 00:51:29,460<br>You’re going to have your own library just move around one, you set up the table like broadcast points out of the data,</p>
<p>1091<br>00:51:29,460 –&gt; 00:51:33,460<br>up over dust and then to your regular drawing, based on the IDs.</p>
<p>1092<br>00:51:33,460 –&gt; 00:51:40,460<br>It’s a question of why can’t you broadcast one of the tables, uh, across all the desks and do the join on regular IDs?</p>
<p>1093<br>00:51:40,460 –&gt; 00:51:50,460<br>Uh, what, you know, read ID and journal with a each and no notes and then you’re going to merge all the defaults together.</p>
<p>1094<br>00:51:50,460 –&gt; 00:51:58,460<br>But I still need to get the, like, I still need to get all the, the S values for giving ID over here.</p>
<p>1095<br>00:51:58,460 –&gt; 00:52:07,460<br>So I, when I do my join, like, you know, I don’t end up missing something that is over here because I didn’t bring it over.</p>
<p>1096<br>00:52:07,460 –&gt; 00:52:14,460<br>Well, okay, so if we just take as, uh, if we just broadcast and are as table, say S is 10 petabytes.</p>
<p>1097<br>00:52:14,460 –&gt; 00:52:24,460<br>Well, both of them have been petabytes, but when I’m going to do any, like, but, but in this case here, I’m sending only a partition of the data to the other nodes.</p>
<p>1098<br>00:52:24,460 –&gt; 00:52:25,460<br>Right?</p>
<p>1099<br>00:52:25,460 –&gt; 00:52:30,460<br>I don’t have to put, like, are you proposing to get S in its entirety replicated on every single node?</p>
<p>1100<br>00:52:30,460 –&gt; 00:52:33,460<br>Right. I mean, we’re good to have any sort of streaming to part of them.</p>
<p>1101<br>00:52:33,460 –&gt; 00:52:34,460<br>Sure.</p>
<p>1102<br>00:52:35,460 –&gt; 00:52:41,460<br>Like, my question is why do we need to re-organize?</p>
<p>1103<br>00:52:41,460 –&gt; 00:53:01,460<br>So, so, in, if you, so it has this, this is actually, we would be more efficient at that because if you’re broadcasting to everyone, then like, say, again, using S as an example, then I’m sending potentially IDs of, of values of S with the ID is never going to match anything here in R.</p>
<p>1104<br>00:53:01,460 –&gt; 00:53:03,460<br>So why send that data?</p>
<p>1105<br>00:53:03,460 –&gt; 00:53:15,460<br>Well, if we re-organize it, don’t get out there, we do everything once to re-organize it and then do this, so it’s sort of moving on to table 2.</p>
<p>1106<br>00:53:15,460 –&gt; 00:53:18,460<br>Yeah, but like, I don’t, I don’t, you’re moving the table 2.</p>
<p>1107<br>00:53:18,460 –&gt; 00:53:19,460<br>No, you’re,</p>
<p>1108<br>00:53:19,460 –&gt; 00:53:26,460<br>Oh, I see. So each single node is going to stand there and turn on, which is corresponding on each of us.</p>
<p>1109<br>00:53:26,460 –&gt; 00:53:27,460<br>Correct. Yes.</p>
<p>1110<br>00:53:27,460 –&gt; 00:53:36,460<br>Because, because, yeah, because, and again, I can do this because it’s SQL, I know what the join clause is, I know what my data looks like, I know, I know where it needs to go.</p>
<p>1111<br>00:53:36,460 –&gt; 00:53:37,460<br>Yeah.</p>
<p>1112<br>00:53:37,460 –&gt; 00:53:42,460<br>So just like in the shuffle join, I’m sorry, in the broadcast join, you’ll see this sometimes called shuffle join.</p>
<p>1113<br>00:53:42,460 –&gt; 00:53:48,460<br>And again, it’s still doing hash join underneath the covers, typically, they’ll say it’s a shuffle hash join.</p>
<p>1114<br>00:53:48,460 –&gt; 00:53:54,460<br>Often, sometimes, the way to say it’s a shuffle join, but it’s really a shuffle hash join, doing the structure step.</p>
<p>1115<br>00:53:54,460 –&gt; 00:53:56,460<br>Right?</p>
<p>1116<br>00:53:56,460 –&gt; 00:54:05,460<br>So, in this example here for this query, it’s a select star, I mean, I’m getting, I want all the columns from R and all the columns of S.</p>
<p>1117<br>00:54:05,460 –&gt; 00:54:06,460<br>So, yes.</p>
<p>1118<br>00:54:06,460 –&gt; 00:54:11,460<br>This is how the opposite question is, like, why do you block columns in scenarios where you have to,</p>
<p>1119<br>00:54:11,460 –&gt; 00:54:14,460<br>a question is, why do you broadcast versus a shuffle?</p>
<p>1120<br>00:54:14,460 –&gt; 00:54:20,460<br>Because, depending on the size of, of the table, right?</p>
<p>1121<br>00:54:21,460 –&gt; 00:54:26,460<br>So, in this example here, they’re all select star queries, I need the columns of R and S.</p>
<p>1122<br>00:54:26,460 –&gt; 00:54:30,460<br>So, therefore, I have to send all the data over.</p>
<p>1123<br>00:54:30,460 –&gt; 00:54:33,460<br>You can do basically some kind of projection push down to say, okay, why?</p>
<p>1124<br>00:54:33,460 –&gt; 00:54:37,460<br>I’m only going to send the actual columns I actually need.</p>
<p>1125<br>00:54:37,460 –&gt; 00:54:41,460<br>And this technique is what is sometimes called a semi-join.</p>
<p>1126<br>00:54:41,460 –&gt; 00:54:45,460<br>So, the SQL standard doesn’t define what a semi-join.</p>
<p>1127<br>00:54:45,460 –&gt; 00:54:47,460<br>Some systems actually have this in their syntax.</p>
<p>1128<br>00:54:47,460 –&gt; 00:54:52,460<br>Like, explicit semi-join clause, like, inner join or outer join.</p>
<p>1129<br>00:54:52,460 –&gt; 00:55:01,460<br>And the basic idea is here is that instead of sending over the actual, all the data from the columns that match during the table,</p>
<p>1130<br>00:55:01,460 –&gt; 00:55:11,460<br>thinking like if it was on a single node, I’m just going to send over the, the bare minimum of the data I need to actually do the join.</p>
<p>1131<br>00:55:11,460 –&gt; 00:55:18,460<br>Again, this is basically just like a projection push down, but in, for whatever reason, they explicitly call it a semi-join.</p>
<p>1132<br>00:55:18,460 –&gt; 00:55:24,460<br>So, in this case here, I’m doing a, a join in R and S with before, but I only want RID.</p>
<p>1133<br>00:55:24,460 –&gt; 00:55:29,460<br>And I, I only want to do matches where RID is not null.</p>
<p>1134<br>00:55:29,460 –&gt; 00:55:37,460<br>So, now, again, if I’m split up across two, two nodes like this, instead of sending all of S over to R to do a join,</p>
<p>1135<br>00:55:37,460 –&gt; 00:55:46,460<br>instead, I’ll, like I said, R, instead of what I can do is to send, here’s the, here’s the IDs that could match, and you send them over to, to the two of them.</p>
<p>1136<br>00:55:46,460 –&gt; 00:55:54,460<br>And this is equivalent to basically writing the query as, with a select one, basically saying, hey, for this given RID, something does match.</p>
<p>1137<br>00:55:54,460 –&gt; 00:55:58,460<br>I’m not telling what, what the rest of the two players, I’m just saying like, something is here.</p>
<p>1138<br>00:55:58,460 –&gt; 00:56:03,460<br>Again, just think of like, I’m, it’s, it’s, it’s doing the join, but instead of getting back the result,</p>
<p>1139<br>00:56:03,460 –&gt; 00:56:09,460<br>you’re just getting back like a true false to say, here’s, or, the set of IDs that did match.</p>
<p>1140<br>00:56:09,460 –&gt; 00:56:16,460<br>And again, some systems will have explicit clauses for this.</p>
<p>1141<br>00:56:16,460 –&gt; 00:56:19,460<br>Okay.</p>
<p>1142<br>00:56:19,460 –&gt; 00:56:22,460<br>Yeah, right, so this is like, you send IDs, yeah, yeah, so, okay.</p>
<p>1143<br>00:56:22,460 –&gt; 00:56:24,460<br>All right.</p>
<p>1144<br>00:56:24,460 –&gt; 00:56:28,460<br>So, in the meantime, I want to talk about cloud systems.</p>
<p>1145<br>00:56:28,460 –&gt; 00:56:35,460<br>And again, there’s, there’s a lot more, there’s a lot more activity, at least in the marketplace for, analytical systems running on the cloud.</p>
<p>1146<br>00:56:35,460 –&gt; 00:56:43,460<br>And part of the has to do with like chasing after all that, you know, the snowflake money, the snowflake IPO, and Databricks will be there pretty soon.</p>
<p>1147<br>00:56:43,460 –&gt; 00:56:51,460<br>So, in, in the cloud systems, they’re going to offer what is called database as a service or abbreviated DBA AS.</p>
<p>1148<br>00:56:52,460 –&gt; 00:57:12,460<br>And the idea here is that they’re going to provide you with a managed database system environment, meaning like, instead of you going allocating an EC2 instance, downloading my SQL, Postgres, whatever you want, and, and running that locally and you managing that entire, that whole VM, plus along with storage and backup and recovery and all that stuff.</p>
<p>1149<br>00:57:13,460 –&gt; 00:57:22,460<br>Instead, they’ll provide you with just a URL where you can connect your application to and interact with the database system.</p>
<p>1150<br>00:57:22,460 –&gt; 00:57:33,460<br>So, you can’t SSH into the box, because that’s all hidden from you, but for most people who cares. And instead, they’re going to manage everything for you, right.</p>
<p>1151<br>00:57:34,460 –&gt; 00:57:47,460<br>And as we said before, a lot of the times that, with between share dicks and share nothing systems, all that gets, looks, underneath the covers, a lot of that looks, looks, looks, looks the same.</p>
<p>1152<br>00:57:47,460 –&gt; 00:57:58,460<br>So, there’s two ways of run basically our cloud database system. The first is what we call, again, whatever sort of said before, a managed database system where they basically took some off the shelf software, like Postgres, my SQL.</p>
<p>1153<br>00:57:58,460 –&gt; 00:58:08,460<br>And instead of you running an EC2, they’re going to run an EC2 for you. They’ll have some management interface in front of it. They’ll handle backups and snapshots and recovery and all that kind of stuff.</p>
<p>1154<br>00:58:08,460 –&gt; 00:58:19,460<br>But for the most part, it’s going to be exactly the same as you would download and run locally. And this means that the database system itself is not going to be aware that it’s running in a cloud environment.</p>
<p>1155<br>00:58:19,460 –&gt; 00:58:25,460<br>I mean, you’re running in a, and a disaggregated architecture, like with share disk.</p>
<p>1156<br>00:58:25,460 –&gt; 00:58:35,460<br>And this is what most vendors do less so in more recently, but most of the time when you see like, hey, here’s like this hot open source project that has like, based on a startup.</p>
<p>1157<br>00:58:35,460 –&gt; 00:58:45,460<br>And then you can download off GitHub and then soon after they have their cloud version of it, this is typically what they’re doing. They’re just taking it and they’re going to run it, run it for you.</p>
<p>1158<br>00:58:45,460 –&gt; 00:59:03,460<br>There’s another category systems called cloud, what I’ll call cloud native systems. And this is not a scientific term. This is sort of what people mean when you say cloud native database, where the data system has been built from the ground up or been modified significantly to be aware that it’s running in a cloud environment specifically with share disk.</p>
<p>1159<br>00:59:03,460 –&gt; 00:59:12,460<br>And it can take advantage of all the sort of flexibility and the scale booty and elasticity of a cloud native system or cloud based system.</p>
<p>1160<br>00:59:12,460 –&gt; 00:59:24,460<br>So snowflake probably was the first in this space. Maybe now, BigQuery came with an internal project called Dremel that started like 2006.</p>
<p>1161<br>00:59:24,460 –&gt; 00:59:33,460<br>Snowflake is 2012ish, 13, but snowflake is the one that really made this architecture popular.</p>
<p>1162<br>00:59:33,460 –&gt; 00:59:41,460<br>And so if you have now a running in the cloud, one of the things you can do is support what’s called a serverless database system.</p>
<p>1163<br>00:59:41,460 –&gt; 00:59:47,460<br>So this seems like a weird thing to attach to a database system is to say like, because obviously you need servers to run it.</p>
<p>1164<br>00:59:47,460 –&gt; 00:59:54,460<br>And so what they really mean is that you’re not going to provision servers ahead of time for each customer for each tenant.</p>
<p>1165<br>00:59:54,460 –&gt; 01:00:06,460<br>And that if it’s ever the case where a tenant becomes idle, meaning they don’t run any queries, you actually can turn off the compute nodes or the resources for the system.</p>
<p>1166<br>01:00:06,460 –&gt; 01:00:17,460<br>And then when they come back later and run a query, you spin all that back up. So it’s like you’re turning machines off for them and not charging them for those compete resources.</p>
<p>1167<br>01:00:17,460 –&gt; 01:00:21,460<br>But then the data system is always available.</p>
<p>1168<br>01:00:21,460 –&gt; 01:00:26,460<br>So assuming we’re like a shared everything system, we allocate some easy to node.</p>
<p>1169<br>01:00:26,460 –&gt; 01:00:32,460<br>We have memory of a disk of CPU and say the application is running queries on it, that’s just fine.</p>
<p>1170<br>01:00:32,460 –&gt; 01:00:39,460<br>But then let’s say the application goes away because someone falls asleep, they’re not running any queries. Nothing’s happening.</p>
<p>1171<br>01:00:39,460 –&gt; 01:00:43,460<br>And in this environment, you pay for this node.</p>
<p>1172<br>01:00:43,460 –&gt; 01:00:51,460<br>Because basically just like in bus tub, there’s a while where this thing’s spinning, waiting for incoming requests.</p>
<p>1173<br>01:00:51,460 –&gt; 01:00:54,460<br>And so you don’t know when the requests are going to come along because you always have to be waiting.</p>
<p>1174<br>01:00:54,460 –&gt; 01:00:57,460<br>But now you’re just burning idle cycles.</p>
<p>1175<br>01:00:57,460 –&gt; 01:01:06,460<br>So in a service environment, again, assuming that it’s shared disk, what you can do is you just run the queries like before, fetch things from the shared disk as needed.</p>
<p>1176<br>01:01:06,460 –&gt; 01:01:15,460<br>But then when people go away, you flush out the contents of the buffer pool and the page table to storage.</p>
<p>1177<br>01:01:15,460 –&gt; 01:01:22,460<br>Basically you take a snapshot of your page table that’s in memory, you can check what page is there and any dirty pages.</p>
<p>1178<br>01:01:22,460 –&gt; 01:01:27,460<br>You write all that out to storage. Then you go ahead and kill this thing.</p>
<p>1179<br>01:01:27,460 –&gt; 01:01:31,460<br>The data is still there, right? Because it’s still on shared disk, everything’s fine.</p>
<p>1180<br>01:01:31,460 –&gt; 01:01:38,460<br>And then when people, when the, and you’re paying less for that. And then when the applications ever waste up, sends a query.</p>
<p>1181<br>01:01:38,460 –&gt; 01:01:49,460<br>It’s as if the system is booting up for the first time. But instead of having a no information on what was in the page table, you go fetch that information back in and you sort of bootstrap the system to say,</p>
<p>1182<br>01:01:49,460 –&gt; 01:01:54,460<br>here’s what the state of the system was before I shut down.</p>
<p>1183<br>01:01:55,460 –&gt; 01:01:59,460<br>So in my example here, I’m showing like, we’re killing the compute node entirely.</p>
<p>1184<br>01:01:59,460 –&gt; 01:02:08,460<br>There are some systems where it’s a multi-tenant setup where it’s the same one sort of instance of the database system is supporting multiple customers.</p>
<p>1185<br>01:02:08,460 –&gt; 01:02:16,460<br>So all you need to do now is look in the buffer pool and figure out for this customer that I know is idle, let me write out its results. But the thing is still running.</p>
<p>1186<br>01:02:16,460 –&gt; 01:02:23,460<br>And so there’s a bunch of databases that are now sort of in this support what are called serverless databases like this.</p>
<p>1187<br>01:02:23,460 –&gt; 01:02:33,460<br>Amazon took Postgres and my sequel and rewrote it for this thing called Aurora. They have a serverless system. Fauna is a serverless database that has portions of Cassandra in it.</p>
<p>1188<br>01:02:33,460 –&gt; 01:02:40,460<br>But everything, a lot of it’s written scratch. Neon is probably the probably one or more famous ones for Postgres.</p>
<p>1189<br>01:02:40,460 –&gt; 01:02:49,460<br>But they took Postgres, ripped out the bottom half, sent them to what Amazon did for Aurora, and then reconfigured it to be based on a shared disk architecture.</p>
<p>1190<br>01:02:49,460 –&gt; 01:02:55,460<br>And PlanetScale and Cochroch have their own own things. PlanetScale is the commercial version of a test. The thing I said before it came out on YouTube.</p>
<p>1191<br>01:02:55,460 –&gt; 01:03:00,460<br>It’s the YouTube guys basically went and forked a company.</p>
<p>1192<br>01:03:00,460 –&gt; 01:03:20,460<br>So the other thing we talked about mentioned before are these data lakes. This is sort of the modern buzzword now to describe basically people using an object store as a data warehouse instead of just having the server management proprietary storage as before.</p>
<p>1193<br>01:03:20,460 –&gt; 01:03:36,460<br>So it’s typically always going to be a shared disk architecture. But in a traditional data warehouse what happened is, and if I want to load any data into my data warehouse, I got to call create table that then updates the catalog.</p>
<p>1194<br>01:03:36,460 –&gt; 01:03:42,460<br>And then I’ve been sort of much data, but that’s all going to be going through this compute node that’s controlled by the database system.</p>
<p>1195<br>01:03:42,460 –&gt; 01:03:47,460<br>So it’s going to know like here’s the data that you’re trying to start in this table. It’s going to go into this location on storage.</p>
<p>1196<br>01:03:47,460 –&gt; 01:03:53,460<br>It has to look at catalog and figure out where to write stuff. But and then any query can do the same thing.</p>
<p>1197<br>01:03:53,460 –&gt; 01:04:02,460<br>But in a data lake architecture, the idea here is that I don’t have the database system be the gatekeeper for all new data coming in.</p>
<p>1198<br>01:04:02,460 –&gt; 01:04:08,460<br>Instead, I have this object store where any application can start writing a bunch of files in there.</p>
<p>1199<br>01:04:08,460 –&gt; 01:04:17,460<br>I think I’m put in CSV files, JSON files, parquet, or which cover in a second, they just start throwing whatever data that they want into this object store.</p>
<p>1200<br>01:04:17,460 –&gt; 01:04:27,460<br>I have to update some catalog somehow to keep track of it. And then now when a select query comes along on this node, we look in this catalog and figure out what was there.</p>
<p>1201<br>01:04:27,460 –&gt; 01:04:35,460<br>And then we know how to go get the data that we actually need. Again, the idea here is we want to remove the gatekeeper that they’ve sent as being the gatekeeper and let anybody write stuff in here.</p>
<p>1202<br>01:04:35,460 –&gt; 01:04:41,460<br>And then we’ll go figure out what was actually in it when we run queries.</p>
<p>1203<br>01:04:41,460 –&gt; 01:04:46,460<br>So this goes back to actually what it said in the beginning of this extract transform load versus track load transform.</p>
<p>1204<br>01:04:46,460 –&gt; 01:04:55,460<br>This is that set up here where anybody can just take, you know, get data from what are fun and application and shove it into s3 as a bunch of files.</p>
<p>1205<br>01:04:55,460 –&gt; 01:05:00,460<br>And then someone’s also going to come along and clean things up and figure out how to make sense of it.</p>
<p>1206<br>01:05:00,460 –&gt; 01:05:04,460<br>So there’s a bunch of people in this space.</p>
<p>1207<br>01:05:04,460 –&gt; 01:05:07,460<br>Databricks is probably at the forefront and he’s some marketing talking about this.</p>
<p>1208<br>01:05:07,460 –&gt; 01:05:16,460<br>And they have the term of this idea that it’s, yes, there’s an object store, but then you have this execution engine and this catalog of the structure on top of it.</p>
<p>1209<br>01:05:16,460 –&gt; 01:05:22,460<br>They would call it the lake house, like a plan of the words of a data warehouse.</p>
<p>1210<br>01:05:22,460 –&gt; 01:05:30,460<br>It’s basically what I’m describing here. So the Databricks has this and everyone has their own own variation of it.</p>
<p>1211<br>01:05:30,460 –&gt; 01:05:33,460<br>Redshift actually didn’t start off being a cloud native system.</p>
<p>1212<br>01:05:33,460 –&gt; 01:05:37,460<br>Like it was a fork of Park cell, which is a fork of Postgres.</p>
<p>1213<br>01:05:37,460 –&gt; 01:05:39,460<br>It was very much a shared nothing system.</p>
<p>1214<br>01:05:39,460 –&gt; 01:05:49,460<br>Over time, they rewritten a lot of it to be shared disk and look very similar to this architecture here.</p>
<p>1215<br>01:05:49,460 –&gt; 01:05:56,460<br>So the last thing I’ll talk about is we’ll be a segue into what 721 will be about if you continue on with this stuff.</p>
<p>1216<br>01:05:56,460 –&gt; 01:06:15,460<br>But one very interesting trend we’ve seen in databases in the last decade is that instead of having these giant monolithic database systems where everything is written by the same vendor or same group organization inside the system itself.</p>
<p>1217<br>01:06:15,460 –&gt; 01:06:29,460<br>The last couple of years, people have been breaking off components of the system and have them being standalone services that you can then connect together in some way to build a larger database system, a data warehouse system.</p>
<p>1218<br>01:06:29,460 –&gt; 01:06:34,460<br>Essentially, sort of like the lake house stuff that I’m talking about here.</p>
<p>1219<br>01:06:34,460 –&gt; 01:06:48,460<br>And what’s interesting about this is that a lot of times these components that people are building, they’re not being built by a database system vendor, which traditionally how data system software has been written for decades.</p>
<p>1220<br>01:06:48,460 –&gt; 01:07:00,460<br>Instead, you have these big tech companies, in some cases, even smaller startups where they have some need for some piece of a database system and they’re not building it, open sourcing it, and other people pick up and start using it.</p>
<p>1221<br>01:07:00,460 –&gt; 01:07:12,460<br>And even again, that company, who’s building a piece of software, they don’t make their money with that software, they make their money doing something else, trying to think of a good example.</p>
<p>1222<br>01:07:12,460 –&gt; 01:07:24,460<br>So like Facebook has this open source execution engine, Facebook is not a database company, but they need the execution engine for their own internal needs, but then they open source it.</p>
<p>1223<br>01:07:24,460 –&gt; 01:07:31,460<br>Actually, Facebook has put out a lot of its software in the last decade or so in the context of databases.</p>
<p>1224<br>01:07:31,460 –&gt; 01:07:44,460<br>So we’ll go through a bunch of examples. So like the idea here is that you can now build, if you want to build a new database system, you don’t want to build everything from scratch anymore, or take Postgres or Clickhouse and fork that, and then try to modify that specifically.</p>
<p>1225<br>01:07:44,460 –&gt; 01:07:49,460<br>You can take these different components and start putting them together.</p>
<p>1226<br>01:07:49,460 –&gt; 01:08:04,460<br>And so what I mean by commoditization is that what made, like, Snowflake Unique 10 years ago, is not really significant anymore, because everyone has it.</p>
<p>1227<br>01:08:04,460 –&gt; 01:08:16,460<br>Like everyone has a vectorized query engine these days, or everyone’s going to be using a column store. The thing that matters the most now is like the user experience, the front end stuff, the query optimizer certainly.</p>
<p>1228<br>01:08:16,460 –&gt; 01:08:24,460<br>So you can put these things together and make a system, but then like to differentiate yourself, you’d have to mostly focus on the front end stuff.</p>
<p>1229<br>01:08:24,460 –&gt; 01:08:32,460<br>You’re using ducty-b as an example. Ducty-b is an amazing piece of software, but the core ideas are well known and not new.</p>
<p>1230<br>01:08:32,460 –&gt; 01:08:42,460<br>They put it in a great form factor that you can run anywhere and connect it to pandas and things like that. That’s not the core database engine stuff. That’s all the user experience stuff.</p>
<p>1231<br>01:08:43,460 –&gt; 01:08:52,460<br>So catalogs we talked about, again, this is how we’re going to keep track of what data is we have, where are files located on storage, what the schemas and so forth.</p>
<p>1232<br>01:08:52,460 –&gt; 01:09:02,460<br>The probably the most famous sort of standalone catalog system, it’s the thing called H catalog. They came out of Facebook, came out of the Hive project, the H stands for Hive or Hadoop.</p>
<p>1233<br>01:09:03,460 –&gt; 01:09:10,460<br>I think the idea here is that I can write a bunch of files in S3 and then I update H catalog and say, hey, by the way, here’s these files that I have and here’s the schema.</p>
<p>1234<br>01:09:10,460 –&gt; 01:09:16,460<br>And then maybe some basic statistical information about what’s in them, because those files sometimes can record that.</p>
<p>1235<br>01:09:16,460 –&gt; 01:09:19,460<br>And then certainly all the cloud vendors will sell you something as well.</p>
<p>1236<br>01:09:19,460 –&gt; 01:09:28,460<br>Databricks has their thing called Unity. It’s not an open source, but they all provide some mechanism to make sense of what data you have in your lake house.</p>
<p>1237<br>01:09:29,460 –&gt; 01:09:33,460<br>Query optimizers we’ve talked about, again, this is the hardest part of building a database system.</p>
<p>1238<br>01:09:33,460 –&gt; 01:09:45,460<br>And to no surprise, most people don’t want to build it themselves. So instead there are at least two that I know about open source projects where like these are just meant to be standalone opt query optimization as a service.</p>
<p>1239<br>01:09:45,460 –&gt; 01:09:48,460<br>CalSites, probably the most famous one.</p>
<p>1240<br>01:09:49,460 –&gt; 01:09:58,460<br>That was a data system in the 2000s called LucidDB, I think it was European. They were a startup. They failed. The company failed.</p>
<p>1241<br>01:09:58,460 –&gt; 01:10:05,460<br>And then for whatever reason, they pulled out the query app from them and they built and then had that be a standalone project that became an Apache project.</p>
<p>1242<br>01:10:05,460 –&gt; 01:10:16,460<br>So the button, CalSite has the query optimizer, but it has the ability to ingest queries from a part of SQL statements from all the Postgres, my SQL, a bunch of different dialects.</p>
<p>1243<br>01:10:16,460 –&gt; 01:10:27,460<br>And in Orca is another one of these optimizers of service that came out of Green Plum, which is bought by Pivotal, which is then I think has been bought by VMware, which has been bought by Broadcom.</p>
<p>1244<br>01:10:27,460 –&gt; 01:10:38,460<br>I think, yeah, but this is open source. This is less common than CalSite. But Green Plum built this originally because they wouldn’t have, there’s the Green Plum, you know, data warehouse system they were building that’s a fork of Postgres.</p>
<p>1245<br>01:10:39,460 –&gt; 01:10:51,460<br>But then they had another system, I think, called Hawk with a queue that was like high was like SQL on top of a Hadoop. And so instead of having to build two separate query optimizers for the two different database systems, they put everything into one.</p>
<p>1246<br>01:10:51,460 –&gt; 01:11:01,460<br>As far as I know, nobody uses Orca other than the Green Plum guys. But I think this, this is the one part of the systems I’m most interested in right now in terms of research.</p>
<p>1247<br>01:11:02,460 –&gt; 01:11:11,460<br>The other cool thing is having the 10 years is that there are now these universal or open source file formats that we can use across different database systems.</p>
<p>1248<br>01:11:11,460 –&gt; 01:11:24,460<br>So until maybe 10 years ago, most databases either had their own proprietary binary format, right, think of like, you know, on your bus top project, you know, when you write out a file, it rates out a dot db file.</p>
<p>1249<br>01:11:24,460 –&gt; 01:11:35,460<br>Do we rename it to that bus hub yet or no? Still dot db, right? I’m assuming, you know, it doesn’t matter. Like the bus have has this own proprietary format that only bus hub knows how to use, right.</p>
<p>1250<br>01:11:35,460 –&gt; 01:11:51,460<br>Oracle, my SQL Postgres, they all have their own proprietary format. But if you want to start sharing data across different systems, again, thinking in the cloud, I have, you know, my front applications, writing some data, I want to put out to S3, and then I want to have something else consume that.</p>
<p>1251<br>01:11:51,460 –&gt; 01:12:07,460<br>I can, I stored as all JSON or text field or CSVs, which would be very inefficient, but maybe if I have a file format, there’s like a columnar binary encoding with compression that I could have the application write that out, then any, any data, any data, some I use can then read that.</p>
<p>1252<br>01:12:07,460 –&gt; 01:12:16,460<br>So there’s a bunch of these file formats that are making easier to access data, generating from one application and shared across other systems.</p>
<p>1253<br>01:12:16,460 –&gt; 01:12:25,460<br>So the most famous one of these is parquet. This one came out of cladder and Twitter. The next most famous one is orc, or came out of Facebook.</p>
<p>1254<br>01:12:25,460 –&gt; 01:12:42,460<br>These are again, they’re basically the column stores that I’ve talked about before. But now they have a bunch of libraries written in whatever query language you, or whatever program language you want, Rust, C++, Java, Python, that you can then read and write these files in.</p>
<p>1255<br>01:12:42,460 –&gt; 01:12:50,460<br>So again, I can have my application generate these files and then without having to talk to the database system and then they can then just have to parse them.</p>
<p>1256<br>01:12:50,460 –&gt; 01:12:58,460<br>Carbon data is at a hallway. That’s a fork of parquet. We did some experimentation on this and it doesn’t work.</p>
<p>1257<br>01:12:58,460 –&gt; 01:13:12,460<br>We said, I’m sure someone doesn’t work. Apache iceberg is parquet files, but additional, keep track of additional metadata so you can do incremental updates and do schema changes where it’s parquet files are like, you know, sort of right ones read many.</p>
<p>1258<br>01:13:12,460 –&gt; 01:13:22,460<br>But this can keep track of this things for you. But this came out of Netflix. This is another good example. Netflix is not a database company. They’re not making money selling you a data system, but they wrote iceberg and they open sourced it.</p>
<p>1259<br>01:13:22,460 –&gt; 01:13:42,460<br>And a lot of people have picked up and started using it. HD5 is not very common in our world in data systems, but this is an array format that’s common in high performance computing, scientific computing, so like all the satellite images of the telescope stuff that usually is written in HD5.</p>
<p>1260<br>01:13:42,460 –&gt; 01:13:52,460<br>And then Apache Arrow is a in memory, a columnar format that allow you to do data exchange between applications running in memory.</p>
<p>1261<br>01:13:52,460 –&gt; 01:14:02,460<br>So think of it like, like, like when duck DB when they read a file in and then you want to query it, get that data in your panas code.</p>
<p>1262<br>01:14:02,460 –&gt; 01:14:14,460<br>Instead of having a per day format, they put it in the Apache Arrow format. Now, pen is going to just read that memory directly without having to write the disc or write your own disk format and parsing it and bringing it back in. Yes.</p>
<p>1263<br>01:14:14,460 –&gt; 01:14:23,460<br>Why are they all Apache? What is Apache? Oh, it’s question is what is it? Why are they all called Apache?</p>
<p>1264<br>01:14:23,460 –&gt; 01:14:34,460<br>So there are the Apache Computing Foundation is this nonprofit open source computer open source software foundation initiative.</p>
<p>1265<br>01:14:34,460 –&gt; 01:14:44,460<br>Think of like if I want to have like an independent person control, the, there’s the Apache license that came with the, there’s also the Apache Computing Foundation.</p>
<p>1266<br>01:14:44,460 –&gt; 01:14:53,460<br>It’s even more confusing because also there’s Apache web server. We’ll get that all the same organization, but it’s basically there’s, there’s Apache’s one of them.</p>
<p>1267<br>01:14:53,460 –&gt; 01:15:04,460<br>There’s also like the Linux Computing Foundation. There’s a cloud computing foundation. They’re basically these nonprofit consortiums that the light you have like a governance, but a governing body for the, for open source software.</p>
<p>1268<br>01:15:04,460 –&gt; 01:15:12,460<br>So even though like Netflix wrote iceberg, people may want to say, I want to use the software, but I don’t want Netflix to be in charge of it.</p>
<p>1269<br>01:15:12,460 –&gt; 01:15:18,460<br>Right? Because then like think of like Netflix, Netflix is competitor wants to start using the software and Netflix starts getting weird about it, right?</p>
<p>1270<br>01:15:18,460 –&gt; 01:15:27,460<br>So you bite by, by you donate the software to this consortium, the foundation, and then they have controlled it.</p>
<p>1271<br>01:15:27,460 –&gt; 01:15:30,460<br>So that’s why you see Apache in front of things. Yes.</p>
<p>1272<br>01:15:30,460 –&gt; 01:15:39,460<br>In a similar vein, why does Apache continue to maintain five of them instead of saying, all right, four of the sucks. Everybody who’s used like to do something right now.</p>
<p>1273<br>01:15:39,460 –&gt; 01:15:52,460<br>This question is why does Apache continue maintain five of these instead of just picking one winner? Because that’s not the model of Apache. It’s not, it’s not a company where like somebody above is making decisions about what’s going to succeed or not.</p>
<p>1274<br>01:15:52,460 –&gt; 01:16:12,460<br>There’s, there’s criteria you have to have about getting your software to be part of Apache foundation. But then, then you have to elect like leaders to like be like, you know, who controls the commits and testing and how you vote for things and keep, you know, there’s a bunch of like bureaucracy stuff that they provide, but they don’t choose who, who can win or not.</p>
<p>1275<br>01:16:12,460 –&gt; 01:16:24,460<br>Things get deprecated, certainly things like you can become like a top ranked or top top level project in Apache. But then if nobody uses it, then it gets relegated.</p>
<p>1276<br>01:16:24,460 –&gt; 01:16:35,460<br>Actually, Gignesh, I was betting that this isn’t what’s consequent quick step and it was it was an incubation process for Apache, but then it didn’t go anywhere and then I got, I got killed off.</p>
<p>1277<br>01:16:35,460 –&gt; 01:16:37,460<br>Yes.</p>
<p>1278<br>01:16:37,460 –&gt; 01:16:46,460<br>So, what’s someone made, you know, which is our open source, but they don’t put out this around the system that they put the under what’s going to be able to use?</p>
<p>1279<br>01:16:46,460 –&gt; 01:16:57,460<br>Yes, question is, statement is good question is if someone makes an almost first file format, but then there’s no software to access it. What’s the use of it?</p>
<p>1280<br>01:16:57,460 –&gt; 01:17:08,460<br>It’s useless. You’re okay, it’s right. And so, like, for parquet, for example, for an orc, like there is, there is an identity system that is the parquet database system.</p>
<p>1281<br>01:17:08,460 –&gt; 01:17:16,460<br>Instead, they provide you low level libraries like in Rust to then parse the data and do some basic manipulation of it.</p>
<p>1282<br>01:17:16,460 –&gt; 01:17:24,460<br>Right? And then you build a larger system around it, potentially using the components that I talked about before. Right?</p>
<p>1283<br>01:17:24,460 –&gt; 01:17:26,460<br>Yes.</p>
<p>1284<br>01:17:26,460 –&gt; 01:17:32,460<br>What is sensitive to example components and then they’ve often developed independently about thinking about the other things that you’ve been doing?</p>
<p>1285<br>01:17:32,460 –&gt; 01:17:34,460<br>Yes.</p>
<p>1286<br>01:17:34,460 –&gt; 01:17:42,460<br>Yes. Yes. The question is, how hard is it to cobble these components together if they were all implemented independently? How would you actually put them together?</p>
<p>1287<br>01:17:42,460 –&gt; 01:17:54,460<br>So there’s like the one, this one we’ll cover in 721, but Apache Arrow helps a lot of this because now I can send data between the services and a universal format.</p>
<p>1288<br>01:17:54,460 –&gt; 01:18:06,460<br>But there’s a much other semantics about what things should look like or what, you know, what does it mean to be a query or what equipment looks like that may be different from one of these components versus another.</p>
<p>1289<br>01:18:06,460 –&gt; 01:18:14,460<br>I would say that it’s no different than having different abstraction layers in your data system that I’ve talked about before.</p>
<p>1290<br>01:18:14,460 –&gt; 01:18:18,460<br>It’s just now that it may be the case that you don’t have full control over this one component.</p>
<p>1291<br>01:18:18,460 –&gt; 01:18:29,460<br>If you want to not do a hard fork of parquet, even though parquet does some things that you think are wrong, you just have to live with it.</p>
<p>1292<br>01:18:29,460 –&gt; 01:18:39,460<br>Actually, a good example was a few years ago we had the founders of Blazing Sequel. They can’t give a talk and they had their own proprietary storage format.</p>
<p>1293<br>01:18:39,460 –&gt; 01:18:48,460<br>But because everyone, they’re running a data system on GPUs, but everyone was coming to them saying, I have my data in, it’s going to be an Arrow format.</p>
<p>1294<br>01:18:48,460 –&gt; 01:18:53,460<br>They then had to drop the Render Partial format and switch to Arrow even though they felt theirs was better.</p>
<p>1295<br>01:18:53,460 –&gt; 01:18:56,460<br>So like, you have to make sacrifices.</p>
<p>1296<br>01:18:56,460 –&gt; 01:18:58,460<br>Yes.</p>
<p>1297<br>01:18:58,460 –&gt; 01:19:09,460<br>So, for example, let’s say for main storage, you want to do parquet, but for large intermediaries, you want to store stuff like Arrow, you want to do better for anything.</p>
<p>1298<br>01:19:09,460 –&gt; 01:19:11,460<br>Is that doable?</p>
<p>1299<br>01:19:11,460 –&gt; 01:19:13,460<br>Yeah, people are always on it. Absolutely.</p>
<p>1300<br>01:19:13,460 –&gt; 01:19:21,460<br>I mean, ductivity does this. Databricks does this. Databricks will, it’ll read any of these file formats, but then when they bring it into memory, they put it into their own format.</p>
<p>1301<br>01:19:22,460 –&gt; 01:19:28,460<br>And then, you know, when it spits out the results to you, then they’ll put it back in parquet or whatever.</p>
<p>1302<br>01:19:28,460 –&gt; 01:19:37,460<br>Yeah, we have paper off the cut this. We have a paper that came out of the ODE basically shows, like parquet and orc were designed 10 years ago.</p>
<p>1303<br>01:19:37,460 –&gt; 01:19:44,460<br>And, you know, the hardware has changed significantly. The bunch of design stations that they make is a bad idea. Let’s finish up.</p>
<p>1304<br>01:19:45,460 –&gt; 01:19:52,460<br>Extrude changes, again, thing of like the things you had to build a bus tub, there’s now libraries that you can download and use.</p>
<p>1305<br>01:19:52,460 –&gt; 01:20:00,460<br>Velox is from Facebook, Data Fusion, I think is from the Apache Error Guys and Inflex TV. And then this Intel thing, OAP, I don’t think anybody uses this.</p>
<p>1306<br>01:20:00,460 –&gt; 01:20:08,460<br>These are going to be the two bigger ones. And then this is what we’re going to play with in, in 721.</p>
<p>1307<br>01:20:09,460 –&gt; 01:20:20,460<br>All right, so the main takeaway from all of this, the cloud is definitely made to show you the basis for OLAF systems, way more common than they used to be.</p>
<p>1308<br>01:20:20,460 –&gt; 01:20:34,460<br>And I think the buy product is just having online applications, the internet, you get more data very quickly. It doesn’t take that much work anyway, any more to like write an application that can take, also take a lot of users and start scaling up and getting a lot of new data.</p>
<p>1309<br>01:20:35,460 –&gt; 01:20:47,460<br>And so there’s a lot of vendors in this space, a lot of VC money, someone’s dying down, focusing on vector databases, but there’s still a lot of problems to be solved in this space.</p>
<p>1310<br>01:20:47,460 –&gt; 01:20:55,460<br>All right, so again, next class will be the single store speaker, that will be on Zoom, as they post on Piazza, just go to that Zoom link.</p>
<p>1311<br>01:20:55,460 –&gt; 01:21:00,460<br>And then if you want, we can come watch it in my office and we run that space, we can spill to the database lab.</p>
<p>1312<br>01:21:01,460 –&gt; 01:21:03,460<br>Any questions? Yes.</p>
<p>1313<br>01:21:03,460 –&gt; 01:21:07,460<br>No, no, sit in your house in your bathtub, whatever you want to do.</p>
<p>1314<br>01:21:07,460 –&gt; 01:21:08,460<br>Okay. Yes.</p>
<p>1315<br>01:21:08,460 –&gt; 01:21:14,460<br>With all these open source components, is there like a consortium like I do believe standards or something to agree on these.</p>
<p>1316<br>01:21:14,460 –&gt; 01:21:23,460<br>This question is, is there a, is there a, like an actually standard to specify what though, sorry.</p>
<p>1317<br>01:21:23,460 –&gt; 01:21:34,460<br>You have, you want to say your output format to be appropriate for taking, like if you take for example, like, see if you architect your for something, you have these companies.</p>
<p>1318<br>01:21:34,460 –&gt; 01:21:46,460<br>I think for the things that are outside the system, like that go outside the internal, you know, the internals of the system, it’s going to be typically arrow.</p>
<p>1319<br>01:21:47,460 –&gt; 01:21:52,460<br>Right. That’s the, that’s the language of Franco for communicating exchanging data between different services.</p>
<p>1320<br>01:21:52,460 –&gt; 01:21:58,460<br>But I was saying, that’s just like, for the encoding, there’s the semantics about what’s actually in there that could change.</p>
<p>1321<br>01:21:58,460 –&gt; 01:22:01,460<br>Okay. All right. Hit it.</p>
<p>1322<br>01:22:16,460 –&gt; 01:22:18,460<br>Yeah. Yeah.</p>
<p>1323<br>01:22:46,460 –&gt; 01:22:48,460<br>Yeah.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CMU15445 P24F202323 DistributedDataWarehouseOLAPDatabases</div>
      <div>http://example.com/2025/10/25/CMU15445 P24F202323-DistributedDataWarehouseOLAPDatabases/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月25日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/25/CMU15445%20P25F202324-SingleStoreDatabaseOverview/" title="CMU15445 P25F202324 SingleStoreDatabaseOverview">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CMU15445 P25F202324 SingleStoreDatabaseOverview</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/25/CMU15445%20P26F202325-PotpourriRedisCockroachDBSnowflakeMangoDBTabDB/" title="CMU15445 P26F202325 PotpourriRedisCockroachDBSnowflakeMangoDBTabDB">
                        <span class="hidden-mobile">CMU15445 P26F202325 PotpourriRedisCockroachDBSnowflakeMangoDBTabDB</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
