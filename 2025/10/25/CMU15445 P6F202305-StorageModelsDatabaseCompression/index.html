

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="100:00:00,600 –&gt; 00:00:03,899ized 200:00:03,899 –&gt; 00:00:27,820All right, let’s get started. 300:00:27,820 –&gt; 00:00:29,820DJ JPL. 400:00:29,820 –&gt; 00:00:34,820I know you got a live show co">
<meta property="og:type" content="article">
<meta property="og:title" content="CMU15445 P6F202305 StorageModelsDatabaseCompression">
<meta property="og:url" content="http://example.com/2025/10/25/CMU15445%20P6F202305-StorageModelsDatabaseCompression/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="100:00:00,600 –&gt; 00:00:03,899ized 200:00:03,899 –&gt; 00:00:27,820All right, let’s get started. 300:00:27,820 –&gt; 00:00:29,820DJ JPL. 400:00:29,820 –&gt; 00:00:34,820I know you got a live show co">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-25T05:03:39.741Z">
<meta property="article:modified_time" content="2025-10-25T05:03:39.741Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>CMU15445 P6F202305 StorageModelsDatabaseCompression - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CMU15445 P6F202305 StorageModelsDatabaseCompression"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-25 13:03" pubdate>
          2025年10月25日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          9k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          76 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CMU15445 P6F202305 StorageModelsDatabaseCompression</h1>
            
            
              <div class="markdown-body">
                
                <p>1<br>00:00:00,600 –&gt; 00:00:03,899<br>ized</p>
<p>2<br>00:00:03,899 –&gt; 00:00:27,820<br>All right, let’s get started.</p>
<p>3<br>00:00:27,820 –&gt; 00:00:29,820<br>DJ JPL.</p>
<p>4<br>00:00:29,820 –&gt; 00:00:34,820<br>I know you got a live show coming up.</p>
<p>5<br>00:00:34,820 –&gt; 00:00:36,820<br>We’ll talk about that in a second.</p>
<p>6<br>00:00:36,820 –&gt; 00:00:38,820<br>So quickly, for you guys in the class,</p>
<p>7<br>00:00:38,820 –&gt; 00:00:41,820<br>again, homework one is do this Friday on the 15th.</p>
<p>8<br>00:00:41,820 –&gt; 00:00:45,820<br>Project one is out and be due on October 1st.</p>
<p>9<br>00:00:45,820 –&gt; 00:00:47,820<br>Even though we haven’t discussed what a buffer pool is yet,</p>
<p>10<br>00:00:47,820 –&gt; 00:00:50,820<br>buffer manager is, you can get started if you want.</p>
<p>11<br>00:00:50,820 –&gt; 00:00:54,820<br>Again, it’s where you’re going to allocate memory that gets you written out the disk.</p>
<p>12<br>00:00:54,820 –&gt; 00:00:55,820<br>Okay.</p>
<p>13<br>00:00:55,820 –&gt; 00:00:59,820<br>The upcoming events next Monday,</p>
<p>14<br>00:00:59,820 –&gt; 00:01:01,820<br>a day event Aiken will be giving a talk with us.</p>
<p>15<br>00:01:01,820 –&gt; 00:01:03,820<br>You see me, alumni, you see me, you see me,</p>
<p>16<br>00:01:03,820 –&gt; 00:01:09,819<br>this group of alumni, giving a talk at our seminar series on the…</p>
<p>17<br>00:01:09,819 –&gt; 00:01:10,819<br>Oh, you ****.</p>
<p>18<br>00:01:10,819 –&gt; 00:01:16,819<br>The show is here, giving a talk at a seminar series on Zoom on Monday.</p>
<p>19<br>00:01:16,819 –&gt; 00:01:23,819<br>And then, before that, DJ JPL is having a concert this Saturday at 9 pm on campus</p>
<p>20<br>00:01:23,819 –&gt; 00:01:26,819<br>in Ragnos in the…</p>
<p>21<br>00:01:26,819 –&gt; 00:01:28,819<br>You see it, right?</p>
<p>22<br>00:01:28,819 –&gt; 00:01:30,819<br>Are you letting you sign autographs or no?</p>
<p>23<br>00:01:30,819 –&gt; 00:01:33,819<br>I mean, like, CBS is still haggling all the price.</p>
<p>24<br>00:01:33,819 –&gt; 00:01:34,819<br>Okay, yeah.</p>
<p>25<br>00:01:34,819 –&gt; 00:01:35,819<br>Just the days.</p>
<p>26<br>00:01:35,819 –&gt; 00:01:36,819<br>CNU is kind of weird.</p>
<p>27<br>00:01:36,819 –&gt; 00:01:39,819<br>They won’t let him, like, it’s a big show and they won’t let him sign autographs afterwards.</p>
<p>28<br>00:01:39,819 –&gt; 00:01:41,819<br>It’s some stupid…</p>
<p>29<br>00:01:41,819 –&gt; 00:01:42,819<br>Yeah.</p>
<p>30<br>00:01:42,819 –&gt; 00:01:43,819<br>Well, I’m going to do that.</p>
<p>31<br>00:01:43,819 –&gt; 00:01:45,819<br>So, we’ll not discuss today.</p>
<p>32<br>00:01:45,819 –&gt; 00:01:47,819<br>Let’s jump right into it.</p>
<p>33<br>00:01:47,819 –&gt; 00:01:52,819<br>So, last class, we were talking about alternative approaches to the two-poor.</p>
<p>34<br>00:01:52,819 –&gt; 00:01:57,819<br>To the two-poor oriented or Slot of Page storage scheme that we presented last week.</p>
<p>35<br>00:01:57,819 –&gt; 00:02:02,819<br>And in particular, we’ve spent a lot of time talking about the log structured storage method</p>
<p>36<br>00:02:02,819 –&gt; 00:02:05,819<br>where instead of storing the actual two-poels, the store…</p>
<p>37<br>00:02:05,819 –&gt; 00:02:10,819<br>The log entries of the changes you’ve made to two-poels.</p>
<p>38<br>00:02:10,819 –&gt; 00:02:14,819<br>And I said that was popular in sort of modern systems that are more right-intensive.</p>
<p>39<br>00:02:14,819 –&gt; 00:02:17,819<br>So, the three approaches we talked about,</p>
<p>40<br>00:02:17,819 –&gt; 00:02:22,819<br>so that the two-poor oriented Slot of Pages, the log structured storage, the index-organized storage.</p>
<p>41<br>00:02:22,819 –&gt; 00:02:28,819<br>These storage approaches are ideal for workloads that are right-heavy.</p>
<p>42<br>00:02:28,819 –&gt; 00:02:32,819<br>Meaning, if you’re doing a lot of inserts, updates, or deletes, right?</p>
<p>43<br>00:02:32,819 –&gt; 00:02:37,819<br>The log structure one is obviously better for this because you just begin your pending to the log.</p>
<p>44<br>00:02:37,819 –&gt; 00:02:40,819<br>And for a lot of applications, or most applications when you start off,</p>
<p>45<br>00:02:40,819 –&gt; 00:02:41,819<br>this is…</p>
<p>46<br>00:02:41,819 –&gt; 00:02:45,819<br>You’re going to be a more potentially right-heavy workload.</p>
<p>47<br>00:02:46,819 –&gt; 00:02:49,819<br>But there’s going to be some applications, or some environments, or some workloads</p>
<p>48<br>00:02:49,819 –&gt; 00:02:53,819<br>where maybe you don’t care about getting the best performance for rights.</p>
<p>49<br>00:02:53,819 –&gt; 00:02:57,819<br>What you really want to do is get the best performance for reads.</p>
<p>50<br>00:02:57,819 –&gt; 00:03:03,819<br>And therefore, these approaches may not be the best way to approach it.</p>
<p>51<br>00:03:03,819 –&gt; 00:03:06,819<br>So, I’m going to spend a little time talking about what sort of broad categories</p>
<p>52<br>00:03:06,819 –&gt; 00:03:08,819<br>of data applications look like.</p>
<p>53<br>00:03:08,819 –&gt; 00:03:12,819<br>And then that’ll be motivation for why we want to look at an alternative storage scheme</p>
<p>54<br>00:03:12,819 –&gt; 00:03:15,819<br>where maybe we don’t want to store everything that’s just rows,</p>
<p>55<br>00:03:15,819 –&gt; 00:03:19,819<br>like the two tables with all the attributes together.</p>
<p>56<br>00:03:19,819 –&gt; 00:03:22,819<br>So, this is a rough categorization.</p>
<p>57<br>00:03:22,819 –&gt; 00:03:26,819<br>But in industry, if you study sort of these three approaches,</p>
<p>58<br>00:03:26,819 –&gt; 00:03:28,819<br>you say you’re one of these three approaches,</p>
<p>59<br>00:03:28,819 –&gt; 00:03:31,819<br>people roughly know what you mean.</p>
<p>60<br>00:03:31,819 –&gt; 00:03:34,819<br>So, the first category of applications are called OLEDTP,</p>
<p>61<br>00:03:34,819 –&gt; 00:03:36,819<br>or online transaction processing.</p>
<p>62<br>00:03:36,819 –&gt; 00:03:41,819<br>And these are applications where you’re ingesting new data from the outside world</p>
<p>63<br>00:03:41,819 –&gt; 00:03:45,819<br>and you’re serving a lot of users at the same time.</p>
<p>64<br>00:03:45,819 –&gt; 00:03:49,819<br>So, again, the example application I always like to use is Amazon.</p>
<p>65<br>00:03:49,819 –&gt; 00:03:53,819<br>When you go to the Amazon website, you look at products,</p>
<p>66<br>00:03:53,819 –&gt; 00:03:56,819<br>then you click things, you add them to your cart, and then you purchase them.</p>
<p>67<br>00:03:56,819 –&gt; 00:04:00,819<br>Maybe you go to your account information, and you go update your mailing address,</p>
<p>68<br>00:04:00,819 –&gt; 00:04:02,819<br>or payment information.</p>
<p>69<br>00:04:02,819 –&gt; 00:04:08,819<br>Those are all considered OLEDTP style workloads because you’re making changes</p>
<p>70<br>00:04:08,819 –&gt; 00:04:11,819<br>to a small subset of the database.</p>
<p>71<br>00:04:11,819 –&gt; 00:04:15,819<br>Like you’re going updating your cart, going updating your payment information.</p>
<p>72<br>00:04:15,819 –&gt; 00:04:19,819<br>So, think of posting things on Reddit or hacker news.</p>
<p>73<br>00:04:19,819 –&gt; 00:04:22,819<br>Those are making small changes, which potentially could be a large database,</p>
<p>74<br>00:04:22,819 –&gt; 00:04:26,819<br>but the amount of change each query or operation is making is small.</p>
<p>75<br>00:04:26,819 –&gt; 00:04:28,819<br>The amount of data that they’re reading is small.</p>
<p>76<br>00:04:28,819 –&gt; 00:04:31,819<br>They’re reading for a single entity.</p>
<p>77<br>00:04:31,819 –&gt; 00:04:35,819<br>So, contrast this with online analytical processing or OLAP workloads,</p>
<p>78<br>00:04:35,819 –&gt; 00:04:40,819<br>and this is where I want to use, I’m going to run queries that are an extract</p>
<p>79<br>00:04:40,819 –&gt; 00:04:45,819<br>or extrapolate new information across the entire data set.</p>
<p>80<br>00:04:45,819 –&gt; 00:04:47,819<br>So, this would be like Amazon running a query that says,</p>
<p>81<br>00:04:47,819 –&gt; 00:04:52,819<br>find me the number one sold product in the state of Pennsylvania on a Saturday</p>
<p>82<br>00:04:52,819 –&gt; 00:04:55,819<br>when the temperature is above 80 degrees.</p>
<p>83<br>00:04:55,819 –&gt; 00:04:59,819<br>It’s not looking at a single person or looking at a single entity,</p>
<p>84<br>00:04:59,819 –&gt; 00:05:01,819<br>but looking across the entire table.</p>
<p>85<br>00:05:01,819 –&gt; 00:05:07,819<br>Potentially doing a lot of joins also with additional information,</p>
<p>86<br>00:05:07,819 –&gt; 00:05:12,819<br>so let me turn to the things you guys have done in homework one.</p>
<p>87<br>00:05:12,819 –&gt; 00:05:17,819<br>So, in these OLAP workloads, they’re going to be primarily re-heavy or read only.</p>
<p>88<br>00:05:17,819 –&gt; 00:05:23,819<br>I’m not doing single updates, I’m going doing large scans of and joins of our big tables.</p>
<p>89<br>00:05:23,819 –&gt; 00:05:30,819<br>And this last one is sort of a buzzword from the industry analyst or Gartner called HTAB.</p>
<p>90<br>00:05:30,819 –&gt; 00:05:34,819<br>And this is basically, there’s some applications where you want to do both the OLAP</p>
<p>91<br>00:05:34,819 –&gt; 00:05:38,819<br>workloads and the OLAP workloads, potentially in the same system.</p>
<p>92<br>00:05:38,819 –&gt; 00:05:41,819<br>So, instead of having to meet, take all my transactional data,</p>
<p>93<br>00:05:41,819 –&gt; 00:05:45,819<br>put it into a separate data warehouse, and then do my analytics on there,</p>
<p>94<br>00:05:45,819 –&gt; 00:05:49,819<br>maybe I could do some analytics directly as the data comes in.</p>
<p>95<br>00:05:49,819 –&gt; 00:05:52,819<br>We’ll discuss this sort of throughout the semester,</p>
<p>96<br>00:05:52,819 –&gt; 00:05:57,819<br>but the main two ones you want to focus on are OLAP.</p>
<p>97<br>00:05:57,819 –&gt; 00:06:02,819<br>Another way to think about the distinction between them is sort of a simple grid like this,</p>
<p>98<br>00:06:02,819 –&gt; 00:06:07,819<br>where along the X-axis, I’m saying whether the workload is meet right heavy,</p>
<p>99<br>00:06:07,819 –&gt; 00:06:11,819<br>versus read heavy, and then the Y-axis says how complex the queries are.</p>
<p>100<br>00:06:11,819 –&gt; 00:06:13,819<br>So, you can sort of divide it up like this.</p>
<p>101<br>00:06:13,819 –&gt; 00:06:17,819<br>OTP would be down in this corner because we’re doing potentially a lot of updates,</p>
<p>102<br>00:06:17,819 –&gt; 00:06:20,819<br>but the queries are going to execute are going to be really simple.</p>
<p>103<br>00:06:20,819 –&gt; 00:06:26,819<br>Like, go select single, you know, go select star from the count table where your ID equals Andy.</p>
<p>104<br>00:06:26,819 –&gt; 00:06:29,819<br>Right, it’s going getting single things.</p>
<p>105<br>00:06:29,819 –&gt; 00:06:32,819<br>OLAP would be on the opposite end of the spectrum here,</p>
<p>106<br>00:06:32,819 –&gt; 00:06:37,819<br>where we’re doing mostly writes, and the writes are mostly reads,</p>
<p>107<br>00:06:37,819 –&gt; 00:06:43,819<br>and the reads, the select queries are going to be executing are going to be much more complex</p>
<p>108<br>00:06:43,819 –&gt; 00:06:45,819<br>than we do in the OLP2B world.</p>
<p>109<br>00:06:45,819 –&gt; 00:06:48,819<br>Think of like Q9, Q10 in homework 1.</p>
<p>110<br>00:06:48,819 –&gt; 00:06:53,819<br>So, OLAP, the OLP term, it goes back to the 80s.</p>
<p>111<br>00:06:53,819 –&gt; 00:06:57,819<br>OLAP comes from the 90s, this guy that named Jim Gray,</p>
<p>112<br>00:06:57,819 –&gt; 00:07:01,819<br>his famous database teacher, who invented a lot of stuff when we talked about this semester,</p>
<p>113<br>00:07:01,819 –&gt; 00:07:06,819<br>he wrote an article saying, hey, there’s this new category of workloads in the early 90s,</p>
<p>114<br>00:07:06,819 –&gt; 00:07:08,819<br>called OLAP, and we should pay attention to it.</p>
<p>115<br>00:07:08,819 –&gt; 00:07:13,819<br>Turns out he was actually getting paid by a company who was trying to sell OLAP database system product</p>
<p>116<br>00:07:13,819 –&gt; 00:07:17,819<br>in the early 90s, and the paper got retracted, but the name still stuck around.</p>
<p>117<br>00:07:17,819 –&gt; 00:07:21,819<br>And then Jim Gray won the touring word in databases in I think 96, right?</p>
<p>118<br>00:07:21,819 –&gt; 00:07:24,819<br>It’s a very famous database researcher.</p>
<p>119<br>00:07:24,819 –&gt; 00:07:29,819<br>And has anybody heard of the story node? Has anybody heard of Jim Gray before?</p>
<p>120<br>00:07:29,819 –&gt; 00:07:30,819<br>It’s one, sort of.</p>
<p>121<br>00:07:30,819 –&gt; 00:07:35,819<br>So, he famously got lost at C in the San Francisco Bay in 2006.</p>
<p>122<br>00:07:35,819 –&gt; 00:07:38,819<br>He was out sailing by himself, he was not a joke, he was out sailing by himself,</p>
<p>123<br>00:07:38,819 –&gt; 00:07:41,819<br>and his boat disappeared.</p>
<p>124<br>00:07:41,819 –&gt; 00:07:44,819<br>And this is actually one of the early examples of crowdsourcing,</p>
<p>125<br>00:07:44,819 –&gt; 00:07:48,819<br>because they actually moved satellites to take pictures of the San Francisco Bay</p>
<p>126<br>00:07:48,819 –&gt; 00:07:53,819<br>and try to, you know, people look at the images, try to find the boat, and they never found them.</p>
<p>127<br>00:07:53,819 –&gt; 00:07:54,819<br>All right?</p>
<p>128<br>00:07:54,819 –&gt; 00:08:00,819<br>And so then, that’s a weird tangent, but I never met him, but like, a lot of the,</p>
<p>129<br>00:08:00,819 –&gt; 00:08:04,819<br>you know, we talked about, like, you know, going to Plutov versus reading the book in front of you,</p>
<p>130<br>00:08:04,819 –&gt; 00:08:08,819<br>you know, that was a Jim Gray metaphor. He had a lot of interesting things like that.</p>
<p>131<br>00:08:08,819 –&gt; 00:08:10,819<br>All right, so, Hdat would be sort of the metal.</p>
<p>132<br>00:08:10,819 –&gt; 00:08:17,819<br>So, today, I want to spend time talking about why the things we talked about so far</p>
<p>133<br>00:08:17,819 –&gt; 00:08:21,819<br>and the pre-stue lectures, they’re going to be good for OTP and not O-Lap,</p>
<p>134<br>00:08:21,819 –&gt; 00:08:27,819<br>and then we’ll design a storage scheme that is better for O-Lap.</p>
<p>135<br>00:08:27,819 –&gt; 00:08:31,819<br>So, to do this, we’re going to do a real simple example using a real database.</p>
<p>136<br>00:08:31,819 –&gt; 00:08:36,819<br>So, this is roughly what the WikiPedia database looks like.</p>
<p>137<br>00:08:36,819 –&gt; 00:08:41,819<br>It runs a software called MediaWiki, it runs off of my SQL and PHP.</p>
<p>138<br>00:08:41,820 –&gt; 00:08:47,820<br>Like, it’s open source, you can go look at it, and the schema roughly looks like this.</p>
<p>139<br>00:08:47,820 –&gt; 00:08:50,820<br>Right? There’ll be user accounts, people that are actually making changes.</p>
<p>140<br>00:08:50,820 –&gt; 00:08:55,820<br>There’ll be pages, like the articles in WikiPedia, and then there’ll be revisions for those articles.</p>
<p>141<br>00:08:55,820 –&gt; 00:08:58,820<br>And so, there’s a foreign key reference for a revision.</p>
<p>142<br>00:08:58,820 –&gt; 00:09:05,820<br>You have the user that created the change, and then the, an ID to the actual, the page itself.</p>
<p>143<br>00:09:05,820 –&gt; 00:09:10,820<br>But all the text itself is going to go in the revision part.</p>
<p>144<br>00:09:10,820 –&gt; 00:09:18,820<br>Right? And there’s a, there’s a, there’s a funky going back from the page to revisions so you can find the latest, latest revision.</p>
<p>145<br>00:09:18,820 –&gt; 00:09:25,820<br>So, I said this before, and I’ll say it again, the relational model does not define or specify that,</p>
<p>146<br>00:09:25,820 –&gt; 00:09:29,820<br>anything about how we should store the data in a table.</p>
<p>147<br>00:09:29,820 –&gt; 00:09:32,820<br>Right? And so, in all the examples I’ve shown so far, we’re just showing the two,</p>
<p>148<br>00:09:32,820 –&gt; 00:09:34,820<br>but every table, all the attributes, one after another.</p>
<p>149<br>00:09:34,820 –&gt; 00:09:39,820<br>Yes, we said there was overflow pages for large attributes, but you know, that’s, that’s, that’s, that’s,</p>
<p>150<br>00:09:39,820 –&gt; 00:09:43,820<br>in general, all the, all the, the smaller attributes will store together.</p>
<p>151<br>00:09:43,820 –&gt; 00:09:46,820<br>But there’s nothing about, again, the relational model says you have to do that,</p>
<p>152<br>00:09:46,820 –&gt; 00:09:48,820<br>just sort of what we as humans came up with first.</p>
<p>153<br>00:09:48,820 –&gt; 00:09:51,820<br>It’s easy for us to conceptually think about.</p>
<p>154<br>00:09:51,820 –&gt; 00:09:54,820<br>But again, for OLAP workloads, this may not be the best thing.</p>
<p>155<br>00:09:54,820 –&gt; 00:09:58,820<br>So, let’s see how it works for, for, for OLTP. Right?</p>
<p>156<br>00:09:58,820 –&gt; 00:10:02,820<br>For, again, for OLTP, it’s going to be a bunch of small queries that are going, sorry,</p>
<p>157<br>00:10:02,820 –&gt; 00:10:07,820<br>it’s going to be a lot of queries that are going to be real simple, and they’re going to read or write a small amount of data,</p>
<p>158<br>00:10:07,820 –&gt; 00:10:10,820<br>relative to all the data in the database. Right?</p>
<p>159<br>00:10:10,820 –&gt; 00:10:15,820<br>So, the first query here is just going get the, for a given page, given, given its page ID,</p>
<p>160<br>00:10:15,820 –&gt; 00:10:18,820<br>go get me the latest revision for it.</p>
<p>161<br>00:10:18,820 –&gt; 00:10:22,820<br>So, it’s a join against the revision table, but it’s one page and one revision.</p>
<p>162<br>00:10:22,820 –&gt; 00:10:24,820<br>It’s, it’s retrieving that.</p>
<p>163<br>00:10:24,820 –&gt; 00:10:28,820<br>The next one is the update query is somebody logging in, you know, you have a user ID,</p>
<p>164<br>00:10:28,820 –&gt; 00:10:33,820<br>assuming they’ve been authenticated, and you update the user count with the time step of the last time they logged in,</p>
<p>165<br>00:10:33,820 –&gt; 00:10:36,820<br>and the host name from where they logged in from.</p>
<p>166<br>00:10:36,820 –&gt; 00:10:41,820<br>Or if I do an insert into the revision table, it’s inserting a single row. Right?</p>
<p>167<br>00:10:41,820 –&gt; 00:10:44,820<br>And this is what usually people end up with when they build a brand new application,</p>
<p>168<br>00:10:44,820 –&gt; 00:10:49,820<br>like if you’re, you know, if you’re going to create a startup and you start building some online service,</p>
<p>169<br>00:10:49,820 –&gt; 00:10:53,820<br>you usually end up with something level like this because you don’t have any data in the beginning,</p>
<p>170<br>00:10:53,820 –&gt; 00:10:58,820<br>you need to get it and you make a website and then your website is going to run these kind of queries.</p>
<p>171<br>00:10:58,820 –&gt; 00:11:06,820<br>Again, for, and for, oh, that, we’re going to do more complicated things that requires to look at larger portions of the table.</p>
<p>172<br>00:11:06,820 –&gt; 00:11:13,820<br>So, this is actually a rough approximation of a real query where people were running,</p>
<p>173<br>00:11:13,820 –&gt; 00:11:20,820<br>you would look at the user accounts in Wikipedia, and you find all the, the, the login attempts from,</p>
<p>174<br>00:11:21,820 –&gt; 00:11:26,820<br>from users that had an IP address or a host name that ended with .gov. Right?</p>
<p>175<br>00:11:26,820 –&gt; 00:11:34,820<br>Because there was like a, there was a scandal late 2000s, early 2010s where like people in Congress are having their staff go update Wikipedia</p>
<p>176<br>00:11:34,820 –&gt; 00:11:38,820<br>to say more flattering things about, you know, the congressman or congresswoman, right?</p>
<p>177<br>00:11:38,820 –&gt; 00:11:43,820<br>Like pens did this, Joe Biden did this, right? So this query could find all people that were doing that.</p>
<p>178<br>00:11:43,820 –&gt; 00:11:49,820<br>So we basically were paying government employees to go update Wikipedia, and I shouldn’t have. Right?</p>
<p>179<br>00:11:49,820 –&gt; 00:11:58,820<br>Again, so this is queries we’re going to execute on data of app we’ve already collected it from, from sort of the old to be portion of the application.</p>
<p>180<br>00:11:58,820 –&gt; 00:12:03,820<br>So the thing we need to talk about now is what I’ll call the, the storage model.</p>
<p>181<br>00:12:03,820 –&gt; 00:12:15,820<br>And this is going to be how the database is going to physically organize tuples in, in disk and in memory relative to their other tuples in their own attributes.</p>
<p>182<br>00:12:15,820 –&gt; 00:12:23,820<br>And so up until now, again, I’ve been assuming that all of the attributes are contiguous for a tuple, and that’s sort of roughly called a, a, a row store.</p>
<p>183<br>00:12:23,820 –&gt; 00:12:29,820<br>But again, for O that, that actually may not be the best thing, and we’ll see why in a, in a second.</p>
<p>184<br>00:12:29,820 –&gt; 00:12:41,820<br>And the reason why we have to discuss this part of a system because there is a clear distinction in the marketplace now or in, in, in the database world, between a row store system and a column store system.</p>
<p>185<br>00:12:41,820 –&gt; 00:12:47,820<br>With a row store system, you’d want to use that for OTP, and for column store system, you’d want to use that for OLAB.</p>
<p>186<br>00:12:47,820 –&gt; 00:12:55,820<br>And then anybody tries to say, hey, I’ve got a fast row store that you can use for analytics, you know, you should be very skeptical.</p>
<p>187<br>00:12:55,820 –&gt; 00:13:04,820<br>All right, so the three choices to do are the, the anary storage model or NSM, that’s the row store decomposition storage model, DSM, that’s the column store.</p>
<p>188<br>00:13:04,820 –&gt; 00:13:12,820<br>And then a hybrid approach is actually, this is the most common one for column stores. Well, we’ll see why in a second. It’s called packs or partition attribute across.</p>
<p>189<br>00:13:12,820 –&gt; 00:13:22,820<br>And most time people say they have a column store, they really have the packs one, but it’s, it’s, it’s, it’s not a major major major major difference.</p>
<p>190<br>00:13:22,820 –&gt; 00:13:26,820<br>All right, let’s start with the first one, the NSM row store.</p>
<p>191<br>00:13:26,820 –&gt; 00:13:36,820<br>Again, this is what we’ve already said so far this semester. We assume that almost all the attributes for a given tuple are going to be stored continuously in a single page, you know, one attribute after another.</p>
<p>192<br>00:13:36,820 –&gt; 00:13:47,820<br>And the idea is, again, you’re going across in the page and you’re laying out all the data for a given tuple, and you don’t start anything, you don’t lay down any bits for the next tuple until you finish the current tuple.</p>
<p>193<br>00:13:48,820 –&gt; 00:13:58,820<br>And the reason why this is going to be better for an OTP system is, as I already said before, OTP application is that most of the queries are going to be accessing a single entry or single tuple.</p>
<p>194<br>00:13:58,820 –&gt; 00:14:07,820<br>Right? And so now I can go to a single page and get all the data I need for that single attribute. And that’s, that’s really all I need for to satisfy that query.</p>
<p>195<br>00:14:08,820 –&gt; 00:14:13,820<br>We’re already talking about page sizes, but again, it’s always been some months multiple of hard pages.</p>
<p>196<br>00:14:13,820 –&gt; 00:14:21,820<br>So this is basically the same layout that we saw before, right? That we have some database page, we have a header in the front with a slot array.</p>
<p>197<br>00:14:21,820 –&gt; 00:14:36,820<br>And then as we start scanning through our table and want to start or just getting through the application, what starts inserting data, it’s just going to go append the entries to the end and keep adding adding more and then till it does think it’s filled up, right?</p>
<p>198<br>00:14:37,820 –&gt; 00:14:50,820<br>And again, now if any query comes along, says select star from this table, where ID equals something, we did go to get this one page, jumped at the offset as defined the slot array, and we get all the data that we need.</p>
<p>199<br>00:14:50,820 –&gt; 00:14:54,820<br>So let’s see now in how this works in a Wikipedia example.</p>
<p>200<br>00:14:55,820 –&gt; 00:15:01,820<br>Say we have a query here, where someone wants to log in, they’re passing in a username and a password, we’re just checking to see whether that matches.</p>
<p>201<br>00:15:01,820 –&gt; 00:15:08,820<br>This is roughly how you log into a database back application, or if you do authentication database, it roughly looks like this.</p>
<p>202<br>00:15:08,820 –&gt; 00:15:19,820<br>And again, to be able to ignore how we actually find the data that we want for one given user, but assume there’s some kind of index, hash table, B plus tree, it doesn’t matter, we’ll cover that and let’s generate.</p>
<p>203<br>00:15:19,820 –&gt; 00:15:25,820<br>And then the way to say for this user account, here’s the record ID and offset.</p>
<p>204<br>00:15:25,820 –&gt; 00:15:38,820<br>So now we go into our page directory and we find a page that has the data we’re looking for, we look in the slot array, we jump to some offset, and now we have all the data that we need for this query query, and we can produce the result.</p>
<p>205<br>00:15:38,820 –&gt; 00:15:42,820<br>Again, so this is the idea for OTP because all the data is just continuous.</p>
<p>206<br>00:15:42,820 –&gt; 00:15:54,820<br>Same thing we want to do in insert, all we need to do is look in our page directory, find a page that has a free slot, go bring it memory, assume it’s this one, and then append it to the end of it.</p>
<p>207<br>00:15:54,820 –&gt; 00:15:56,820<br>That’s fine.</p>
<p>208<br>00:15:56,820 –&gt; 00:16:06,820<br>But now if I try to run that query before, again, when I’m trying to find all the, get the number of times you log in per month if they end with a hostname with .gov,</p>
<p>209<br>00:16:06,820 –&gt; 00:16:14,820<br>now you see in this case here, I got to scan all the pages in the table because I need to look at everything, all the user accounts.</p>
<p>210<br>00:16:14,820 –&gt; 00:16:26,820<br>And then when I bring a page in, the way we’re going to roughly execute the query, we haven’t got to have a new query execution yet, but the roughly right idea is that we got this wear clause thing that’s looking up on hostname.</p>
<p>211<br>00:16:26,820 –&gt; 00:16:33,820<br>So we need to go find the tuples in the page that, where that that predicate on the hostname is satisfied.</p>
<p>212<br>00:16:33,820 –&gt; 00:16:40,820<br>So the only data we really need to look at is just the hostname here.</p>
<p>213<br>00:16:40,820 –&gt; 00:16:46,820<br>Then we got to do the aggregate on the last log in for the group by.</p>
<p>214<br>00:16:46,820 –&gt; 00:16:53,820<br>And so that means the only data we really need to look at for that push in the query is just these attributes here.</p>
<p>215<br>00:16:53,820 –&gt; 00:16:57,820<br>So what’s the obvious problem?</p>
<p>216<br>00:16:58,820 –&gt; 00:17:03,820<br>Get through all the rows and you brought a bunch of data and you actually don’t need, right?</p>
<p>217<br>00:17:03,820 –&gt; 00:17:13,820<br>So in order to get just the attributes I needed, I had to bring in the entire page, but the entire page brings on a bunch of attributes, user ID, user name, user pass, that I don’t need for this query.</p>
<p>218<br>00:17:13,820 –&gt; 00:17:21,820<br>So I’m basically doing useless IO. I’m fetching data in from disk and I don’t even need it at all.</p>
<p>219<br>00:17:21,819 –&gt; 00:17:26,819<br>So not only is that slow, but in some systems you pay per disk IOPS.</p>
<p>220<br>00:17:26,819 –&gt; 00:17:33,819<br>In Aurora on Amazon, if you read something from disk, you pay per the number of IO operations you’re doing for a query.</p>
<p>221<br>00:17:33,819 –&gt; 00:17:40,819<br>So in this case here, I’d be paying for data that I don’t actually even need.</p>
<p>222<br>00:17:40,819 –&gt; 00:17:43,819<br>So that’s the obvious problem with NSM.</p>
<p>223<br>00:17:43,819 –&gt; 00:17:49,819<br>Again, great for inserts update, it’s great for queries that need to get the entire, all the data for a single entity in the database.</p>
<p>224<br>00:17:49,819 –&gt; 00:17:59,819<br>But if you want to scan large portions of a table and you only need a subset of the attributes which most OLAP queries only need, right?</p>
<p>225<br>00:17:59,819 –&gt; 00:18:05,819<br>It’s very rare if you call a select star on a really wide, huge table because you’re basically dumping the whole thing out.</p>
<p>226<br>00:18:05,819 –&gt; 00:18:08,819<br>There’s utilities to make that go faster other than the select star.</p>
<p>227<br>00:18:08,819 –&gt; 00:18:14,819<br>So this can be bad for OLAP because we’re bringing in data that we don’t need.</p>
<p>228<br>00:18:14,819 –&gt; 00:18:26,819<br>This is a low level detail, but going back to this portion here, if you think about how you would actually execute the query to do this, to run this predicate,</p>
<p>229<br>00:18:26,819 –&gt; 00:18:31,819<br>I’m jumping around to different locations in memory to do my scan.</p>
<p>230<br>00:18:31,819 –&gt; 00:18:38,819<br>So I’ve got to read this header for the first tuple, figure out how far I need to jump over to get the host name.</p>
<p>231<br>00:18:38,819 –&gt; 00:18:42,819<br>Then I can maybe look at the last log and I’m computing the aggregate as I go along.</p>
<p>232<br>00:18:42,819 –&gt; 00:18:48,819<br>But then I jump down to the next tuple and then jump over to get to the, you know, it’s a host name attribute.</p>
<p>233<br>00:18:48,819 –&gt; 00:19:02,819<br>So in a modern super scale, I see, this is terrible because there’s a bunch of these non-sequential operations that could also be non-deterministic where my memory locations that I’m accessing is going to be sort of,</p>
<p>234<br>00:19:02,819 –&gt; 00:19:11,819<br>it’s not random because you’re always going in an increasing order, but it’s not going to be like, I’m just reading strides of memory and crunching through it very quickly.</p>
<p>235<br>00:19:11,819 –&gt; 00:19:17,819<br>That’s a low level detail that we don’t really cover in this semester, but it’s at least worth discussing.</p>
<p>236<br>00:19:17,819 –&gt; 00:19:26,819<br>And then we’ll see this at, we’ll cover compression later in this lecture, but in this case here, we’re not going to be able to get a compression rate if you want to reduce the amount of,</p>
<p>237<br>00:19:26,819 –&gt; 00:19:41,819<br>pack in more data within a single page because all the attributes for a given table are just thrown together in that page and there’s no, there’s going to be less chance for repeatability or less chance for identifying, hey, these values are the same, I can compress them really well.</p>
<p>238<br>00:19:41,819 –&gt; 00:19:54,819<br>Right, again, it’s going back, we have, we have a user ID, that’s going to be integer, user names can be random string, user pass random string, right, it’s going to be all sort of different value domains and that’s not going to be ideal for compression.</p>
<p>239<br>00:19:54,819 –&gt; 00:20:18,819<br>So the alternative approach is the DSM, the columnar storage, the composition storage model, and the idea here is that instead of storing all the attributes for a single tuple together in a page, we’re going to store all the attributes for all tuples, sorry, for all tuples, we’re going to store a single attribute in a single page, right, I just had that last log in field, instead of having all intermixed with the other attributes within a single page, we’re going to store all the attributes for a single page.</p>
<p>240<br>00:20:18,819 –&gt; 00:20:28,819<br>Right, I just had that last log in field, instead of having all intermixed with the other attributes within a single page, we’re only just had that last log in attribute.</p>
<p>241<br>00:20:28,819 –&gt; 00:20:47,819<br>And this is going to be ideal now for OLAP queries because they’re going to scan the entire table and only for only a subset of the attributes, and now when I go fetch a page from disk, I’m only getting data, I know I’m only getting data for the has attributes that actually need, and not for other things that just sort of gets carried along for the right.</p>
<p>242<br>00:20:48,819 –&gt; 00:21:00,819<br>So the, again, the benefit of a declared language like SQL is that you don’t have to know, not to care whether you’re running on a row store system versus a column store system, your same SQL query works just fine, this is the same.</p>
<p>243<br>00:21:00,819 –&gt; 00:21:15,819<br>But now it’s the database system for responsibility, meaning us, people actually building it, it’s our responsibility to take data, split it up into separate columns, separate attributes, and then stitch it back together to when we need to produce results.</p>
<p>244<br>00:21:18,819 –&gt; 00:21:22,819<br>All right, so this is just another the same diagram I showed before.</p>
<p>245<br>00:21:22,819 –&gt; 00:21:28,819<br>Again, the way to think about this is that for the first column here, for its attribute column A, we will have a separate file with a bunch of pages.</p>
<p>246<br>00:21:28,819 –&gt; 00:21:32,819<br>It’ll have a header now just to tell us what’s inside the page.</p>
<p>247<br>00:21:32,819 –&gt; 00:21:46,819<br>And then now we’ll have the null bitmap for all the columns, sorry, for all the values within that within within this column, followed by now the continuous values for all the two tables in the table.</p>
<p>248<br>00:21:47,819 –&gt; 00:21:51,819<br>And we just do it for the next one and the next one.</p>
<p>249<br>00:21:51,819 –&gt; 00:21:53,819<br>All right.</p>
<p>250<br>00:21:53,819 –&gt; 00:22:02,819<br>And so these are still, these files are still be broken up as database pages like we talked about before, so either 4 kilobytes, 8 kilobytes, whatever, whatever the system supports.</p>
<p>251<br>00:22:02,819 –&gt; 00:22:09,819<br>But the file itself will contain, again, just just the data for a single attribute.</p>
<p>252<br>00:22:10,819 –&gt; 00:22:29,819<br>And now the metadata overhead for these different files is actually much less than a row store because I don’t have to keep track of like every single column, whether it could be null or not, the different information about the offsets or what to find things.</p>
<p>253<br>00:22:29,819 –&gt; 00:22:40,819<br>Right. These are all going to be the metadata at the store because it’s just all the same value domains are all the same attribute type is we much less than a row store.</p>
<p>254<br>00:22:40,819 –&gt; 00:22:53,819<br>All right. So the idea here again, so we go back to our Wikipedia example, we just take the the every column for for our table and then we’re going to store that as a separate page.</p>
<p>255<br>00:22:54,819 –&gt; 00:23:03,819<br>Right. So if you go back to the, you know, for the host name example, within a single page, again, we’re only storing values for the host name column.</p>
<p>256<br>00:23:03,819 –&gt; 00:23:07,819<br>And we’ll have separate pages for all all our attributes.</p>
<p>257<br>00:23:07,819 –&gt; 00:23:17,819<br>So now if you go back to this, this, this, the other lab query here again, we’re doing the look up the counting number logins per month with the from government addresses.</p>
<p>258<br>00:23:17,819 –&gt; 00:23:32,819<br>The first part X, unit query is going get the host name, well, that’s assuming it’s, it’s one page per attribute that’s going fetching that one page and then doing the scan is ripping through the column and identifying all the matches for that, for that host name.</p>
<p>259<br>00:23:32,819 –&gt; 00:23:40,819<br>And again, I have 100 complete utilization of all the data that I brought in because I’m only bringing in the data I need for, for this query.</p>
<p>260<br>00:23:40,819 –&gt; 00:23:49,819<br>I’m not bringing in attributes that I don’t care about. And then now we’ll talk about this later how we do, we talk about query execution, how we, how we match things up.</p>
<p>261<br>00:23:49,819 –&gt; 00:23:55,819<br>But assuming I keep track of a list of here’s the offsets of the tuples within this column that match my predicate.</p>
<p>262<br>00:23:55,819 –&gt; 00:24:15,819<br>Then I go to now to the last log in page, go fetch that again, that only has data that we need. And then now I know how to jump to the different offsets of the matching host names to find the right offset for the log in the log in time stamp and then compute whatever I need for the query.</p>
<p>263<br>00:24:15,819 –&gt; 00:24:18,819<br>So this is clear.</p>
<p>264<br>00:24:18,819 –&gt; 00:24:23,819<br>Who here is here to column storms before today? Less than 10%.</p>
<p>265<br>00:24:23,819 –&gt; 00:24:37,819<br>Again, so this is a, it sort of seems obvious now. This is clear the way you want to do this, but up to like 15 years ago, I mean 20 years ago, this is not how any Davies system was actually built. It was very, very rare.</p>
<p>266<br>00:24:37,819 –&gt; 00:24:38,819<br>Yes.</p>
<p>267<br>00:24:38,819 –&gt; 00:24:56,819<br>So your question is, if I go back to the, the road store example.</p>
<p>268<br>00:24:56,819 –&gt; 00:25:01,819<br>This one here, your question is what, sorry, if even in this one, do I have to.</p>
<p>269<br>00:25:01,819 –&gt; 00:25:09,819<br>I was like, this one.</p>
<p>270<br>00:25:09,819 –&gt; 00:25:14,819<br>Oh, like this, this one. Yeah, yeah, literally this, okay, yes. Why is it?</p>
<p>271<br>00:25:14,819 –&gt; 00:25:16,819<br>How do you know the index from like that?</p>
<p>272<br>00:25:16,819 –&gt; 00:25:22,819<br>Where? How does it know where that you can password equals?</p>
<p>273<br>00:25:22,819 –&gt; 00:25:31,819<br>Oh, okay. This question is, I said there’s some index. I didn’t say what it was. There’s some magic way to say, look at the wear clause where it says username equals something, right?</p>
<p>274<br>00:25:31,819 –&gt; 00:25:39,819<br>Because you would go to index and username. And I magically got to the single record, again, the record ID page ID and offset. How did I do that?</p>
<p>275<br>00:25:39,819 –&gt; 00:25:44,819<br>That’s what the index does. That’s next week. Right? It’s just a key value.</p>
<p>276<br>00:25:44,819 –&gt; 00:25:54,819<br>You can think of a key value map or associative array for giving key, the username, give be the record ID or record IDs if it’s non unique. That matched this.</p>
<p>277<br>00:25:54,819 –&gt; 00:26:01,819<br>Then I, then so I get that. My next gives me the record ID. I look at my page directory, said, okay, I need page 123. Where’s that?</p>
<p>278<br>00:26:01,819 –&gt; 00:26:08,819<br>It’s on here. It’s either in memory or in disk. I go get it. And now I have the slot number from the record ID and I look in that page and jump to that slot.</p>
<p>279<br>00:26:08,819 –&gt; 00:26:15,819<br>So that allows me to jump exactly to the page I need. And then within that page, go to get exactly the record I need.</p>
<p>280<br>00:26:15,819 –&gt; 00:26:23,819<br>But again, like I only need one one assuming user names are unique. I only need one username.</p>
<p>281<br>00:26:23,819 –&gt; 00:26:28,819<br>So when I had to go fetch all these other rows, I don’t actually need.</p>
<p>282<br>00:26:28,819 –&gt; 00:26:29,819<br>Yes.</p>
<p>283<br>00:26:29,819 –&gt; 00:26:40,819<br>So just to understand like the only difference between a column base, like the only disadvantage of column base would have is slightly more implementation on the DBMS side.</p>
<p>284<br>00:26:40,819 –&gt; 00:26:42,819<br>What do you like slight more imitation?</p>
<p>285<br>00:26:42,819 –&gt; 00:26:49,819<br>I mean, your DBMS has to be more intelligent because now it has to keep map of different columns and stitch them together.</p>
<p>286<br>00:26:49,819 –&gt; 00:26:58,819<br>Where in a row base, you don’t really care. You want to have the access of the whole row. You can compare the host name. You can compare the login at a single instance.</p>
<p>287<br>00:26:58,819 –&gt; 00:27:08,819<br>Where if you are doing it column based, you will have to have some more intelligence to understand what I’m putting out because I’m matching two different columns.</p>
<p>288<br>00:27:08,819 –&gt; 00:27:10,819<br>I would say they’re equally as hard.</p>
<p>289<br>00:27:10,819 –&gt; 00:27:17,819<br>If you don’t care about other, but much other protections like we’re not going to transactions, but like.</p>
<p>290<br>00:27:18,819 –&gt; 00:27:24,819<br>If you don’t care about those things, then yeah, I would agree that like a row store would be easier to.</p>
<p>291<br>00:27:24,819 –&gt; 00:27:35,819<br>It’s again, you just like, everything’s here. I assume it all packs in then yeah, then assuming that every record can fit into a single page ignoring overflows, then a row store would be potentially easier.</p>
<p>292<br>00:27:35,819 –&gt; 00:27:36,819<br>Yes.</p>
<p>293<br>00:27:36,819 –&gt; 00:27:37,819<br>Yes.</p>
<p>294<br>00:27:37,819 –&gt; 00:27:44,819<br>So the fact that like a data store and like a row, then it will come from very different, considerable by the whole word, which it can probably have in the title.</p>
<p>295<br>00:27:44,819 –&gt; 00:27:52,819<br>So this question is, is the fact that a, should the data system store is a row store versus a column store?</p>
<p>296<br>00:27:52,819 –&gt; 00:27:56,819<br>Is this something that’s configurable by table or is this, it’s sort of all there nothing?</p>
<p>297<br>00:27:56,819 –&gt; 00:28:04,819<br>So most systems are going to be row only or column only, right?</p>
<p>298<br>00:28:05,819 –&gt; 00:28:09,819<br>The HTAP stuff, the hybrid stuff, sort of tries to do sort of, sort of, both.</p>
<p>299<br>00:28:13,819 –&gt; 00:28:20,819<br>So typically you would say, yeah, so in most systems you would say, I know I’m using this system, it’s going to be a column store.</p>
<p>300<br>00:28:20,819 –&gt; 00:28:22,819<br>So I’ll store everything in there, right?</p>
<p>301<br>00:28:22,819 –&gt; 00:28:25,819<br>The tables will be column, column up.</p>
<p>302<br>00:28:26,819 –&gt; 00:28:38,819<br>Now, even though I said like the, like, even though it’s a column store and we’re going to be optimized for read only queries, people obviously want to update data, right?</p>
<p>303<br>00:28:38,819 –&gt; 00:28:52,819<br>And so the way you typically get around that is these systems would have a sort of row store buffer area and it typically is log structure, where if I have any updates, I apply them to that row portion.</p>
<p>304<br>00:28:53,819 –&gt; 00:28:56,819<br>And then periodically I would then merge them into the column store, right?</p>
<p>305<br>00:28:56,819 –&gt; 00:28:58,819<br>That’s one approach to this.</p>
<p>306<br>00:28:58,819 –&gt; 00:29:09,819<br>Oracle does a different approach where the row store is considered the primary storage location of the database, but then they’ll make a copy of your tables in a column store format.</p>
<p>307<br>00:29:09,819 –&gt; 00:29:12,819<br>And they, they’re responsible for keeping the sort of things updated for you.</p>
<p>308<br>00:29:12,819 –&gt; 00:29:21,819<br>So different approach to these different things, but typically, if the system supports, I want a row store versus a column store, you could define it on a portable basis, but most systems don’t do that.</p>
<p>309<br>00:29:22,819 –&gt; 00:29:23,819<br>Yes.</p>
<p>310<br>00:29:23,819 –&gt; 00:29:33,819<br>Many of you think column store systems have the data if you have the number of as many disks that you have applied for every column.</p>
<p>311<br>00:29:33,819 –&gt; 00:29:36,819<br>Would you have the right to update the abstractivable system?</p>
<p>312<br>00:29:36,819 –&gt; 00:29:48,819<br>This question is, if I have as many disks as I have columns, assuming I break up in a column store or a table, and every attribute goes to a separate disk, would that be as fast as a row store?</p>
<p>313<br>00:29:49,819 –&gt; 00:29:55,819<br>Well, no, because you have to, you still have to do that, that’s splitting apart and writing it all out, right?</p>
<p>314<br>00:29:55,819 –&gt; 00:30:03,819<br>And then you also would have more pressure in memory because, because again, say I have a thousand attributes.</p>
<p>315<br>00:30:03,819 –&gt; 00:30:14,819<br>So now, if I have to update a thousand pages, I have to have a thousand pages in my buffer pool to do the update to put, you know, the each, updating each of them with the new instant attribute, and then write them all out.</p>
<p>316<br>00:30:15,819 –&gt; 00:30:25,819<br>Typically, again, doing updates in a column store system without this sort of buffer thing I just mentioned is always going to be slow.</p>
<p>317<br>00:30:25,819 –&gt; 00:30:26,819<br>Yes.</p>
<p>318<br>00:30:31,819 –&gt; 00:30:36,819<br>Her question is, what is the null bitmap in the recurrence in Roasters?</p>
<p>319<br>00:30:37,819 –&gt; 00:30:43,819<br>Yeah, we discussed this last class. It basically, it’s a way to represent which attribute is null, right?</p>
<p>320<br>00:30:43,819 –&gt; 00:30:52,819<br>So I’m not drawing here, but they had the diagram last class. In the header of every row, there’ll be a bitmap that says which attribute is null or not.</p>
<p>321<br>00:30:52,819 –&gt; 00:31:00,819<br>That’s one approach to do it. That’s in this common one, right? You could do this, we had to talk about the special value, or each attribute could have a little flag in front of it.</p>
<p>322<br>00:31:00,819 –&gt; 00:31:05,819<br>But the null bitmap basically says, for this two, what attribute one is null, attribute two is null.</p>
<p>323<br>00:31:05,819 –&gt; 00:31:13,819<br>And so, think of that instead of having that bitmap per in the header per tuple, in the comms store, it’s just for the entire column, here’s the null bitmap.</p>
<p>324<br>00:31:16,819 –&gt; 00:31:17,819<br>Yes.</p>
<p>325<br>00:31:17,819 –&gt; 00:31:25,819<br>If word index in a comms store is it instead of here, where to find this one, where to find these 10 columns?</p>
<p>326<br>00:31:25,819 –&gt; 00:31:31,819<br>This question is, in a comms store, what does the index actually do?</p>
<p>327<br>00:31:31,819 –&gt; 00:31:36,819<br>So some systems, some O-Lat systems that are comms stores, you don’t get any indexes.</p>
<p>328<br>00:31:36,819 –&gt; 00:31:43,819<br>I don’t think Snowflake gives you an index. You can’t create one. It might have changed. Vertica, you couldn’t have an indexes, right?</p>
<p>329<br>00:31:44,819 –&gt; 00:31:51,819<br>Because again, they’re not trying to do point queries or single thing lookups, right? It’s to do complete scans.</p>
<p>330<br>00:31:52,819 –&gt; 00:31:58,819<br>And so now, you’re at the point in your correct, you could have indexes that are range indexes, right?</p>
<p>331<br>00:31:58,819 –&gt; 00:32:03,819<br>So here’s where to go find, if your ID is when 0 and 100 go to this page, right?</p>
<p>332<br>00:32:03,819 –&gt; 00:32:12,819<br>Or there’s things like that. There’s inverted indexes, like find me all the records where the keyword Andy exists, right?</p>
<p>333<br>00:32:13,819 –&gt; 00:32:21,819<br>And that doesn’t look like a tree structure that’s usually hashable. Like there’s different types of indexes, but you would not, maybe you wouldn’t have the index of the point query lookups.</p>
<p>334<br>00:32:26,819 –&gt; 00:32:28,819<br>All right, cool.</p>
<p>335<br>00:32:29,819 –&gt; 00:32:32,819<br>So let’s jump back.</p>
<p>336<br>00:32:33,819 –&gt; 00:32:52,819<br>All right, so I was kind of hand wavy about this part here, where I said, okay, let me go fetch the page that has the host name, run my where calls, I’ll get a bunch of matches, and then let me go fetch the last log in page, and then I had a magic way finding the matches there.</p>
<p>337<br>00:32:52,819 –&gt; 00:32:56,819<br>Right? How did I do that? Well, there’s two approaches.</p>
<p>338<br>00:32:57,819 –&gt; 00:33:09,819<br>The most common one is to do fixed length offsets, and that means that the, the, you identify rows not by a slot number, but you identify unique tuples.</p>
<p>339<br>00:33:09,819 –&gt; 00:33:17,819<br>This is why I don’t want to say row versus like instead of two borders record, because like, what does a row look like in a comm store, right?</p>
<p>340<br>00:33:17,819 –&gt; 00:33:26,819<br>It’s two pulls the better term, but the unique idea for a for a tuple is going to be its offset within the table.</p>
<p>341<br>00:33:26,819 –&gt; 00:33:37,819<br>So now if I’m at like offset three in one column, I would then know how to jump to the offset three in another column, and then I can stitch that tuple back together.</p>
<p>342<br>00:33:37,819 –&gt; 00:33:51,819<br>Again, this only works if the values are fixed length. Of course, what breaks that assumption? Very length, varchar strings, blobs, texts, right? So we’ll talk about how to handle that in a second.</p>
<p>343<br>00:33:51,819 –&gt; 00:33:56,819<br>So that, that, this approach, the fixed length column one, that’s the most, that’s the most common one.</p>
<p>344<br>00:33:56,819 –&gt; 00:34:11,819<br>So a legacy approach is to use embedded IDs where with every single value, you have some unique tuple identifier, like, you know, sort of like the log structure stuff, like some counter encummented by one.</p>
<p>345<br>00:34:11,819 –&gt; 00:34:20,819<br>And then there’s some index structure, and I’m not showing here where for a given record ID, for a given column, it tells you where to jump to this.</p>
<p>346<br>00:34:20,820 –&gt; 00:34:32,820<br>This is rare. You’re probably shouldn’t even mention it, but like, it is one way to do it. There was some system I forget in the old days that did do this because they were kind of like contorting a rose toward to make it a column store.</p>
<p>347<br>00:34:32,820 –&gt; 00:34:41,820<br>But everyone that uses fixed length offsets, of course, the problem you got to deal with now again is the, is how to convert variable length values into fixed length values.</p>
<p>348<br>00:34:41,820 –&gt; 00:34:45,820<br>And we think I guess how you do that.</p>
<p>349<br>00:34:45,820 –&gt; 00:34:48,820<br>He says pointers, pointers to what?</p>
<p>350<br>00:34:48,820 –&gt; 00:34:55,820<br>For some other place, let me have been sure that for every column, the data is also contiguous.</p>
<p>351<br>00:34:55,820 –&gt; 00:34:59,820<br>Yeah, see, for every comment, sure, every data is also contiguous.</p>
<p>352<br>00:34:59,820 –&gt; 00:35:07,820<br>Yeah, that would, that actually would potentially work. The problem with that one is like, if you do like, in place updates, right?</p>
<p>353<br>00:35:07,820 –&gt; 00:35:16,820<br>If you’re packing all the data in, if it’s not, if it’s immutable, you don’t have this problem, but if it is immutable, then you have, you could have fragmentation.</p>
<p>354<br>00:35:16,820 –&gt; 00:35:17,820<br>So slot arrays again?</p>
<p>355<br>00:35:17,820 –&gt; 00:35:20,820<br>He says slot arrays.</p>
<p>356<br>00:35:20,820 –&gt; 00:35:22,820<br>But what’s the pointy to?</p>
<p>357<br>00:35:22,820 –&gt; 00:35:33,820<br>Yeah, so there, so slot array you have an array with at the start of every page, and then that has the starting address of every column entry.</p>
<p>358<br>00:35:33,820 –&gt; 00:35:35,820<br>So how be hard?</p>
<p>359<br>00:35:35,820 –&gt; 00:35:37,820<br>Yeah, yeah, yeah, yeah, yeah.</p>
<p>360<br>00:35:37,820 –&gt; 00:35:45,820<br>That’s sort of similar to what he was saying, that would potentially work.</p>
<p>361<br>00:35:45,820 –&gt; 00:35:47,820<br>Compression.</p>
<p>362<br>00:35:47,820 –&gt; 00:35:54,820<br>Right? So the, the, the, the point approach that would work, I don’t think everybody does that.</p>
<p>363<br>00:35:54,820 –&gt; 00:35:57,820<br>You could just pat things out, but that’s going to be wasteful as we said before.</p>
<p>364<br>00:35:57,820 –&gt; 00:36:00,820<br>But this is basically how dictionary compression works. Right?</p>
<p>365<br>00:36:00,820 –&gt; 00:36:10,820<br>Dictionary compression is replacing some, some very length value with a, with the integer code, which is, which is going to be fixed length in usually 32 bits.</p>
<p>366<br>00:36:10,820 –&gt; 00:36:16,820<br>That we can use to, to, to then do some predicates on the dictionary code.</p>
<p>367<br>00:36:16,820 –&gt; 00:36:22,820<br>And if necessary, if it matches something we’re looking for, go do a look up and find, find what the actual value is.</p>
<p>368<br>00:36:22,820 –&gt; 00:36:26,820<br>And that’s a typo, it’s not more in this next week, it’s more in this, more in this hour.</p>
<p>369<br>00:36:26,820 –&gt; 00:36:28,820<br>Like we’ll discuss this now, so sorry.</p>
<p>370<br>00:36:28,820 –&gt; 00:36:31,820<br>Right? So that’s how we’re going to be able to get, solve this problem.</p>
<p>371<br>00:36:31,820 –&gt; 00:36:35,820<br>And this, pretty much the way everyone does it in a modern system.</p>
<p>372<br>00:36:35,820 –&gt; 00:36:40,820<br>So this comms or idea is not new.</p>
<p>373<br>00:36:40,820 –&gt; 00:36:46,820<br>According to the literature, the very first version of this, goes back to 1970s.</p>
<p>374<br>00:36:46,820 –&gt; 00:36:51,820<br>There is this project out of the, the Swedish Defense Ministry called Cantor.</p>
<p>375<br>00:36:51,820 –&gt; 00:36:54,820<br>It was more of a file system than a, than a database system.</p>
<p>376<br>00:36:54,820 –&gt; 00:36:59,820<br>But this is considered to be the first documented, like, proposal for a column store system.</p>
<p>377<br>00:36:59,820 –&gt; 00:37:02,820<br>And I don’t know whether it, it, it, it exists today.</p>
<p>378<br>00:37:02,820 –&gt; 00:37:10,820<br>In the 1980s, there was a paper that actually sort of mapped out the theoretical properties of what the decomposition storage model looked like.</p>
<p>379<br>00:37:10,820 –&gt; 00:37:14,820<br>But again, it was still mostly only, only an academia.</p>
<p>380<br>00:37:15,820 –&gt; 00:37:22,820<br>The roughly was considered the first commercial implementation of a column store system was this thing called Sybase IQ.</p>
<p>381<br>00:37:22,820 –&gt; 00:37:26,820<br>But it wasn’t really a full-fledged database system.</p>
<p>382<br>00:37:26,820 –&gt; 00:37:28,820<br>It was more like a query accelerator.</p>
<p>383<br>00:37:28,820 –&gt; 00:37:31,820<br>And so it’s sort of similar to what I was saying before about Oracle.</p>
<p>384<br>00:37:31,820 –&gt; 00:37:34,820<br>They make a copy of your Rostor into a, in a memory column store.</p>
<p>385<br>00:37:34,820 –&gt; 00:37:37,820<br>This is basically what Sybase was doing back in the 1990s.</p>
<p>386<br>00:37:37,820 –&gt; 00:37:38,820<br>So your query would show up.</p>
<p>387<br>00:37:38,820 –&gt; 00:37:43,820<br>And then Sybase would figure out, should I go to the, the Rostor and maybe do something there,</p>
<p>388<br>00:37:43,820 –&gt; 00:37:48,820<br>or should I run the query only on the, on the, on the, in the recall store.</p>
<p>389<br>00:37:48,820 –&gt; 00:37:52,820<br>And the 2000s went is when the comms are still really took off.</p>
<p>390<br>00:37:52,820 –&gt; 00:38:00,820<br>And the three sort of key systems in the space were Vertica, which is, and it was found by my, my peachy, peachy advisors, Stan Zanodak, and my, my, my snowbreaker.</p>
<p>391<br>00:38:00,820 –&gt; 00:38:06,820<br>Dr. Wise was a, out of, there’s a four-community B, but now that, that was out of CWI.</p>
<p>392<br>00:38:06,820 –&gt; 00:38:12,820<br>And, and MoodyDB was, was, was a major economic project at CWI as well.</p>
<p>393<br>00:38:12,820 –&gt; 00:38:14,820<br>DuckDB is from CWI.</p>
<p>394<br>00:38:14,820 –&gt; 00:38:19,820<br>So the, the first version of DuckDB was actually called MoodyDB Lite.</p>
<p>395<br>00:38:19,820 –&gt; 00:38:24,820<br>They threw all the code away and then started ducking even scratch after learning with the, with MoodyDB Lite.</p>
<p>396<br>00:38:24,820 –&gt; 00:38:29,820<br>The act of Wise was started by some people that I worked on MoodyDB.</p>
<p>397<br>00:38:29,820 –&gt; 00:38:35,820<br>And then the, the, the two main people I vectorized, one of them left and was a co-founder of Snowflake.</p>
<p>398<br>00:38:35,820 –&gt; 00:38:38,820<br>Right, so a lot of the early ideas that vectorized developed is in Snowflake.</p>
<p>399<br>00:38:38,820 –&gt; 00:38:45,820<br>And then the other guy Peter Bonds, he went back to CWI and then he, you know, helped him advise the, the DuckDB project.</p>
<p>400<br>00:38:45,820 –&gt; 00:38:51,820<br>So there was a bunch of other systems at the time, but I’ve considered these the three major ones, and, and the pioneers in the space.</p>
<p>401<br>00:38:51,820 –&gt; 00:39:03,820<br>And actually how this all sort of came out, the way Mike tells it was, or Strenberger tells it, he was consulting for Walmart Labs in the early 2000s.</p>
<p>402<br>00:39:03,820 –&gt; 00:39:08,820<br>And they were struggling trying to scale their, uh, teradata database, which at the time was a row store.</p>
<p>403<br>00:39:08,820 –&gt; 00:39:12,820<br>Right, I think Walmart was, it was, it was, it was, you know, multiple petabytes.</p>
<p>404<br>00:39:12,820 –&gt; 00:39:14,820<br>It was, it was a database of every single transaction.</p>
<p>405<br>00:39:14,820 –&gt; 00:39:19,820<br>And sometimes somebody bought something at a store, like Scansman to Cash Usher, it was in that database.</p>
<p>406<br>00:39:19,820 –&gt; 00:39:24,820<br>And they were struggling to get teradata and run fast. And then Mike was like, oh, we should make this a column store.</p>
<p>407<br>00:39:24,820 –&gt; 00:39:29,820<br>And then he, you know, and then he found the C store project that became Vertica.</p>
<p>408<br>00:39:29,820 –&gt; 00:39:33,820<br>And then, you know, it was a pretty famous project.</p>
<p>409<br>00:39:33,820 –&gt; 00:39:35,820<br>Now pretty much everybody does this.</p>
<p>410<br>00:39:35,820 –&gt; 00:39:42,820<br>So, you know, this is just a sample of the, a bunch of your database systems that are out there that, that are considered column stores.</p>
<p>411<br>00:39:42,820 –&gt; 00:39:50,820<br>Um, but the two key things that are also interesting about the 2010s is there’s these open source file formats, Parquet and Ork.</p>
<p>412<br>00:39:50,820 –&gt; 00:39:54,820<br>Parquet came out of Dremio and somebody else I’m forgetting.</p>
<p>413<br>00:39:54,820 –&gt; 00:40:01,820<br>Uh, and then Ork came out of Facebook. Right, these are open source file formats that are columnar based.</p>
<p>414<br>00:40:01,820 –&gt; 00:40:09,820<br>And now you can build database systems that can read and write, uh, yeah, Parquet and Ork files.</p>
<p>415<br>00:40:09,820 –&gt; 00:40:21,820<br>Right. So the advantages for the, the columnar or the, the composition storage model is that we’re going to greatly reduce the amount of waste of IO we have to do for analytical queries because we’re only reading the exact data that we need.</p>
<p>416<br>00:40:21,820 –&gt; 00:40:35,820<br>Right. Um, and we’re going to get better cash reuse and better locality for our access patterns because again, we’re literally just going to rip through columns one after another and not have to jump around, uh, within, within memory, which is better for CPUs.</p>
<p>417<br>00:40:35,820 –&gt; 00:40:50,820<br>And again, we’ll get better compression, which we’re coming up to because the downside is going to be it’s going to slow for point queries, slow for insert updates delete because we’re going to have to split things up and write out multiple, you know, data to multiple locations and then bring it back in if you want to put it back together.</p>
<p>418<br>00:40:50,820 –&gt; 00:40:52,820<br>Yes.</p>
<p>419<br>00:40:52,820 –&gt; 00:40:59,820<br>The previous slide that this really did subway build their own database question is that subway a builder in database. No.</p>
<p>420<br>00:40:59,820 –&gt; 00:41:08,820<br>It’s a, consider that each rig. Um, but pancake DB is real. That’s a real system.</p>
<p>421<br>00:41:08,820 –&gt; 00:41:34,820<br>So one thing to point out though is that in my, my earlier example, the way I showed I ran that that one query, I did the scan on the, on the, on the, the hosting, you know, the column, then I ran the scan portion of the query on the, uh, on the log in one.</p>
<p>422<br>00:41:34,820 –&gt; 00:41:42,820<br>And you sort of think of that like it was, I did one and then I moved on to another. A lot of cases, though queries, you actually want to look at multiple columns at the same time.</p>
<p>423<br>00:41:42,820 –&gt; 00:41:52,820<br>Right. My wear clause only had was only reference one attribute. But as you’ve seen in the queries you’ve written for homework one, oftentimes you have multiple columns or multiple attributes referenced in your wear clause.</p>
<p>424<br>00:41:52,820 –&gt; 00:42:05,820<br>And so it would be kind of, kind of expensive, a cumbersome to sort of now me maintaining, uh, as I’m scanning along one column, fetching another column at the same time and trying to patch things together.</p>
<p>425<br>00:42:05,820 –&gt; 00:42:25,820<br>And so we still want to, we want to be able to have data or we want to wait and have the attributes that are so much that are going to use together so much close to each other on disk and in our files, but still get all the benefit of a, of a comm store layout.</p>
<p>426<br>00:42:26,820 –&gt; 00:42:30,820<br>Right. And so this is what the Pax model model is.</p>
<p>427<br>00:42:30,820 –&gt; 00:42:39,820<br>Well, again, and as I said, in most systems, they say they’re a comm store, they’re really doing this. Parking or are really, really doing this.</p>
<p>428<br>00:42:39,820 –&gt; 00:42:52,820<br>And the idea is not like, you know, mind blowing, it’s just basically saying instead of having a separate file for every single at column, actually by itself, I’ll have the, I’ll break them up into chunks,</p>
<p>429<br>00:42:52,820 –&gt; 00:42:58,820<br>uh, into row groups and have data that are within the same tuple close to each other.</p>
<p>430<br>00:42:58,820 –&gt; 00:43:04,820<br>You know, in the same file, just sort of spaced out in separate pages.</p>
<p>431<br>00:43:04,820 –&gt; 00:43:13,820<br>Right. So we go back to our sort of mock example here. All we’re going to do is just horizontally partition the table into, to row groups.</p>
<p>432<br>00:43:13,820 –&gt; 00:43:17,820<br>And then within that row group, we’re going to partition it based on columns.</p>
<p>433<br>00:43:17,820 –&gt; 00:43:25,820<br>Right. So you think of this, the first three rows here, I’ll have some portion in my giant file, my, uh, define the row group.</p>
<p>434<br>00:43:25,820 –&gt; 00:43:37,820<br>I have a header for that row group. And then I have all the extra to column A together, then all the batch of these column, or all the values for column A, all of that ones come B followed by all the values of column C.</p>
<p>435<br>00:43:37,820 –&gt; 00:43:50,820<br>Right. So now again, if I have a wear clause that needs to access, you know, both column A and column C, when I go fetch these pages for this row group, I, you know, I have all the data for the, that I need close to each other.</p>
<p>436<br>00:43:50,820 –&gt; 00:44:01,820<br>But I’m also getting the benefit of a scrunch or IO because this row group is going to be, you know, in the tens of megabytes instead of like, you know, four kilobyte or eight kilobyte pages.</p>
<p>437<br>00:44:01,820 –&gt; 00:44:13,820<br>Right. Same thing for the next guy and so forth. And so this is roughly how park a work. There’s a lot of diagrams or, you know, presentations about parking or look like that.</p>
<p>438<br>00:44:13,820 –&gt; 00:44:29,820<br>And again, they’re basically using all the same language that we’re using here. Right. And here they say the default size of a page is one megabyte because they want to have, you know, group things together and have as much scrunch or IOs again. And then a row group is going to be 128 megabytes.</p>
<p>439<br>00:44:30,820 –&gt; 00:44:32,820<br>All right. Yes.</p>
<p>440<br>00:44:32,820 –&gt; 00:44:38,820<br>One of the things from like the same problem where you’re still in the field of IOs, I think it’s the same thing.</p>
<p>441<br>00:44:38,820 –&gt; 00:44:44,820<br>The statement is, couldn’t this also have those, you’re still going to have the problem where you have a much useless IOs if you’re doing a full table scan.</p>
<p>442<br>00:44:44,820 –&gt; 00:44:52,820<br>So the header tells you where things are. Right. And because it’s so huge, like I can bring in, I bring in this first header here.</p>
<p>443<br>00:44:52,820 –&gt; 00:45:03,820<br>Actually, I’m showing the header, but like in real systems on parking or it’s actually at the footer because assuming the files immutable. So I don’t know what, like, I don’t know what where everything’s going to be until I finish writing it.</p>
<p>444<br>00:45:03,820 –&gt; 00:45:06,820<br>So it’s in the footer. That’s a, that’s a, that’s on the side.</p>
<p>445<br>00:45:06,820 –&gt; 00:45:16,820<br>So his statement is, don’t have the same problem as a row store here if I’m doing this packs thing because now if I bring this entire row group in, am I going to read a bunch of stuff I don’t need.</p>
<p>446<br>00:45:16,820 –&gt; 00:45:32,820<br>So you don’t bring the whole row group in, you bring the header in, and you say, okay, here’s the offsets now of where my attributes are. And then I can go ahead and fetch those.</p>
<p>447<br>00:45:32,820 –&gt; 00:45:46,820<br>Actually, here you can see here, here’s the, you see the footer here. This is, is that of the header saying the metadata was in here, like the file, row group and column metadata, like what the offsets are, is that of the header. It’s just in the footer for park.</p>
<p>448<br>00:45:46,820 –&gt; 00:45:49,820<br>Or is the same way.</p>
<p>449<br>00:45:50,820 –&gt; 00:46:09,820<br>All right, so as you say, multiple times, I always, always been the main bottleneck we have, especially for analytical queries. And if we assume the data is on the press, that means like the, you know, whatever the exact size of a tuple for in, you know, in a table.</p>
<p>450<br>00:46:09,820 –&gt; 00:46:26,820<br>Every page that is going to bring exactly that data in. And so the most obvious way to reduce the, well, speed up queries, you can, you can, you can basically skip data or you can make the data do fetch, bring more things into memory.</p>
<p>451<br>00:46:26,820 –&gt; 00:46:38,820<br>So skipping data is what the comm store stuff helps with because you avoid having to read, meet attributes you don’t need. Compression is another way to say, okay, for every page I fetch, I get more tuples than I would if it was uncompressed.</p>
<p>452<br>00:46:38,820 –&gt; 00:46:48,820<br>Now, there’s going to be this trade-off between speed up and the pressure ratio. Obviously, this is going to be potentially slower than CPU, especially in the cloud setting.</p>
<p>453<br>00:46:48,820 –&gt; 00:46:59,820<br>And so I’m willing to pay the extra cost of having to decompress and compress data because now again, it’ll reduce my amount of IOPS, amount of time and wasting on IOPS to bring the fetch things in.</p>
<p>454<br>00:47:00,820 –&gt; 00:47:15,820<br>Things are slightly getting the sort of the difference between this speed and CPU speed is getting smaller where in some cases, this is actually getting so fast lately where maybe you don’t want things to be compressed.</p>
<p>455<br>00:47:15,820 –&gt; 00:47:22,820<br>There will be some other benefits we’ll see in a second where we do keep things compressed, the data system actually can run faster when it actually processes things in memory.</p>
<p>456<br>00:47:22,820 –&gt; 00:47:30,820<br>And we’ll cover that in a few weeks. But in general, for most systems, compressing things on disk is always going to be a win.</p>
<p>457<br>00:47:32,820 –&gt; 00:47:36,820<br>So any compression scheme we need to use has to produce fixed length values.</p>
<p>458<br>00:47:36,820 –&gt; 00:47:42,820<br>As we said before, because if you want to store this in a column store, we want to make sure that we always have fixed length all sets.</p>
<p>459<br>00:47:43,820 –&gt; 00:47:51,820<br>In some cases, too, we want to delay when we actually decompress things or as long as possible while we execute queries.</p>
<p>460<br>00:47:51,820 –&gt; 00:48:11,820<br>And we’ll see this again. We’ll talk about this more when we talk about query execution. But the idea here is that if I have a bunch of these one megabyte strings that are in my table, but I can convert them to 32-bit integers, I want to process 32-bit integers for as long as possible because I have to copy data from one operative in the next as I execute the query.</p>
<p>461<br>00:48:12,820 –&gt; 00:48:23,820<br>If it’s a distributed system copied over the network, I want to keep things compressed as long as possible and only decompress it when I actually have to show something back to something needs it, DB decompressed or the user needs the output.</p>
<p>462<br>00:48:23,820 –&gt; 00:48:27,820<br>Joins makes that harder, but we’ll cover that later in a second.</p>
<p>463<br>00:48:27,820 –&gt; 00:48:34,820<br>And then the obvious most important thing we need for any compression scheme in our data system is we need to ensure that we’re using a lossless scheme.</p>
<p>464<br>00:48:34,820 –&gt; 00:48:36,820<br>And they know what that means.</p>
<p>465<br>00:48:37,820 –&gt; 00:48:39,820<br>Lossy versus lossless. Yes.</p>
<p>466<br>00:48:39,820 –&gt; 00:48:45,820<br>There’s no information loss when you’re decompressed things, right? Or decompress them, right?</p>
<p>467<br>00:48:45,820 –&gt; 00:48:58,820<br>So a lossy scheme would be like MP3, MP4, JPEG, where they’re doing some tricks about how the human perceives audio data or visual data to compress things down to a much smaller size.</p>
<p>468<br>00:48:58,820 –&gt; 00:49:08,820<br>So that means if you have the raw image you took or the raw sound file you took, when you compress it, you’re not going to get back the same values, if you will, when you decompress it.</p>
<p>469<br>00:49:08,820 –&gt; 00:49:14,820<br>We don’t want to do that in a database system because, as we said before, people don’t want losing data.</p>
<p>470<br>00:49:14,820 –&gt; 00:49:23,820<br>If you have $100 in your bank account and then they compress the data and when it gets decompressed and now you have $90, you’re going to notice you’re going to complain.</p>
<p>471<br>00:49:23,820 –&gt; 00:49:32,820<br>So typically, you know, most systems will not use a lossy scheme just because, you know, you don’t have problems.</p>
<p>472<br>00:49:32,820 –&gt; 00:49:40,820<br>You can do lossy compression yourself, right? So think of like, I mean, like the application could do this.</p>
<p>473<br>00:49:40,820 –&gt; 00:49:53,820<br>If I have a keeping track of the temperature of this room every second, right, and I do this for 10 years, do I really need to know what the exact temperature was at a one second interval, you know, a year from now?</p>
<p>474<br>00:49:53,820 –&gt; 00:49:58,820<br>No, I can maybe compress it down to, here’s the average temperature per minute, right?</p>
<p>475<br>00:49:58,820 –&gt; 00:50:04,820<br>So I can’t get back the original data because it’s been compressed or aggregated and that might be okay.</p>
<p>476<br>00:50:04,820 –&gt; 00:50:09,820<br>But again, that’s something you, as a user in the application, a humus to know whether that’s an okay thing to do.</p>
<p>477<br>00:50:09,820 –&gt; 00:50:14,820<br>The data system doesn’t, therefore the database system is always going to be using a lossless scheme.</p>
<p>478<br>00:50:14,820 –&gt; 00:50:20,820<br>So now the question is, you know, what do we actually want to compress? And there’s a couple of different choices.</p>
<p>479<br>00:50:20,820 –&gt; 00:50:27,820<br>One is we can compress a single page or a block of data. So that’s all the two balls within the same table.</p>
<p>480<br>00:50:27,820 –&gt; 00:50:32,820<br>We can compress a single tool by itself if it’s a a Rostor system.</p>
<p>481<br>00:50:32,820 –&gt; 00:50:40,820<br>We can go even more fine grain that we can say, all compress within one, two, well, one single attribute and compress that.</p>
<p>482<br>00:50:40,820 –&gt; 00:50:53,820<br>So think of like the overflow tables we said before, if you’re storing, you know, huge text attributes or like in Wikipedia, the revisions, it’s the, you know, it could be a lot of text.</p>
<p>483<br>00:50:53,820 –&gt; 00:50:58,820<br>I forget what the largest Wikipedia article is. And it’s like someone with Star Wars, right?</p>
<p>484<br>00:50:58,820 –&gt; 00:51:05,820<br>So like that’s, you know, it could be kilobytes of text data. I could compress just for that, you know, that one entry.</p>
<p>485<br>00:51:05,820 –&gt; 00:51:08,820<br>I’m post-ghosted, there’s a bunch of other systems do this.</p>
<p>486<br>00:51:08,820 –&gt; 00:51:15,820<br>Or alternatively, I could compress for a single column if it’s a column source system.</p>
<p>487<br>00:51:15,820 –&gt; 00:51:18,820<br>So let’s talk about how you can do this for the block level.</p>
<p>488<br>00:51:19,820 –&gt; 00:51:26,820<br>And then we’ll spend most of our time talking about the column level because that that matters the most for in a columnar system.</p>
<p>489<br>00:51:26,820 –&gt; 00:51:30,820<br>So to do it at a block level, we essentially need to use a naive compression scheme.</p>
<p>490<br>00:51:30,820 –&gt; 00:51:39,820<br>And by naive, I mean that the, it’s the database system is making a call to like a third party library like GZIP.</p>
<p>491<br>00:51:39,820 –&gt; 00:51:47,820<br>You wouldn’t want to use that because it’s slow, but it’s a third party library that’s going to take the page and then compress it down to some binary form where the database is going to be.</p>
<p>492<br>00:51:47,820 –&gt; 00:51:55,820<br>Where the database system has no way to interpret or can do any introspection into the compressed version of the block.</p>
<p>493<br>00:51:55,820 –&gt; 00:52:01,820<br>Right? Again, so I call, you know, call GZIP on a file.</p>
<p>494<br>00:52:01,820 –&gt; 00:52:04,820<br>The data center doesn’t know how to go read inside that compressed file.</p>
<p>495<br>00:52:04,820 –&gt; 00:52:08,820<br>It has to decompress it in order to get back the original version of it.</p>
<p>496<br>00:52:08,820 –&gt; 00:52:09,820<br>Right?</p>
<p>497<br>00:52:09,820 –&gt; 00:52:12,820<br>So again, you wouldn’t want to use GZIP. There’s a bunch of these faster alternatives.</p>
<p>498<br>00:52:12,820 –&gt; 00:52:19,820<br>And that sort of all came out with LZO was a sort of big breakthrough in the 1990s.</p>
<p>499<br>00:52:19,820 –&gt; 00:52:24,820<br>Z standard is considered the state of the art compression scheme now from Facebook.</p>
<p>500<br>00:52:24,820 –&gt; 00:52:27,820<br>They’re actually working on a new version. It’s not public yet.</p>
<p>501<br>00:52:30,820 –&gt; 00:52:33,820<br>It’s even faster and better. I’ve got a thun out yet.</p>
<p>502<br>00:52:33,820 –&gt; 00:52:36,820<br>But Z standards would you should be using?</p>
<p>503<br>00:52:36,820 –&gt; 00:52:39,820<br>So let’s see how my SQL does this.</p>
<p>504<br>00:52:39,820 –&gt; 00:52:42,820<br>So my SQL act you can support table compression.</p>
<p>505<br>00:52:42,820 –&gt; 00:52:44,820<br>You declare it on a per table basis.</p>
<p>506<br>00:52:44,820 –&gt; 00:52:46,820<br>It’s I don’t think it’s owned by default.</p>
<p>507<br>00:52:46,820 –&gt; 00:52:54,820<br>And the way it works is that all your pages when they’re written a disk, they’re going to be compressed into some multi,</p>
<p>508<br>00:52:54,820 –&gt; 00:53:02,820<br>a page size that’s going to be some multiple of four or two up to eight kilobytes.</p>
<p>509<br>00:53:03,820 –&gt; 00:53:10,820<br>And then each page, they’re going to have a header portion called the mod log where it’s sort of like the row store thing I mentioned before,</p>
<p>510<br>00:53:10,820 –&gt; 00:53:16,820<br>where I can do a bunch of rights and make changes to the page without having to decompress it first.</p>
<p>511<br>00:53:16,820 –&gt; 00:53:20,820<br>So there’s a little extra space in the beginning.</p>
<p>512<br>00:53:22,820 –&gt; 00:53:27,820<br>And obviously also too. Say, if your pages, like when the after is compressed, it’s six kilobytes.</p>
<p>513<br>00:53:28,820 –&gt; 00:53:32,820<br>They’ll pat it up to the next highest value within one, two, four, eight.</p>
<p>514<br>00:53:32,820 –&gt; 00:53:38,820<br>And this ensures that you have, you don’t have any fragmentation in your layout on disk.</p>
<p>515<br>00:53:38,820 –&gt; 00:53:40,820<br>And when you bring things into memory.</p>
<p>516<br>00:53:40,820 –&gt; 00:53:44,820<br>So say I query runs and wants to read something in page zero.</p>
<p>517<br>00:53:44,820 –&gt; 00:53:52,820<br>Right? If I’m doing a blind right, like an insert or a delete or even update assuming I have the values,</p>
<p>518<br>00:53:53,820 –&gt; 00:53:57,820<br>I don’t need to decompress the page. I just write that change to the mod log.</p>
<p>519<br>00:53:57,820 –&gt; 00:53:59,820<br>And again, it’s just log structure, like we talked about before.</p>
<p>520<br>00:53:59,820 –&gt; 00:54:02,820<br>I said we were going to see this idea throughout the rest of the semester.</p>
<p>521<br>00:54:02,820 –&gt; 00:54:06,820<br>Right? You can think the mod log is just the log structure storage we talked about before.</p>
<p>522<br>00:54:07,820 –&gt; 00:54:15,820<br>And in some cases too, I can do reads on the mod log because if the data I need was just inserted and within the mod log,</p>
<p>523<br>00:54:15,820 –&gt; 00:54:18,820<br>I don’t have to decompress the rest of the page.</p>
<p>524<br>00:54:19,820 –&gt; 00:54:22,820<br>But then if I do need to read the page, though decompress it,</p>
<p>525<br>00:54:22,820 –&gt; 00:54:29,820<br>store it as a regular 16 kilobite page in memory in the buffer pool because that’s the default size for my SQL.</p>
<p>526<br>00:54:29,820 –&gt; 00:54:32,820<br>And then I can do whatever reads I want on that.</p>
<p>527<br>00:54:32,820 –&gt; 00:54:35,820<br>Right? But I still keep the compressed version around.</p>
<p>528<br>00:54:35,820 –&gt; 00:54:41,820<br>And I think also to when it gets decompressed, they apply the changes to the mod log to the page there.</p>
<p>529<br>00:54:41,820 –&gt; 00:54:42,820<br>Right?</p>
<p>530<br>00:54:44,820 –&gt; 00:54:46,820<br>This is a good idea or a bad idea?</p>
<p>531<br>00:54:49,820 –&gt; 00:54:51,820<br>Postgres doesn’t do this.</p>
<p>532<br>00:54:56,820 –&gt; 00:54:57,820<br>Yes?</p>
<p>533<br>00:54:57,820 –&gt; 00:54:59,820<br>I would say you’re reading itself to the grade.</p>
<p>534<br>00:54:59,820 –&gt; 00:55:01,820<br>You said it’s a reading, it’s not super great. Why?</p>
<p>535<br>00:55:01,820 –&gt; 00:55:08,820<br>Well, because everything on the computer is one thing, you’ll have to decompress a bunch of stuff.</p>
<p>536<br>00:55:08,820 –&gt; 00:55:12,820<br>Not necessarily. Like, if, like, going back here,</p>
<p>537<br>00:55:13,820 –&gt; 00:55:17,820<br>if I do an insert and it lands on the mod log, right?</p>
<p>538<br>00:55:17,820 –&gt; 00:55:19,820<br>I don’t have to decompress it.</p>
<p>539<br>00:55:19,820 –&gt; 00:55:22,820<br>My index, like, there’s some bookkeeping and doing saying, okay,</p>
<p>540<br>00:55:22,820 –&gt; 00:55:26,820<br>like, I updated the index now so the record ID points to this page and then you look in the mod log,</p>
<p>541<br>00:55:26,820 –&gt; 00:55:30,820<br>oh, for that slot number, that record ID, it’s really in the mod log and the full page.</p>
<p>542<br>00:55:30,820 –&gt; 00:55:32,820<br>So you don’t have to decompress it.</p>
<p>543<br>00:55:35,820 –&gt; 00:55:37,820<br>All right, let’s say I actually think it’s a decent idea.</p>
<p>544<br>00:55:37,820 –&gt; 00:55:41,820<br>And I say Postgres doesn’t do it, not because, like, oh, Postgres,</p>
<p>545<br>00:55:41,820 –&gt; 00:55:43,820<br>like Postgres is not the gospel, right?</p>
<p>546<br>00:55:43,820 –&gt; 00:55:46,820<br>Postgres doesn’t do something, doesn’t mean like, you shouldn’t be doing it.</p>
<p>547<br>00:55:46,820 –&gt; 00:55:51,820<br>Postgres is actually an amazing front end, the back end is actually pretty terrible, right?</p>
<p>548<br>00:55:51,820 –&gt; 00:55:55,820<br>Because a lot of the design is remnants from the 1980s.</p>
<p>549<br>00:55:55,820 –&gt; 00:55:58,820<br>And it’s not how you would vote a modern system today.</p>
<p>550<br>00:55:58,820 –&gt; 00:56:03,820<br>So, and they don’t support compression for pages, you know, for regular data pages like this,</p>
<p>551<br>00:56:03,820 –&gt; 00:56:05,820<br>only for toast tables, the overflow pages.</p>
<p>552<br>00:56:05,820 –&gt; 00:56:07,820<br>So this is actually a decent idea.</p>
<p>553<br>00:56:07,820 –&gt; 00:56:08,820<br>Right?</p>
<p>554<br>00:56:08,820 –&gt; 00:56:10,820<br>It does have some challenges though, right?</p>
<p>555<br>00:56:10,820 –&gt; 00:56:18,820<br>Because my SQL is a row store, that’s why you have to use a 90-degree compression scheme,</p>
<p>556<br>00:56:18,820 –&gt; 00:56:23,820<br>because you can’t do anything fancy, because the values you’re storing in the tables themselves,</p>
<p>557<br>00:56:23,820 –&gt; 00:56:26,820<br>or sorry, in the pages of cell, from all the different attributes,</p>
<p>558<br>00:56:26,820 –&gt; 00:56:30,820<br>and you’re not going to be able to do all the native compression scheme we’ll see in a second.</p>
<p>559<br>00:56:30,820 –&gt; 00:56:31,820<br>Right?</p>
<p>560<br>00:56:31,820 –&gt; 00:56:36,820<br>And again, because we’re just using, I think they use snappy or Z-standard,</p>
<p>561<br>00:56:37,820 –&gt; 00:56:40,820<br>because you’re using a general-purpose compression algorithm,</p>
<p>562<br>00:56:40,820 –&gt; 00:56:45,820<br>the datacent doesn’t know how to interpret what those compressed bytes actually mean.</p>
<p>563<br>00:56:45,820 –&gt; 00:56:48,820<br>And the spoiler is, all those compression items are talking about four,</p>
<p>564<br>00:56:48,820 –&gt; 00:56:53,820<br>they’re basically doing some variant of dictionary compression.</p>
<p>565<br>00:56:53,820 –&gt; 00:56:54,820<br>Right?</p>
<p>566<br>00:56:54,820 –&gt; 00:56:57,820<br>It’s going to build its own dictionary of repeated byte sequences.</p>
<p>567<br>00:56:57,820 –&gt; 00:57:00,820<br>But again, my SQL doesn’t know how to read that dictionary,</p>
<p>568<br>00:57:00,820 –&gt; 00:57:03,820<br>and so it has to decompress the whole thing.</p>
<p>569<br>00:57:04,820 –&gt; 00:57:08,820<br>So for some workloads, I think this is actually a good idea,</p>
<p>570<br>00:57:08,820 –&gt; 00:57:12,820<br>and I kind of wish Postgres did do some compression.</p>
<p>571<br>00:57:12,820 –&gt; 00:57:13,820<br>Right?</p>
<p>572<br>00:57:13,820 –&gt; 00:57:21,820<br>So, if we’re doing OLAP, ideally, we want to be able to run our query directly in the compressed data</p>
<p>573<br>00:57:21,820 –&gt; 00:57:23,820<br>without having to decompress it first.</p>
<p>574<br>00:57:23,820 –&gt; 00:57:27,820<br>So say something I like this, I have my salary, 2PL salary,</p>
<p>575<br>00:57:27,820 –&gt; 00:57:30,820<br>assuming I have some compression algorithm, I’m not saying what it is,</p>
<p>576<br>00:57:30,820 –&gt; 00:57:33,820<br>but I have a compressed form of the database.</p>
<p>577<br>00:57:33,820 –&gt; 00:57:36,820<br>Well, if my query shows up, or I want to get my salary,</p>
<p>578<br>00:57:36,820 –&gt; 00:57:42,820<br>I do some kind of magic that converts the query to convert this constant string</p>
<p>579<br>00:57:42,820 –&gt; 00:57:48,820<br>andy into the compressed form, and then now I can do a direct lookup on my compressed table</p>
<p>580<br>00:57:48,820 –&gt; 00:57:53,820<br>using my compressed constant, and not have to decompress every single page as I’m going along.</p>
<p>581<br>00:57:55,820 –&gt; 00:57:58,820<br>Now I’m going to do some IOA at the due,</p>
<p>582<br>00:57:58,820 –&gt; 00:58:04,820<br>because I’m fetching in compressed pages, I don’t have to decompress them in order to do a lookups into them.</p>
<p>583<br>00:58:07,820 –&gt; 00:58:13,820<br>So this is ideally what we want, and the easiest way to do this is going to be in a columnar system.</p>
<p>584<br>00:58:15,820 –&gt; 00:58:20,820<br>So this is just a quick overview of a bunch of different compression algorithms you could possibly have.</p>
<p>585<br>00:58:21,820 –&gt; 00:58:24,820<br>Again, the spouter is going to be dictionary compression,</p>
<p>586<br>00:58:24,820 –&gt; 00:58:27,820<br>and dictionary encoding is the default choice for most systems.</p>
<p>587<br>00:58:27,820 –&gt; 00:58:34,820<br>But what you can do, you may not want to compress a single column using these other schemes,</p>
<p>588<br>00:58:34,820 –&gt; 00:58:37,820<br>and we’ll see what see some examples where it does make sense.</p>
<p>589<br>00:58:37,820 –&gt; 00:58:43,820<br>But after you do dictionary encoding, you can apply all of these other compression schemes on the dictionary itself,</p>
<p>590<br>00:58:43,820 –&gt; 00:58:47,820<br>or you’re still dictionary coded values, and get even further compression.</p>
<p>591<br>00:58:47,820 –&gt; 00:58:50,820<br>So you can get sort of a multiple-plicative effect where you do compression one way,</p>
<p>592<br>00:58:50,820 –&gt; 00:58:53,820<br>and then you run another compression algorithm on the compressed data and get even better compression.</p>
<p>593<br>00:58:53,820 –&gt; 00:59:02,820<br>And it’s still done in a way where the data system can natively interpret what those bytes actually mean in the compressed form without having to decompress it first.</p>
<p>594<br>00:59:03,820 –&gt; 00:59:08,820<br>And again, this is why the data is going to do everything, and don’t want the OS to do anything or anybody else to do anything,</p>
<p>595<br>00:59:08,820 –&gt; 00:59:13,820<br>because again, because we can do this native compression.</p>
<p>596<br>00:59:15,820 –&gt; 00:59:17,820<br>So let’s do some quick examples here.</p>
<p>597<br>00:59:18,820 –&gt; 00:59:31,820<br>So one approach you do is called run length encoding RLE, and this is the basic idea here is that if we have contiguous runs of values that are the same thing,</p>
<p>598<br>00:59:31,820 –&gt; 00:59:36,820<br>or literally the same value, instead of starting that value over and over again for every single tuple,</p>
<p>599<br>00:59:36,820 –&gt; 00:59:46,820<br>all instead store a compressed summary that says, for this value, at this offset, here’s how many currencies it has.</p>
<p>600<br>00:59:47,820 –&gt; 00:59:53,820<br>Now, this works great if your data is sorted based on whatever the column you’re trying to compress.</p>
<p>601<br>00:59:53,820 –&gt; 01:00:02,820<br>You can’t always do this, but again, if you sort things, then you can maximize the amount of the maximized of the repeated runs.</p>
<p>602<br>01:00:02,820 –&gt; 01:00:08,820<br>Let’s say I have a single table where it has an ID field, and then has a column that says whether somebody’s dead or not, yes or no,</p>
<p>603<br>01:00:08,820 –&gt; 01:00:12,820<br>it’s yes or no, there’s no null, there’s no maybe.</p>
<p>604<br>01:00:13,820 –&gt; 01:00:15,820<br>So we can compress this guy here.</p>
<p>605<br>01:00:15,820 –&gt; 01:00:27,820<br>So a compressed form would just take, essentially just scanning through the column, and finding the contiguous attributes or contiguous tuples that have the same value,</p>
<p>606<br>01:00:27,820 –&gt; 01:00:33,820<br>and then converting it into this triplet that says, here’s the value, we’re at this offset, and here’s the size of the run.</p>
<p>607<br>01:00:34,820 –&gt; 01:00:48,820<br>And so now, if I have a query that comes along, like, count the number of people that are dead versus not dead, I can just rip through that is dead column,</p>
<p>608<br>01:00:48,820 –&gt; 01:00:57,820<br>and compute my aggregation by just summing up the length of the run, and then along with the value there.</p>
<p>609<br>01:00:58,820 –&gt; 01:01:02,820<br>I actually can do even better.</p>
<p>610<br>01:01:02,820 –&gt; 01:01:10,820<br>So I have this little part here, I have somebody’s not dead, and then they’re dead, and then not dead.</p>
<p>611<br>01:01:10,820 –&gt; 01:01:15,820<br>So I have now these three triplets here where the run size is one.</p>
<p>612<br>01:01:15,820 –&gt; 01:01:23,820<br>So in this case here, I’m actually doing worse because I’m storing a triplet when I could just store a single value by itself.</p>
<p>613<br>01:01:24,820 –&gt; 01:01:33,820<br>So if I sort the data based on whether somebody’s dead or not, now my RL compression only has, you know, compressed column only has two entries.</p>
<p>614<br>01:01:33,820 –&gt; 01:01:37,820<br>Here’s all the dead people, and here’s all the non-dead people.</p>
<p>615<br>01:01:37,820 –&gt; 01:01:41,820<br>And this greatly reduces the amount of data that’s stored now.</p>
<p>616<br>01:01:41,820 –&gt; 01:01:49,820<br>So maybe the case, and say I have, always thinking of the extremes, my example is that the fit them on the slides, I have 10, 8 or 9 tuples here,</p>
<p>617<br>01:01:50,820 –&gt; 01:02:03,820<br>if I have a billion tuples or a billion people, I can compress now down, you know, keeping track of like, you know, whose ever a billion people is dead or not dead into a small number of bytes, and that’ll fit on one page.</p>
<p>618<br>01:02:06,820 –&gt; 01:02:18,820<br>You need the length in the triplet because again, assuming that we always have fixed length offsets, this allows you to figure out, okay, like if I need to find for a single tuple, a single entry, or they dead or not,</p>
<p>619<br>01:02:18,820 –&gt; 01:02:24,820<br>like it allows you to do the math, to reverse it back and say, okay, I would be at this offset if I was uncompressed.</p>
<p>620<br>01:02:24,820 –&gt; 01:02:26,820<br>And that’s just simple arithmetic.</p>
<p>621<br>01:02:29,820 –&gt; 01:02:32,820<br>Now the compression scheme you can do is called bitpacking.</p>
<p>622<br>01:02:32,820 –&gt; 01:02:43,820<br>And the idea here is that people oftentimes declare attributes or columns in a certain type that is larger than they actually need.</p>
<p>623<br>01:02:44,820 –&gt; 01:02:54,820<br>So, an idea would be like, if I have a column where I’m keeping track of some number, and I declare it as an integer type, that’s in SQL, that’s a 32 bit integer.</p>
<p>624<br>01:02:54,820 –&gt; 01:03:00,820<br>So that means that even if it’s a small value, I’m still going to allocate 32 bits to store it.</p>
<p>625<br>01:03:00,820 –&gt; 01:03:06,820<br>So for these numbers here, none of them are very big, but I’m always going to store it as 32 bits.</p>
<p>626<br>01:03:06,820 –&gt; 01:03:12,820<br>So in this case here, to store these 8 or 9 numbers, 8 numbers have to store 26 bits.</p>
<p>627<br>01:03:13,820 –&gt; 01:03:21,820<br>But again, the only thing that matters is actually these lower portion of the bits here, because this is the actual data that they need.</p>
<p>628<br>01:03:21,820 –&gt; 01:03:29,820<br>All this other stuff, the other 24 bits is just waste of space.</p>
<p>629<br>01:03:29,820 –&gt; 01:03:38,820<br>So instead, what I can do is, even though you declare it as a 32 bit integer, I’m going to store it as an 8 bit integer.</p>
<p>630<br>01:03:39,820 –&gt; 01:03:44,820<br>And then now that greatly reduces the size down by a factor of 4.</p>
<p>631<br>01:03:45,820 –&gt; 01:03:49,820<br>So I was able to go again from 26 bits to 64 bits.</p>
<p>632<br>01:03:49,820 –&gt; 01:04:00,820<br>And you can do a bunch of tricks with bit shifting operators and SIMD, which we can tell at LinusMester, to actually now, as I’m scanning along and sitting trying to find matches on a certain number,</p>
<p>633<br>01:04:01,820 –&gt; 01:04:11,820<br>because these are now 8 bit integers, I could put them into a single 32 bit integer, and I’m keeping track of inside my system, oh, it’s really at this offset, it’s these different values.</p>
<p>634<br>01:04:11,820 –&gt; 01:04:16,820<br>And then with now a single CPU instruction, I can operate on 4 values at once.</p>
<p>635<br>01:04:20,820 –&gt; 01:04:21,820<br>What’s the problem with this? Yes.</p>
<p>636<br>01:04:22,820 –&gt; 01:04:25,820<br>What happens if you add a 32 bit integer to the database?</p>
<p>637<br>01:04:25,820 –&gt; 01:04:27,820<br>Boom. Okay, excellent. Thank you.</p>
<p>638<br>01:04:28,820 –&gt; 01:04:34,820<br>So his statement is, well, what happens if I have a number that can’t be stored in those 8 bits that I’m trying to pack them into?</p>
<p>639<br>01:04:34,820 –&gt; 01:04:45,820<br>And so the way you get around this is a technique from Amazon for Redshift, called mostly encoding, where you say, the idea is basically to say, most of the data in my comms is going to be small enough.</p>
<p>640<br>01:04:45,820 –&gt; 01:04:51,820<br>But in the cases where it’s not, they will keep track of that and store that as a separate dictionary.</p>
<p>641<br>01:04:51,820 –&gt; 01:04:56,820<br>So again, I have these 32 bit numbers, but I have this 19999 here that’s really big.</p>
<p>642<br>01:04:56,820 –&gt; 01:05:11,820<br>So all stores still store them as 8 bits, but then I’ll have a special marker value, thinking like all the bits are set to 1, and then I have a separate table that says, for a given offset, here’s what the original value should be.</p>
<p>643<br>01:05:12,820 –&gt; 01:05:21,820<br>So now as I’m scanning along this column, if I see my special marker value, I know that I should look in this offset table and find out what the real value should have been.</p>
<p>644<br>01:05:23,820 –&gt; 01:05:24,820<br>Yes.</p>
<p>645<br>01:05:25,820 –&gt; 01:05:34,820<br>So the triplets where you just say the next couple things are 4 bits, the next couple are 8 or the next couple are 3 bits.</p>
<p>646<br>01:05:34,820 –&gt; 01:05:49,820<br>Yes, so the same as, couldn’t you do something with the triplets where, instead of just saying, everything’s always eat bits, could you say, I have 1000 values that are contiguous that are, you store them as 4 bits, then I can store them as 12 bits or whatever.</p>
<p>647<br>01:05:49,820 –&gt; 01:05:55,820<br>So that going back to the packs thing, because they break it up into row groups, each row group could have its own compression scheme.</p>
<p>648<br>01:05:55,820 –&gt; 01:05:57,820<br>So you could do something like that.</p>
<p>649<br>01:05:59,820 –&gt; 01:06:05,820<br>I think parquet is more aggressive compression than orc, it’s more complicated, or maybe the other way around.</p>
<p>650<br>01:06:05,820 –&gt; 01:06:09,820<br>One of them is very simple, one of them has a bunch of the various tricks you’re talking about.</p>
<p>651<br>01:06:10,820 –&gt; 01:06:35,820<br>So in this example here, the original size is 256 bits, but then if I do the most thing coding, I just have to store 8 by 8 bits for the most 8 column, and then assuming that I only need 16 bits for the offset and the 32 bits for the value for this lookup table, which is not true, because obviously I’ll locate more for additional metadata, but assuming you get it down to that, it’s 112 bits.</p>
<p>652<br>01:06:36,820 –&gt; 01:06:38,820<br>So that’s pretty good.</p>
<p>653<br>01:06:40,820 –&gt; 01:07:04,820<br>Another trick you knew is called bitmap encoding, and the idea here is that if you have an attribute that has low card inality, meaning it has a small number of unique values, where now instead of storing for every single tuple in a column, here’s the actual value, what I’m stagnant to do is maintain bitmaps, where I have one bitmap for every possible value I could have in the column,</p>
<p>654<br>01:07:04,820 –&gt; 01:07:13,820<br>and the bit is set to one based on whether the column or the attribute the tuple at that offset has that particular value.</p>
<p>655<br>01:07:14,820 –&gt; 01:07:25,820<br>So there are some database systems that provide bitmap indexes that essentially give the same things, you still have the original column, but then they maintain bitmap indexes that will do the same technique that we’re seeing here.</p>
<p>656<br>01:07:26,820 –&gt; 01:07:34,820<br>There’s a company that’s going to come talk about their data, some later this semester, I think it’s either feature based or feature form.</p>
<p>657<br>01:07:35,820 –&gt; 01:07:43,820<br>There’s two different databases, the same mean feature in them. One of them only stores bitmap indexes. You can’t actually store real data or the base data.</p>
<p>658<br>01:07:44,820 –&gt; 01:07:51,820<br>So the idea is here, so say we go back to our is dead column, right? Again, there’s only two possible values, either dead or not dead.</p>
<p>659<br>01:07:52,820 –&gt; 01:08:08,820<br>So instead of storing actual single values themselves, I have two bitmaps. One says, for yes, one says no, and then there’s a bit here that’s set in this bitmap that corresponds to whether the original value has that bitmap or not.</p>
<p>660<br>01:08:09,820 –&gt; 01:08:10,820<br>Or has that particular value or not.</p>
<p>661<br>01:08:11,820 –&gt; 01:08:25,819<br>So I only need now two eight bits, 16 bits to store the yes or no, and then now my bitmap is just 18 bits, because I have nine values and I need two bits each.</p>
<p>662<br>01:08:26,819 –&gt; 01:08:28,819<br>So I can get this down down to 34 bits.</p>
<p>663<br>01:08:31,819 –&gt; 01:08:33,819<br>What’s an obvious problem with this approach?</p>
<p>664<br>01:08:34,819 –&gt; 01:08:35,819<br>Yes.</p>
<p>665<br>01:08:36,819 –&gt; 01:08:43,819<br>If your data is high-carbonality, this is a terrible idea. Indeed, yes, it is. Let’s look at an example.</p>
<p>666<br>01:08:44,819 –&gt; 01:08:47,819<br>So the same, we have a customer table like this, and we have this zip code column.</p>
<p>667<br>01:08:48,819 –&gt; 01:08:54,819<br>How many zip codes are in the United States? I’m going to give you a guess. I hear 10,000 now more.</p>
<p>668<br>01:08:55,819 –&gt; 01:09:00,819<br>100,000 less. Now we’re doing binary search. It’s 43,000.</p>
<p>669<br>01:09:01,819 –&gt; 01:09:10,819<br>Assuming we have a table with 10 million rows, and I’m going to build a bitmap for every single unique possible zip code I have, well, I’m going to need…</p>
<p>670<br>01:09:12,819 –&gt; 01:09:27,819<br>Let’s just store the data. Assuming the raw data, assuming we can sort the zip code as 32 bits, the raw data is 40 megabytes, but if I had to have a 10 million size bitmap for every single zip code, now we’re at 53 gigs.</p>
<p>671<br>01:09:28,819 –&gt; 01:09:30,819<br>So clearly, this is a bad idea.</p>
<p>672<br>01:09:32,819 –&gt; 01:09:41,819<br>Furthermore, every time somebody adds a new tuple, I have to extend that bitmap because all of us have to match. I keep adding more to it.</p>
<p>673<br>01:09:42,819 –&gt; 01:09:44,819<br>So I have to do that for every possible bitmap.</p>
<p>674<br>01:09:45,819 –&gt; 01:09:52,819<br>So bitmap indexes can make a huge difference, but it’s really for when you have a really small number card or not, like less than maybe 10.</p>
<p>675<br>01:09:53,819 –&gt; 01:09:58,819<br>You’d want to do this. And then again, most systems don’t do this by default.</p>
<p>676<br>01:10:00,819 –&gt; 01:10:19,819<br>Delta coding, the idea here is that if the values from one attribute to the next, from one tuple to the next, sorry, if they’re close enough to each other, maybe again, I don’t need to store the entire value for one tuple, I just need to store the difference of the delta between the previous value.</p>
<p>677<br>01:10:20,819 –&gt; 01:10:28,819<br>So let’s say again, I have a sensor reading where I’m keeping track of the temperature in the room and every minute I’m storing the temperature.</p>
<p>678<br>01:10:29,819 –&gt; 01:10:38,819<br>So at this timestamp column here, assuming that we’re storing 64 bits, we know that the time is always incrementing by one.</p>
<p>679<br>01:10:39,819 –&gt; 01:10:46,819<br>Furthermore, assuming that it came track of the temperature in the room or outside, from one minute to the next, there’s not going to be dramatic temperature swings.</p>
<p>680<br>01:10:47,819 –&gt; 01:10:52,819<br>We’re not going to go from like 99 degrees to zero degrees within a minute.</p>
<p>681<br>01:10:53,819 –&gt; 01:11:00,819<br>And so what I can just do now is to store from one tuple to the next, what was the difference between the previous one here?</p>
<p>682<br>01:11:02,819 –&gt; 01:11:11,819<br>So in the time stamp, it’s just plus one adding a minute, in case the temperature is a decimal difference between the previous one.</p>
<p>683<br>01:11:12,819 –&gt; 01:11:18,819<br>I compress this even further now, because what do I have in this first column here at the time stamp, what do I have?</p>
<p>684<br>01:11:19,819 –&gt; 01:11:22,819<br>A bunch of plus ones, how can we compress that?</p>
<p>685<br>01:11:24,819 –&gt; 01:11:26,819<br>Run length encoding, right?</p>
<p>686<br>01:11:27,819 –&gt; 01:11:39,819<br>So we can compress this even further now and convert this into, you know, convert the combination of the delta encoding and the run length encoding to tell you how many plus ones I have afterwards.</p>
<p>687<br>01:11:42,819 –&gt; 01:11:52,819<br>So this is a good example, again, we can have this multiple clicker effect where we can compress the compressed data even further, because we’re putting it to a form that can take advantage of it.</p>
<p>688<br>01:11:53,819 –&gt; 01:12:06,819<br>So if you go back to our original data size, just for the time stamp column itself, we were at 320 bits, but if we do the delta encoding followed by the earlier coding, we get it down to 96 bits.</p>
<p>689<br>01:12:07,819 –&gt; 01:12:15,819<br>Again, I’m showing six or seven tuples here, it’s not that big, but again, think of it extreme, think of like a billion records. This would be a massive savings.</p>
<p>690<br>01:12:19,819 –&gt; 01:12:26,819<br>The last one discussed is dictionic oppression, because again, I said this is the most common one, this is how most systems are going to compress data.</p>
<p>691<br>01:12:27,819 –&gt; 01:12:29,819<br>Even for things that aren’t strings, right?</p>
<p>692<br>01:12:30,819 –&gt; 01:12:37,819<br>In some cases, there are some columnar systems will compress integer data, flow data, and putting them to dictionary codes.</p>
<p>693<br>01:12:38,819 –&gt; 01:12:50,819<br>The idea here is that if we have vows that we see over and over again, instead of storing that value repeatedly within a column, we’re going to convert that into some 32 bit integer.</p>
<p>694<br>01:12:51,819 –&gt; 01:13:01,819<br>And then we maintain a mapping data structure, the dictionary, that knows how to take that dictionary code, the 32 bit integer, and convert it back into an original value.</p>
<p>695<br>01:13:02,819 –&gt; 01:13:19,819<br>Typically, we’re going to have, it’s a one-to-one correspondence, for one value, we’ll have one dictionary code. There is some techniques, I don’t think any commercial system does this, where you can say, if I see multiple attributes that the patterns together, I’ll convert the combination, the two of them, or three of them into a single dictionary code.</p>
<p>696<br>01:13:20,819 –&gt; 01:13:25,819<br>I don’t think any further compression, but again, I’ve only seen that in the academic literature.</p>
<p>697<br>01:13:26,819 –&gt; 01:13:33,819<br>And then we need a way to do fast encoding and decoding on the fly that allows us to do both range and point queries.</p>
<p>698<br>01:13:34,819 –&gt; 01:13:42,819<br>So point queries are obvious, like I want to be able to say, the string-and-e maps to code 101, I know how to do that exact click-ups in those.</p>
<p>699<br>01:13:42,819 –&gt; 01:13:47,819<br>But ideally, I want to be able to also be able to do range queries on compressed data.</p>
<p>700<br>01:13:47,819 –&gt; 01:13:53,819<br>And so I want my dictionary code to have the same ordering that the original values actually did, too.</p>
<p>701<br>01:13:54,819 –&gt; 01:13:56,819<br>And we’ll see how to do that in a second.</p>
<p>702<br>01:13:57,819 –&gt; 01:14:00,819<br>So say this is my original data, a bunch of names of my former students.</p>
<p>703<br>01:14:02,819 –&gt; 01:14:10,819<br>Then the compressed version of this could be, again, I have my original column, convert those into 32 bit integers.</p>
<p>704<br>01:14:10,819 –&gt; 01:14:21,819<br>And then I just have this mapping table here that converts the last video lookups to say, for a given code, what’s the original value, or for a given original value, what’s the code, and that’s the dictionary.</p>
<p>705<br>01:14:22,819 –&gt; 01:14:36,819<br>So now we can go back to my example that I had in the very beginning with me, and DJ2PL, where select star from users, where name equals Andy, I can convert the string-and-e into the dictionary code by doing a lookup first in the dictionary.</p>
<p>706<br>01:14:37,819 –&gt; 01:14:43,819<br>Then now I scan through my column and just do lookups or do comparisons based on the integers.</p>
<p>707<br>01:14:44,819 –&gt; 01:14:53,819<br>So I don’t need to go through as I’m scanning along. If I don’t compress my constant, then as I scan along, I got to go decompress each of these one by one, and then do my lookups.</p>
<p>708<br>01:14:54,819 –&gt; 01:14:57,819<br>I’m basically losing all the benefit of any compression.</p>
<p>709<br>01:14:57,819 –&gt; 01:15:03,819<br>And that’s what my SQL has to do, because they can’t interpret what’s actually in the dictionary. They can’t interpret what the compressed bytes actually mean.</p>
<p>710<br>01:15:04,819 –&gt; 01:15:19,819<br>But in this case here, because we’re the data system, we built the dictionary, we control it, we know how to read and interpret it, and we can, and in SQL, so we know what the query wants to do, we know how to take that constant, convert it to the dictionary code, then do our scan directly on the compressed data.</p>
<p>711<br>01:15:20,819 –&gt; 01:15:34,819<br>So how do we actually do this? Do the encoding and decoding? Well, again, for a given, given, given, unquest value, we know how to go, go, get, press form, and then reverse it.</p>
<p>712<br>01:15:34,819 –&gt; 01:15:38,819<br>So the key thing you point out is there’s not going to be a magic hash function that can do this for us.</p>
<p>713<br>01:15:39,819 –&gt; 01:15:51,819<br>Right? Any reverse will hash function is going to generate something that’s going to be much larger likely than the original value, or so they not get it down to a 30-bit integer.</p>
<p>714<br>01:15:51,819 –&gt; 01:15:57,819<br>So we’re going to have to build a data structure that we maintain that allows us to do this.</p>
<p>715<br>01:15:58,819 –&gt; 01:16:12,819<br>And as I said, we want something that’s going to be preserved the ordering of the original values such that the compressed data, the compressed dictionary codes, those things have the same ordering electrographically or, yeah, as the original data does.</p>
<p>716<br>01:16:13,819 –&gt; 01:16:34,819<br>So going back here, right, if I have, again, I have a bunch of these names, I want the dictionary that I’m generating to the codes that have this such that if one, if the original value comes before in the ordering before another original value is dictionary code should come before it as well.</p>
<p>717<br>01:16:35,819 –&gt; 01:16:41,819<br>So I would have my dictionary is basically sorted. So now this allows me to do queries like this.</p>
<p>718<br>01:16:41,819 –&gt; 01:16:46,819<br>Slackstar from users were named like Andy, A&amp;D, followed by the wildcard.</p>
<p>719<br>01:16:46,819 –&gt; 01:16:57,819<br>And so if we operate directly on the compressed data, we can convert this like clause into a between clause, because we can look up in the dictionary,</p>
<p>720<br>01:16:57,819 –&gt; 01:17:15,819<br>and run the like portion just on the dictionary values, find the ones that match, find the min and max values for the matching values, and then rewrite the like into a between clause, and then now rip through my column while it’s still compressed.</p>
<p>721<br>01:17:16,819 –&gt; 01:17:32,819<br>Again, we can do this because it’s SQL, we know what the, we know it’s in the where clause, it’s not arbitrary Python code or C code, we know exactly what the, what the, what the where clause wants to do, and we can be smart, intelligent, and convert this into, you know, do the rewriting for us.</p>
<p>722<br>01:17:32,819 –&gt; 01:17:46,819<br>And again, you as the application program, or not you guys, but some JavaScript program, they don’t have to know what the hell’s going on in the covers, right, data’s right, the like clause, and the data system can be smart and we write it for you and get better performance.</p>
<p>723<br>01:17:49,819 –&gt; 01:17:58,819<br>So in some cases here, you still have to do questions, whether it’s, you know, still in the performance and original column.</p>
<p>724<br>01:17:58,819 –&gt; 01:18:10,819<br>In this case here, since I need the, I need the output of the, of the name attribute, I still have to go rip through the column and actually look at them.</p>
<p>725<br>01:18:10,819 –&gt; 01:18:21,819<br>In some cases though, the data system can even even smarter and it can answer queries without actually looking at the compressed data, but just operate directly on the dictionary.</p>
<p>726<br>01:18:22,819 –&gt; 01:18:32,819<br>So instead of saying select name from users, if it was distinct name from users, where I don’t need to get the actual tuples themselves, I just need to get the actual values that are unique.</p>
<p>727<br>01:18:32,819 –&gt; 01:18:48,819<br>Then for this query here, after I do my conversion to the, you know, converting to the between, or convert the, this, this wildcard here into the dictionary values, I only need to know what, what values actually exist in the dictionary.</p>
<p>728<br>01:18:49,819 –&gt; 01:19:03,819<br>And I don’t need to go look at the actual column for this query here with a distinct, you know, assuming I only have four names in my, in my table, but I have a billion rows, I only need to look at four rows in the dictionary to answer it.</p>
<p>729<br>01:19:04,819 –&gt; 01:19:12,819<br>And again, I’m set this multiple times, we can do this because the data, because the data is responsible for compressing this.</p>
<p>730<br>01:19:13,819 –&gt; 01:19:21,819<br>Now, parking or one of the big limitations that they have is they don’t actually expose the dictionary to you when you use their, their libraries and utilities.</p>
<p>731<br>01:19:21,819 –&gt; 01:19:30,819<br>So you can’t do this trick, I’m talking about here if you’re using parking or, right, parking or or decompress the data when it gives it back to you, you can’t operate directly on compressed data.</p>
<p>732<br>01:19:30,819 –&gt; 01:19:35,819<br>And that’s actually one of the biggest limitations of my, my opinion of those two formats.</p>
<p>733<br>01:19:35,819 –&gt; 01:19:40,819<br>But again, other systems that do native compression without parking or can, can, can do this trick.</p>
<p>734<br>01:19:41,819 –&gt; 01:19:47,819<br>All right, so what, what is the data, what, what, what, what, what, what is the data, what we’re going to use for our dictionary?</p>
<p>735<br>01:19:47,819 –&gt; 01:19:50,819<br>So the most common approach is going to be a really simple array.</p>
<p>736<br>01:19:50,819 –&gt; 01:20:00,819<br>And this works great if it, if it follows our meta, because I build the array once and I never to resize things and, in, certain things in, in, in place and, in, to move things around, I can just build on once that I’m done.</p>
<p>737<br>01:20:01,819 –&gt; 01:20:06,819<br>If you need something that’s dynamic and can support updates, you need either hash table or B plus tree.</p>
<p>738<br>01:20:06,819 –&gt; 01:20:11,819<br>These are, hash table is, is, I think less common. There is, actually, these things are less common.</p>
<p>739<br>01:20:11,819 –&gt; 01:20:17,819<br>Most people do the array and, and assume the, the blocks are going to be, the compressed blocks are going to be immutable.</p>
<p>740<br>01:20:17,819 –&gt; 01:20:21,819<br>And only if I need to, to, to rebuild it, then I’ll rebuild the array.</p>
<p>741<br>01:20:21,819 –&gt; 01:20:24,819<br>And I realize them over time. Let me show, roughly what it looks like.</p>
<p>742<br>01:20:25,819 –&gt; 01:20:30,819<br>So basically you have your original data in your column. So the first thing you do is, is build your dictionary.</p>
<p>743<br>01:20:30,819 –&gt; 01:20:35,819<br>And again, all that’s going to be is a sorted list of the, of the values you have.</p>
<p>744<br>01:20:35,819 –&gt; 01:20:42,819<br>And then the, and you store the length of the, of the, of the string.</p>
<p>745<br>01:20:42,819 –&gt; 01:20:48,819<br>And then now the dictionary code is going to be, it’s just an offset into this, this array here.</p>
<p>746<br>01:20:49,819 –&gt; 01:20:55,819<br>So my compressed data would look, look like this. And these are just offsets, the byte offset into the array.</p>
<p>747<br>01:20:55,819 –&gt; 01:21:05,819<br>And so now when I’m doing a scan, and I want to say, okay, if I, if I have, if I, if I, if that’s the second one, the second entry, it’s 17.</p>
<p>748<br>01:21:05,819 –&gt; 01:21:14,819<br>I jumped a byte, offset at 17. And I can look in the, it’s down here. I can look in the, in the header and tell me how big is the string afterwards.</p>
<p>749<br>01:21:15,819 –&gt; 01:21:19,819<br>So the dictionary itself is literally just an array packed of bytes like that.</p>
<p>750<br>01:21:19,819 –&gt; 01:21:20,819<br>Okay.</p>
<p>751<br>01:21:21,819 –&gt; 01:21:27,819<br>All right. So, to finish up, the, we, you know, this row store versus comms are going to be really important.</p>
<p>752<br>01:21:27,819 –&gt; 01:21:31,819<br>And we’ll see this show up when we talk about query execution and other things.</p>
<p>753<br>01:21:31,819 –&gt; 01:21:38,819<br>Mostly before, before the midterm, because again, the, the distinction of the difference between a row store and a comms store system,</p>
<p>754<br>01:21:38,819 –&gt; 01:21:47,819<br>have ramifications through, throughout all other parts of the database system, how you do recovery, how you could do query execution, how you want to run transactions, how you want to optimize your queries.</p>
<p>755<br>01:21:47,819 –&gt; 01:21:55,819<br>And so it’s really important to understand this now. And we’ll, we’ll, we’ll see the trade-offs between the two approaches again and again throughout the entire semester.</p>
<p>756<br>01:21:55,819 –&gt; 01:22:02,819<br>And then most database systems to get the best compression ratio, you want to do it natively, you want to do yourself, and dictionary coding is Ms. Common One.</p>
<p>757<br>01:22:02,819 –&gt; 01:22:07,819<br>So, I show this three, three lectures ago that there was sort of two problems with the data storage.</p>
<p>758<br>01:22:07,819 –&gt; 01:22:11,819<br>First, how are we going to represent data on disk? We’ve covered that so far.</p>
<p>759<br>01:22:11,819 –&gt; 01:22:17,819<br>So starting next week, now tell me, okay, when we bring things into memory, what do we do with it?</p>
<p>760<br>01:22:17,819 –&gt; 01:22:24,819<br>How do we store it? And how do we write things back out safely when, when we make changes? Okay? All right. Hit it.</p>
<p>761<br>01:22:55,819 –&gt; 01:23:01,819<br>I got a block on tap, the Feds can’t trace that. Style is like temp, but proof. You can’t lace that at the Dominican.</p>
<p>762<br>01:23:01,819 –&gt; 01:23:08,819<br>Oh, you could call me Dominican. Black, Skelly, Black, another, Black, Swade, Timberlands. My old Black, Dirty Eight, send you to the Purdy Gates.</p>
<p>763<br>01:23:08,819 –&gt; 01:23:13,819<br>You get your zombie trying to skate, and that’s your first mistake. I ain’t lying for that cake, you’re famous, see your weight.</p>
<p>764<br>01:23:13,819 –&gt; 01:23:19,819<br>My grants is heavy weight, the Randthorough Ebbe State. When they actin’ how I’m livin’, I tell them I’m livin’ great.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CMU15445 P6F202305 StorageModelsDatabaseCompression</div>
      <div>http://example.com/2025/10/25/CMU15445 P6F202305-StorageModelsDatabaseCompression/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月25日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/25/CMU15445%20P5F202304-DatabaseStoragePart2/" title="CMU15445 P5F202304 DatabaseStoragePart2">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CMU15445 P5F202304 DatabaseStoragePart2</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/25/CMU15445%20P7F202306-DatabaseMemoryDiskI%E2%A7%B8OManagement/" title="CMU15445 P7F202306 DatabaseMemoryDiskI⧸OManagement">
                        <span class="hidden-mobile">CMU15445 P7F202306 DatabaseMemoryDiskI⧸OManagement</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
