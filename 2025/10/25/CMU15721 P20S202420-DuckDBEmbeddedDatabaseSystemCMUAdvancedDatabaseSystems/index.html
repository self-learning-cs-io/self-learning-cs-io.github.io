

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="100:00:00,000 –&gt; 00:00:06,000Carnegie Mellon University’s Advanced Database Systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio audience. 300:00:09,000 –&gt; 00:00:18,0">
<meta property="og:type" content="article">
<meta property="og:title" content="CMU15721 P20S202420 DuckDBEmbeddedDatabaseSystemCMUAdvancedDatabaseSystems">
<meta property="og:url" content="http://example.com/2025/10/25/CMU15721%20P20S202420-DuckDBEmbeddedDatabaseSystemCMUAdvancedDatabaseSystems/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="100:00:00,000 –&gt; 00:00:06,000Carnegie Mellon University’s Advanced Database Systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio audience. 300:00:09,000 –&gt; 00:00:18,0">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-25T05:03:39.752Z">
<meta property="article:modified_time" content="2025-10-25T05:03:39.752Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>CMU15721 P20S202420 DuckDBEmbeddedDatabaseSystemCMUAdvancedDatabaseSystems - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CMU15721 P20S202420 DuckDBEmbeddedDatabaseSystemCMUAdvancedDatabaseSystems"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-25 13:03" pubdate>
          2025年10月25日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.7k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          64 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CMU15721 P20S202420 DuckDBEmbeddedDatabaseSystemCMUAdvancedDatabaseSystems</h1>
            
            
              <div class="markdown-body">
                
                <p>1<br>00:00:00,000 –&gt; 00:00:06,000<br>Carnegie Mellon University’s Advanced Database Systems courses</p>
<p>2<br>00:00:06,000 –&gt; 00:00:09,000<br>filming front of the live studio audience.</p>
<p>3<br>00:00:09,000 –&gt; 00:00:18,000<br>Let’s open two discussion today on Dr. D.</p>
<p>4<br>00:00:18,000 –&gt; 00:00:24,000<br>And again, as I said last time, this is going to be a lot different than all the other systems we’ve been talking about,</p>
<p>5<br>00:00:24,000 –&gt; 00:00:30,000<br>in the last week or so because all of those release giant distributed warehouse is running in the cloud,</p>
<p>6<br>00:00:30,000 –&gt; 00:00:35,000<br>and then now I had to read a paper about WB who’s most of running on a single node.</p>
<p>7<br>00:00:35,000 –&gt; 00:00:40,000<br>But we’ll talk about Mother Duck at the end, how they not necessarily go distributed,</p>
<p>8<br>00:00:40,000 –&gt; 00:00:49,000<br>meaning fanning out, scaling out the queries themselves, but at least now be able to leverage cloud compute infrastructure for query execution.</p>
<p>9<br>00:00:49,000 –&gt; 00:00:51,000<br>But we’ll see that at the end.</p>
<p>10<br>00:00:51,000 –&gt; 00:00:54,000<br>All right, so last class we were talking about the Smith-like data warehouse.</p>
<p>11<br>00:00:54,000 –&gt; 00:01:02,000<br>And as I said, this was along with Dremel, so that one of the first, this is what I call classical cloud native, OLAP engine,</p>
<p>12<br>00:01:02,000 –&gt; 00:01:05,000<br>that did all the various things we talked about through the entire semester.</p>
<p>13<br>00:01:05,000 –&gt; 00:01:13,000<br>Precompah primitive push-based execution, separating computing storage, all those nice things.</p>
<p>14<br>00:01:13,000 –&gt; 00:01:18,000<br>And so this showed up actually I think yesterday, which I think worth just looking at,</p>
<p>15<br>00:01:18,000 –&gt; 00:01:23,000<br>because I think somebody else asked me like, okay, well, like all these systems at a high look at the same, how do you pick them?</p>
<p>16<br>00:01:23,000 –&gt; 00:01:29,000<br>So data engineering subreddit is actually really good, because there’s people who actually using the systems and talking about the pros and cons of them.</p>
<p>17<br>00:01:29,000 –&gt; 00:01:30,000<br>So I highly recommend it.</p>
<p>18<br>00:01:30,000 –&gt; 00:01:34,000<br>And so somebody’s asking, hey, how do I pick between snowflake, data bricks, BigQuery, and Redshift?</p>
<p>19<br>00:01:34,000 –&gt; 00:01:37,000<br>We haven’t talked about Redshift yet.</p>
<p>20<br>00:01:37,000 –&gt; 00:01:38,000<br>That’ll be next week.</p>
<p>21<br>00:01:38,000 –&gt; 00:01:43,000<br>And this boot basically says, hey, don’t worry about the native-goody details.</p>
<p>22<br>00:01:43,000 –&gt; 00:01:48,000<br>The way to really think about whether you want one system and another is, first of all, what cloud are you already running on?</p>
<p>23<br>00:01:48,000 –&gt; 00:01:51,000<br>If you’re already running on GCP or Google, then just use BigQuery.</p>
<p>24<br>00:01:51,000 –&gt; 00:01:56,000<br>If you’re already running on AWS, you could probably just use Redshift otherwise.</p>
<p>25<br>00:01:56,000 –&gt; 00:02:00,000<br>And then if you’re already using Spark, use data bricks.</p>
<p>26<br>00:02:00,000 –&gt; 00:02:06,000<br>And then if you have a lot of money, I’m looking for a good time, then that’s snowflake.</p>
<p>27<br>00:02:06,000 –&gt; 00:02:10,000<br>And I would say that’s one of the differentiators I think that snowflake has done really well,</p>
<p>28<br>00:02:10,000 –&gt; 00:02:20,000<br>that separates them from the other systems is just that the user experience is much, much better and cleaner than these other cloud systems.</p>
<p>29<br>00:02:20,000 –&gt; 00:02:25,000<br>And so even though, again, the core architecture might still be the same underneath the covers at a high level,</p>
<p>30<br>00:02:25,000 –&gt; 00:02:27,000<br>based on the things that we talked about.</p>
<p>31<br>00:02:27,000 –&gt; 00:02:30,000<br>As I said before, the user experience is going to last matter a lot.</p>
<p>32<br>00:02:30,000 –&gt; 00:02:33,000<br>And also how good the query optimizer is.</p>
<p>33<br>00:02:33,000 –&gt; 00:02:36,000<br>Is there no tuning in snowflake?</p>
<p>34<br>00:02:36,000 –&gt; 00:02:38,000<br>It’s question, there’s no tuning snowflake.</p>
<p>35<br>00:02:38,000 –&gt; 00:02:43,000<br>Yeah, they don’t expose really any knobs, you can’t even do query plain hands.</p>
<p>36<br>00:02:43,000 –&gt; 00:02:47,000<br>I think there’s one knob you can jack up the compute size.</p>
<p>37<br>00:02:47,000 –&gt; 00:02:50,000<br>You can turn off auto scaling.</p>
<p>38<br>00:02:50,000 –&gt; 00:02:52,000<br>I think there’s three things.</p>
<p>39<br>00:02:52,000 –&gt; 00:02:55,000<br>Now, it doesn’t mean in the actual implementation itself, there aren’t a bunch of other knobs.</p>
<p>40<br>00:02:55,000 –&gt; 00:02:58,000<br>And they’ve told me this. They’re like, yeah, we have hundreds of knobs on the inside.</p>
<p>41<br>00:02:58,000 –&gt; 00:03:00,000<br>They don’t expose them to the users.</p>
<p>42<br>00:03:00,000 –&gt; 00:03:04,000<br>So then typically what happens is, they said,</p>
<p>43<br>00:03:04,000 –&gt; 00:03:08,000<br>if somebody calls a sales engineer, says my query is running slow, the sales engineer can then get in touch,</p>
<p>44<br>00:03:08,000 –&gt; 00:03:12,000<br>or the customer service can get in touch with a database system engineer,</p>
<p>45<br>00:03:12,000 –&gt; 00:03:17,000<br>and then they’ll recommend like, hey, tune these three four knobs for this one particular customer.</p>
<p>46<br>00:03:17,000 –&gt; 00:03:19,000<br>But it’s very ad hoc.</p>
<p>47<br>00:03:19,000 –&gt; 00:03:21,000<br>At least this is what they told me a few years ago.</p>
<p>48<br>00:03:21,000 –&gt; 00:03:24,000<br>So underneath the covers, there’s all the tuning knobs that they would expect,</p>
<p>49<br>00:03:24,000 –&gt; 00:03:27,000<br>but they don’t make it your life easier as the user of it.</p>
<p>50<br>00:03:27,000 –&gt; 00:03:29,000<br>They don’t expose it to you.</p>
<p>51<br>00:03:29,000 –&gt; 00:03:31,000<br>How would they actually tune in themselves?</p>
<p>52<br>00:03:31,000 –&gt; 00:03:33,000<br>It must be really hard to do that.</p>
<p>53<br>00:03:33,000 –&gt; 00:03:36,000<br>How would they actually tune in themselves? What do you mean, how are they tuning themselves?</p>
<p>54<br>00:03:36,000 –&gt; 00:03:38,000<br>I mean, they have tons of knobs, right?</p>
<p>55<br>00:03:38,000 –&gt; 00:03:40,000<br>And like, getting those values out of your sentence.</p>
<p>56<br>00:03:40,000 –&gt; 00:03:45,000<br>So their sort of design philosophy would be like, you want things to be adaptive.</p>
<p>57<br>00:03:45,000 –&gt; 00:03:50,000<br>And so, again, it’s hard to quantify this, say how much it is actually adaptive,</p>
<p>58<br>00:03:50,000 –&gt; 00:03:54,000<br>but you can think about it like, instead of being lazy, not lazy in the other right word,</p>
<p>59<br>00:03:54,000 –&gt; 00:04:00,000<br>but instead of saying, okay, here’s some value for a knob I need to know about how to set correctly for the workload.</p>
<p>60<br>00:04:00,000 –&gt; 00:04:03,000<br>Instead of just saying, all right, let’s pound to fine somebody else that said it for me,</p>
<p>61<br>00:04:03,000 –&gt; 00:04:06,000<br>you could, there’s ways to make, try to make things adaptive.</p>
<p>62<br>00:04:06,000 –&gt; 00:04:08,000<br>It’s more engineering, certainly, but it makes you more robust.</p>
<p>63<br>00:04:08,000 –&gt; 00:04:09,000<br>Yes.</p>
<p>64<br>00:04:09,000 –&gt; 00:04:11,000<br>Why is Retchip not a positive experience?</p>
<p>65<br>00:04:11,000 –&gt; 00:04:14,000<br>The question is, why is Retchip not a positive experience?</p>
<p>66<br>00:04:14,000 –&gt; 00:04:18,000<br>Well, we’ll come to that next week.</p>
<p>67<br>00:04:18,000 –&gt; 00:04:21,000<br>Hey, it’s not…</p>
<p>68<br>00:04:21,000 –&gt; 00:04:25,000<br>Again, I think there’s nothing particularly wrong with the architecture itself,</p>
<p>69<br>00:04:25,000 –&gt; 00:04:28,000<br>because they fixed a lot of it.</p>
<p>70<br>00:04:28,000 –&gt; 00:04:32,000<br>Again, it’s the user experience, and that’s hard to quantify, right?</p>
<p>71<br>00:04:32,000 –&gt; 00:04:38,000<br>It could be like, okay, my query went slow, I did the right tools for me to figure out why it ran slow.</p>
<p>72<br>00:04:38,000 –&gt; 00:04:41,000<br>Or is it, you know, how stable is the system in terms of performance?</p>
<p>73<br>00:04:41,000 –&gt; 00:04:45,000<br>Like, I run the query today, I run it tomorrow, and like, it’s an order mag 2 difference of performance.</p>
<p>74<br>00:04:45,000 –&gt; 00:04:47,000<br>Like, you know, is that what people are seeing?</p>
<p>75<br>00:04:47,000 –&gt; 00:04:51,000<br>Again, this is purely anecdotal, right?</p>
<p>76<br>00:04:51,000 –&gt; 00:04:54,000<br>I just thought it was an interesting quip.</p>
<p>77<br>00:04:54,000 –&gt; 00:05:00,000<br>And, like I said, the data engineering, this is where you see a lot of people at the sort of bleeding edge talking about how using these systems,</p>
<p>78<br>00:05:00,000 –&gt; 00:05:04,000<br>and also the ecosystems around it, like airflow, DBT, and things like that,</p>
<p>79<br>00:05:04,000 –&gt; 00:05:07,000<br>and how they integrate with these cloud unders.</p>
<p>80<br>00:05:07,000 –&gt; 00:05:09,000<br>Okay?</p>
<p>81<br>00:05:11,000 –&gt; 00:05:16,000<br>All right, so, for today’s discussion, we need to go back a time, and just a little bit,</p>
<p>82<br>00:05:16,000 –&gt; 00:05:21,000<br>to talk about what led to the creation of WDB.</p>
<p>83<br>00:05:21,000 –&gt; 00:05:28,000<br>And, you actually already read one of the papers that was a precursor or a catalyst for what, you know,</p>
<p>84<br>00:05:28,000 –&gt; 00:05:34,000<br>what set off the development of Dr. B, and that was the networking paper, like, don’t hold my data hostage.</p>
<p>85<br>00:05:34,000 –&gt; 00:05:38,000<br>Right? And that paper came out of this project they were building at CWI,</p>
<p>86<br>00:05:38,000 –&gt; 00:05:42,000<br>to make an embedded version of MoNDDB called MoNDDB Lite.</p>
<p>87<br>00:05:42,000 –&gt; 00:05:47,000<br>Basically, MoNDDB, again, was one of these early comms store systems at academia, at a CWI,</p>
<p>88<br>00:05:47,000 –&gt; 00:05:50,000<br>that then, it was open source, and people were actually using.</p>
<p>89<br>00:05:50,000 –&gt; 00:05:54,000<br>But it was like, you know, it was like a post-gress or any other sort of shared everything system,</p>
<p>90<br>00:05:54,000 –&gt; 00:06:00,000<br>you had to prop it up, configure it, set it up, and you would connect to it through JDBC over the network.</p>
<p>91<br>00:06:00,000 –&gt; 00:06:08,000<br>And so, what they were trying to do was to make it faster or easier for data scientists to use a database system</p>
<p>92<br>00:06:08,000 –&gt; 00:06:14,000<br>inside of, like, Python pandas, or R, I think there’s explicitly for R, they were doing this.</p>
<p>93<br>00:06:14,000 –&gt; 00:06:17,000<br>They tried to make an embedded version that ran in process.</p>
<p>94<br>00:06:17,000 –&gt; 00:06:21,000<br>I haven’t really talked about what an embedded database system looks like, but basically,</p>
<p>95<br>00:06:21,000 –&gt; 00:06:26,000<br>it’s, you know, there’s no main function, like, you would starting the post-gress or my sequel,</p>
<p>96<br>00:06:26,000 –&gt; 00:06:33,000<br>it’s running, it only runs whenever the hosting process then invokes inside, down into it.</p>
<p>97<br>00:06:33,000 –&gt; 00:06:37,000<br>Now, it can spin up its own threads in the background and do other stuff,</p>
<p>98<br>00:06:37,000 –&gt; 00:06:45,000<br>but, like, it’s not something you would just start, like, as a demon on your server and just run all the time.</p>
<p>99<br>00:06:45,000 –&gt; 00:06:51,000<br>So, they were trying to make this run directly inside of the R runtime or pandas runtime,</p>
<p>100<br>00:06:51,000 –&gt; 00:07:00,000<br>and they were trying to reduce the cost of going back and forth between the R-lang infrastructure and the database system.</p>
<p>101<br>00:07:00,000 –&gt; 00:07:07,000<br>Because, typically, very often, how data scientists use databases is they just grab a giant CSV or a parquet file,</p>
<p>102<br>00:07:07,000 –&gt; 00:07:12,000<br>whatever, from the server itself, bring that down to your local laptop and do all the crunching on that.</p>
<p>103<br>00:07:12,000 –&gt; 00:07:16,000<br>And so, they’re not leveraging the database system to do all the fast calculations and stuff that, you know,</p>
<p>104<br>00:07:16,000 –&gt; 00:07:22,000<br>we know how to do really well, they’re basically rolling all that crap themselves inside of their set of user code.</p>
<p>105<br>00:07:22,000 –&gt; 00:07:27,000<br>Pandas isn’t particularly known to be, you know, very, very swift or fast.</p>
<p>106<br>00:07:28,000 –&gt; 00:07:32,000<br>So, that was the goal. They’re trying to have an embedded version of MoDV Lite that got all the advantages of the column store,</p>
<p>107<br>00:07:32,000 –&gt; 00:07:40,000<br>that would expose that to the data scientists, but the problem of MoDV, you know, at this point, it was 15, 20 years old,</p>
<p>108<br>00:07:40,000 –&gt; 00:07:54,000<br>and it just had too much sort of legacy infrastructure and legacy code that it was too much to rip things out and strip it down to get it to be a more simplistic package that, again, you could then embed.</p>
<p>109<br>00:07:55,000 –&gt; 00:07:58,000<br>So, that’s what led the C2R researchers to develop.db.</p>
<p>110<br>00:07:58,000 –&gt; 00:08:06,000<br>Right, it’s basically the time that the efforts that they took for MoDV Lite, they learned a bunch of lessons from that and said,</p>
<p>111<br>00:08:06,000 –&gt; 00:08:15,000<br>okay, we should build a system from scratch, specifically designed from the beginning to be embedded inside of, you know, other applications.</p>
<p>112<br>00:08:15,000 –&gt; 00:08:21,000<br>So, you could call this an embedded data system, sometimes it’s called in process, you could technically call it serverless, right,</p>
<p>113<br>00:08:22,000 –&gt; 00:08:36,000<br>because again, it’s not a demon that’s always running, but the idea is that they’re trying to be a, provide a fast SQL execution engine on any possible data file you could find, that you possibly want to query.</p>
<p>114<br>00:08:36,000 –&gt; 00:08:42,000<br>Right, again, this is sort of the same idea in Dremel, they want to take any data file that someone may have in their object store,</p>
<p>115<br>00:08:42,000 –&gt; 00:08:44,000<br>anyone would run queries real fast on it.</p>
<p>116<br>00:08:45,000 –&gt; 00:08:49,000<br>So, the SQL dialect search support is based on Postgres.</p>
<p>117<br>00:08:49,000 –&gt; 00:08:56,000<br>They do what we did in an hourly system, we took the Postgres grammar file and just embedded that inside of our system,</p>
<p>118<br>00:08:56,000 –&gt; 00:09:03,000<br>but then over the years, what I’ve liked is that, well, this is how you get, you know, there’s, so why there isn’t a single SQL standard,</p>
<p>119<br>00:09:03,000 –&gt; 00:09:10,000<br>they’ve added some nice quality of life enhancements that are specific to WDB, like you just type from and then the table name,</p>
<p>120<br>00:09:11,000 –&gt; 00:09:13,000<br>that does the same things like the select star, things like that.</p>
<p>121<br>00:09:13,000 –&gt; 00:09:19,000<br>And the pitch, which I think is fantastic from a marketing standpoint, to really understand what WDB is trying to be,</p>
<p>122<br>00:09:19,000 –&gt; 00:09:22,000<br>is trying to be the SQL light for analytics.</p>
<p>123<br>00:09:22,000 –&gt; 00:09:29,000<br>Right, SQL light is the most widely deployed embedded database system, it’s on all your phones and your laptops right here right now,</p>
<p>124<br>00:09:29,000 –&gt; 00:09:35,000<br>it’s running satellites in space, it’s in every plane, it’s designed to do transactions.</p>
<p>125<br>00:09:36,000 –&gt; 00:09:41,000<br>And so, they wanted to sort of have the same ubiquity of SQL light, the proliferation of it being used by everyone,</p>
<p>126<br>00:09:41,000 –&gt; 00:09:44,000<br>but specifically for doing fast analytics.</p>
<p>127<br>00:09:44,000 –&gt; 00:09:52,000<br>Now, Jignesh has a paper with his students, I think, came out two or three years ago, they’ve added some enhancements to SQL light to improve the, you know,</p>
<p>128<br>00:09:52,000 –&gt; 00:09:57,000<br>it’s support to do analytics, but I don’t think it comes close to what WDB can do.</p>
<p>129<br>00:09:58,000 –&gt; 00:10:07,000<br>So, another important design decision in terms of the implementation of it, is that it’s all going to be custom SQL’s less code that they’ve written for WDB.</p>
<p>130<br>00:10:07,000 –&gt; 00:10:12,000<br>So, that means they’re going to try to avoid bringing in third party dependencies when necessary.</p>
<p>131<br>00:10:12,000 –&gt; 00:10:17,000<br>I think for encryption SSL stuff, like that stuff you want to bring in, you don’t want to write that yourself.</p>
<p>132<br>00:10:17,000 –&gt; 00:10:24,000<br>But like, a bunch of other stuff, they’re not going to rely on third party packages or libraries, they’re going to write everything themselves.</p>
<p>133<br>00:10:25,000 –&gt; 00:10:32,000<br>And this is going to make the system more lightweight, easier to manage from the engineering side of things.</p>
<p>134<br>00:10:32,000 –&gt; 00:10:45,000<br>And you have the additional capability of like, now because it’s all C++ code that you wrote, and not some weird party library, third party library, they can actually compile it on the wasm, and it can run in your browser very easily.</p>
<p>135<br>00:10:45,000 –&gt; 00:10:52,000<br>There’s been attempts to get like SQL light, and actually SQL light will run on the wasm, but people would try to get post-gust run on wasm,</p>
<p>136<br>00:10:52,000 –&gt; 00:10:58,000<br>and it always seems like a huge hack to get that to work, whereas in productive it’s quite simple.</p>
<p>137<br>00:10:58,000 –&gt; 00:11:08,000<br>And so the way they’re going to be able to expand the, what the system can actually do, beyond the sort of core runtime engine, they’re going to rely heavily on an extension ecosystem.</p>
<p>138<br>00:11:08,000 –&gt; 00:11:09,000<br>We’ll talk about in a second.</p>
<p>139<br>00:11:09,000 –&gt; 00:11:15,000<br>So they’ll have like the official ones that they support, but also you can download arbitrary ones and install them.</p>
<p>140<br>00:11:15,000 –&gt; 00:11:19,000<br>And that, again, that makes the core engine lightweight and easier for them to manage.</p>
<p>141<br>00:11:20,000 –&gt; 00:11:29,000<br>So again, like we talked about when we talked about photon, like the design philosophy of like, okay, they have to maintain all the spark Java crap, and they want to avoid Java.</p>
<p>142<br>00:11:29,000 –&gt; 00:11:33,000<br>So photon was the supposed engine that they evoked down through JNI.</p>
<p>143<br>00:11:33,000 –&gt; 00:11:34,000<br>Yes.</p>
<p>144<br>00:11:34,000 –&gt; 00:11:36,000<br>So let’s work exactly what we mean.</p>
<p>145<br>00:11:36,000 –&gt; 00:11:37,000<br>Are we looking like, like, lambda function, something like that?</p>
<p>146<br>00:11:37,000 –&gt; 00:11:46,000<br>Yeah, so service will be like a lambda function, meaning like, the process goes away when like, if nothing is actively using it.</p>
<p>147<br>00:11:46,000 –&gt; 00:11:50,000<br>So embedded, embedded is a better one.</p>
<p>148<br>00:11:50,000 –&gt; 00:11:52,000<br>Like, embedded is sort of the same idea.</p>
<p>149<br>00:11:52,000 –&gt; 00:12:00,000<br>Like, there has to be some other process already running that then links into the data system, and then it is, I don’t think you hand it threads where you do in SQL light.</p>
<p>150<br>00:12:00,000 –&gt; 00:12:06,000<br>I think you spin up a zone threads, but like, it’s all within the same address space as the host process.</p>
<p>151<br>00:12:06,000 –&gt; 00:12:08,000<br>It’s like a library.</p>
<p>152<br>00:12:08,000 –&gt; 00:12:10,000<br>It’s like a library, yes.</p>
<p>153<br>00:12:10,000 –&gt; 00:12:12,000<br>Yeah, it’s a library, yes.</p>
<p>154<br>00:12:12,000 –&gt; 00:12:20,000<br>All right, so here’s at high level what the major features that are in, in ducty B.</p>
<p>155<br>00:12:20,000 –&gt; 00:12:28,000<br>And again, the number one difference at the top is going to be that it shared everything versus all the shared disk or disaggregated storage stuff we talked about before.</p>
<p>156<br>00:12:28,000 –&gt; 00:12:30,000<br>Because again, it’s an embedded database system.</p>
<p>157<br>00:12:30,000 –&gt; 00:12:33,000<br>It has no notion of a separate and compute in storage.</p>
<p>158<br>00:12:33,000 –&gt; 00:12:38,000<br>I mean, you’ll see that they can use extensions to talk to the cloud platforms and so forth.</p>
<p>159<br>00:12:38,000 –&gt; 00:12:48,000<br>But the, you know, at its core, it is sort of just like just a, just the query engine.</p>
<p>160<br>00:12:48,000 –&gt; 00:12:50,000<br>It’s not in time to do because they knew transactions.</p>
<p>161<br>00:12:50,000 –&gt; 00:12:52,000<br>They have them in file format.</p>
<p>162<br>00:12:52,000 –&gt; 00:12:58,000<br>So, but it’s again, it’s not the like scaled out architecture that we saw in Dremel and others.</p>
<p>163<br>00:12:58,000 –&gt; 00:13:04,000<br>Their new push based vectorized query processing will spend most of our time talking about this because they actually started doing pull based.</p>
<p>164<br>00:13:04,000 –&gt; 00:13:12,000<br>And then they switched over two years ago, three years ago to push base and they talk about all of the challenges they face scaling out.</p>
<p>165<br>00:13:12,000 –&gt; 00:13:17,000<br>The pull based model and why switching to a bush based one was better.</p>
<p>166<br>00:13:17,000 –&gt; 00:13:23,000<br>Again, given the, the, the, the provenance of coming from the CWI vector wise was invented.</p>
<p>167<br>00:13:23,000 –&gt; 00:13:26,000<br>They’re going to pre-compile primitives. That’s not, that’s not a surprise.</p>
<p>168<br>00:13:26,000 –&gt; 00:13:31,000<br>They use a solid MVCC that’s actually based on what the Germans do in Umbra or sorry in hyper.</p>
<p>169<br>00:13:31,000 –&gt; 00:13:36,000<br>We’re not going to cover this paper, but I’ll actually a lot of these are based on what the Germans did.</p>
<p>170<br>00:13:36,000 –&gt; 00:13:42,000<br>Some, some ways you can say like they let me just took the papers and re-implement the stuff that the hype, that we, for all the papers we talked about today.</p>
<p>171<br>00:13:42,000 –&gt; 00:13:48,000<br>For example, they’re doing morsels. They’re going to do the, the, the, the un-nesting the arbitrary sub queries.</p>
<p>172<br>00:13:48,000 –&gt; 00:13:52,000<br>They’re the only databases other than hyper and Umbra that can do this.</p>
<p>173<br>00:13:52,000 –&gt; 00:13:54,000<br>Yes.</p>
<p>174<br>00:13:54,000 –&gt; 00:14:01,000<br>Why would you want morsel fellows to work something that’s intended to be better, maybe like on a pizza box?</p>
<p>175<br>00:14:01,000 –&gt; 00:14:06,000<br>So actually, why would you want morsel parallelism, if you, if you tend to, if you tend to be running on a pizza box.</p>
<p>176<br>00:14:06,000 –&gt; 00:14:10,000<br>But a pizza box with nowadays has a sheet little cores, right?</p>
<p>177<br>00:14:10,000 –&gt; 00:14:12,000<br>Doesn’t have one of the ones.</p>
<p>178<br>00:14:12,000 –&gt; 00:14:18,000<br>When it’s a pizza box, I mean like one unit rack, rack unit, you can put like multiple sockets in those things.</p>
<p>179<br>00:14:18,000 –&gt; 00:14:23,000<br>Or even the, whatever, the AMD, the Ryzen, the latest one, it’s a ton of cores.</p>
<p>180<br>00:14:23,000 –&gt; 00:14:27,000<br>The thread, I forget the exact number now. You can get a ton of cores.</p>
<p>181<br>00:14:27,000 –&gt; 00:14:30,000<br>So why, why wouldn’t you want to use them?</p>
<p>182<br>00:14:30,000 –&gt; 00:14:35,000<br>And it’s specifically like not, not new like, the way it’s called.</p>
<p>183<br>00:14:35,000 –&gt; 00:14:38,000<br>Well, of course, they’re on the same like talking.</p>
<p>184<br>00:14:38,000 –&gt; 00:14:42,000<br>Actually, I don’t know whether this is actually new more aware. I don’t, the paper doesn’t say.</p>
<p>185<br>00:14:42,000 –&gt; 00:14:49,000<br>But I imagine it’s not hard to figure that out, right? You also tell you.</p>
<p>186<br>00:14:49,000 –&gt; 00:14:59,000<br>Like SQLite, I would say, SQLite really was designed to run on like a one core very-parent CPU, like from the, like from the 2000s.</p>
<p>187<br>00:14:59,000 –&gt; 00:15:11,000<br>There’s a, even more stripped down data system called extreme DB, that’s running like on like S.O.C.s in like missiles and shit like that.</p>
<p>188<br>00:15:11,000 –&gt; 00:15:15,000<br>Right? Where like you don’t even have an operating system.</p>
<p>189<br>00:15:15,000 –&gt; 00:15:19,000<br>So there’s system, even more like low level than that.</p>
<p>190<br>00:15:19,000 –&gt; 00:15:25,000<br>SQLite’s a little bit more, right? And in that case, SQLite can’t do parallel query execution.</p>
<p>191<br>00:15:25,000 –&gt; 00:15:29,000<br>Like, whatever thread makes the SQL request, that’s the one that runs it.</p>
<p>192<br>00:15:29,000 –&gt; 00:15:34,000<br>I’m fairly certain. Whereas in ductyb, like one thread would make the request.</p>
<p>193<br>00:15:34,000 –&gt; 00:15:46,000<br>But then if you tell it how many threads you’re allowed to spread the query, query across, and they use more cells to do that schedule.</p>
<p>194<br>00:15:46,000 –&gt; 00:15:52,000<br>Right? So the pack stuff that we’ve covered that many, many times, they’re going to do the sort merge join and hash joins.</p>
<p>195<br>00:15:52,000 –&gt; 00:15:57,000<br>And then the stratified query optimizer is, looks very similar to what we talked about before.</p>
<p>196<br>00:15:57,000 –&gt; 00:16:09,000<br>And if you’re running on arbitrary files, they may not have any statistics for, so they’re using much of rules to figure out some basic join order or heuristics or things like that.</p>
<p>197<br>00:16:09,000 –&gt; 00:16:13,000<br>But the one thing they do well is the unnesting arbitrary subqueries.</p>
<p>198<br>00:16:13,000 –&gt; 00:16:18,000<br>They didn’t support unnesting arbitrary subqueries for lateral joins.</p>
<p>199<br>00:16:18,000 –&gt; 00:16:22,000<br>And as I said last year in 721, the students actually did that.</p>
<p>200<br>00:16:22,000 –&gt; 00:16:25,000<br>Like Sam arched the PGA student here. They got that merge into ductyb.</p>
<p>201<br>00:16:25,000 –&gt; 00:16:28,000<br>So they can handle all possible subqueries now.</p>
<p>202<br>00:16:28,000 –&gt; 00:16:40,000<br>All right, so we’re going to spend those right time talking about the push based architecture because they’re going to talk about its public about actually how they’re going to pass data between the operators, which is something we haven’t really discussed.</p>
<p>203<br>00:16:40,000 –&gt; 00:16:53,000<br>So as I mentioned, the original version of ductyb prior to 2021 was using a pull based vectorized extrusion model with precomp默 primitives.</p>
<p>204<br>00:16:53,000 –&gt; 00:17:13,000<br>But then over the years, they found that it was turned out to be cumbersome to maintain and work on because every single time that you wanted to add a new operator and make it parallel, you had to modify this control plane piece to say, OK, here’s this parallel thing we can now we now run.</p>
<p>205<br>00:17:13,000 –&gt; 00:17:17,000<br>It was more infrastructure that they had to maintain for every single time to add something new.</p>
<p>206<br>00:17:17,000 –&gt; 00:17:29,000<br>The other challenge they had is that because now, since they want to be able to support reading data from not necessarily local disk, right, remote files, and they can support HTTPS or S3 and so forth.</p>
<p>207<br>00:17:29,000 –&gt; 00:17:39,000<br>Now you have this challenge where some pipeline may be blocked because it’s getting data over the network, but there are other pipelines you could start running because they don’t need to wait for that data.</p>
<p>208<br>00:17:39,000 –&gt; 00:17:50,000<br>But if you do a pull based model, like the volcano approach, where you’re calling get next, get next, get next, the call stack down between the query plan, that’s the century, the state of where things are being executed.</p>
<p>209<br>00:17:50,000 –&gt; 00:18:01,000<br>So if you reach the bottom to a leaf node and that leaf node wants to go get remote data, you have no way to like pause it, unwind it, go back up the stack, and then maybe call down another pipeline.</p>
<p>210<br>00:18:01,000 –&gt; 00:18:11,000<br>Basically, the control flow of the execution of the query plan is implicitly within that call stack.</p>
<p>211<br>00:18:11,000 –&gt; 00:18:23,000<br>So if they switch to a push based model, then now you have the centralized scheduler using morsels that I can say are these are the pipelines or tasks that I can run right now, go ahead and run them.</p>
<p>212<br>00:18:23,000 –&gt; 00:18:29,000<br>And here’s the ones where I know I’m waiting for IO for whatever reason, and we go, we pause them and wait.</p>
<p>213<br>00:18:29,000 –&gt; 00:18:44,000<br>So for them, they found it is a not just in terms of performance, but in terms of the engineering of not assuming the data is always readily available, switching to a push based model, trying to be a much better approach for them.</p>
<p>214<br>00:18:44,000 –&gt; 00:18:58,000<br>And the great thing about it is you can read the actual PR or Mark, the co-creators of it, added the push based model, and now talks about all the great things they were able to achieve with it.</p>
<p>215<br>00:18:59,000 –&gt; 00:19:03,000<br>What’s wrong?</p>
<p>216<br>00:19:03,000 –&gt; 00:19:05,000<br>What’s that?</p>
<p>217<br>00:19:05,000 –&gt; 00:19:06,000<br>It’s getting ours as huge.</p>
<p>218<br>00:19:06,000 –&gt; 00:19:11,000<br>Yeah, I mean, because they ripped out all the pull based stuff and switched to a push based stuff.</p>
<p>219<br>00:19:11,000 –&gt; 00:19:12,000<br>Who are you?</p>
<p>220<br>00:19:12,000 –&gt; 00:19:13,000<br>I don’t know.</p>
<p>221<br>00:19:13,000 –&gt; 00:19:16,000<br>Anyway.</p>
<p>222<br>00:19:16,000 –&gt; 00:19:19,000<br>Probably a bunch of people are coming for it.</p>
<p>223<br>00:19:19,000 –&gt; 00:19:21,000<br>There’s only just a little wallet on screen.</p>
<p>224<br>00:19:21,000 –&gt; 00:19:23,000<br>They probably did all the ones.</p>
<p>225<br>00:19:23,000 –&gt; 00:19:25,000<br>Mark’s amazing.</p>
<p>226<br>00:19:25,000 –&gt; 00:19:30,000<br>I think in this point, 2021, I think he was still a postdoc.</p>
<p>227<br>00:19:30,000 –&gt; 00:19:34,000<br>The Dutch model is kind of weird as a teacher student.</p>
<p>228<br>00:19:34,000 –&gt; 00:19:36,000<br>CWI is not a university.</p>
<p>229<br>00:19:36,000 –&gt; 00:19:41,000<br>It’s like a research consortium of a bunch of other universities.</p>
<p>230<br>00:19:41,000 –&gt; 00:19:46,000<br>So technically, all the people there, like Peter Bonds, Hannes, Martin, he’s died.</p>
<p>231<br>00:19:46,000 –&gt; 00:19:54,000<br>They had affiliations with other universities, but they didn’t teach classes really, and they just wrote code at CWI.</p>
<p>232<br>00:19:54,000 –&gt; 00:20:02,000<br>Again, this is one of the pros and cons of the American academic model versus the European one.</p>
<p>233<br>00:20:02,000 –&gt; 00:20:07,000<br>In case of German, the Germans, they get six free PG students a year.</p>
<p>234<br>00:20:07,000 –&gt; 00:20:14,000<br>Each PG student here costs me $120,000 to $1,000 a year.</p>
<p>235<br>00:20:14,000 –&gt; 00:20:19,000<br>So they’re getting a ton of money and their top students that can write a ton of code for them.</p>
<p>236<br>00:20:19,000 –&gt; 00:20:24,000<br>Whereas it’s hard to scale up at the same level.</p>
<p>237<br>00:20:24,000 –&gt; 00:20:26,000<br>CWI is the same sort of thing.</p>
<p>238<br>00:20:26,000 –&gt; 00:20:30,000<br>Mark basically did a wrote code full time on duck DB.</p>
<p>239<br>00:20:30,000 –&gt; 00:20:34,000<br>It doesn’t surprise me that he did this entire PR.</p>
<p>240<br>00:20:34,000 –&gt; 00:20:36,000<br>It means he’s also very smart.</p>
<p>241<br>00:20:36,000 –&gt; 00:20:38,000<br>I’m not surprised that he did it.</p>
<p>242<br>00:20:38,000 –&gt; 00:20:44,000<br>He’s just saying he had the opportunity to do this, even though he’s a PG student.</p>
<p>243<br>00:20:45,000 –&gt; 00:20:58,000<br>So because now they switch to a push-based model, they talk about how this opens up a bunch of opportunities to do additional optimizations that you would be very difficult to do in a pool-based model.</p>
<p>244<br>00:20:58,000 –&gt; 00:21:01,000<br>Again, some of these, I think we talked about, but not all of them.</p>
<p>245<br>00:21:02,000 –&gt; 00:21:11,000<br>The first one is that because now you have explicit control of when you can basically pause a pipeline.</p>
<p>246<br>00:21:11,000 –&gt; 00:21:19,000<br>If you, since all the tasks are in this centralized scheduler table, if you will, or list, you say,</p>
<p>247<br>00:21:19,000 –&gt; 00:21:22,000<br>OK, well, this thing is going too fast or going too slow.</p>
<p>248<br>00:21:22,000 –&gt; 00:21:26,000<br>Let me just prevent any more tasks for that pipeline from executing.</p>
<p>249<br>00:21:26,000 –&gt; 00:21:34,000<br>So you can do things like, if my scan is running too fast, I can do that back pressure.</p>
<p>250<br>00:21:34,000 –&gt; 00:21:45,000<br>But if my scan is producing not a lot of data as it’s going up, rather than having the filter send up a bunch of data that’s not…</p>
<p>251<br>00:21:46,000 –&gt; 00:21:55,000<br>On half empty or semi-full vectors, you basically pause things, have it buffer the output between the filter and the aggregate.</p>
<p>252<br>00:21:55,000 –&gt; 00:21:59,000<br>And then when that fills up, then you can say, OK, now you can start executing again.</p>
<p>253<br>00:21:59,000 –&gt; 00:22:02,000<br>We talked a little bit about the mediated or lack of operator fusion.</p>
<p>254<br>00:22:02,000 –&gt; 00:22:04,000<br>We talked about vectorization query compilation.</p>
<p>255<br>00:22:04,000 –&gt; 00:22:13,000<br>This is the paper that we wrote where in order to maximize the vectorization between operators in a push-based model, you have a little buffer in between.</p>
<p>256<br>00:22:13,000 –&gt; 00:22:19,000<br>So they can introduce that, but then not just fill it up and then pass it along, they can say, OK, well, this thing’s not full yet.</p>
<p>257<br>00:22:19,000 –&gt; 00:22:24,000<br>You’re allowed to keep executing already. It is full. Let me go ahead and pause you.</p>
<p>258<br>00:22:24,000 –&gt; 00:22:29,000<br>They can also do scan sharing because all the query plans are DAGs.</p>
<p>259<br>00:22:29,000 –&gt; 00:22:39,000<br>So you could have your scan start producing results, fill up some buffers that you can then reuse for this parent operator and this parent operator.</p>
<p>260<br>00:22:39,000 –&gt; 00:22:45,000<br>Again, in a pull-based model, if you’re calling getnext, getnext, every child has to have one parent.</p>
<p>261<br>00:22:45,000 –&gt; 00:22:51,000<br>So how would you pass along that information? It would have to be this weird, like, sideways information passing.</p>
<p>262<br>00:22:51,000 –&gt; 00:22:57,000<br>So in this case here, they just fill the buffer up and say, OK, well, the buffer is full. This parent task can run.</p>
<p>263<br>00:22:57,000 –&gt; 00:23:03,000<br>OK, before I throw away the intermediate result from this child, then you go and invoke the other one.</p>
<p>264<br>00:23:03,000 –&gt; 00:23:08,000<br>And again, through the centralized coordinator, you can turn these things on and off as needed.</p>
<p>265<br>00:23:10,000 –&gt; 00:23:24,000<br>And then the last one is, if you recognize that the operators on the top of the query plan can’t consume or process the data you’re passing up quickly as possible, you can just pause things.</p>
<p>266<br>00:23:24,000 –&gt; 00:23:32,000<br>So you can introduce a buffer here that says, OK, when this thing gets more than 10 megabytes, even though I have more memory, I could keep going.</p>
<p>267<br>00:23:33,000 –&gt; 00:23:40,000<br>Rather than letting balloon indefinitely, I just can pause the whole pipeline and don’t let any more tasks execute for it.</p>
<p>268<br>00:23:40,000 –&gt; 00:23:46,000<br>The other one is also super useful, is like, because again, you’re reading remote data over HTTP.</p>
<p>269<br>00:23:46,000 –&gt; 00:23:59,000<br>Instead of having this entire task just paused while you’re thread being blocked while you’re fetching this, you do the background IO, fill up some buffer, and then when the data is available, then you can kick off the execution again.</p>
<p>270<br>00:24:00,000 –&gt; 00:24:08,000<br>Think about how you would do that with GetNext. I call GetNext down here. They make a remote call at HTTP to go get some data.</p>
<p>271<br>00:24:08,000 –&gt; 00:24:17,000<br>And then I need to go back up and say, OK, I don’t have the data you’re looking for, but call me back again when it’s actually available. You have to make another GetNext call.</p>
<p>272<br>00:24:17,000 –&gt; 00:24:26,000<br>It gets really awkward and weird. Whereas the push-based model, because now the control flow and the data flow are separate, this makes us all much easier to do.</p>
<p>273<br>00:24:30,000 –&gt; 00:24:35,000<br>The next thing that’s interesting is how they, what their intermediate result vectors look like.</p>
<p>274<br>00:24:35,000 –&gt; 00:24:45,000<br>So there’s the data, obviously, on disk. We’ll talk about it in a second. That’s going to be more heavily compressed, because you want to reduce the size of the data itself.</p>
<p>275<br>00:24:45,000 –&gt; 00:24:54,000<br>But once everything’s in memory, when you’re going from one operator to the next, they want to do a lightweight encoding similar to what we talked about before to pass data from one operator to the next.</p>
<p>276<br>00:24:55,000 –&gt; 00:25:01,000<br>And they’re basically going to have four vector types that are specialized or three of them are specialized to different types of data.</p>
<p>277<br>00:25:01,000 –&gt; 00:25:11,000<br>So without any compression, they call the vector on compressed or flat, and it’s just the listing in a columnar order of the values.</p>
<p>278<br>00:25:11,000 –&gt; 00:25:21,000<br>But if they recognize that within the vector you’re passing along, it only has one value. Then rather than passing one of that repeated value over and over again, they can have what’s called a constant vector.</p>
<p>279<br>00:25:22,000 –&gt; 00:25:28,000<br>And it’s just a single value that says this entire vector has 1,000 tuples, and they all have one single value.</p>
<p>280<br>00:25:28,000 –&gt; 00:25:37,000<br>They have a dictionary vector like we talked about before, but the selection vector that says what offset in the dictionary corresponds to the data you’re looking for.</p>
<p>281<br>00:25:37,000 –&gt; 00:25:47,000<br>And they have what they call as a sequence vector. This is basically some variation of a special case of delta encoding where you have the starting values, the base.</p>
<p>282<br>00:25:48,000 –&gt; 00:25:52,000<br>And then you just say for every single value that comes after that, incremented by this amount.</p>
<p>283<br>00:25:52,000 –&gt; 00:25:54,000<br>So you have the other case.</p>
<p>284<br>00:25:54,000 –&gt; 00:26:01,000<br>Sometimes you have other increment keys as the primary key for a column error or sequence. It’s like 1,2,3,4,5,6,7,8,9,10.</p>
<p>285<br>00:26:01,000 –&gt; 00:26:08,000<br>If you recognize that, you just need to store two values to say, here’s the base and here’s how it’s being incremented.</p>
<p>286<br>00:26:09,000 –&gt; 00:26:19,000<br>So again, they’ll figure this out on the fly while you’re actually processing the data between what operative the next, which of these versions you can use.</p>
<p>287<br>00:26:19,000 –&gt; 00:26:23,000<br>And then the default has always fall back to flat because it’s just the simplest one.</p>
<p>288<br>00:26:23,000 –&gt; 00:26:29,000<br>So their, actually in memory layout is, they actually designed this in conjunction with Velox.</p>
<p>289<br>00:26:29,000 –&gt; 00:26:33,000<br>So they’re actually compatible with the vectors that Velox passes around.</p>
<p>290<br>00:26:33,000 –&gt; 00:26:41,000<br>And again, at a high level, it smells like arrow. But my understanding it’s not exactly completely 100% in the memory layout isn’t 100% compatible.</p>
<p>291<br>00:26:43,000 –&gt; 00:27:02,000<br>So now the challenge though is that because it’s a pre-compiled primitives, if you have all these different variations of these different vector types, for all the possible combinations of data types you could have, now you have this comment-torn explosion of the number of possible primitives you would actually need.</p>
<p>292<br>00:27:03,000 –&gt; 00:27:13,000<br>And even if you template it everything, now this is going to balloon up your code base when it gets compiled, which makes a compilation slower, but also expands the size of the code in memory.</p>
<p>293<br>00:27:13,000 –&gt; 00:27:16,000<br>It makes the process more heavier.</p>
<p>294<br>00:27:16,000 –&gt; 00:27:28,000<br>So what they want to do for three of these vector types is that they want to get them into what they call a unified format, where there is a, now you have a single primitive that knows how to do whatever processing you need,</p>
<p>295<br>00:27:28,000 –&gt; 00:27:33,000<br>or in that data without having to do any conversion or memory copying.</p>
<p>296<br>00:27:33,000 –&gt; 00:27:38,000<br>So for the flat one, it’s super easy because it’s just exactly the same.</p>
<p>297<br>00:27:38,000 –&gt; 00:27:49,000<br>But then now they’re going to add this selection vector just to say, here’s the data, and then for the, you know, here’s the offset that corresponds to the data that the tuple that offset matches for.</p>
<p>298<br>00:27:49,000 –&gt; 00:27:52,000<br>Right? That looks a lot like dictionary encoding.</p>
<p>299<br>00:27:53,000 –&gt; 00:27:57,000<br>So this is actually how the represent things in memory and pass things along.</p>
<p>300<br>00:27:57,000 –&gt; 00:28:02,000<br>And then now they don’t have to decode it if they’re matching up against another dictionary one.</p>
<p>301<br>00:28:02,000 –&gt; 00:28:10,000<br>For constant same thing, right? It’s basically dictionary coding. I have, that’s backwards. Sorry.</p>
<p>302<br>00:28:10,000 –&gt; 00:28:20,000<br>Yeah, sorry, that’s constant. The constant one, again, it’s just, I have a single value as my data as it is as if it was a dictionary, and then here’s the selection vector, which is all zeros because they’re all pulling into the same one.</p>
<p>303<br>00:28:20,000 –&gt; 00:28:23,000<br>And then the dictionary one, it’s basically the same thing.</p>
<p>304<br>00:28:23,000 –&gt; 00:28:35,000<br>So again, like even though they have different compression schemes for, for these different, three different vector types, the sequence ones, I think they always have to unroll into the flat one.</p>
<p>305<br>00:28:35,000 –&gt; 00:28:42,000<br>But all this looks the same now, and then you have a primitive that operates exactly on this data, and you don’t have to do any extra memory copying.</p>
<p>306<br>00:28:42,000 –&gt; 00:28:49,000<br>And you get the benefit of like, oh, if it’s constant, you know, you’re passing along less data than before.</p>
<p>307<br>00:28:49,000 –&gt; 00:28:58,000<br>So this is what they call the unified, unified vector format. And again, this is for the intimate results that are going from one operator to the next.</p>
<p>308<br>00:28:58,000 –&gt; 00:28:59,000<br>But the increment one?</p>
<p>309<br>00:28:59,000 –&gt; 00:29:08,000<br>The increment one, I think they have to unroll it. But I might, that might be a year out of date. I haven’t looked.</p>
<p>310<br>00:29:08,000 –&gt; 00:29:23,000<br>So the other interesting that that we’re talking about, Dr. B, the way I haven’t really talked about before, is how they enter the, they can work with this, you know, the non-seco ecosystem that data scientists are coming using.</p>
<p>311<br>00:29:23,000 –&gt; 00:29:36,000<br>And so, you know, if you’re being like Python Pandas, people operating on data frames, and they want to, you know, data frames provides this API to do the manipulations, but at a high level, it basically looks like relational algebra.</p>
<p>312<br>00:29:36,000 –&gt; 00:29:56,000<br>And so what they want to be able to do is for people that can link in, or, you know, instantiate Dr. B within their Python or our program, write to some common API based on data frames, and then have that get translated into the corresponding SQL command that can then retrieve the data that you’re looking for.</p>
<p>313<br>00:29:57,000 –&gt; 00:30:08,000<br>And so there’s, they say support two different libraries, one for R, one for Python, one’s called Deplier Piler, and it goes, Ibus, Ibus was developed by the guy in the Apache Arrow, I forget where Deplier came from.</p>
<p>314<br>00:30:08,000 –&gt; 00:30:17,000<br>Did anybody ever heard of these before or no? No. Let me show you what it looks like.</p>
<p>315<br>00:30:17,000 –&gt; 00:30:33,000<br>But basically, it’s a, again, it’s a, it looks like if you ever use like Spark, PySports and things like that, right, it’s going to have, you know, APIs that manipulate data frames like this, right, and sort of like this, the fashion things get the head, right, and then this is the output.</p>
<p>316<br>00:30:33,000 –&gt; 00:30:40,000<br>So again, it’s like the procedural language to manipulate data frames, which are just basically relations or tables in a database.</p>
<p>317<br>00:30:40,000 –&gt; 00:30:45,000<br>And then would Deplier, sorry.</p>
<p>318<br>00:30:50,000 –&gt; 00:30:51,000<br>What’s wrong? Yes.</p>
<p>319<br>00:30:51,000 –&gt; 00:31:00,000<br>The difference something that is still on the Apache Arrow is the other way around, the Apache Arrow integrated data frames.</p>
<p>320<br>00:31:00,000 –&gt; 00:31:04,000<br>So question is, is data frames like working for?</p>
<p>321<br>00:31:04,000 –&gt; 00:31:13,000<br>So data frames came from pandas, I think, and the guy invented pandas also invented Arrow.</p>
<p>322<br>00:31:13,000 –&gt; 00:31:26,000<br>Yeah, so that’s the connection there. But like the idea, like through like the pandas API or Deplier Piler, Ivis, like you can manipulate what you think are data frames and memory.</p>
<p>323<br>00:31:26,000 –&gt; 00:31:30,000<br>That could be though in the Apache Arrow format.</p>
<p>324<br>00:31:30,000 –&gt; 00:31:34,000<br>Because the whole point is like, like again, going back to that paper guy’s red, don’t hold my data hostage.</p>
<p>325<br>00:31:34,000 –&gt; 00:31:42,000<br>Like they make a big deal, like, okay, well, if you just use JDBC to go run some query on a database, you’re going to get it back back a bunch of rows.</p>
<p>326<br>00:31:42,000 –&gt; 00:31:49,000<br>And then if you’re using like pandas or some Python program, then that’s kind of then do a pivot convert it now to a column store.</p>
<p>327<br>00:31:49,000 –&gt; 00:31:59,000<br>So what you really want to do is hand things off as arrow, and then now your Python code or our code can manipulate directly on those arrow buffers, even though you’re still operating on the data frame API.</p>
<p>328<br>00:31:59,000 –&gt; 00:32:08,000<br>So that’s the idea with these integrations is that I mean the deep pile of the one has it.</p>
<p>329<br>00:32:08,000 –&gt; 00:32:13,000<br>First go down, like you see that they have these basic primitives.</p>
<p>330<br>00:32:13,000 –&gt; 00:32:17,000<br>And then they have different back ends and then here’s the one for for ducty B, right?</p>
<p>331<br>00:32:17,000 –&gt; 00:32:28,000<br>Dropping replacement for for deep pile that uses the same API. But what’s going to happen is when I write code against this thing, it’s actually not going to generate sequel.</p>
<p>332<br>00:32:28,000 –&gt; 00:32:38,000<br>Where is something to look at? Right, yeah, so here, like, that’s the data frame.</p>
<p>333<br>00:32:38,000 –&gt; 00:32:47,000<br>Here’s the query. So here’s some query here. More or less looks like some, you know, bash it as version of sequel, which is, you know, instead of where calls is called filter, right?</p>
<p>334<br>00:32:47,000 –&gt; 00:32:52,000<br>Instead of average is called mean. But at a high level, this basically looks like sequel.</p>
<p>335<br>00:32:52,000 –&gt; 00:33:09,000<br>And so what these integrations do with with ducty B is that is that converting these commands to sequel the library will convert this action to a logical plan of the internal representation that that ducty B has for queries.</p>
<p>336<br>00:33:09,000 –&gt; 00:33:18,000<br>And then as if it got parsed from from the sequel from the command line or whatever, and then it hands that off now to the optimizer who can then convert that to the physical plan. Yes.</p>
<p>337<br>00:33:18,000 –&gt; 00:33:25,000<br>People hate writing sequel that much. David is do people hate writing sequel that much.</p>
<p>338<br>00:33:25,000 –&gt; 00:33:38,000<br>You’re talking the wrong dude. So I mean, no, so it’s it’s similar to we saw with UDS right there’s certain things that are hard to express in in sequel.</p>
<p>339<br>00:33:38,000 –&gt; 00:33:49,000<br>Yeah, but like, yeah, so like there’s a lot of there’s a lot of data scientists that prefer to use pandas and Python APIs and Python notebooks.</p>
<p>340<br>00:33:49,000 –&gt; 00:34:01,000<br>And certainly for some things, like it doesn’t make sense to run that database. Like if you’re going to call it PyTorch stuff, like it may not make make sense to have that run the database system could run that locally or farm it out to.</p>
<p>341<br>00:34:01,000 –&gt; 00:34:22,000<br>To like spark and actually that’s one of the one of the one of the advantages of like data bricks, although snowflake has snowpark. Forget what the Google one is like this this single environment where you could do run your sequel query on the overlap engine plus also run like scale out machine learning jobs all together instead of one interface.</p>
<p>342<br>00:34:22,000 –&gt; 00:34:34,000<br>That’s that’s very common now, but it’s also very common like you know for this if you’re running a one off experiments just to download the whole file locally crunch on it using Python and then maybe upload results or handle somebody else.</p>
<p>343<br>00:34:34,000 –&gt; 00:34:40,000<br>So the idea here is now like instead of having like the pandas runtime is actually very slow.</p>
<p>344<br>00:34:40,000 –&gt; 00:34:57,000<br>But instead of you so now if you want to manipulate data frames if you put your data in like park a files then like duck DB do the crunching of the park a files and set a pandas itself and you get all the advantages of modern OLAP system that we’ve talked about in the entire semester directly in your Python notebook.</p>
<p>345<br>00:34:57,000 –&gt; 00:35:01,000<br>So that that’s that’s the idea here.</p>
<p>346<br>00:35:01,000 –&gt; 00:35:21,000<br>And again the zero copy is the big idea this is what Apache arrow sort of this is what the was the ritual foundation or the motivation of Apache arrow that you could pass around data between your disparate applications running the same same address base without having to do serialization decilization.</p>
<p>347<br>00:35:21,000 –&gt; 00:35:44,000<br>Now as I said it’s basically being used as the transport protocol or format between different nodes running in a system right so but in this case here can duck DB’s in process you can get data in and out of duck DB back and forth between duck DB and in Python are through these different APIs through all Apache arrow.</p>
<p>348<br>00:35:44,000 –&gt; 00:35:50,000<br>Because again same address base if I malock in duck DB hand you some buffer.</p>
<p>349<br>00:35:50,000 –&gt; 00:36:01,000<br>You know how I actually keep track of who has to free it that’s a separate thing but like I don’t have to do a mem copy to be able to manipulate it in Python.</p>
<p>350<br>00:36:01,000 –&gt; 00:36:08,000<br>Duck DB also supports the execution of substrate plans as well but I think that’s done through that’s done through an extension.</p>
<p>351<br>00:36:08,000 –&gt; 00:36:24,000<br>So I think you can take a substrate serialized plan and then handle the duck DB just as if it was a logical plan generated by one of these guys and then it’ll convert it to its physical plan that that can then execute.</p>
<p>352<br>00:36:25,000 –&gt; 00:36:45,000<br>So this thing I mean it goes back to the very beginning we were talking about before like that red post like quality of life things or like the ease of use like if I have a bunch of Python notebooks and I want to use some data that I have stored out in in my object store but I want to run them in you know like an old that engine or something like that.</p>
<p>353<br>00:36:46,000 –&gt; 00:36:57,000<br>If I can reduce the friction how much manipulation I have to do for the existing code that the better user experience is that I’m saying okay rewrite all your Python stuff to see well no one’s going to do that willingly.</p>
<p>354<br>00:36:57,000 –&gt; 00:37:11,000<br>So I’ve already sort of been into multiple times that like duck DB does support reading a bunch of different file formats that we’ve covered but it also has its own proprietary custom file format.</p>
<p>355<br>00:37:11,000 –&gt; 00:37:26,000<br>Similar to like you know if you create you know open up SQL light you call create table it’ll write out the dot db dot SQL life file you’ll get the same thing duck DB has his own proprietary file format is meant to run as a single generate everything is a single file.</p>
<p>356<br>00:37:26,000 –&gt; 00:37:41,000<br>Now when you do update on it just like in SQL light they’ll maintain a maintain a right-of-hand log as a separate file and for temp files or temp data that gets built to separate files as well but the core database itself that you attached to is always going to be a single file.</p>
<p>357<br>00:37:42,000 –&gt; 00:38:01,000<br>So no surprise it’s going to be packs their row group is set to be 120,000 tuples and important thing to understand is that there to be more aggressive with the compression schemes that they’re going to use when it goes to disk versus once in memory in particular they’re going to do bit packing or the frame of reference optimizations that we talked about before.</p>
<p>358<br>00:38:02,000 –&gt; 00:38:30,000<br>So when you actually start writing data at the disk I think you do much inserts are copying into drb that then writes out to the database they’re going to do something sort of similar we told with better blocks where they’ll have a sort of an initial pass to look at the data figure out what it looks like use some kind of ranking algorithm to decide which is the best and most efficient encoding scheme for for that data and then once they figured out then they compress it and then write it out the disk and they’re going to do this on a per column basis.</p>
<p>359<br>00:38:30,000 –&gt; 00:38:59,000<br>I think within a row group itself. So this table is a bit at a date this is from 2022 but you can see over the years over the different versions when they’ve added different compression schemes and even the latest one I think this came out in March they have they had the latest compression scheme from another project at CWI called Alps that we didn’t cover for floating point numbers but you can sort of see how the compare against like you know base compression in parquet.</p>
<p>360<br>00:39:00,000 –&gt; 00:39:06,000<br>With snappy as standard you know for the different light weight encoding schemes.</p>
<p>361<br>00:39:06,000 –&gt; 00:39:14,000<br>Yes sorry the columns are these are different like well known data sets of benchmarks so the tax.</p>
<p>362<br>00:39:14,000 –&gt; 00:39:28,000<br>What’s that you line up light item is tch tbch taxi is the New York City taxi database it’s like every single taxi every single taxi ride over like a one year period like and like what you know.</p>
<p>363<br>00:39:28,000 –&gt; 00:39:37,000<br>The pick up in the drop of location and then on time I think that’s a flight data but I’m not sure.</p>
<p>364<br>00:39:37,000 –&gt; 00:39:49,000<br>But again this is this is the own disc format they only have those four vector types once everything is in memory because you want that to be that processing to go as fast as possible.</p>
<p>365<br>00:39:49,000 –&gt; 00:40:00,000<br>So so in addition to be a support again reading their own proprietary file format on the local disk as I said they can read data in other file formats.</p>
<p>366<br>00:40:00,000 –&gt; 00:40:05,000<br>Parquet is not surprising I don’t know the I don’t think they support orc.</p>
<p>367<br>00:40:05,000 –&gt; 00:40:08,000<br>I don’t see it here.</p>
<p>368<br>00:40:08,000 –&gt; 00:40:19,000<br>Obviously can support arrow another cool thing they can do is they can attach to a SQL light database and actually read that directly and that can manipulate it directly.</p>
<p>369<br>00:40:19,000 –&gt; 00:40:27,000<br>And you know you attach to the to the database and you see it as if it was a you see the catalog you see all the scheme and everything.</p>
<p>370<br>00:40:27,000 –&gt; 00:40:34,000<br>You know from your from your perspective you don’t see that you don’t know that it’s actually SQL like data SQL like database versus a.</p>
<p>371<br>00:40:34,000 –&gt; 00:40:38,000<br>versus a ducty B database.</p>
<p>372<br>00:40:38,000 –&gt; 00:40:44,000<br>They also support obviously reading JSON and then the for the postgres one day I think they connect to the postgres.</p>
<p>373<br>00:40:44,000 –&gt; 00:40:54,000<br>Over like JDBC but again it sucks in all the catalogs or within ducty B you can see all the tables you have in in postgres and run queries on them.</p>
<p>374<br>00:40:54,000 –&gt; 00:41:03,000<br>But I don’t know how much is like they they push and pull between pushing down the query into postgres versus sucking the data in and running more quickly inside a ducty B.</p>
<p>375<br>00:41:03,000 –&gt; 00:41:11,000<br>Like because the ducty B query engines is going to be way faster than then postgres for doing analytics.</p>
<p>376<br>00:41:11,000 –&gt; 00:41:21,000<br>So these are the if you call from it’s hard to see it from ducty B extensions like this will give you all the listing of the official built in ones and whether they’re learning or not.</p>
<p>377<br>00:41:21,000 –&gt; 00:41:35,000<br>And as I was saying before they’re trying to minimize the the sort of the size of the core engine binary itself for not bringing in additional binaries or additional code.</p>
<p>378<br>00:41:35,000 –&gt; 00:41:40,000<br>But they if you do need it then you get it through these extensions.</p>
<p>379<br>00:41:40,000 –&gt; 00:41:50,000<br>One in particular is is ICU like that thing is super important that’s like the international like timestamps and time zones and things like that and date formats.</p>
<p>380<br>00:41:50,000 –&gt; 00:41:55,000<br>That one you don’t want to write yourself the Germans did I’ll talk about that in a second.</p>
<p>381<br>00:41:55,000 –&gt; 00:42:01,000<br>But like so but a lot of people need it so like you get it through one of these extensions because it’s going to be a third party library.</p>
<p>382<br>00:42:01,000 –&gt; 00:42:11,000<br>The Germans told me that one time they were going to do a sales call this is back when it was hyper and they needed the customer they were living on the plane flying to meet some customer.</p>
<p>383<br>00:42:11,000 –&gt; 00:42:18,000<br>I don’t know where in Europe and they were running queries with the ICU library and it was super slow.</p>
<p>384<br>00:42:18,000 –&gt; 00:42:26,000<br>So then Thomas within the flight without internet rewrote the ICU library on the flight and they had it you know ten times faster when they landed.</p>
<p>385<br>00:42:26,000 –&gt; 00:42:38,000<br>The same right so the one I want to point out though here is this one called mother duck and again this is like you just download duck duck db and you call show extensions these are all the things you can you can get.</p>
<p>386<br>00:42:38,000 –&gt; 00:42:45,000<br>And I think some of these again they’re not they’re not shipped in the binary you say load them it’ll pull them down from the duck to the website.</p>
<p>387<br>00:42:45,000 –&gt; 00:42:52,000<br>But this one here again this duck to be so this comes along the same called mother duck. So what is that yes.</p>
<p>388<br>00:42:52,000 –&gt; 00:43:07,000<br>How does the extension work? How do you load these extensions? It’s shared objects you call load and then load a create extension.</p>
<p>389<br>00:43:07,000 –&gt; 00:43:14,000<br>And then you want and then I think it will pull down for you from the one.</p>
<p>390<br>00:43:14,000 –&gt; 00:43:23,000<br>I think when you call create extension yes. I think because I think the binary you know when you download it’s not that big.</p>
<p>391<br>00:43:23,000 –&gt; 00:43:27,000<br>There’s not like a new style of creating something like you.</p>
<p>392<br>00:43:27,000 –&gt; 00:43:36,000<br>Yeah correct yes. I think actually I think some of them are shipped with it but they’re not.</p>
<p>393<br>00:43:36,000 –&gt; 00:43:44,000<br>They’re not they’re not loading automatically. I mean some of these say built in and that would be equivalent to like in Postgres within the Postgres</p>
<p>394<br>00:43:44,000 –&gt; 00:43:51,000<br>Source tree they have a contrib directory and that has like official third party extensions that are shipped with Postgres itself.</p>
<p>395<br>00:43:51,000 –&gt; 00:43:57,000<br>But obviously things like PG vector you just download that from GitHub and install yourself. So it’s similar model.</p>
<p>396<br>00:43:57,000 –&gt; 00:44:06,000<br>Yeah you can of course you know that Postgres too. It’s just linking in a shared object.</p>
<p>397<br>00:44:06,000 –&gt; 00:44:11,000<br>And then that shared object has to implement this kind of extension API to note the entry point when you want to book something.</p>
<p>398<br>00:44:11,000 –&gt; 00:44:21,000<br>Yeah that’s yes. The question is like they may be asking like when you download duck to be and get the executor doesn’t come with these built in.</p>
<p>399<br>00:44:21,000 –&gt; 00:44:25,000<br>I think some of them yes other ones I think you get from their website.</p>
<p>400<br>00:44:25,000 –&gt; 00:44:32,000<br>Is there actually is at least one extension that can be loads or one that’s one that overwrite the malloc implementation.</p>
<p>401<br>00:44:32,000 –&gt; 00:44:41,000<br>Overwrite the what sorry. The malloc implementation to give you a malloc. Yeah but that’s that comes with it automatically.</p>
<p>402<br>00:44:41,000 –&gt; 00:44:45,000<br>I’m sorry we don’t actually do not say that.</p>
<p>403<br>00:44:45,000 –&gt; 00:44:50,000<br>We don’t only talk about malloc. You never want to use lip see malloc for your database system.</p>
<p>404<br>00:44:50,000 –&gt; 00:44:55,000<br>You always almost always want to use J email. In rare cases maybe you maybe want to use TC malloc.</p>
<p>405<br>00:44:55,000 –&gt; 00:45:04,000<br>And that one’s from Google J email is from Facebook. It’s just like it’s it’s it’s just way more efficient.</p>
<p>406<br>00:45:04,000 –&gt; 00:45:11,000<br>It’s less it’s more scalable for multiple cores like it takes less the latches are less expensive.</p>
<p>407<br>00:45:11,000 –&gt; 00:45:18,000<br>TC malloc is thought to be better for for multi-fitted applications. If you have a lot of cores.</p>
<p>408<br>00:45:18,000 –&gt; 00:45:26,000<br>But J email is always the right choice. So pretty much everyone uses uses this in their data system.</p>
<p>409<br>00:45:26,000 –&gt; 00:45:30,000<br>We didn’t talk about huge pages. That’s another one that most assessments don’t do that.</p>
<p>410<br>00:45:30,000 –&gt; 00:45:34,000<br>But you never want to use transparent huge pages in the OS. That’s always a nightmare.</p>
<p>411<br>00:45:34,000 –&gt; 00:45:40,000<br>But I think it’s gotten better. But we can talk about it. If people are curious about these things we can talk about a bit more.</p>
<p>412<br>00:45:40,000 –&gt; 00:45:43,000<br>But we’re going to do this use gmail. Yes.</p>
<p>413<br>00:45:43,000 –&gt; 00:45:49,000<br>What’s the problem with lip see what’s the big one? What’s the big problem with the lip see malloc?</p>
<p>414<br>00:45:49,000 –&gt; 00:45:53,000<br>Yeah, it’s too many latches inside of it.</p>
<p>415<br>00:45:53,000 –&gt; 00:45:54,000<br>No, not a lot.</p>
<p>416<br>00:45:54,000 –&gt; 00:46:00,000<br>J email is basically designed for like taking lightweight latches. Small critical section is to be the scale of multiple cores.</p>
<p>417<br>00:46:00,000 –&gt; 00:46:02,000<br>And you use it like.</p>
<p>418<br>00:46:02,000 –&gt; 00:46:03,000<br>It’s not.</p>
<p>419<br>00:46:03,000 –&gt; 00:46:09,000<br>It’s a question. Why is there any reason why it’s not the default in Linux?</p>
<p>420<br>00:46:09,000 –&gt; 00:46:12,000<br>That’s why we didn’t want to use malloc.</p>
<p>421<br>00:46:12,000 –&gt; 00:46:17,000<br>The jmailoc is not written explicitly for databases.</p>
<p>422<br>00:46:17,000 –&gt; 00:46:19,000<br>Because all the data is used because it’s much better.</p>
<p>423<br>00:46:19,000 –&gt; 00:46:22,000<br>It pretty much any high performance application is going to use jmailoc.</p>
<p>424<br>00:46:22,000 –&gt; 00:46:24,000<br>And why it has not.</p>
<p>425<br>00:46:24,000 –&gt; 00:46:26,000<br>Can you see?</p>
<p>426<br>00:46:26,000 –&gt; 00:46:28,000<br>I think of a form of form.</p>
<p>427<br>00:46:28,000 –&gt; 00:46:30,000<br>I have no idea.</p>
<p>428<br>00:46:30,000 –&gt; 00:46:39,000<br>Licensing may be licensing issues. Maybe this is like MIT and it has a GPL or whatever.</p>
<p>429<br>00:46:39,000 –&gt; 00:46:44,000<br>Yeah, there’s a paper we didn’t read this year.</p>
<p>430<br>00:46:44,000 –&gt; 00:46:49,000<br>There’s a system called scuba at Facebook. It was in memory database.</p>
<p>431<br>00:46:49,000 –&gt; 00:46:53,000<br>And one of the things they do is they want to do rolling upgrades.</p>
<p>432<br>00:46:53,000 –&gt; 00:46:55,000<br>I want to restart the server.</p>
<p>433<br>00:46:55,000 –&gt; 00:47:01,000<br>Since it’s memory database, if I kill the process and start it back up, I get a little bit of the data back in.</p>
<p>434<br>00:47:01,000 –&gt; 00:47:06,000<br>So the trick they do is they write everything all the condos of memory to shared memory in the OS.</p>
<p>435<br>00:47:06,000 –&gt; 00:47:10,000<br>Kill the process, come back, reattach that shared memory and everything is there.</p>
<p>436<br>00:47:10,000 –&gt; 00:47:17,000<br>And they talk about how they tried having, because they own the employee of a person that writes jmailoc,</p>
<p>437<br>00:47:17,000 –&gt; 00:47:20,000<br>they hadn’t tried to write some tricks in jmailoc to make it better.</p>
<p>438<br>00:47:20,000 –&gt; 00:47:23,000<br>So they shared memory and restart. It was a huge nightmare.</p>
<p>439<br>00:47:23,000 –&gt; 00:47:26,000<br>For that case, they did rely on the OS to do it.</p>
<p>440<br>00:47:26,000 –&gt; 00:47:31,000<br>But so there are some optimizations that think in jmailoc, that are for data bases that are rolling.</p>
<p>441<br>00:47:31,000 –&gt; 00:47:33,000<br>Because I know Facebook puts stuff in there.</p>
<p>442<br>00:47:33,000 –&gt; 00:47:34,000<br>Yes?</p>
<p>443<br>00:47:34,000 –&gt; 00:47:37,000<br>Also, the lip-see rather because it is because it uses less memory.</p>
<p>444<br>00:47:37,000 –&gt; 00:47:39,000<br>Jmailoc is a lot of memory.</p>
<p>445<br>00:47:39,000 –&gt; 00:47:41,000<br>You just a lot of memory.</p>
<p>446<br>00:47:41,000 –&gt; 00:47:44,000<br>And then eagerly allocates ahead of time.</p>
<p>447<br>00:47:44,000 –&gt; 00:47:47,000<br>Get this used jmailoc.</p>
<p>448<br>00:47:47,000 –&gt; 00:47:49,000<br>Sorry, yes. So this mother duck thing, what is that?</p>
<p>449<br>00:47:49,000 –&gt; 00:47:59,000<br>So the, this is during the pandemic, the HANA-SEN mark we’re thinking about doing a start-up on duck DB.</p>
<p>450<br>00:47:59,000 –&gt; 00:48:04,000<br>But at least when I talk to them what they really wanted to do is just keep building duck DB,</p>
<p>451<br>00:48:04,000 –&gt; 00:48:08,000<br>you know, as is running on embedded devices and so forth.</p>
<p>452<br>00:48:08,000 –&gt; 00:48:13,000<br>And all the DCs were one of them to make a cloud version of it.</p>
<p>453<br>00:48:13,000 –&gt; 00:48:16,000<br>That end up looking something like snowflake.</p>
<p>454<br>00:48:16,000 –&gt; 00:48:23,000<br>But that would be a major rewrite and go against the ethos of the original design of duck DB.</p>
<p>455<br>00:48:23,000 –&gt; 00:48:25,000<br>So there is duck DB labs.</p>
<p>456<br>00:48:25,000 –&gt; 00:48:31,000<br>And that’s basically the spin-off of CWI that is doing most of the development on duck DB.</p>
<p>457<br>00:48:31,000 –&gt; 00:48:36,000<br>And employees, a bunch of former students, Mark and HANA, to build, you know, building duck DB.</p>
<p>458<br>00:48:36,000 –&gt; 00:48:37,000<br>And that’s where you get official.</p>
<p>459<br>00:48:37,000 –&gt; 00:48:40,000<br>If you need official support for duck DB, you contract out the them.</p>
<p>460<br>00:48:40,000 –&gt; 00:48:50,000<br>Then year two ago there was a spin-off, there was a startup that was created called MotherDuck,</p>
<p>461<br>00:48:50,000 –&gt; 00:48:53,000<br>to provide a cloud version of duck DB.</p>
<p>462<br>00:48:53,000 –&gt; 00:49:00,000<br>But again, it’s not a scalable, you know, scale a version of duck DB like a snowflake or a dremel.</p>
<p>463<br>00:49:00,000 –&gt; 00:49:06,000<br>Instead, it’s more like a remote compute option that you can get now in duck DB,</p>
<p>464<br>00:49:06,000 –&gt; 00:49:09,000<br>where you still run duck DB locally.</p>
<p>465<br>00:49:09,000 –&gt; 00:49:14,000<br>But if your data is already in the cloud, you basically can run it query locally</p>
<p>466<br>00:49:14,000 –&gt; 00:49:18,000<br>that then gets shipped over the wire to MotherDuck, who’s running duck DB there,</p>
<p>467<br>00:49:18,000 –&gt; 00:49:23,000<br>and do some processing and then send back the result to you.</p>
<p>468<br>00:49:23,000 –&gt; 00:49:24,000<br>All right.</p>
<p>469<br>00:49:24,000 –&gt; 00:49:30,000<br>So again, going back here, when you download duck DB, this comes along with it.</p>
<p>470<br>00:49:30,000 –&gt; 00:49:33,000<br>This is official MotherDuck extension.</p>
<p>471<br>00:49:34,000 –&gt; 00:49:39,000<br>And so that means everybody’s running duck DB now can connect directly to MotherDuck,</p>
<p>472<br>00:49:39,000 –&gt; 00:49:41,000<br>assuming they have account and API key.</p>
<p>473<br>00:49:41,000 –&gt; 00:49:46,000<br>And so, the MotherDuck sends down to the local duck DB, like here’s the catalog,</p>
<p>474<br>00:49:46,000 –&gt; 00:49:48,000<br>everything, all the files that I’m available.</p>
<p>475<br>00:49:48,000 –&gt; 00:49:52,000<br>And then I can write queries on them as if it was a local file.</p>
<p>476<br>00:49:52,000 –&gt; 00:49:56,000<br>And the system figures out what part of the query needs to run in the cloud,</p>
<p>477<br>00:49:56,000 –&gt; 00:49:59,000<br>what part of the query needs to run locally.</p>
<p>478<br>00:49:59,000 –&gt; 00:50:00,000<br>Yes.</p>
<p>479<br>00:50:00,000 –&gt; 00:50:05,000<br>Why would you come and use this instead of any other all-app system?</p>
<p>480<br>00:50:05,000 –&gt; 00:50:08,000<br>It’s a question of why would anybody use this for some of the all-app system?</p>
<p>481<br>00:50:08,000 –&gt; 00:50:11,000<br>Yeah, it’s on the, because all of them have cloud offerings.</p>
<p>482<br>00:50:13,000 –&gt; 00:50:18,000<br>But if you’re already doing much analytics locally in duck DB,</p>
<p>483<br>00:50:18,000 –&gt; 00:50:21,000<br>because it connects to Python or whatever,</p>
<p>484<br>00:50:21,000 –&gt; 00:50:25,000<br>to stop whatever you’re doing, then switch over to BigQuery,</p>
<p>485<br>00:50:25,000 –&gt; 00:50:27,000<br>and where the query might actually work anymore.</p>
<p>486<br>00:50:27,000 –&gt; 00:50:32,000<br>You might not even be running SQL anymore, because you can use an iBus or DePire.</p>
<p>487<br>00:50:32,000 –&gt; 00:50:38,000<br>So it seems to integration, like it’s still the duck DB client-side interface,</p>
<p>488<br>00:50:38,000 –&gt; 00:50:42,000<br>but you don’t know necessarily where that query is going to run anymore.</p>
<p>489<br>00:50:42,000 –&gt; 00:50:43,000<br>Yes.</p>
<p>490<br>00:50:43,000 –&gt; 00:50:47,000<br>So, to say there’s no interquerialism going on here,</p>
<p>491<br>00:50:47,000 –&gt; 00:50:50,000<br>like it wants to put some old notes at all in the cloud?</p>
<p>492<br>00:50:50,000 –&gt; 00:50:53,000<br>This question, is there any, well,</p>
<p>493<br>00:50:53,000 –&gt; 00:50:55,000<br>we’ll scale hard on like to multiple notes in the cloud.</p>
<p>494<br>00:50:55,000 –&gt; 00:50:58,000<br>As far as they know, these are term version now, right?</p>
<p>495<br>00:50:58,000 –&gt; 00:51:00,000<br>Is there a reason why not?</p>
<p>496<br>00:51:00,000 –&gt; 00:51:02,000<br>The question is the reason why not, because yeah,</p>
<p>497<br>00:51:02,000 –&gt; 00:51:06,000<br>you’d have to rewrite a lot of duck DB to make that work, right?</p>
<p>498<br>00:51:06,000 –&gt; 00:51:09,000<br>Maybe you could just, if they’re just sending the query plan up to the cloud,</p>
<p>499<br>00:51:09,000 –&gt; 00:51:12,000<br>maybe you could identify pipeline breakers.</p>
<p>500<br>00:51:12,000 –&gt; 00:51:14,000<br>Yeah, right off parts of the query plan,</p>
<p>501<br>00:51:14,000 –&gt; 00:51:16,000<br>and then just run different duck DB instances.</p>
<p>502<br>00:51:16,000 –&gt; 00:51:17,000<br>Yeah, sorry, sorry, sorry.</p>
<p>503<br>00:51:17,000 –&gt; 00:51:19,000<br>So, to this point, actually, you’re right.</p>
<p>504<br>00:51:19,000 –&gt; 00:51:20,000<br>They could do that.</p>
<p>505<br>00:51:20,000 –&gt; 00:51:21,000<br>I don’t know whether they do that though.</p>
<p>506<br>00:51:21,000 –&gt; 00:51:22,000<br>I’ll show that in the next slide.</p>
<p>507<br>00:51:22,000 –&gt; 00:51:24,000<br>Basically, they know what parts of remote and local,</p>
<p>508<br>00:51:24,000 –&gt; 00:51:29,000<br>and then the local duck DB instance is responsible for figuring out,</p>
<p>509<br>00:51:29,000 –&gt; 00:51:31,000<br>okay, like this data is remote,</p>
<p>510<br>00:51:31,000 –&gt; 00:51:33,000<br>and it’s too big for me to suck down locally,</p>
<p>511<br>00:51:33,000 –&gt; 00:51:35,000<br>so I’ll send my query, my plan fragment over there,</p>
<p>512<br>00:51:35,000 –&gt; 00:51:37,000<br>and process and get back the result.</p>
<p>513<br>00:51:37,000 –&gt; 00:51:39,000<br>So, yes, in that point, you could say,</p>
<p>514<br>00:51:39,000 –&gt; 00:51:45,000<br>okay, I could take portions of this and fan it across multiple duck DB instances.</p>
<p>515<br>00:51:45,000 –&gt; 00:51:48,000<br>I just don’t know whether they do that or not.</p>
<p>516<br>00:51:48,000 –&gt; 00:51:51,000<br>So, this is from the paper that came out this year,</p>
<p>517<br>00:51:51,000 –&gt; 00:51:54,000<br>right, the idea is that again, you have the client side, the duck DB,</p>
<p>518<br>00:51:54,000 –&gt; 00:51:57,000<br>you install that mother duck extension,</p>
<p>519<br>00:51:57,000 –&gt; 00:52:02,000<br>that then can send query plans up to the mother duck cloud service,</p>
<p>520<br>00:52:02,000 –&gt; 00:52:06,000<br>where they’re running duck DB inside of Docker containers,</p>
<p>521<br>00:52:06,000 –&gt; 00:52:11,000<br>and they’re doing client side caching similar to what we saw in,</p>
<p>522<br>00:52:11,000 –&gt; 00:52:14,000<br>I think in Dremel, sorry, sorry, in snowflake,</p>
<p>523<br>00:52:14,000 –&gt; 00:52:17,000<br>and then of course, you can always read data from your object store,</p>
<p>524<br>00:52:17,000 –&gt; 00:52:19,000<br>whatever you want.</p>
<p>525<br>00:52:19,000 –&gt; 00:52:25,000<br>And how the client’s, sorry, the mother duck service is aware of what data is in here,</p>
<p>526<br>00:52:25,000 –&gt; 00:52:32,000<br>I think they can connect to Ithberg and other things that suck out that metadata as well.</p>
<p>527<br>00:52:32,000 –&gt; 00:52:37,000<br>So, the way this is going to work is that they’re going to introduce what they call a bridge operator</p>
<p>528<br>00:52:37,000 –&gt; 00:52:43,000<br>in the query plans now that is capable of sending and receiving data</p>
<p>529<br>00:52:43,000 –&gt; 00:52:48,000<br>from the local duck DB instance to the cloud version of it.</p>
<p>530<br>00:52:48,000 –&gt; 00:52:51,000<br>And so, again, when you invoke a query on the local side,</p>
<p>531<br>00:52:51,000 –&gt; 00:52:54,000<br>it’ll do all the planning that it normally would,</p>
<p>532<br>00:52:54,000 –&gt; 00:52:56,000<br>and then with the mother duck extension installed,</p>
<p>533<br>00:52:56,000 –&gt; 00:52:58,000<br>they’ll then take a second pass on it and say,</p>
<p>534<br>00:52:58,000 –&gt; 00:53:01,000<br>okay, well, this data you’re looking, you’re trying to access in this pipeline,</p>
<p>535<br>00:53:01,000 –&gt; 00:53:03,000<br>is local, this data is remote,</p>
<p>536<br>00:53:03,000 –&gt; 00:53:06,000<br>and then they do, in a cost calculation, the side,</p>
<p>537<br>00:53:06,000 –&gt; 00:53:10,000<br>is it better to push the query to the data out on the remote storage</p>
<p>538<br>00:53:10,000 –&gt; 00:53:14,000<br>or pull the data down to the local machine?</p>
<p>539<br>00:53:14,000 –&gt; 00:53:19,000<br>So, I’m doing a join between some customer table and a sales table.</p>
<p>540<br>00:53:19,000 –&gt; 00:53:22,000<br>The customer table is remote, the sales data is local,</p>
<p>541<br>00:53:22,000 –&gt; 00:53:25,000<br>and so, to say the customer data is huge,</p>
<p>542<br>00:53:25,000 –&gt; 00:53:29,000<br>it’s typically the opposite of this, the sales one is always much, much bigger.</p>
<p>543<br>00:53:29,000 –&gt; 00:53:36,000<br>So, duck DB would say, okay, well, since I already have the customer data and that’s remote,</p>
<p>544<br>00:53:36,000 –&gt; 00:53:42,000<br>let me go send the sales data over the wire up to the remote service,</p>
<p>545<br>00:53:42,000 –&gt; 00:53:45,000<br>the remote service then, computer to the hash join,</p>
<p>546<br>00:53:45,000 –&gt; 00:53:48,000<br>using the duck DB instance that’s running on the cloud,</p>
<p>547<br>00:53:48,000 –&gt; 00:53:50,000<br>then it has to get the result back to you and the client,</p>
<p>548<br>00:53:50,000 –&gt; 00:53:55,000<br>so then they send the data back to the source and sync operators in between these things.</p>
<p>549<br>00:53:55,000 –&gt; 00:53:59,000<br>Again, now you see, they don’t have to do anything extra to support this,</p>
<p>550<br>00:53:59,000 –&gt; 00:54:04,000<br>in terms of scheduling and running these pipelines,</p>
<p>551<br>00:54:04,000 –&gt; 00:54:08,000<br>because we just made a big deal about how they switched to the push-based model,</p>
<p>552<br>00:54:08,000 –&gt; 00:54:11,000<br>they can pause things and do a synchronous IO,</p>
<p>553<br>00:54:11,000 –&gt; 00:54:14,000<br>because now the control flow is separate from the data flow.</p>
<p>554<br>00:54:14,000 –&gt; 00:54:17,000<br>So, I can, you know, I can, I can,</p>
<p>555<br>00:54:17,000 –&gt; 00:54:20,000<br>I can have this thing start running, sending data up,</p>
<p>556<br>00:54:20,000 –&gt; 00:54:23,000<br>and, you know, not have to, like, you know,</p>
<p>557<br>00:54:23,000 –&gt; 00:54:25,000<br>have this weird call stack thing with it,</p>
<p>558<br>00:54:25,000 –&gt; 00:54:29,000<br>like, pause and unpause as I’m pushing data at how.</p>
<p>559<br>00:54:29,000 –&gt; 00:54:31,000<br>All right?</p>
<p>560<br>00:54:31,000 –&gt; 00:54:36,000<br>So, that calls model for that second pass to decide whether what runs remote,</p>
<p>561<br>00:54:36,000 –&gt; 00:54:40,000<br>remote local is based purely on not computational complexity,</p>
<p>562<br>00:54:40,000 –&gt; 00:54:43,000<br>but transfer cost of the data, the transfer time of the data.</p>
<p>563<br>00:54:43,000 –&gt; 00:54:46,000<br>Obviously, if I have, you know, 10 terabytes in the cloud,</p>
<p>564<br>00:54:46,000 –&gt; 00:54:49,000<br>and one kill-light file locally, I don’t want to suck down the 10 terabytes,</p>
<p>565<br>00:54:49,000 –&gt; 00:54:51,000<br>I want to send the one kill-light data up,</p>
<p>566<br>00:54:51,000 –&gt; 00:54:54,000<br>and run everything remotely there.</p>
<p>567<br>00:54:54,000 –&gt; 00:54:55,000<br>Yes?</p>
<p>568<br>00:54:55,000 –&gt; 00:55:00,000<br>Besides, you’ve got to be having this client layer that connects to the model that I can see with,</p>
<p>569<br>00:55:00,000 –&gt; 00:55:03,000<br>what’s the difference between this and something like the one where the on is,</p>
<p>570<br>00:55:03,000 –&gt; 00:55:06,000<br>is sort of, like, scaling out postgres.</p>
<p>571<br>00:55:06,000 –&gt; 00:55:11,000<br>I think they actually have a lot of horizontal and without a scaling going on, right?</p>
<p>572<br>00:55:11,000 –&gt; 00:55:16,000<br>This question is, what is the difference between duct-DB and neon?</p>
<p>573<br>00:55:16,000 –&gt; 00:55:22,000<br>I mean, neon is doing, neon’s taking, ripped up the storage layer of postgres,</p>
<p>574<br>00:55:22,000 –&gt; 00:55:30,000<br>and can make it, basically, make a shared disk architecture that can scale out horizontally.</p>
<p>575<br>00:55:30,000 –&gt; 00:55:35,000<br>But I’m pretty sure that the compute side for the queries themselves are still running on a single node.</p>
<p>576<br>00:55:35,000 –&gt; 00:55:37,000<br>So that would look like this.</p>
<p>577<br>00:55:37,000 –&gt; 00:55:40,000<br>So the horizontal scaling and actually it’s happening at the storage layer?</p>
<p>578<br>00:55:40,000 –&gt; 00:55:42,000<br>The horizontal scaling is having the storage layer.</p>
<p>579<br>00:55:42,000 –&gt; 00:55:45,000<br>This is still, the mother duct, still runs duct-DB as a shared nothing.</p>
<p>580<br>00:55:45,000 –&gt; 00:55:49,000<br>So that’s why the, oh, go back here.</p>
<p>581<br>00:55:49,000 –&gt; 00:55:51,000<br>Sorry, you’re saying the…</p>
<p>582<br>00:55:51,000 –&gt; 00:55:58,000<br>I’m just trying to understand why the scaling is in viable here,</p>
<p>583<br>00:55:58,000 –&gt; 00:56:01,000<br>but something neon does.</p>
<p>584<br>00:56:01,000 –&gt; 00:56:02,000<br>So I misspoke.</p>
<p>585<br>00:56:02,000 –&gt; 00:56:06,000<br>I don’t know whether they’re actually, they’re scaling horizontal and going on a compute side,</p>
<p>586<br>00:56:06,000 –&gt; 00:56:10,000<br>but you could, as you said, you say, if I have like four pipelines that I’m all going to run remotely,</p>
<p>587<br>00:56:10,000 –&gt; 00:56:14,000<br>is that going to hand them all be on a single instance of duct-DB?</p>
<p>588<br>00:56:14,000 –&gt; 00:56:16,000<br>You could have them run on multiple instances, separate containers.</p>
<p>589<br>00:56:16,000 –&gt; 00:56:19,000<br>You have like a million two-fold things, but it’s 250 to 250 to 250,</p>
<p>590<br>00:56:19,000 –&gt; 00:56:20,000<br>so it’s 450.</p>
<p>591<br>00:56:20,000 –&gt; 00:56:21,000<br>Yeah, you could do that, yes.</p>
<p>592<br>00:56:21,000 –&gt; 00:56:22,000<br>Yes.</p>
<p>593<br>00:56:22,000 –&gt; 00:56:24,000<br>I don’t know whether they’re doing that though.</p>
<p>594<br>00:56:24,000 –&gt; 00:56:25,000<br>Okay.</p>
<p>595<br>00:56:25,000 –&gt; 00:56:32,000<br>I think, again, I think the initial version of it just,</p>
<p>596<br>00:56:32,000 –&gt; 00:56:35,000<br>here’s the whole, here’s the, you know, here’s the pipeline, just run it,</p>
<p>597<br>00:56:35,000 –&gt; 00:56:39,000<br>because you would need an extra step to say, okay, chop it up and scale it out,</p>
<p>598<br>00:56:39,000 –&gt; 00:56:41,000<br>and then stitch it back together.</p>
<p>599<br>00:56:41,000 –&gt; 00:56:46,000<br>And that’s, eventually they could do that, I’m sure.</p>
<p>600<br>00:56:46,000 –&gt; 00:56:50,000<br>The neon architecture is more similar to like Aurora,</p>
<p>601<br>00:56:51,000 –&gt; 00:56:55,000<br>that you have a single primary, all the rights go to,</p>
<p>602<br>00:56:55,000 –&gt; 00:56:59,000<br>and then you propagate the updates through the storage layer,</p>
<p>603<br>00:56:59,000 –&gt; 00:57:01,000<br>and then you’d have read-only replicas, you know,</p>
<p>604<br>00:57:01,000 –&gt; 00:57:03,000<br>server-service those queries.</p>
<p>605<br>00:57:03,000 –&gt; 00:57:04,000<br>Oh, so…</p>
<p>606<br>00:57:04,000 –&gt; 00:57:06,000<br>In a transaction-consistent manner.</p>
<p>607<br>00:57:06,000 –&gt; 00:57:09,000<br>So neon is more heavy on that, on cash?</p>
<p>608<br>00:57:09,000 –&gt; 00:57:16,000<br>No, I’m saying that like, the neon is…</p>
<p>609<br>00:57:16,000 –&gt; 00:57:21,000<br>The neon’s trying to have a, you know, primary multiple replicas.</p>
<p>610<br>00:57:21,000 –&gt; 00:57:23,000<br>So the primary is get, absorbs all the rights,</p>
<p>611<br>00:57:23,000 –&gt; 00:57:27,000<br>because it’s an old-to-be workload, and then the changes get propagated to the replicas,</p>
<p>612<br>00:57:27,000 –&gt; 00:57:30,000<br>and you can run read-only queries on those in a transaction-consistent manner,</p>
<p>613<br>00:57:30,000 –&gt; 00:57:31,000<br>or snapshot isolation.</p>
<p>614<br>00:57:31,000 –&gt; 00:57:32,000<br>Thank you.</p>
<p>615<br>00:57:33,000 –&gt; 00:57:38,000<br>And then what Amazon does, when Aurora is, they,</p>
<p>616<br>00:57:38,000 –&gt; 00:57:44,000<br>where neon does all the, that, that propagation of the updates through,</p>
<p>617<br>00:57:45,000 –&gt; 00:57:48,000<br>through code-pitch-intensity-sitting-bub, the file system,</p>
<p>618<br>00:57:48,000 –&gt; 00:57:52,000<br>Amazon puts that propagation directly within like EBS itself,</p>
<p>619<br>00:57:52,000 –&gt; 00:57:55,000<br>and they can do that because they could control the whole stack.</p>
<p>620<br>00:57:55,000 –&gt; 00:57:58,000<br>Not exactly directly in EBS, but, there’s a little bit above it.</p>
<p>621<br>00:57:58,000 –&gt; 00:57:59,000<br>Yes?</p>
<p>622<br>00:57:59,000 –&gt; 00:58:01,000<br>This is a bit of a tangent, which is how it goes.</p>
<p>623<br>00:58:01,000 –&gt; 00:58:04,000<br>If you do the primary thing, aren’t you, you’re doing eventual persistence in data,</p>
<p>624<br>00:58:04,000 –&gt; 00:58:05,000<br>and asset-like?</p>
<p>625<br>00:58:05,000 –&gt; 00:58:07,000<br>Statement is, if you’re doing, if you’re doing, what I’ve said before,</p>
<p>626<br>00:58:07,000 –&gt; 00:58:08,000<br>you’re doing a bit of a…</p>
<p>627<br>00:58:08,000 –&gt; 00:58:09,000<br>The primary primary…</p>
<p>628<br>00:58:09,000 –&gt; 00:58:10,000<br>No.</p>
<p>629<br>00:58:10,000 –&gt; 00:58:16,000<br>Because I could do a commit on the primary, and then not acknowledge the commit</p>
<p>630<br>00:58:16,000 –&gt; 00:58:19,000<br>until all the replicas of acknowledge that they got the update.</p>
<p>631<br>00:58:19,000 –&gt; 00:58:20,000<br>Right?</p>
<p>632<br>00:58:22,000 –&gt; 00:58:26,000<br>And then, but again, for read-only queries, sometimes you don’t maybe need,</p>
<p>633<br>00:58:26,000 –&gt; 00:58:29,000<br>you don’t need that sort of strong consistency, or the,</p>
<p>634<br>00:58:29,000 –&gt; 00:58:34,000<br>the better, the higher guarantees, like snapshot isolation might be enough.</p>
<p>635<br>00:58:34,000 –&gt; 00:58:38,000<br>So I don’t care that I’m reading data that’s 10 milliseconds behind,</p>
<p>636<br>00:58:38,000 –&gt; 00:58:41,000<br>as long as I have a consistent snapshot.</p>
<p>637<br>00:58:41,000 –&gt; 00:58:42,000<br>That’s a whole…</p>
<p>638<br>00:58:42,000 –&gt; 00:58:43,000<br>Yeah.</p>
<p>639<br>00:58:43,000 –&gt; 00:58:45,000<br>Yeah, that’s something that’s class.</p>
<p>640<br>00:58:45,000 –&gt; 00:58:48,000<br>Okay.</p>
<p>641<br>00:58:48,000 –&gt; 00:58:53,000<br>All right, so, definition of inductive E, and then I’ll talk about the worst degree of system,</p>
<p>642<br>00:58:53,000 –&gt; 00:58:56,000<br>the worst idea of everything in data is afterwards.</p>
<p>643<br>00:58:56,000 –&gt; 00:58:58,000<br>I think inductive E is amazing, right?</p>
<p>644<br>00:58:58,000 –&gt; 00:59:02,000<br>The amount of adoption that they’ve had in the last couple of years is phenomenal,</p>
<p>645<br>00:59:02,000 –&gt; 00:59:04,000<br>and I think it was the combination of three things, right?</p>
<p>646<br>00:59:05,000 –&gt; 00:59:07,000<br>They were at the right place at the right time, where people…</p>
<p>647<br>00:59:07,000 –&gt; 00:59:13,000<br>The pendulum was sort of swinging back, where SQL is the default choice for a lot of applications.</p>
<p>648<br>00:59:13,000 –&gt; 00:59:15,000<br>He was sort of asking why people would not want to write SQL,</p>
<p>649<br>00:59:15,000 –&gt; 00:59:17,000<br>and that’s a…</p>
<p>650<br>00:59:17,000 –&gt; 00:59:21,000<br>I think the artifact of data scientists that the people in the no-sequel world</p>
<p>651<br>00:59:21,000 –&gt; 00:59:25,000<br>are issuing SQL back in the day, but it definitely has changed over time.</p>
<p>652<br>00:59:25,000 –&gt; 00:59:27,000<br>So they were at the right time for this.</p>
<p>653<br>00:59:27,000 –&gt; 00:59:30,000<br>They were solving the right problem, like, hey, we need another…</p>
<p>654<br>00:59:30,000 –&gt; 00:59:33,000<br>We need an embedded database to do analytics, not another Dremel,</p>
<p>655<br>00:59:33,000 –&gt; 00:59:35,000<br>not another snowflake.</p>
<p>656<br>00:59:35,000 –&gt; 00:59:42,000<br>And they basically borrowed a talk a lot of the ideas that the Germans were developing.</p>
<p>657<br>00:59:42,000 –&gt; 00:59:45,000<br>I mean, Hanna says German, but I mean the Munich Germans, the hyper guys.</p>
<p>658<br>00:59:45,000 –&gt; 00:59:49,000<br>Took those papers and built an open source implementation of it,</p>
<p>659<br>00:59:49,000 –&gt; 00:59:52,000<br>and had hyper-Umbra been open source.</p>
<p>660<br>00:59:52,000 –&gt; 00:59:55,000<br>I mean, the embedded one is also…</p>
<p>661<br>00:59:55,000 –&gt; 01:00:00,000<br>That was a great idea too, like, you know, SQL library analytics to be embedded database,</p>
<p>662<br>01:00:00,000 –&gt; 01:00:02,000<br>hyper-number or not.</p>
<p>663<br>01:00:02,000 –&gt; 01:00:06,000<br>So the combination of that packaging plus the idea of some hyper-number,</p>
<p>664<br>01:00:06,000 –&gt; 01:00:10,000<br>and improvements over it, certainly, you know, the phenomenal.</p>
<p>665<br>01:00:10,000 –&gt; 01:00:12,000<br>That was a really good idea.</p>
<p>666<br>01:00:12,000 –&gt; 01:00:15,000<br>And for me, like, we were betting a system that I wanted to be, like, you know,</p>
<p>667<br>01:00:15,000 –&gt; 01:00:17,000<br>have… see adoption outside of CMU,</p>
<p>668<br>01:00:17,000 –&gt; 01:00:21,000<br>but I was putting all my eggs in the basket on in memory databases.</p>
<p>669<br>01:00:21,000 –&gt; 01:00:23,000<br>And that certainly did not pan out,</p>
<p>670<br>01:00:23,000 –&gt; 01:00:26,000<br>because Deer and Prices kind of stay stagnated,</p>
<p>671<br>01:00:26,000 –&gt; 01:00:30,000<br>and SSDs got really fast and really cheap.</p>
<p>672<br>01:00:30,000 –&gt; 01:00:35,000<br>And so, memory databases aren’t, you know, aren’t invoked anymore.</p>
<p>673<br>01:00:35,000 –&gt; 01:00:38,000<br>Everything is always SSD-based.</p>
<p>674<br>01:00:38,000 –&gt; 01:00:42,000<br>And we had other complications that I could take offline.</p>
<p>675<br>01:00:42,000 –&gt; 01:00:45,000<br>So, again, I think Dr. B is a great system, and I use it.</p>
<p>676<br>01:00:45,000 –&gt; 01:00:47,000<br>Like, this is my default choice of, like, oh, I got a CSV.</p>
<p>677<br>01:00:47,000 –&gt; 01:00:49,000<br>I got to, you know, do some analyzing on it.</p>
<p>678<br>01:00:49,000 –&gt; 01:00:53,000<br>It used to be open up Excel or whatever, Google Sheets, Nounc.tv.</p>
<p>679<br>01:00:53,000 –&gt; 01:00:59,000<br>So you should have that in your mindset, like, throw Dr. B at everything for quick and dirty things.</p>
<p>680<br>01:00:59,000 –&gt; 01:01:02,000<br>All right, so next class, we’re going to read about Yellowbrick.</p>
<p>681<br>01:01:02,000 –&gt; 01:01:04,000<br>And as I said before, the reason why we’re reading this paper,</p>
<p>682<br>01:01:04,000 –&gt; 01:01:08,000<br>which also just came out in 2024, is they’re going to do a bunch of low-level things</p>
<p>683<br>01:01:08,000 –&gt; 01:01:12,000<br>that nobody else does, that we talked a little bit about,</p>
<p>684<br>01:01:12,000 –&gt; 01:01:15,000<br>to have this semester, but you see how there’s going to be super hardcore about it.</p>
<p>685<br>01:01:15,000 –&gt; 01:01:18,000<br>It’s a very fascinating system.</p>
<p>686<br>01:01:18,000 –&gt; 01:01:21,000<br>And, like I said, they have real numbers in there that nobody else does.</p>
<p>687<br>01:01:21,000 –&gt; 01:01:23,000<br>Okay?</p>
<p>688<br>01:01:23,000 –&gt; 01:01:26,000<br>All right, so I’ll cut this.</p>
<p>689<br>01:01:26,000 –&gt; 01:01:29,000<br>The worst idea I’ve ever heard.</p>
<p>690<br>01:01:57,000 –&gt; 01:01:59,000<br>The worst idea I’ve ever heard.</p>
<p>691<br>01:01:59,000 –&gt; 01:02:02,000<br>The worst idea I’ve ever heard.</p>
<p>692<br>01:02:02,000 –&gt; 01:02:05,000<br>The worst idea I’ve ever heard.</p>
<p>693<br>01:02:05,000 –&gt; 01:02:08,000<br>The worst idea I’ve ever heard.</p>
<p>694<br>01:02:08,000 –&gt; 01:02:11,000<br>The worst idea I’ve ever heard.</p>
<p>695<br>01:02:11,000 –&gt; 01:02:14,000<br>The worst idea I’ve ever heard.</p>
<p>696<br>01:02:14,000 –&gt; 01:02:17,000<br>The worst idea I’ve ever heard.</p>
<p>697<br>01:02:17,000 –&gt; 01:02:20,000<br>The worst idea I’ve ever heard.</p>
<p>698<br>01:02:20,000 –&gt; 01:02:23,000<br>The worst idea I’ve ever heard.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CMU15721 P20S202420 DuckDBEmbeddedDatabaseSystemCMUAdvancedDatabaseSystems</div>
      <div>http://example.com/2025/10/25/CMU15721 P20S202420-DuckDBEmbeddedDatabaseSystemCMUAdvancedDatabaseSystems/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月25日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/25/CMU15721%20P1S202400-CourseOverviewLogisticsCMUAdvancedDatabaseSystems/" title="CMU15721 P1S202400 CourseOverviewLogisticsCMUAdvancedDatabaseSystems">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CMU15721 P1S202400 CourseOverviewLogisticsCMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/25/CMU15721%20P21S202421-YellowbrickDataWarehouseSystemCMUAdvancedDatabaseSystems/" title="CMU15721 P21S202421 YellowbrickDataWarehouseSystemCMUAdvancedDatabaseSystems">
                        <span class="hidden-mobile">CMU15721 P21S202421 YellowbrickDataWarehouseSystemCMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
