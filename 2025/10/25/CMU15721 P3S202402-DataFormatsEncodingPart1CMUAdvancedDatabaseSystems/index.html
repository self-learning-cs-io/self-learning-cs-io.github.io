

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="100:00:00,000 –&gt; 00:00:06,000Carnegie Mellon University’s Advanced Database Systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio audience. 300:00:09,000 –&gt; 00:00:11,0">
<meta property="og:type" content="article">
<meta property="og:title" content="CMU15721 P3S202402 DataFormatsEncodingPart1CMUAdvancedDatabaseSystems">
<meta property="og:url" content="http://example.com/2025/10/25/CMU15721%20P3S202402-DataFormatsEncodingPart1CMUAdvancedDatabaseSystems/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="100:00:00,000 –&gt; 00:00:06,000Carnegie Mellon University’s Advanced Database Systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio audience. 300:00:09,000 –&gt; 00:00:11,0">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-25T05:03:39.754Z">
<meta property="article:modified_time" content="2025-10-25T05:03:39.755Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>CMU15721 P3S202402 DataFormatsEncodingPart1CMUAdvancedDatabaseSystems - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CMU15721 P3S202402 DataFormatsEncodingPart1CMUAdvancedDatabaseSystems"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-25 13:03" pubdate>
          2025年10月25日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          9.6k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          80 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CMU15721 P3S202402 DataFormatsEncodingPart1CMUAdvancedDatabaseSystems</h1>
            
            
              <div class="markdown-body">
                
                <p>1<br>00:00:00,000 –&gt; 00:00:06,000<br>Carnegie Mellon University’s Advanced Database Systems courses</p>
<p>2<br>00:00:06,000 –&gt; 00:00:09,000<br>filming front of the live studio audience.</p>
<p>3<br>00:00:09,000 –&gt; 00:00:11,000<br>I want to know what kind of club it is.</p>
<p>4<br>00:00:11,000 –&gt; 00:00:13,000<br>I want to know what kind of club it is.</p>
<p>5<br>00:00:13,000 –&gt; 00:00:15,000<br>I want to know what kind of club it is.</p>
<p>6<br>00:00:15,000 –&gt; 00:00:18,000<br>We’re starting the bottom of the system stack of this concept</p>
<p>7<br>00:00:18,000 –&gt; 00:00:22,000<br>of the ecosystem with the women building and sort of work our way up</p>
<p>8<br>00:00:22,000 –&gt; 00:00:25,000<br>to actually maybe start running queries and producing results.</p>
<p>9<br>00:00:25,000 –&gt; 00:00:28,000<br>So we’re going to start the very bottom of the system</p>
<p>10<br>00:00:28,000 –&gt; 00:00:31,000<br>and describe what is the data actually going to look like.</p>
<p>11<br>00:00:31,000 –&gt; 00:00:35,000<br>So the first thing you understand is, again, just remind ourselves</p>
<p>12<br>00:00:35,000 –&gt; 00:00:37,000<br>what workload we’re targeting.</p>
<p>13<br>00:00:37,000 –&gt; 00:00:41,000<br>Right? We’ve been talking about these OLAP systems</p>
<p>14<br>00:00:41,000 –&gt; 00:00:44,000<br>and their workload is going to look different than an OLTP system.</p>
<p>15<br>00:00:44,000 –&gt; 00:00:49,000<br>And that’s going to inform us how we want to design, again, the data,</p>
<p>16<br>00:00:49,000 –&gt; 00:00:52,000<br>how we want to lay out data on the disk or in memory,</p>
<p>17<br>00:00:52,000 –&gt; 00:00:56,000<br>and then again, all the auxiliary things you need to do to support that.</p>
<p>18<br>00:00:57,000 –&gt; 00:01:02,000<br>So the primary sort of access method or access pattern</p>
<p>19<br>00:01:02,000 –&gt; 00:01:07,000<br>we’re going to have in a OLAP workload is going to be sequential scans.</p>
<p>20<br>00:01:07,000 –&gt; 00:01:10,000<br>Meaning we’re going to be taking large chunks of data</p>
<p>21<br>00:01:10,000 –&gt; 00:01:14,000<br>and scanning some subset of the columns that may be in the table.</p>
<p>22<br>00:01:14,000 –&gt; 00:01:17,000<br>But just again, scanning large segments of them at a time.</p>
<p>23<br>00:01:17,000 –&gt; 00:01:19,000<br>And therefore, we’re going to have to do a bunch of stuff</p>
<p>24<br>00:01:19,000 –&gt; 00:01:21,000<br>to make sure that runs as fast as possible.</p>
<p>25<br>00:01:21,000 –&gt; 00:01:25,000<br>There isn’t going to be a data structure</p>
<p>26<br>00:01:26,000 –&gt; 00:01:30,000<br>like a B-plus tree or a skip list or other things we would do want to use</p>
<p>27<br>00:01:30,000 –&gt; 00:01:34,000<br>to help us find individual tuples, because we don’t care about individual tuples</p>
<p>28<br>00:01:34,000 –&gt; 00:01:35,000<br>in OLAP workload.</p>
<p>29<br>00:01:35,000 –&gt; 00:01:38,000<br>We care about aggregate information.</p>
<p>30<br>00:01:38,000 –&gt; 00:01:40,000<br>We care about sequential scans.</p>
<p>31<br>00:01:40,000 –&gt; 00:01:43,000<br>And the only time we’re ever going to care about finding individual tuples</p>
<p>32<br>00:01:43,000 –&gt; 00:01:46,000<br>is that we have to stitch the results back together.</p>
<p>33<br>00:01:46,000 –&gt; 00:01:49,000<br>Right? Because we’re going to use a decomposition storage model</p>
<p>34<br>00:01:49,000 –&gt; 00:01:52,000<br>or column store, right? We’re going to break up the attributes for a single tuple.</p>
<p>35<br>00:01:53,000 –&gt; 00:01:56,000<br>And that’s fine because a lot of the processing will do in our sequential scans.</p>
<p>36<br>00:01:56,000 –&gt; 00:01:58,000<br>We’ll be again on a subset of those columns.</p>
<p>37<br>00:01:58,000 –&gt; 00:02:01,000<br>But at the end, when we need to produce a final result,</p>
<p>38<br>00:02:01,000 –&gt; 00:02:05,000<br>we may need to go find the other tuples or sorry, other attributes for a given tuple</p>
<p>39<br>00:02:05,000 –&gt; 00:02:07,000<br>and put things back together.</p>
<p>40<br>00:02:07,000 –&gt; 00:02:12,000<br>We’re not going to want to use a B-plus tree or another data structure</p>
<p>41<br>00:02:12,000 –&gt; 00:02:14,000<br>to do that for us again, or hash table.</p>
<p>42<br>00:02:14,000 –&gt; 00:02:16,000<br>Again, just a contrast reminder, cells from last semester,</p>
<p>43<br>00:02:16,000 –&gt; 00:02:19,000<br>in an OLAP environment or OLAP workloads,</p>
<p>44<br>00:02:20,000 –&gt; 00:02:22,000<br>they care about finding individual things.</p>
<p>45<br>00:02:22,000 –&gt; 00:02:26,000<br>Like go find Andy’s order orders, go find Andy’s bank account.</p>
<p>46<br>00:02:26,000 –&gt; 00:02:30,000<br>And in this, in that world, we want to use again,</p>
<p>47<br>00:02:30,000 –&gt; 00:02:33,000<br>like something like a B-plus tree, be able to go find those things efficiently.</p>
<p>48<br>00:02:33,000 –&gt; 00:02:37,000<br>Because OLTP systems also need to be able to support updates,</p>
<p>49<br>00:02:37,000 –&gt; 00:02:41,000<br>inserts updates, deletes, these data structures have to be dynamic,</p>
<p>50<br>00:02:41,000 –&gt; 00:02:45,000<br>automatically resize themselves as we insert new data.</p>
<p>51<br>00:02:46,000 –&gt; 00:02:49,000<br>We don’t care about any of those things in our world, yes.</p>
<p>52<br>00:02:49,000 –&gt; 00:02:52,000<br>So by stitching, why would we need to do it at the very end?</p>
<p>53<br>00:02:52,000 –&gt; 00:02:53,000<br>Why would we want to…</p>
<p>54<br>00:02:53,000 –&gt; 00:02:56,000<br>This question is, why would I have to do stitching at the very end?</p>
<p>55<br>00:02:56,000 –&gt; 00:03:01,000<br>That’s the late materialization, which we’ll cover two weeks.</p>
<p>56<br>00:03:01,000 –&gt; 00:03:02,000<br>Yeah.</p>
<p>57<br>00:03:02,000 –&gt; 00:03:06,000<br>But that’s a basic idea is that like I want to avoid having stitched the tuple together</p>
<p>58<br>00:03:06,000 –&gt; 00:03:09,000<br>for as long as possible, because I don’t know whether I’m going to need</p>
<p>59<br>00:03:09,000 –&gt; 00:03:11,000<br>even that tuple as I’m going up the query plan.</p>
<p>60<br>00:03:11,000 –&gt; 00:03:14,000<br>So if I can hold off, actually, you need to have to put it back together</p>
<p>61<br>00:03:14,000 –&gt; 00:03:17,000<br>to the very end, then I avoid unnecessary IO.</p>
<p>62<br>00:03:19,000 –&gt; 00:03:21,000<br>I think I’m going to be so understanding with stitching.</p>
<p>63<br>00:03:21,000 –&gt; 00:03:24,000<br>Again, it’s going to be a comms sort of.</p>
<p>64<br>00:03:24,000 –&gt; 00:03:26,000<br>Today’s talk is about we’re going to break up it,</p>
<p>65<br>00:03:26,000 –&gt; 00:03:28,000<br>two-bone two different attributes.</p>
<p>66<br>00:03:28,000 –&gt; 00:03:30,000<br>We’ve got to put it back together at the end.</p>
<p>67<br>00:03:30,000 –&gt; 00:03:31,000<br>Stitch.</p>
<p>68<br>00:03:31,000 –&gt; 00:03:32,000<br>Question?</p>
<p>69<br>00:03:32,000 –&gt; 00:03:33,000<br>I’m just going to just like…</p>
<p>70<br>00:03:33,000 –&gt; 00:03:38,000<br>If we’re not using the season all over the lab for those…</p>
<p>71<br>00:03:38,000 –&gt; 00:03:39,000<br>Yes.</p>
<p>72<br>00:03:39,000 –&gt; 00:03:40,000<br>How do we…</p>
<p>73<br>00:03:40,000 –&gt; 00:03:41,000<br>We go to the…</p>
<p>74<br>00:03:41,000 –&gt; 00:03:43,000<br>Like, sequential scan, fly all the tuple that we want.</p>
<p>75<br>00:03:43,000 –&gt; 00:03:46,000<br>And we’re already passing the tuple as we see it.</p>
<p>76<br>00:03:46,000 –&gt; 00:03:47,000<br>And we’re like, you want that?</p>
<p>77<br>00:03:47,000 –&gt; 00:03:48,000<br>Or is it like later?</p>
<p>78<br>00:03:48,000 –&gt; 00:03:51,000<br>Yes. The question is, like, how can we stitch things back together?</p>
<p>79<br>00:03:51,000 –&gt; 00:03:53,000<br>Like, what additional metadata we need to know is like,</p>
<p>80<br>00:03:53,000 –&gt; 00:03:55,000<br>hey, you’re part of this tuple, put that together.</p>
<p>81<br>00:03:55,000 –&gt; 00:03:56,000<br>We’ll cover that later.</p>
<p>82<br>00:03:56,000 –&gt; 00:03:59,000<br>The basic idea is that you just record offsets.</p>
<p>83<br>00:03:59,000 –&gt; 00:04:02,000<br>Good. That’s the query processing.</p>
<p>84<br>00:04:02,000 –&gt; 00:04:03,000<br>That’s later.</p>
<p>85<br>00:04:03,000 –&gt; 00:04:06,000<br>Right. So…</p>
<p>86<br>00:04:06,000 –&gt; 00:04:07,000<br>Okay.</p>
<p>87<br>00:04:07,000 –&gt; 00:04:13,000<br>So if all we’re going to do for O-Lup workloads is mostly run sequential scans.</p>
<p>88<br>00:04:13,000 –&gt; 00:04:14,000<br>Again, it’s not entirely true.</p>
<p>89<br>00:04:14,000 –&gt; 00:04:16,000<br>Sometimes there’s smaller range scans.</p>
<p>90<br>00:04:16,000 –&gt; 00:04:17,000<br>We don’t scan the entire thing.</p>
<p>91<br>00:04:17,000 –&gt; 00:04:20,000<br>Sometimes you do need to go find individual tuples.</p>
<p>92<br>00:04:20,000 –&gt; 00:04:22,000<br>And for all that, we would have additional things.</p>
<p>93<br>00:04:22,000 –&gt; 00:04:24,000<br>We could add to our database system.</p>
<p>94<br>00:04:24,000 –&gt; 00:04:27,000<br>For now, we’re going to ignore that.</p>
<p>95<br>00:04:27,000 –&gt; 00:04:30,000<br>Right. So if all you’re going to do is execute central scans,</p>
<p>96<br>00:04:30,000 –&gt; 00:04:32,000<br>you know, how can you actually optimize it?</p>
<p>97<br>00:04:32,000 –&gt; 00:04:35,000<br>So this list here is basically what we covered a lot.</p>
<p>98<br>00:04:35,000 –&gt; 00:04:38,000<br>We talked a little bit about in the intro class,</p>
<p>99<br>00:04:38,000 –&gt; 00:04:41,000<br>but a lot of these other things we’re going to talk about in this class.</p>
<p>100<br>00:04:41,000 –&gt; 00:04:42,000<br>Right.</p>
<p>101<br>00:04:42,000 –&gt; 00:04:44,000<br>This is basically the menu what’s available to us.</p>
<p>102<br>00:04:44,000 –&gt; 00:04:45,000<br>Right.</p>
<p>103<br>00:04:45,000 –&gt; 00:04:47,000<br>So today’s class is about data encoding and compression.</p>
<p>104<br>00:04:47,000 –&gt; 00:04:52,000<br>How to minimize the amount of storage space it takes to represent data.</p>
<p>105<br>00:04:52,000 –&gt; 00:04:55,000<br>Represent tuples.</p>
<p>106<br>00:04:55,000 –&gt; 00:05:00,000<br>Prefetching is identifying what data I’m going to need as I’m scanning along the table</p>
<p>107<br>00:05:00,000 –&gt; 00:05:03,000<br>and go ahead and bring those things into memory before the…</p>
<p>108<br>00:05:03,000 –&gt; 00:05:06,000<br>before the execution ended and actually needs it.</p>
<p>109<br>00:05:06,000 –&gt; 00:05:08,000<br>So when it goes and says, hey, I need this block.</p>
<p>110<br>00:05:08,000 –&gt; 00:05:12,000<br>Well, it’s already here in memory or already in a local cache for us.</p>
<p>111<br>00:05:12,000 –&gt; 00:05:15,000<br>Parallelization is going to allow us to run multiple queries at the same time</p>
<p>112<br>00:05:15,000 –&gt; 00:05:19,000<br>and within that single query run multiple tasks, a query plan for segments</p>
<p>113<br>00:05:19,000 –&gt; 00:05:22,000<br>or different portions of a query plan at the same time.</p>
<p>114<br>00:05:22,000 –&gt; 00:05:25,000<br>Either across, you’ve been threads, different processes, different nodes.</p>
<p>115<br>00:05:25,000 –&gt; 00:05:26,000<br>Kind of doesn’t matter.</p>
<p>116<br>00:05:26,000 –&gt; 00:05:31,000<br>Clustering sorting is identifying that the data can be stored in such a way</p>
<p>117<br>00:05:31,000 –&gt; 00:05:36,000<br>that when queries go start asking for data within a given range or something,</p>
<p>118<br>00:05:36,000 –&gt; 00:05:39,000<br>you can minimize the amount of data you have to look at because you know</p>
<p>119<br>00:05:39,000 –&gt; 00:05:44,000<br>within some range it’s located spatially close to each other.</p>
<p>120<br>00:05:44,000 –&gt; 00:05:46,000<br>Late materialization is what he was asking about.</p>
<p>121<br>00:05:46,000 –&gt; 00:05:51,000<br>How do I… can I delay having to stick the tuple back together to the very end?</p>
<p>122<br>00:05:51,000 –&gt; 00:05:55,000<br>Because I don’t want to pay the cost of going reading things from disk or from memory</p>
<p>123<br>00:05:55,000 –&gt; 00:05:58,000<br>if I know I’m actually not going to need it.</p>
<p>124<br>00:05:58,000 –&gt; 00:06:02,000<br>And I don’t need to materialize, you know, take up memory to put things together either.</p>
<p>125<br>00:06:02,000 –&gt; 00:06:05,000<br>MaterializeViews, a result caching, is basically identifying that</p>
<p>126<br>00:06:05,000 –&gt; 00:06:08,000<br>I’m going to execute basically the same query over and over again.</p>
<p>127<br>00:06:08,000 –&gt; 00:06:12,000<br>Therefore I can keep the result of that query around and that way in someone</p>
<p>128<br>00:06:12,000 –&gt; 00:06:14,000<br>to ask for it again, it’s already there.</p>
<p>129<br>00:06:14,000 –&gt; 00:06:17,000<br>Or materializeViews, sort of specialized case of this where you recognize that</p>
<p>130<br>00:06:17,000 –&gt; 00:06:23,000<br>there’s a bunch of queries that need to operate or process the same subset of data.</p>
<p>131<br>00:06:23,000 –&gt; 00:06:27,000<br>Like give me all the orders within today’s month or this month.</p>
<p>132<br>00:06:28,000 –&gt; 00:06:31,000<br>So maybe I can pre-compute that portion of the query for the…</p>
<p>133<br>00:06:31,000 –&gt; 00:06:33,000<br>give me all the orders for today’s for this month.</p>
<p>134<br>00:06:33,000 –&gt; 00:06:37,000<br>And even though the queries may do different things on that month’s worth of data,</p>
<p>135<br>00:06:37,000 –&gt; 00:06:41,000<br>I’ve already done sort of the basic work of getting the month.</p>
<p>136<br>00:06:41,000 –&gt; 00:06:42,000<br>Yes.</p>
<p>137<br>00:06:42,000 –&gt; 00:06:44,000<br>How is that different from the data to your stuff?</p>
<p>138<br>00:06:44,000 –&gt; 00:06:46,000<br>It’s questions, how is it different than data-cube stuff?</p>
<p>139<br>00:06:46,000 –&gt; 00:06:48,000<br>So it’s basically the same thing.</p>
<p>140<br>00:06:48,000 –&gt; 00:06:52,000<br>In the data-cube world back in the day, even now, they’re probably not all very dynamic.</p>
<p>141<br>00:06:52,000 –&gt; 00:06:55,000<br>But like you basically have to manually refresh.</p>
<p>142<br>00:06:55,000 –&gt; 00:07:00,000<br>And materializeViews, at least the ideas that if I make incremental changes,</p>
<p>143<br>00:07:00,000 –&gt; 00:07:03,000<br>I can then refresh the view.</p>
<p>144<br>00:07:03,000 –&gt; 00:07:08,000<br>Now, in the ideal case, you want to do this without re-computing the entire query.</p>
<p>145<br>00:07:08,000 –&gt; 00:07:10,000<br>Because that’s the dumbest thing you can do, right?</p>
<p>146<br>00:07:10,000 –&gt; 00:07:14,000<br>Like if I insert one tube into a row and re-compute this one second query or ten second query,</p>
<p>147<br>00:07:14,000 –&gt; 00:07:17,000<br>now there are some systems that can do incremental updates.</p>
<p>148<br>00:07:17,000 –&gt; 00:07:19,000<br>Admittedly, this is the part of data systems I know the least about.</p>
<p>149<br>00:07:19,000 –&gt; 00:07:20,000<br>This is super hard.</p>
<p>150<br>00:07:20,000 –&gt; 00:07:23,000<br>And we’re actually not going to cover that this semester.</p>
<p>151<br>00:07:23,000 –&gt; 00:07:24,000<br>Consult caching is obvious.</p>
<p>152<br>00:07:24,000 –&gt; 00:07:26,000<br>You just do pattern matching with the strings.</p>
<p>153<br>00:07:26,000 –&gt; 00:07:27,000<br>They say, it’s the same thing.</p>
<p>154<br>00:07:27,000 –&gt; 00:07:28,000<br>And then we use it.</p>
<p>155<br>00:07:28,000 –&gt; 00:07:32,000<br>This is not that common either.</p>
<p>156<br>00:07:32,000 –&gt; 00:07:37,000<br>Data skipping is being able to identify before actually looking at the data that I don’t need the data.</p>
<p>157<br>00:07:37,000 –&gt; 00:07:41,000<br>And not have to process it.</p>
<p>158<br>00:07:41,000 –&gt; 00:07:42,000<br>Data parallelization or vectorization.</p>
<p>159<br>00:07:42,000 –&gt; 00:07:47,000<br>This is going to be sort of specialization of regular parallelization or task parallelization.</p>
<p>160<br>00:07:47,000 –&gt; 00:07:51,000<br>The idea here is that within a single piece of data,</p>
<p>161<br>00:07:52,000 –&gt; 00:07:55,000<br>or yeah, the chunk of data, maybe multiple two-blows,</p>
<p>162<br>00:07:55,000 –&gt; 00:08:03,000<br>can I use things like SIMD to process multiple units at the same time or multiple pieces of the data at the same time?</p>
<p>163<br>00:08:03,000 –&gt; 00:08:07,000<br>And then code specialization compilation, again, we’ll cover this later this semester.</p>
<p>164<br>00:08:07,000 –&gt; 00:08:12,000<br>This idea is that since I’m, if I know the type of data I’m processing,</p>
<p>165<br>00:08:12,000 –&gt; 00:08:19,000<br>I know what the query is going to be, rather than have this execution to interpret what the query wants to do,</p>
<p>166<br>00:08:19,000 –&gt; 00:08:25,000<br>I could just literally generate C code that does exactly what the query wants, compile that, and run that.</p>
<p>167<br>00:08:25,000 –&gt; 00:08:29,000<br>Does not be C, could use intermediate languages and things like that.</p>
<p>168<br>00:08:29,000 –&gt; 00:08:32,000<br>But this will be a big thing with a little cover later on.</p>
<p>169<br>00:08:32,000 –&gt; 00:08:35,000<br>So again, for this semester, we’re going to not talk about materialized views,</p>
<p>170<br>00:08:35,000 –&gt; 00:08:40,000<br>and we’re not going to talk about the pre-vegting one.</p>
<p>171<br>00:08:40,000 –&gt; 00:08:42,000<br>And so for this class, though, we’re going to talk about this lecture today,</p>
<p>172<br>00:08:42,000 –&gt; 00:08:44,000<br>we’re going to talk about data encoding compression,</p>
<p>173<br>00:08:44,000 –&gt; 00:08:54,000<br>and then the data encoding part is going to then help us for next class to talk about how can we encode data in such a way that we can get better parallelism through vectorization.</p>
<p>174<br>00:08:54,000 –&gt; 00:08:57,000<br>Can I get the summary of materialized views?</p>
<p>175<br>00:08:57,000 –&gt; 00:09:01,000<br>This question, can I get the summary of materialized views? The basic idea of materialized views is like,</p>
<p>176<br>00:09:01,000 –&gt; 00:09:07,000<br>think of a regular view, right? Views are almost like a macro of a query.</p>
<p>177<br>00:09:07,000 –&gt; 00:09:11,000<br>So instead of having to, like in case of a query, a view would say,</p>
<p>178<br>00:09:11,000 –&gt; 00:09:13,000<br>just a select query, create a view for it.</p>
<p>179<br>00:09:13,000 –&gt; 00:09:16,000<br>Now, anytime that someone does a, any treat it like a virtual table,</p>
<p>180<br>00:09:16,000 –&gt; 00:09:24,000<br>and anytime you do a select on it, the database system is basically replacing the name of the table with the nested query that’s defined in your view.</p>
<p>181<br>00:09:24,000 –&gt; 00:09:28,000<br>But in that, in that world, in regular views, every single time you run the query,</p>
<p>182<br>00:09:28,000 –&gt; 00:09:32,000<br>you’re re-computing whatever the select query that the view points do.</p>
<p>183<br>00:09:32,000 –&gt; 00:09:39,000<br>A materialized view, basically, says, generate the result when you declare the view,</p>
<p>184<br>00:09:39,000 –&gt; 00:09:45,000<br>and then anytime anybody wants query that view, you don’t have to re-comput it, you have the result already.</p>
<p>185<br>00:09:45,000 –&gt; 00:09:51,000<br>But then the thing you can do is you can find materialized views on,</p>
<p>186<br>00:09:51,000 –&gt; 00:09:57,000<br>maybe they don’t exactly match with the query once, like, give me all the orders for this month.</p>
<p>187<br>00:09:57,000 –&gt; 00:10:00,000<br>And then you define that as a materialized view.</p>
<p>188<br>00:10:00,000 –&gt; 00:10:04,000<br>Anytime anybody inserts a new order for this month, you have to then refresh that materialized view.</p>
<p>189<br>00:10:04,000 –&gt; 00:10:06,000<br>And the database system will do this automatically.</p>
<p>190<br>00:10:06,000 –&gt; 00:10:10,000<br>If they support automatic refresh, in Postgres, you have to manually refresh it.</p>
<p>191<br>00:10:10,000 –&gt; 00:10:13,000<br>And SQL Server and other systems that will do it will free automatically.</p>
<p>192<br>00:10:13,000 –&gt; 00:10:15,000<br>There’s probably the index and you have a separate storage or a penny plug.</p>
<p>193<br>00:10:15,000 –&gt; 00:10:20,000<br>It’s question is, yeah, it’s statement, it sounds like an index. Yes, it’s like a supplemental data structure.</p>
<p>194<br>00:10:20,000 –&gt; 00:10:24,000<br>Yeah, that’s a subset of the data I looked for.</p>
<p>195<br>00:10:24,000 –&gt; 00:10:27,000<br>But the way the database is maintained as it’s just like a temp table.</p>
<p>196<br>00:10:27,000 –&gt; 00:10:30,000<br>But if you restart it, it comes back.</p>
<p>197<br>00:10:30,000 –&gt; 00:10:31,000<br>Yeah.</p>
<p>198<br>00:10:32,000 –&gt; 00:10:34,000<br>But the hard part is doing that incremental updates.</p>
<p>199<br>00:10:34,000 –&gt; 00:10:41,000<br>If I insert one two, like, say my view is like the running total of the number sales for this month.</p>
<p>200<br>00:10:41,000 –&gt; 00:10:45,000<br>Well, if I insert a new order, I want to update that total.</p>
<p>201<br>00:10:45,000 –&gt; 00:10:50,000<br>But I know I, you know, if I’m dumb, I’ll just run the whole aggregation query from beginning to end.</p>
<p>202<br>00:10:50,000 –&gt; 00:10:55,000<br>But if I’m clever about it and recognize, oh, well, this is just a sum of the total order amount.</p>
<p>203<br>00:10:55,000 –&gt; 00:10:58,000<br>I know what this order amount is, just increment it.</p>
<p>204<br>00:10:58,000 –&gt; 00:11:03,000<br>Okay, but not every system can do that.</p>
<p>205<br>00:11:03,000 –&gt; 00:11:05,000<br>Okay, again, so this class, we’re talking about these two things.</p>
<p>206<br>00:11:05,000 –&gt; 00:11:11,000<br>And then how we do data encoding will influence how we can do data parallelization and vectorization later.</p>
<p>207<br>00:11:11,000 –&gt; 00:11:17,000<br>And then we’ll cover vectorization in more detail when we talk about query processing further along.</p>
<p>208<br>00:11:17,000 –&gt; 00:11:23,000<br>But it’s hard to, like, try to avoid talking about simd, this lecture and vectorization, this lecture.</p>
<p>209<br>00:11:23,000 –&gt; 00:11:26,000<br>Next lecture that you read the fast lanes paper and it’s all about it.</p>
<p>210<br>00:11:26,000 –&gt; 00:11:33,000<br>So we’ll see how to do this for, you know, do use vectorization for other things like joins and sorting and other stuff later in the two weeks.</p>
<p>211<br>00:11:33,000 –&gt; 00:11:38,000<br>All right, so today we’re going to talk about storage models and then persistent data formats.</p>
<p>212<br>00:11:38,000 –&gt; 00:11:40,000<br>I’m not going to talk about intermediate data formats.</p>
<p>213<br>00:11:40,000 –&gt; 00:11:46,000<br>We’ll cover that when we talk about a little bit about arrow today, but we’ll cover that in more detail when we talk about query processing.</p>
<p>214<br>00:11:46,000 –&gt; 00:11:52,000<br>Right, it’s really about these are the files that are on disk, whether it’s an object store or a local file system, it doesn’t matter.</p>
<p>215<br>00:11:52,000 –&gt; 00:11:57,000<br>Right, these are the actual bits that are getting put out in persistent storage.</p>
<p>216<br>00:11:57,000 –&gt; 00:11:58,000<br>Okay.</p>
<p>217<br>00:11:58,000 –&gt; 00:12:04,000<br>All right, so the first thing we got to discuss is what storage model we’re going to use for these data files.</p>
<p>218<br>00:12:04,000 –&gt; 00:12:11,000<br>And again, this will be some of a, you know, repeat what we talked about in intro class, but it’s important to go over this again more detail,</p>
<p>219<br>00:12:11,000 –&gt; 00:12:17,000<br>to understand again what packs is going to do for us later on when we start constructing the file formats.</p>
<p>220<br>00:12:18,000 –&gt; 00:12:24,000<br>So the storage model is going to find how we’re physically going to store the tuples on disk and in memory.</p>
<p>221<br>00:12:24,000 –&gt; 00:12:27,000<br>And again, this is not the actual bytes we’re actually storing.</p>
<p>222<br>00:12:27,000 –&gt; 00:12:30,000<br>It just says like, okay, here’s some tuple, here’s some attributes for them.</p>
<p>223<br>00:12:30,000 –&gt; 00:12:40,000<br>I don’t really care what those bytes are in those attributes, but how should I organize them in relation to both the attributes within the same tuple and then across other tuples.</p>
<p>224<br>00:12:41,000 –&gt; 00:12:45,000<br>So the default storage model in most systems, or most people think about what, when they think about a data system,</p>
<p>225<br>00:12:45,000 –&gt; 00:12:48,000<br>this is going to be the anary storage model at the row store.</p>
<p>226<br>00:12:48,000 –&gt; 00:12:55,000<br>Again, this is what bus tub is, this is what postgres, my SQL, SQLite, Oracle, and this is what most people get.</p>
<p>227<br>00:12:55,000 –&gt; 00:13:01,000<br>Geekop is an storage model is the sort of pure column store, and then the packs model will be a hybrid of the two of these.</p>
<p>228<br>00:13:01,000 –&gt; 00:13:09,000<br>And you’ll see that we want to use packs because we’ll have better locality for attributes within the same tuple, like the Libby in the same block.</p>
<p>229<br>00:13:10,000 –&gt; 00:13:20,000<br>Again, so the e-sys, or not e-sys, the default storage model that every system can think most people, when they think about a data system,</p>
<p>230<br>00:13:20,000 –&gt; 00:13:23,000<br>they think about the storage model as going to be the anary or the row store.</p>
<p>231<br>00:13:23,000 –&gt; 00:13:34,000<br>And again, the idea here is that we’re going to store almost all the attributes for a single tuple, continuously in our pages or in our file or in memory, one after another.</p>
<p>232<br>00:13:34,000 –&gt; 00:13:44,000<br>Now, I’m saying almost because again, there are sometimes in some systems, if you have oversized attributes, like something more than like four kilobytes that doesn’t fit in a single page,</p>
<p>233<br>00:13:44,000 –&gt; 00:13:49,000<br>they’ll have an auxiliary storage, postgres calls this toasterage, I think some systems call this secondary storage.</p>
<p>234<br>00:13:49,000 –&gt; 00:13:56,000<br>It’s basically you just have a pointer to some other blob store thing that has the large attributes, but we don’t really care about that.</p>
<p>235<br>00:13:57,000 –&gt; 00:14:09,000<br>And again, this is going to be ideal for OTP workloads because in that environment these applications, the transactions of the queries are most going to be concerned about getting single tuples, like go get Andy’s order record.</p>
<p>236<br>00:14:09,000 –&gt; 00:14:23,000<br>And because we’re going to want to, you know, digesting new information from the outside world, when new inserts up in deletes, it’s really easy for us to take any new tuple that someone inserted, go find a free slot in some page and just write it all out there continuously.</p>
<p>237<br>00:14:23,000 –&gt; 00:14:33,000<br>And then when we want to commit the transaction, or we later need to flush the journey page, assuming one tuple fits in a single page, it’s one disc write to put that out there.</p>
<p>238<br>00:14:33,000 –&gt; 00:14:43,000<br>So the page size in this world is typically going to be some constant factor of four kilobytes, right? Postgres by default is eight kilobytes or equals four kilobytes, although I think you can tune that.</p>
<p>239<br>00:14:44,000 –&gt; 00:14:51,000<br>DB2, you can tune this. My SQL is the biggest one, maybe at 16 kilobytes. It might be one that’s 32 kilobytes, you have to figure which one.</p>
<p>240<br>00:14:51,000 –&gt; 00:14:56,000<br>But it’s always going to be some, it’s going to be measured in kilobytes, like single digits kilobytes in this world.</p>
<p>241<br>00:14:56,000 –&gt; 00:15:04,000<br>Because again, if you think about the workload they’re trying to support, go get Andy’s order record. My record isn’t going to be that big. It’s going to, isn’t going to be megabytes.</p>
<p>242<br>00:15:05,000 –&gt; 00:15:12,000<br>So it doesn’t make sense for me to have these really big pages to store this row data because I got to go get that entire page and bring it in.</p>
<p>243<br>00:15:12,000 –&gt; 00:15:21,000<br>I can’t, you know, in the NSM world, I can’t bring in partial pages. I got to bring the whole thing because I need the header and what else the data is in that because I need to protect it in the context of transactions.</p>
<p>244<br>00:15:21,000 –&gt; 00:15:26,000<br>Right. So there are always the page sizes are going to be much smaller than we’ll see in for a network list.</p>
<p>245<br>00:15:27,000 –&gt; 00:15:35,000<br>And because, again, because we’re trying to do single to proper processing for the most part, we’ll want to use the iterator, the volcano processing model in this world.</p>
<p>246<br>00:15:35,000 –&gt; 00:15:39,000<br>Again, I don’t want to talk about too much about query processing, but that’ll come later.</p>
<p>247<br>00:15:39,000 –&gt; 00:15:48,000<br>And obviously this sucks for OLAP, as I said, because in OLAP, we’re doing large, large, substantial scans on a subset of the columns for the most part.</p>
<p>248<br>00:15:48,000 –&gt; 00:15:58,000<br>So if I have 100 columns in my table, but now my query only needs four of them, well, I got to bring in the other 96 columns that I don’t need because it’s all packed into the single page.</p>
<p>249<br>00:15:59,000 –&gt; 00:16:11,000<br>So this is where the column store, the DSM decomposition storage model stuff came along where people recognize that, oh, for the different class of workloads, for OLAP workloads, it doesn’t make sense to store everything continuously.</p>
<p>250<br>00:16:11,000 –&gt; 00:16:20,000<br>Instead, you want to break up the two pool based on its attributes. Then now you store all the data for that attribute across multiple tuples, contiguously.</p>
<p>251<br>00:16:20,000 –&gt; 00:16:25,000<br>And that’s going to open up a bunch of advantages for compression and other things that we can take advantage of.</p>
<p>252<br>00:16:26,000 –&gt; 00:16:31,000<br>And this, we can do this because we’re, again, we’re mostly running, we don’t link queries.</p>
<p>253<br>00:16:32,000 –&gt; 00:16:39,000<br>So we’re not worried about how to do incremental inserts into our database. If someone was certain new data, you know, again, some systems will handle that,</p>
<p>254<br>00:16:39,000 –&gt; 00:16:46,000<br>but oftentimes it’d be like a bulk load of like, here’s a bunch of results, a bunch of data we’ve gotten, and then you don’t have to worry about incremental updates.</p>
<p>255<br>00:16:47,000 –&gt; 00:17:00,000<br>Because again, if you had to do a transaction, update this one record that had 100 attributes, if I’m doing the decomposition storage model, I got to update 100 pages at least and write them all out, which is actually.</p>
<p>256<br>00:17:00,000 –&gt; 00:17:04,000<br>And that’s going to be slow, that’s going to be terrible. So we don’t have to worry about that.</p>
<p>257<br>00:17:05,000 –&gt; 00:17:11,000<br>These file sizes are going to be typically larger, hundreds of megabytes. Now within this file will break it up.</p>
<p>258<br>00:17:11,000 –&gt; 00:17:22,000<br>This was the row group stuff that you read in the paper, you’ll break it up into smaller chunks and identify which of these pieces of the file actually need, but the overall file itself is going to be, could be quite large.</p>
<p>259<br>00:17:23,000 –&gt; 00:17:32,000<br>So let’s give an example what this looks like. Say we have a really simple table, three attributes and six rows, six tuples.</p>
<p>260<br>00:17:33,000 –&gt; 00:17:41,000<br>So what we’re going to want to do is we’re going to store all of the values for a game in column all within a single file.</p>
<p>261<br>00:17:41,000 –&gt; 00:17:51,000<br>There’ll be some metadata header at the front that maybe tell us additional information like what version of the data system wrote this data, any additional statistics, like the zone map stuff that we want to store for that.</p>
<p>262<br>00:17:51,000 –&gt; 00:17:58,000<br>There’ll be a null bitmap to keep track of which of these attributes are actually null because we need a way to represent that.</p>
<p>263<br>00:17:58,000 –&gt; 00:18:03,000<br>But again, we’ll just store a separate file for each of the three attributes.</p>
<p>264<br>00:18:03,000 –&gt; 00:18:11,000<br>Now we’ll see why we need to do this in a second, but we’re going to make sure that all the data we’re storing has to be fixed length.</p>
<p>265<br>00:18:11,000 –&gt; 00:18:16,000<br>Right? I’m going to think of us why or knows why from last semester.</p>
<p>266<br>00:18:17,000 –&gt; 00:18:28,000<br>So you can get the offset. So that going back to the stitching thing, if I need to get for the for row zero, tuples zero, I need to get its data and I’m processing, you know, I’m at this position here.</p>
<p>267<br>00:18:28,000 –&gt; 00:18:33,000<br>I know how to do simple arithmetic to jump down to the other column files.</p>
<p>268<br>00:18:33,000 –&gt; 00:18:41,000<br>I know the length of the data that I’m storing of each value and I know what offset a map. So I just do the simple map and jump to the right offset.</p>
<p>269<br>00:18:42,000 –&gt; 00:18:48,000<br>But of course, how we handle that for variable length data, like strings, which are super common. We’ll see that in a second.</p>
<p>270<br>00:18:48,000 –&gt; 00:18:55,000<br>So fixed length offsets isn’t the only way to do this. It’s the most common one. This one pretty much almost everyone does, especially if you have a sort of pure,</p>
<p>271<br>00:18:55,000 –&gt; 00:19:02,000<br>a lot system or an old system that is designed from the ground up to be to run these analytical workloads as a column store.</p>
<p>272<br>00:19:03,000 –&gt; 00:19:14,000<br>The alternative is to embed tuple IDs. So for every single value in a column, you’ll have a little prefix that says, I’m, you know, I’m tuple zero, tuple one, tuple three, tuple four.</p>
<p>273<br>00:19:14,000 –&gt; 00:19:28,000<br>And now if you ever want to find the corresponding data or attributes for that same tuple across all the columns, you need an auxiliary data structure, like a hash table that says, okay, for this column, for this tuple ID, here’s the offset you want to jump to.</p>
<p>274<br>00:19:28,000 –&gt; 00:19:33,000<br>Or at least very least look, here’s the range you should start out looking for it.</p>
<p>275<br>00:19:33,000 –&gt; 00:19:34,000<br>Yes.</p>
<p>276<br>00:19:38,000 –&gt; 00:19:45,000<br>The same question is, if I’m saying everything has to be fixed length, but then you have strings, does that mean you can’t do this?</p>
<p>277<br>00:19:45,000 –&gt; 00:19:55,000<br>No, we’ll see how this handle that in a second. The answer’s going to be dictionary coding. Because we want to convert anything that’s variable length to fixed length.</p>
<p>278<br>00:19:56,000 –&gt; 00:20:00,000<br>The next one is going to be what I’m highlighting here.</p>
<p>279<br>00:20:02,000 –&gt; 00:20:09,000<br>So, like I said, I’m only bringing stuff to the bottom one. I’m saying, don’t do this. I’m only bringing out because there are some systems to do it.</p>
<p>280<br>00:20:09,000 –&gt; 00:20:20,000<br>And as far as I know, it’s only done in cases where it was like a row store system. And then they added like a little specialized storage manager or storage engine and say, oh, yeah, we also have a column store piece.</p>
<p>281<br>00:20:20,000 –&gt; 00:20:29,000<br>And they need to be able to handle the need to reuse some existing depotons that they had to do a process on column store data.</p>
<p>282<br>00:20:29,000 –&gt; 00:20:37,000<br>So they add these two more ideas. But this is what you want to do. You want to use the fixed length. And in parkaian or they’re all fixed length.</p>
<p>283<br>00:20:39,000 –&gt; 00:20:44,000<br>So basically what he asked me was, how do I handle the error length data? The answer’s going to be dictionary. Yes, question.</p>
<p>284<br>00:20:51,000 –&gt; 00:20:58,000<br>So his question is, or say, but is if I’m doing run length encoding, do I still have the advantage of fixed length? Yeah, why wouldn’t you?</p>
<p>285<br>00:21:07,000 –&gt; 00:21:16,000<br>We won’t tell how to process run length encoding today. But the basic idea is that you do have to, like, you scan through to know where to jump. Like, okay, I’m looking for this value.</p>
<p>286<br>00:21:16,000 –&gt; 00:21:29,000<br>I scan through and I know that if I’m looking for a two point this offset, I keep scanning to I find the run length encoding entry that covers my offset.</p>
<p>287<br>00:21:33,000 –&gt; 00:21:45,000<br>So you just came through all the file again, we’ll break this pure DSM. Yes, you would have that problem. Like worst case scenario, you get the scan to the very bottom because the thing you look for is at the very bottom.</p>
<p>288<br>00:21:45,000 –&gt; 00:22:02,000<br>This is why another reason why I would use packs because they’re basically going to break things up to the row groups. So that in worst case scenario, yes, we have the scan in entire column. It’s early compressed. But it’s only going to be 10 megs or something like that. It’s not that big.</p>
<p>289<br>00:22:02,000 –&gt; 00:22:21,000<br>Right, again, so what are you dictionary encoding? And that allows convert very length data into fixed length files. And if you read the paper, you pick this up. Parquet is very aggressive actually on the dictionary encoding and their addiction encoded everything.</p>
<p>290<br>00:22:21,000 –&gt; 00:22:42,000<br>Even if it’s already fixed length, right, or it only does this for for for strings for very length things. But then it turns out, Parquet does pretty good still because there’s still you can convert dictionary coding. We’ll see in a second, you’re going to allow you to convert data that may be in a really large domain down to a much smaller domain.</p>
<p>291<br>00:22:42,000 –&gt; 00:22:47,000<br>And then you can apply additional compression techniques on top of that and you still get a win.</p>
<p>292<br>00:22:47,000 –&gt; 00:22:53,000<br>It seems counter to that I would want to compress flow. It says dictionary codes, but we found that actually works.</p>
<p>293<br>00:22:53,000 –&gt; 00:23:03,000<br>This doesn’t solve the problem of how to handle semi structured data in this world. The dumbest thing you do would be just treat semi structured data as a text field.</p>
<p>294<br>00:23:03,000 –&gt; 00:23:15,000<br>All right, and just have the query engine do parse the JSON or whatever it is as it goes along. That’s going to be super slow. Again, we’ll see how to handle that in a second. Well, we can convert everything to columns.</p>
<p>295<br>00:23:15,000 –&gt; 00:23:27,000<br>And then again, and if there’s string, string fields in the values of the nested data, the JSON values. Again, we’ll just dictionary code that to treat it like a regular column.</p>
<p>296<br>00:23:28,000 –&gt; 00:23:34,000<br>So as I said, the most OLAQ queries are never going to access a single column in a table by itself.</p>
<p>297<br>00:23:34,000 –&gt; 00:23:43,000<br>It’s very rare to say select some average in a single column without any where clause, without any group by, without any sorting, or anything like that.</p>
<p>298<br>00:23:43,000 –&gt; 00:23:55,000<br>I’m not saying that single column queries don’t exist. I’m saying they’re not that common, not as common as multi column queries.</p>
<p>299<br>00:23:55,000 –&gt; 00:24:08,000<br>So if we do the decomposition storage model where we’re storing single files per column, then I’m going to have to jump through different files if I need to start putting things back together in order to process my query.</p>
<p>300<br>00:24:08,000 –&gt; 00:24:16,000<br>My where clause might reference four columns, I got to go jump to those four files at different offsets and go give the data that I need.</p>
<p>301<br>00:24:17,000 –&gt; 00:24:28,000<br>So we want a way to get all the benefits we get from having columnar data, either through getting better compression, or also doing vectorized execution.</p>
<p>302<br>00:24:28,000 –&gt; 00:24:35,000<br>We still want all that, but we don’t want the downsides of having separate files.</p>
<p>303<br>00:24:35,000 –&gt; 00:24:38,000<br>So this is what packs is going to solve for us.</p>
<p>304<br>00:24:39,000 –&gt; 00:24:48,000<br>So this was invented, actually I think here. So this is the paper is from Natasa Alamaki. She was the database professor before I was.</p>
<p>305<br>00:24:48,000 –&gt; 00:24:56,000<br>Before I showed up, she was teaching 721 in 2006. Then she left to go to EPL, and I revised him 21 when I showed up.</p>
<p>306<br>00:24:56,000 –&gt; 00:25:07,000<br>But there’s a paper she wrote in 2002 that basically recognized that there is this problem if you have a lot of memory, then you don’t want to pay this penalty of, sorry, not that they have a lot of memory.</p>
<p>307<br>00:25:07,000 –&gt; 00:25:12,000<br>You don’t pay this penalty, you have to jump between these different columnar files to put things back together.</p>
<p>308<br>00:25:12,000 –&gt; 00:25:19,000<br>If you have enough memory to bring in a big chunk of data, you can still keep things in a columnar format.</p>
<p>309<br>00:25:19,000 –&gt; 00:25:25,000<br>But then now the data for a that’s related to a single tuple will be close to each other.</p>
<p>310<br>00:25:25,000 –&gt; 00:25:33,000<br>So again, we get all the benefit of colonizing our storage, but still maintain the spatial locality of the row store.</p>
<p>311<br>00:25:34,000 –&gt; 00:25:43,000<br>So the way this could work is that we have our example table before. So we’re going to horizontally partition the table into row groups.</p>
<p>312<br>00:25:43,000 –&gt; 00:25:52,000<br>And the size of the row group will vary per implementation, right? For now, it’s just assumed that we did some fixed number tuples, like three tuples.</p>
<p>313<br>00:25:52,000 –&gt; 00:25:58,000<br>And now within that row group, we’re going to have a header, of course, that’s going to tell us information about what’s in this row group.</p>
<p>314<br>00:25:58,000 –&gt; 00:26:04,000<br>But then now we’re going to lay out the data for a single column or some attribute sequentially or contiguously.</p>
<p>315<br>00:26:04,000 –&gt; 00:26:10,000<br>And then once we’re done for all the tuples in that column, then we jump to the next one.</p>
<p>316<br>00:26:10,000 –&gt; 00:26:14,000<br>So we’re going to call this piece here within the row group. We’re going to call this a column chunk.</p>
<p>317<br>00:26:14,000 –&gt; 00:26:18,000<br>I think the work paper calls them, or the work system calls them stripes.</p>
<p>318<br>00:26:18,000 –&gt; 00:26:24,000<br>Roe groups and column chunks are what’s in parquet. What does use that?</p>
<p>319<br>00:26:24,000 –&gt; 00:26:28,000<br>And then once we lay everything out for the first row group, then we do the same thing for the other.</p>
<p>320<br>00:26:28,000 –&gt; 00:26:34,000<br>And then in the folder, we’re going to have metadata that’s going to tell us what’s in this file. Yes?</p>
<p>321<br>00:26:34,000 –&gt; 00:26:40,000<br>So what’s the reason that we have multiple row groups instead of having different multiple files?</p>
<p>322<br>00:26:40,000 –&gt; 00:26:46,000<br>What’s the reason for having different row groups versus having a row group be a single file?</p>
<p>323<br>00:26:46,000 –&gt; 00:26:56,000<br>Yes. So you could have a, right, so you could define your parquet or you could say, I want my file to only have one row group.</p>
<p>324<br>00:26:56,000 –&gt; 00:27:02,000<br>But then now you basically have a bunch of metadata that’s very narrowed just for that one row group.</p>
<p>325<br>00:27:02,000 –&gt; 00:27:08,000<br>So the idea here is that if I put a bunch of row groups, the right amount, again, it’s probably the empty complete problem.</p>
<p>326<br>00:27:08,000 –&gt; 00:27:16,000<br>It’s difficult to know. The right amount of row groups to have the right granularity for the scope of the zone map and other metadata maintaining, that’s hard to know.</p>
<p>327<br>00:27:16,000 –&gt; 00:27:22,000<br>I guess my question is, what’s the data that I’m having these multiple within a single file?</p>
<p>328<br>00:27:22,000 –&gt; 00:27:25,000<br>In a single file versus having multiple files?</p>
<p>329<br>00:27:25,000 –&gt; 00:27:32,000<br>So I could have a zone map in the footer, the metadata, that tells me within my row groups, here’s the data that I have.</p>
<p>330<br>00:27:32,000 –&gt; 00:27:43,000<br>I could have it for like, you know, for some columns in the date. Here’s the min and the max value for all the values in that single column across all the row groups.</p>
<p>331<br>00:27:43,000 –&gt; 00:27:46,000<br>So like you have that like bottom metadata inside?</p>
<p>332<br>00:27:46,000 –&gt; 00:28:01,000<br>Right, now what I’m saying is that so now like if I want to go, if I’m looking for all the orders from this month and my files have been sort of squentially based on time, now I just go read this and I can figure out, oh, this thing contains all the orders from 2023.</p>
<p>333<br>00:28:01,000 –&gt; 00:28:12,000<br>I don’t need anything in there, I’m looking for, you know, January 2024. So I can skip it entirely. If I do what you’re proposing, then I have to go read this header from every single, you know, from multiple files.</p>
<p>334<br>00:28:12,000 –&gt; 00:28:14,000<br>So this is another level of granularity?</p>
<p>335<br>00:28:14,000 –&gt; 00:28:18,000<br>To another level granularity. But again, when I’m saying it’s like, what’s the right size? It depends.</p>
<p>336<br>00:28:18,000 –&gt; 00:28:24,000<br>Which is a cop out answer database is beginning to answer a lot of things. It depends on the query, it depends on the data, it depends on what you’re trying to do.</p>
<p>337<br>00:28:24,000 –&gt; 00:28:29,000<br>So is the row group, what single row group would it be, one, four kilobytes?</p>
<p>338<br>00:28:29,000 –&gt; 00:28:36,000<br>Yeah, so his question is, is a row group one, one four kilobytes page? No, it’s going to be tens of megabytes.</p>
<p>339<br>00:28:36,000 –&gt; 00:28:37,000<br>Okay.</p>
<p>340<br>00:28:37,000 –&gt; 00:28:50,000<br>Yeah. Again, it’s the same thing he brought up, it’s the granularity of this, right? If I’m storing just four kilobytes of data, but I have to have this metadata thing and then like say the metadata is, I don’t know, half a kilobytes, or sorry, kilobytes.</p>
<p>341<br>00:28:50,000 –&gt; 00:28:58,000<br>Then I can find three kilobytes of data in a row group. And now I’m basically have a bunch of overhead of metadata that I don’t actually need.</p>
<p>342<br>00:28:58,000 –&gt; 00:28:59,000<br>Yes.</p>
<p>343<br>00:28:59,000 –&gt; 00:29:12,000<br>Then when there really is a moment like you want each like strike to be like at least one page, then you pull back home and like get that benefit of not pulling in data that you don’t need, if you’re just doing like a subset of columns that you’re querying over.</p>
<p>344<br>00:29:12,000 –&gt; 00:29:17,000<br>So your question is like shouldn’t the size of the column group or the size of the row group be a certain size or what?</p>
<p>345<br>00:29:17,000 –&gt; 00:29:22,000<br>Like a single strike, like a single column shock should be like at least a page.</p>
<p>346<br>00:29:22,000 –&gt; 00:29:23,000<br>At least a page yes.</p>
<p>347<br>00:29:23,000 –&gt; 00:29:24,000<br>Yeah.</p>
<p>348<br>00:29:24,000 –&gt; 00:29:26,000<br>Yes.</p>
<p>349<br>00:29:26,000 –&gt; 00:29:31,000<br>So what does zone map be in the photo of the entire file that I’m using in the metadata of each row of the row?</p>
<p>350<br>00:29:31,000 –&gt; 00:29:37,000<br>Well, we’ll cover about, I’m going to send a second, but there will be a zone map per row group in this metadata thing.</p>
<p>351<br>00:29:37,000 –&gt; 00:29:43,000<br>And then optionally you can have a zone map that covers the columns in the metadata.</p>
<p>352<br>00:29:43,000 –&gt; 00:29:50,000<br>This is actually one of the problems of parking or is that like they added this later a bunch of limitations don’t actually support it.</p>
<p>353<br>00:29:50,000 –&gt; 00:29:57,000<br>So like it’s like it’s in the specs as there, but it may not actually be there.</p>
<p>354<br>00:29:57,000 –&gt; 00:29:58,000<br>Yes.</p>
<p>355<br>00:29:58,000 –&gt; 00:30:06,000<br>Are those random or maybe based on the form?</p>
<p>356<br>00:30:06,000 –&gt; 00:30:09,000<br>A question is a row group is random? What do you mean like random?</p>
<p>357<br>00:30:09,000 –&gt; 00:30:11,000<br>Like are there boundaries?</p>
<p>358<br>00:30:11,000 –&gt; 00:30:12,000<br>The boundaries you mean?</p>
<p>359<br>00:30:12,000 –&gt; 00:30:13,000<br>Yeah.</p>
<p>360<br>00:30:13,000 –&gt; 00:30:15,000<br>We’ll cover that in a second.</p>
<p>361<br>00:30:15,000 –&gt; 00:30:19,000<br>Like there’s a sizing protocol that they have.</p>
<p>362<br>00:30:19,000 –&gt; 00:30:24,000<br>For these different formats they’ll say like I want my row group to be at least this size or this number of tuples.</p>
<p>363<br>00:30:24,000 –&gt; 00:30:27,000<br>And there’s tradeouts for them for these choices.</p>
<p>364<br>00:30:27,000 –&gt; 00:30:28,000<br>Yes.</p>
<p>365<br>00:30:28,000 –&gt; 00:30:31,000<br>Are these actually the same length and cost of the different chunks?</p>
<p>366<br>00:30:31,000 –&gt; 00:30:34,000<br>These are the same length that goes to different chunks.</p>
<p>367<br>00:30:34,000 –&gt; 00:30:35,000<br>Yes.</p>
<p>368<br>00:30:35,000 –&gt; 00:30:39,000<br>Let me think about that.</p>
<p>369<br>00:30:39,000 –&gt; 00:30:42,000<br>I think you can actually have different within a row group.</p>
<p>370<br>00:30:42,000 –&gt; 00:30:45,000<br>I think you can vary the encoding scheme you’re using.</p>
<p>371<br>00:30:45,000 –&gt; 00:30:47,000<br>So I think they could be different.</p>
<p>372<br>00:30:47,000 –&gt; 00:30:50,000<br>But since I have my metadata what encoding scheme I’m using,</p>
<p>373<br>00:30:50,000 –&gt; 00:30:54,000<br>if I say I want two-board offset six, like logical offset six,</p>
<p>374<br>00:30:54,000 –&gt; 00:30:58,000<br>and I jump to this row group, I know it’s in here,</p>
<p>375<br>00:30:58,000 –&gt; 00:31:02,000<br>I use the metadata and then figure, okay, the size of the attributes is,</p>
<p>376<br>00:31:02,000 –&gt; 00:31:05,000<br>for this column is this.</p>
<p>377<br>00:31:05,000 –&gt; 00:31:07,000<br>Therefore I can do my math to go find that.</p>
<p>378<br>00:31:07,000 –&gt; 00:31:10,000<br>So it could be different per row group and certainly different per file.</p>
<p>379<br>00:31:10,000 –&gt; 00:31:13,000<br>But again, I just do the same arithmetic that I before.</p>
<p>380<br>00:31:13,000 –&gt; 00:31:17,000<br>But in the column group they all be the same size, or the column chunk.</p>
<p>381<br>00:31:17,000 –&gt; 00:31:22,000<br>Okay.</p>
<p>382<br>00:31:22,000 –&gt; 00:31:27,000<br>And it says, sort of his question is there’s a global metadata that can tell me,</p>
<p>383<br>00:31:27,000 –&gt; 00:31:32,000<br>like, what’s in my file, how things are encoded.</p>
<p>384<br>00:31:32,000 –&gt; 00:31:35,000<br>You would think it’s essentially the header, right?</p>
<p>385<br>00:31:35,000 –&gt; 00:31:37,000<br>Remembering this lot of pages stuff from last semester,</p>
<p>386<br>00:31:37,000 –&gt; 00:31:40,000<br>that was literally in the header, the beginning bytes of a page.</p>
<p>387<br>00:31:40,000 –&gt; 00:31:42,000<br>The putting this at the bottom.</p>
<p>388<br>00:31:42,000 –&gt; 00:31:49,000<br>It might take us a while.</p>
<p>389<br>00:31:49,000 –&gt; 00:31:51,000<br>He says the size is not fixed.</p>
<p>390<br>00:31:51,000 –&gt; 00:31:54,000<br>The size of what?</p>
<p>391<br>00:31:54,000 –&gt; 00:31:59,000<br>The size of the metadata is not fixed.</p>
<p>392<br>00:31:59,000 –&gt; 00:32:04,000<br>Again, these files are big, and I don’t know what the metadata is going to be,</p>
<p>393<br>00:32:04,000 –&gt; 00:32:08,000<br>like what’s the minmax value for all the data that I have,</p>
<p>394<br>00:32:08,000 –&gt; 00:32:11,000<br>until I process all the data.</p>
<p>395<br>00:32:11,000 –&gt; 00:32:14,000<br>And then also, the reason why they’re putting it at the end and at the beginning,</p>
<p>396<br>00:32:14,000 –&gt; 00:32:18,000<br>because this comes from the Hadoop world, or HDFS world,</p>
<p>397<br>00:32:18,000 –&gt; 00:32:21,000<br>from 10 years ago, which is a pen-only file system.</p>
<p>398<br>00:32:21,000 –&gt; 00:32:25,000<br>I can’t make in place updates to the file without rewriting it.</p>
<p>399<br>00:32:25,000 –&gt; 00:32:29,000<br>So if I have my, if I’m bulk loading much of data to generate a parquet and orc file,</p>
<p>400<br>00:32:29,000 –&gt; 00:32:32,000<br>I got to scan through all the data, and I’m writing up these row groups incrementally.</p>
<p>401<br>00:32:32,000 –&gt; 00:32:35,000<br>And then when I’m done, all right, let me close the file,</p>
<p>402<br>00:32:35,000 –&gt; 00:32:39,000<br>the metadata, you can’t go back and put it back up here,</p>
<p>403<br>00:32:39,000 –&gt; 00:32:42,000<br>because you’ve already written out the beginning of the file.</p>
<p>404<br>00:32:42,000 –&gt; 00:32:45,000<br>S3 is the same thing, right?</p>
<p>405<br>00:32:45,000 –&gt; 00:32:50,000<br>You can’t do in-place updates, you have to basically rewrite something entirely.</p>
<p>406<br>00:32:50,000 –&gt; 00:32:54,000<br>And that would be expensive, because then basically if I wrote a 1D-giby parquet file,</p>
<p>407<br>00:32:54,000 –&gt; 00:32:59,000<br>I don’t want to go write it back out again just to update the metadata.</p>
<p>408<br>00:32:59,000 –&gt; 00:33:03,000<br>So they always put it at the bottom.</p>
<p>409<br>00:33:03,000 –&gt; 00:33:07,000<br>The zoom-app stuff actually in the bottom, for parquet here,</p>
<p>410<br>00:33:07,000 –&gt; 00:33:13,000<br>again, I think the paper talks about that was added, wow, 2018, 2019.</p>
<p>411<br>00:33:13,000 –&gt; 00:33:16,000<br>That was actually my student at Claudero that added that.</p>
<p>412<br>00:33:16,000 –&gt; 00:33:18,000<br>She took this class.</p>
<p>413<br>00:33:18,000 –&gt; 00:33:21,000<br>Now she’s a PhD student at University of Maryland.</p>
<p>414<br>00:33:21,000 –&gt; 00:33:24,000<br>So there’s some connection there with 721.</p>
<p>415<br>00:33:24,000 –&gt; 00:33:25,000<br>Okay.</p>
<p>416<br>00:33:25,000 –&gt; 00:33:31,000<br>So up until again 10 years ago, sorry, yes.</p>
<p>417<br>00:33:31,000 –&gt; 00:33:34,000<br>So what do you think about the end of the file?</p>
<p>418<br>00:33:34,000 –&gt; 00:33:37,000<br>How do you, like, if it is different to the metadata,</p>
<p>419<br>00:33:37,000 –&gt; 00:33:40,000<br>what do you think is different?</p>
<p>420<br>00:33:40,000 –&gt; 00:33:43,000<br>You just go to the end and file and bring the last couple of pages in the end.</p>
<p>421<br>00:33:43,000 –&gt; 00:33:48,000<br>Yes, question is, like, if it’s at the end, how do I start processing the file?</p>
<p>422<br>00:33:48,000 –&gt; 00:33:51,000<br>Yeah, so the entry point for the file is the footer.</p>
<p>423<br>00:33:51,000 –&gt; 00:33:57,000<br>So I think the way it works is that the last 32 bits of the file is the length of the footer.</p>
<p>424<br>00:33:57,000 –&gt; 00:34:04,000<br>So you read that and that will tell you how much back you have read to get the complete view of the metadata.</p>
<p>425<br>00:34:04,000 –&gt; 00:34:05,000<br>Yes.</p>
<p>426<br>00:34:05,000 –&gt; 00:34:10,000<br>You’re saying this is a kind of file that’s basically similar to a log structure, which is really?</p>
<p>427<br>00:34:10,000 –&gt; 00:34:12,000<br>Is there a question that my saying this is like a log structure, a nursery?</p>
<p>428<br>00:34:12,000 –&gt; 00:34:13,000<br>No.</p>
<p>429<br>00:34:13,000 –&gt; 00:34:15,000<br>Why would I say that?</p>
<p>430<br>00:34:15,000 –&gt; 00:34:22,000<br>What we mentioned in context of the new file, the end only, I thought it was.</p>
<p>431<br>00:34:22,000 –&gt; 00:34:28,000<br>So a pen only means like in Hadoop, the file isn’t only allows for like, like, open a file and there’s the pen bytes to it.</p>
<p>432<br>00:34:28,000 –&gt; 00:34:31,000<br>So like, but like Hadoop doesn’t know which are storing.</p>
<p>433<br>00:34:31,000 –&gt; 00:34:33,000<br>It just sees bytes, right?</p>
<p>434<br>00:34:33,000 –&gt; 00:34:36,000<br>The, this would not be, like, this is not LSM.</p>
<p>435<br>00:34:36,000 –&gt; 00:34:41,000<br>This is literally like, there’s one version of a piece of data of Tupel,</p>
<p>436<br>00:34:41,000 –&gt; 00:34:49,000<br>and I’m just writing it out sequentially as I go, but I’m going to organize it in this pack format.</p>
<p>437<br>00:34:49,000 –&gt; 00:34:50,000<br>Okay.</p>
<p>438<br>00:34:50,000 –&gt; 00:34:56,000<br>So prior to 10 years ago, even the early commsure systems, like the Vertica, the Green Flums and so forth,</p>
<p>439<br>00:34:56,000 –&gt; 00:35:00,000<br>they were, they had their own proprietary data format.</p>
<p>440<br>00:35:00,000 –&gt; 00:35:02,000<br>And then I mentioned this last class.</p>
<p>441<br>00:35:02,000 –&gt; 00:35:07,000<br>Like, most data systems you think about, SQL Light, Postgres, MySQL, Oracle and so forth,</p>
<p>442<br>00:35:07,000 –&gt; 00:35:13,000<br>like, the, when they put bits down on disk, the format is going to be proprietary to that database system.</p>
<p>443<br>00:35:13,000 –&gt; 00:35:14,000<br>Right?</p>
<p>444<br>00:35:14,000 –&gt; 00:35:16,000<br>Like, the systems designed to, to read and write that data.</p>
<p>445<br>00:35:16,000 –&gt; 00:35:19,000<br>Now, there are some systems like, like, like, DuckDB is really good about this.</p>
<p>446<br>00:35:19,000 –&gt; 00:35:22,000<br>They can read SQL Light database files. They can read other data files.</p>
<p>447<br>00:35:22,000 –&gt; 00:35:24,000<br>Right? But most systems don’t do that.</p>
<p>448<br>00:35:24,000 –&gt; 00:35:25,000<br>Right?</p>
<p>449<br>00:35:25,000 –&gt; 00:35:30,000<br>And so the problem with this is that since all these different data systems have their own proprietary data format,</p>
<p>450<br>00:35:30,000 –&gt; 00:35:33,000<br>you can’t share data across different disparate systems.</p>
<p>451<br>00:35:33,000 –&gt; 00:35:38,000<br>Like, I can’t take a Postgres data directory files and plop it down, I’ll let MySQL read it.</p>
<p>452<br>00:35:38,000 –&gt; 00:35:41,000<br>Again, DuckDB is trying to be like a Swiss Army knife kind of thing.</p>
<p>453<br>00:35:41,000 –&gt; 00:35:42,000<br>We can ignore that.</p>
<p>454<br>00:35:42,000 –&gt; 00:35:44,000<br>But that’s how, not, not how most systems work.</p>
<p>455<br>00:35:44,000 –&gt; 00:35:50,000<br>So that means the only way I could get data out of one system and put it to another system is to write a SQL query,</p>
<p>456<br>00:35:50,000 –&gt; 00:35:57,000<br>dump it out, and then convert it to a CSV, TSE, or JSON XML file, some other format,</p>
<p>457<br>00:35:57,000 –&gt; 00:36:01,000<br>and then, then do bulk insertion on the, for the other naming system,</p>
<p>458<br>00:36:01,000 –&gt; 00:36:04,000<br>to then convert it into, you know, their own proprietary format.</p>
<p>459<br>00:36:04,000 –&gt; 00:36:09,000<br>But again, with the, with the Hadoop and the Cloud stuff taking off in early 2010s,</p>
<p>460<br>00:36:09,000 –&gt; 00:36:13,000<br>you had now, as I showed in the last class, you had all these operational databases</p>
<p>461<br>00:36:13,000 –&gt; 00:36:17,000<br>that want to start writing data out so that you could read it into your data warehouse.</p>
<p>462<br>00:36:17,000 –&gt; 00:36:18,000<br>But again, you don’t want to do this conversion.</p>
<p>463<br>00:36:18,000 –&gt; 00:36:24,000<br>I just want to put things out to my object store and not worry about having to do additional conversions.</p>
<p>464<br>00:36:24,000 –&gt; 00:36:30,000<br>So this is where the, the Parkane orcs stuff comes, comes into play because they’re, that’s the problem they were trying to solve.</p>
<p>465<br>00:36:30,000 –&gt; 00:36:39,000<br>They’re trying to be a universal file format that, that, you know, one application or some other thing upstream in your applications stack could generate.</p>
<p>466<br>00:36:39,000 –&gt; 00:36:42,000<br>And then you wouldn’t have to do that conversion to be able to read it.</p>
<p>467<br>00:36:42,000 –&gt; 00:36:48,000<br>And you would get all the benefits of a binary encoding scheme that you would want in like a column store or a packs layout</p>
<p>468<br>00:36:48,000 –&gt; 00:36:51,000<br>without having to, to, revert it to a text format like this.</p>
<p>469<br>00:36:51,000 –&gt; 00:36:57,000<br>Like, see if it’s the worst thing you can do because now basically all your binary data gets converted into ASCII characters.</p>
<p>470<br>00:36:57,000 –&gt; 00:37:00,000<br>And, and you’re going to parse that and deal with that when you load it back in.</p>
<p>471<br>00:37:00,000 –&gt; 00:37:02,000<br>Same thing with JSON.</p>
<p>472<br>00:37:02,000 –&gt; 00:37:07,000<br>So the idea is that, that you define a spec, here’s the file format is,</p>
<p>473<br>00:37:08,000 –&gt; 00:37:12,000<br>and then whoever, whatever system wants to be able to read it could either use an off the shelf and the notation of it,</p>
<p>474<br>00:37:12,000 –&gt; 00:37:17,000<br>which of, of, they have question or quality, or write the run.</p>
<p>475<br>00:37:17,000 –&gt; 00:37:20,000<br>Which again, of question or quality, right?</p>
<p>476<br>00:37:20,000 –&gt; 00:37:27,000<br>So the, this is not entirely a new idea, but it actually goes back to the 1990s.</p>
<p>477<br>00:37:27,000 –&gt; 00:37:32,000<br>In the high, high performance computing world of the scientific community, there was this thing called HDF5.</p>
<p>478<br>00:37:33,000 –&gt; 00:37:38,000<br>That means there was 4, 3, 2, 1, there was previous versions to this, but 5 is the current one I’m still uses.</p>
<p>479<br>00:37:38,000 –&gt; 00:37:43,000<br>But this was a binary format to store compressed multimensional arrays.</p>
<p>480<br>00:37:43,000 –&gt; 00:37:50,000<br>And then you could have, you know, whatever random 4chan program you had to do processing on, you know, the data from scientific instruments or satellite data,</p>
<p>481<br>00:37:50,000 –&gt; 00:37:54,000<br>you could use this universal file format and use it for different experiments.</p>
<p>482<br>00:37:54,000 –&gt; 00:37:59,000<br>But this is almost entirely ignored by the database community, right?</p>
<p>483<br>00:38:00,000 –&gt; 00:38:08,000<br>So 2009, people recognized that, oh, Hadoop wants to generate a bunch of these files that we want to be used for different purposes.</p>
<p>484<br>00:38:08,000 –&gt; 00:38:15,000<br>And so the original version of the data format in Hadoop, which is a thing called sequential sequence files, is literally key value pairs, right?</p>
<p>485<br>00:38:15,000 –&gt; 00:38:23,000<br>Like serialized value strings that only the function of the Hadoop code knew how to process like your application code.</p>
<p>486<br>00:38:23,000 –&gt; 00:38:25,000<br>So there was no embedded scheme information.</p>
<p>487<br>00:38:26,000 –&gt; 00:38:32,000<br>So they replaced this with this thing called Avro, and I think this came out of maybe CloudDero or the Hadoop project.</p>
<p>488<br>00:38:32,000 –&gt; 00:38:42,000<br>But if this was still role-oriented, so, you know, even though the commsource systems existed at the time, Hadoop was still writing things out and processing data as rows, which sucked.</p>
<p>489<br>00:38:42,000 –&gt; 00:38:50,000<br>So then in, there was another version before Park K called RC file, actually before Ork from Meta.</p>
<p>490<br>00:38:51,000 –&gt; 00:39:00,000<br>But then at the same time in 2013, CloudDero and Twitter working on Apollo, they put out Park K, and then Meta put out Ork for Apache Hive.</p>
<p>491<br>00:39:00,000 –&gt; 00:39:05,000<br>Hive is basically a SQL engine or SQL query engine on top of Hadoop or Meta produce.</p>
<p>492<br>00:39:05,000 –&gt; 00:39:07,000<br>So it converts your SQL queries to Meta produce jobs.</p>
<p>493<br>00:39:07,000 –&gt; 00:39:10,000<br>It’s still around today, but you don’t want to use it.</p>
<p>494<br>00:39:10,000 –&gt; 00:39:16,000<br>So, again, Park K and Ork is the dominant ones, but at this point they’re over 10 years old.</p>
<p>495<br>00:39:16,000 –&gt; 00:39:18,000<br>And they’re still widely used today.</p>
<p>496<br>00:39:18,000 –&gt; 00:39:21,000<br>Carbon data is an extension of Park K from Hwale.</p>
<p>497<br>00:39:21,000 –&gt; 00:39:26,000<br>It adds additional metadata for keeping track of schema versions and so forth.</p>
<p>498<br>00:39:26,000 –&gt; 00:39:29,000<br>I don’t know anybody who uses this other than Hwale.</p>
<p>499<br>00:39:29,000 –&gt; 00:39:32,000<br>The open source version that we tried doesn’t work, it doesn’t compile.</p>
<p>500<br>00:39:32,000 –&gt; 00:39:35,000<br>But somebody’s still working on it.</p>
<p>501<br>00:39:35,000 –&gt; 00:39:38,000<br>And then Arrow, again, I mentioned this before, this is going to be an in-memory.</p>
<p>502<br>00:39:38,000 –&gt; 00:39:41,000<br>So, thinking this is like, I’m just in-memory version of Park K.</p>
<p>503<br>00:39:41,000 –&gt; 00:39:47,000<br>And it’s going to allow me to do change data between different processes over the network or in memory on the memory.</p>
<p>504<br>00:39:47,000 –&gt; 00:39:49,000<br>On the same box.</p>
<p>505<br>00:39:49,000 –&gt; 00:39:53,000<br>And this came out of Dremio and from the Panersky.</p>
<p>506<br>00:39:53,000 –&gt; 00:39:57,000<br>So, again, for this class, we’re only going to focus on these two.</p>
<p>507<br>00:39:57,000 –&gt; 00:39:59,000<br>And then there’s a bunch of other ones I think mentioned the paper.</p>
<p>508<br>00:39:59,000 –&gt; 00:40:04,000<br>There’s an extension or newer version of Ork called Dwarf from Facebook.</p>
<p>509<br>00:40:04,000 –&gt; 00:40:07,000<br>And then there’s this thing called Alpha that we’ve been talking about.</p>
<p>510<br>00:40:07,000 –&gt; 00:40:09,000<br>It’s sort of the next generation one.</p>
<p>511<br>00:40:09,000 –&gt; 00:40:10,000<br>We can ignore all that.</p>
<p>512<br>00:40:10,000 –&gt; 00:40:12,000<br>We want to focus on Park K and Ork.</p>
<p>513<br>00:40:12,000 –&gt; 00:40:15,000<br>That’s what pretty much everyone’s using today.</p>
<p>514<br>00:40:15,000 –&gt; 00:40:18,000<br>And it’s not to say that these formats are the best.</p>
<p>515<br>00:40:18,000 –&gt; 00:40:20,000<br>And this is what every system should be using.</p>
<p>516<br>00:40:20,000 –&gt; 00:40:23,000<br>It’s just what’s commonly being used.</p>
<p>517<br>00:40:23,000 –&gt; 00:40:25,000<br>Like, is SQL the best query language?</p>
<p>518<br>00:40:25,000 –&gt; 00:40:27,000<br>No. It has problems.</p>
<p>519<br>00:40:27,000 –&gt; 00:40:29,000<br>But is it widely used? And therefore, it’s not going away?</p>
<p>520<br>00:40:29,000 –&gt; 00:40:30,000<br>Yes. So, same thing.</p>
<p>521<br>00:40:30,000 –&gt; 00:40:31,000<br>Park K and Ork are widely used.</p>
<p>522<br>00:40:31,000 –&gt; 00:40:33,000<br>We have to be able to handle them.</p>
<p>523<br>00:40:33,000 –&gt; 00:40:40,000<br>Like, on Hugging Face, you can get native data directly out in a Park K format directly.</p>
<p>524<br>00:40:40,000 –&gt; 00:40:47,000<br>So, the thing we’re talking about is when we design a file format is what metadata we want to maintain.</p>
<p>525<br>00:40:47,000 –&gt; 00:40:49,000<br>The layout of the data.</p>
<p>526<br>00:40:49,000 –&gt; 00:40:56,000<br>With TySystem, we’re going to support the encoding teams or the compression schemes we would use for the data itself.</p>
<p>527<br>00:40:56,000 –&gt; 00:40:59,000<br>And then block compression is sort of the after I’ve done the encoding,</p>
<p>528<br>00:40:59,000 –&gt; 00:41:02,000<br>can I run like GZip or something on the compress it?</p>
<p>529<br>00:41:02,000 –&gt; 00:41:05,000<br>Talk about filters and then we’ll finish off talking about the nested data.</p>
<p>530<br>00:41:05,000 –&gt; 00:41:09,000<br>So, the paper I had you guys read, again, this is something that I wrote with my former student,</p>
<p>531<br>00:41:09,000 –&gt; 00:41:13,000<br>my former student, P.C. student here, who’s at Shinhua, and his P.C. student.</p>
<p>532<br>00:41:13,000 –&gt; 00:41:18,000<br>And then the guy that I met at Python Pandas, who was at Voltron,</p>
<p>533<br>00:41:18,000 –&gt; 00:41:23,000<br>he and I’ve been talking about, hey, Park K has some problems because he worked on the inventor of Arrow.</p>
<p>534<br>00:41:23,000 –&gt; 00:41:28,000<br>He worked on some Park K stuff too and realized that we wanted to investigate why</p>
<p>535<br>00:41:28,000 –&gt; 00:41:33,000<br>what are the problems in the modern environment for these two, the most popular storage formats.</p>
<p>536<br>00:41:33,000 –&gt; 00:41:39,000<br>At the same time, Microsoft was actually doing the same evaluation, same experiments that we were.</p>
<p>537<br>00:41:39,000 –&gt; 00:41:44,000<br>Their paper came out, I think, in October, in ours came out like November.</p>
<p>538<br>00:41:44,000 –&gt; 00:41:47,000<br>So, like, we dodged a bullet.</p>
<p>539<br>00:41:47,000 –&gt; 00:41:51,000<br>They approached the problem differently and did different kind of evaluation.</p>
<p>540<br>00:41:51,000 –&gt; 00:41:52,000<br>So, they’re complimentary.</p>
<p>541<br>00:41:52,000 –&gt; 00:41:58,000<br>I’ll talk a little about them, but the main take away I got from them is their findings corroborate what we found in ours.</p>
<p>542<br>00:41:58,000 –&gt; 00:42:02,000<br>So, I felt good as a scientist that had worked out.</p>
<p>543<br>00:42:02,000 –&gt; 00:42:08,000<br>All right, so the first thing is when, and the meditated, what meditated we want to put in the file.</p>
<p>544<br>00:42:08,000 –&gt; 00:42:11,000<br>So, I already mentioned the ZOMAP stuff.</p>
<p>545<br>00:42:11,000 –&gt; 00:42:18,000<br>But the basic ideas for Park K network, these are meant to be self-describing or self-contained file formats.</p>
<p>546<br>00:42:18,000 –&gt; 00:42:26,000<br>Meaning, everything I need to know to interpret what the bytes mean in my file are being contained in the file itself.</p>
<p>547<br>00:42:27,000 –&gt; 00:42:33,000<br>And this is a post you’re like thinking like in a, a system like bus tub or Postgres or my SQL,</p>
<p>548<br>00:42:33,000 –&gt; 00:42:39,000<br>you have a bunch of files that keep track of like the catalog, the blocks in the catalog keeps track of the scheme of the tables,</p>
<p>549<br>00:42:39,000 –&gt; 00:42:40,000<br>what the types are and so forth.</p>
<p>550<br>00:42:40,000 –&gt; 00:42:42,000<br>And then you have pages for the actual data.</p>
<p>551<br>00:42:42,000 –&gt; 00:42:49,000<br>So, in order for me to understand what’s in my data pages, I got to go read the pages for the catalog.</p>
<p>552<br>00:42:49,000 –&gt; 00:42:53,000<br>Oracle is the only system that actually packs everything within the page itself for like disaster recovery.</p>
<p>553<br>00:42:53,000 –&gt; 00:42:58,000<br>So, that way if your Oracle game gets destroyed, your disk gets destroyed, and you can recover some pages,</p>
<p>554<br>00:42:58,000 –&gt; 00:43:03,000<br>you can still interpret those bytes because everything you need to know what are in those bytes, what those bytes mean,</p>
<p>555<br>00:43:03,000 –&gt; 00:43:05,000<br>are actually in the page itself.</p>
<p>556<br>00:43:05,000 –&gt; 00:43:07,000<br>And then there’s other limitations in Oracle.</p>
<p>557<br>00:43:07,000 –&gt; 00:43:09,000<br>Like you can only have a thousand columns in Oracle.</p>
<p>558<br>00:43:09,000 –&gt; 00:43:14,000<br>Not just because they want to be self-contained, there’s other reasons why they have that problem.</p>
<p>559<br>00:43:14,000 –&gt; 00:43:19,000<br>But again, the bottom line is everything we need to know within the file is in the file itself.</p>
<p>560<br>00:43:19,000 –&gt; 00:43:23,000<br>We don’t need to go read some external catalog or external data.</p>
<p>561<br>00:43:23,000 –&gt; 00:43:31,000<br>And so we’ll keep track of the table schema, and the way they’re going to do this is to basically serialize,</p>
<p>562<br>00:43:31,000 –&gt; 00:43:37,000<br>here’s my columns, there are this type and this size and so forth, and additional annotations about them.</p>
<p>563<br>00:43:37,000 –&gt; 00:43:41,000<br>And then within the row gives us a keep track of how it’s being coded.</p>
<p>564<br>00:43:41,000 –&gt; 00:43:46,000<br>So that when I read the file, I know how to then process this bytes and put it back to its original form.</p>
<p>565<br>00:43:47,000 –&gt; 00:43:49,000<br>Does everybody here use thrift or protobuff?</p>
<p>566<br>00:43:49,000 –&gt; 00:43:51,000<br>One, few.</p>
<p>567<br>00:43:51,000 –&gt; 00:43:55,000<br>But protobuff is from Google, thrift is from meta, sorry.</p>
<p>568<br>00:43:55,000 –&gt; 00:44:03,000<br>Basically again, you define like a schema, almost like a great table statement, you define like, here’s my columns,</p>
<p>569<br>00:44:03,000 –&gt; 00:44:14,000<br>here’s other types and so forth, their names, and then they have a way to then generate a binary encoding for what that schema actually is.</p>
<p>570<br>00:44:15,000 –&gt; 00:44:21,000<br>And so they basically just piggyback off of this, serialize the bytes and then embed that in the file and metadata.</p>
<p>571<br>00:44:21,000 –&gt; 00:44:30,000<br>The big problem is going to be though, if I have a really wide table and say I only want to learn about two or three columns out of a thousand columns,</p>
<p>572<br>00:44:30,000 –&gt; 00:44:37,000<br>I have to destabilize the entire protobuff for the thrift message to convert that.</p>
<p>573<br>00:44:37,000 –&gt; 00:44:40,000<br>So that’s going to be a big problem for these file formats as well.</p>
<p>574<br>00:44:40,000 –&gt; 00:44:52,000<br>There’s newer versions of like, there’s better versions like flatbuff, flatbuff or some Google, there’s other, there’s better versions of these things, but at the time this is what existed and the file formats carry that legacy.</p>
<p>575<br>00:44:52,000 –&gt; 00:44:59,000<br>The real group all sets and length, we talked about this, again, this is going to give us a direct view of how to jump in the file to find the beginning of each row group.</p>
<p>576<br>00:44:59,000 –&gt; 00:45:05,000<br>And then there’ll be some basic metadata like the number of two balls that I have in each row group with the zone maps are potentially.</p>
<p>577<br>00:45:06,000 –&gt; 00:45:12,000<br>And that can use this information to determine whether I need to even read the rest of the file.</p>
<p>578<br>00:45:12,000 –&gt; 00:45:22,000<br>So the formats are what we’ve mostly already talked about, we’re going to use packs for this and then we’re going to split it up based on two row groups that will contain one of our column chunks.</p>
<p>579<br>00:45:22,000 –&gt; 00:45:26,000<br>And the question has come up is how, what size of the row group we should use.</p>
<p>580<br>00:45:27,000 –&gt; 00:45:33,000<br>And in the case of parquet, they’re going to use just on the number of two balls that you actually have.</p>
<p>581<br>00:45:33,000 –&gt; 00:45:39,000<br>And you can change that, you can specify as you’re creating the file, the guy on what my row group would be a million two balls or 10 million two balls for so forth.</p>
<p>582<br>00:45:39,000 –&gt; 00:45:43,000<br>But it’s always based on the number of two balls.</p>
<p>583<br>00:45:43,000 –&gt; 00:45:48,000<br>Ork takes a different approach and they specify it based on the size of the data.</p>
<p>584<br>00:45:48,000 –&gt; 00:45:52,000<br>So the default I think is 250 megs.</p>
<p>585<br>00:45:53,000 –&gt; 00:45:57,000<br>So what are some pros and cons about these? We already talked a little bit about this.</p>
<p>586<br>00:45:57,000 –&gt; 00:46:01,000<br>The two balls are massive parquet and then you have the massive file.</p>
<p>587<br>00:46:01,000 –&gt; 00:46:07,000<br>Yes, the two balls are massive or like if I have a lot of attributes, right, ignore like storing wops.</p>
<p>588<br>00:46:07,000 –&gt; 00:46:15,000<br>If I had just say, I have a 10,000 columns, then a million two balls with times 10,000 columns is going to be pretty big row group.</p>
<p>589<br>00:46:15,000 –&gt; 00:46:20,000<br>So what happens in that case? Why is that bad?</p>
<p>590<br>00:46:21,000 –&gt; 00:46:29,000<br>At no point, sorry, his statement is, well, even fit in your storage.</p>
<p>591<br>00:46:29,000 –&gt; 00:46:33,000<br>At no point this semester should we ever say, are we going to run out of disk?</p>
<p>592<br>00:46:33,000 –&gt; 00:46:36,000<br>I mean, S3 for our purposes is infinite.</p>
<p>593<br>00:46:36,000 –&gt; 00:46:42,000<br>Like, long as your credit card keeps, you know, as long as Amazon keeps charging your credit card, you’re never going to run out of storage.</p>
<p>594<br>00:46:42,000 –&gt; 00:46:49,000<br>And then you’re at the point where like they start running out of storage, they will call you and be like, hey, who are you, what are you doing?</p>
<p>595<br>00:46:49,000 –&gt; 00:46:52,000<br>Storage and infinite for us, yes.</p>
<p>596<br>00:46:52,000 –&gt; 00:46:55,000<br>Could it be the data split across multiple pages?</p>
<p>597<br>00:46:55,000 –&gt; 00:46:59,000<br>They could be split across multiple pages. That’s assumed, right?</p>
<p>598<br>00:46:59,000 –&gt; 00:47:02,000<br>Like page size of four kilobytes in hardware, but that’s assumed.</p>
<p>599<br>00:47:02,000 –&gt; 00:47:04,000<br>Your zone map, you know less helpful?</p>
<p>600<br>00:47:04,000 –&gt; 00:47:06,000<br>He says your zone map is a couple of less helpful. Yes, that’s one. Yes.</p>
<p>601<br>00:47:06,000 –&gt; 00:47:16,000<br>Like again, the granularity of the scope of the zone map is so large that like anything, like anything I look in there, be like, oh yeah, your value range for this number is zero to an infinity.</p>
<p>602<br>00:47:16,000 –&gt; 00:47:19,000<br>Great, like that’s useless.</p>
<p>603<br>00:47:19,000 –&gt; 00:47:23,000<br>Missing a key thing, one more.</p>
<p>604<br>00:47:23,000 –&gt; 00:47:26,000<br>I have to bring in the entire row group, right?</p>
<p>605<br>00:47:26,000 –&gt; 00:47:34,000<br>So if I have a massive row group, then I’m just going to have this choosing a memory that I’ve got to bring in in order to start processing and understand what’s going on.</p>
<p>606<br>00:47:34,000 –&gt; 00:47:35,000<br>Right?</p>
<p>607<br>00:47:36,000 –&gt; 00:47:43,000<br>The benefit though is that I can size this in such a way that I’m guaranteed to, or at least I can maximize my chances to do vectorized processing.</p>
<p>608<br>00:47:43,000 –&gt; 00:47:53,000<br>That like, I’m always going to have enough data to put in my simd lanes or scan across multiple threads process and paralyze it.</p>
<p>609<br>00:47:53,000 –&gt; 00:47:54,000<br>Yes.</p>
<p>610<br>00:47:54,000 –&gt; 00:47:57,000<br>What are you pulling from S3? What should you be pulling from S3?</p>
<p>611<br>00:47:57,000 –&gt; 00:48:00,000<br>Is David as if you’re pulling from S3, would you be pulling entire part K5? No.</p>
<p>612<br>00:48:00,000 –&gt; 00:48:03,000<br>So like again, in S3, you can do bite offsets and lengths.</p>
<p>613<br>00:48:03,000 –&gt; 00:48:08,000<br>So I get to the header, sorry, get the footer, get the metadata, I know what row groups I have.</p>
<p>614<br>00:48:08,000 –&gt; 00:48:15,000<br>And then if my zone maps are selective enough, I can then say, oh, I don’t need this row group and this row group, then these will get the bite ranges that I need.</p>
<p>615<br>00:48:15,000 –&gt; 00:48:27,000<br>When would you not have enough values?</p>
<p>616<br>00:48:27,000 –&gt; 00:48:30,000<br>It’s a question, when would you not have enough data put your simd lanes?</p>
<p>617<br>00:48:30,000 –&gt; 00:48:39,000<br>So again, say if I have a really wide tuple with a lot of attributes, then I’m only going to have, this is an extreme example.</p>
<p>618<br>00:48:39,000 –&gt; 00:48:44,000<br>I’m going to have four tuples in my row group because it’s just so huge.</p>
<p>619<br>00:48:44,000 –&gt; 00:48:55,000<br>Now, maybe simd lanes, maybe two fine graded layer, but if I’m processing this multiple threads, I don’t think again, I’m jumping ahead to what we’re talking about in the semester.</p>
<p>620<br>00:48:55,000 –&gt; 00:49:02,000<br>But like, don’t think in the bus tub world where one thread is running this one operator, it’s one page from memory, and then does whatever one’s on it.</p>
<p>621<br>00:49:02,000 –&gt; 00:49:21,000<br>Think of like, I’m going to have some other piece of the system, the I always schedule or what everyone call it, I need this file, you go get the blocks in, and then maybe some coordinator figures like, okay, yeah, we do want to process this and oh, yeah, it is, it’s half a gig, so this thread process is the first half, so the other thread process is the second half.</p>
<p>622<br>00:49:21,000 –&gt; 00:49:24,000<br>So I want to have enough work for everyone to do.</p>
<p>623<br>00:49:24,000 –&gt; 00:49:31,000<br>Yes. The decision of like, what are the use like number two, which is the score time, is the efficiency is pretty good, the size of the two pools?</p>
<p>624<br>00:49:31,000 –&gt; 00:49:40,000<br>Yes. Like, if people are like, like, hybrid systems, you choose which one you use based off of, what the end you’re playing in your specific file.</p>
<p>625<br>00:49:40,000 –&gt; 00:49:50,000<br>So this question is, like, again, the cop-on answer in the end is it depends, so clearly it depends on, sometimes this is good, and then this is good, is anybody trying to do a hybrid solution, we support both.</p>
<p>626<br>00:49:50,000 –&gt; 00:49:54,000<br>But that’s going to be another theme we’ll see as we go along, although we’re short in time.</p>
<p>627<br>00:49:54,000 –&gt; 00:50:06,000<br>The increasing the complexity of the file format means that when you’re actually processing two pools, then you have to like, you have branched with predictions, because now you’re different code bats, you mean you consider.</p>
<p>628<br>00:50:06,000 –&gt; 00:50:12,000<br>Plus, there’s the engineering complexity, now to support both. So, parquet does this or does that?</p>
<p>629<br>00:50:12,000 –&gt; 00:50:26,000<br>You see, there’s other things too, like, in bringing like, for like a transactional system, should you flush the log buffer when you’ve written so many transactions, or when the log files are certain size, or when you’ve run for a certain amount of time?</p>
<p>630<br>00:50:26,000 –&gt; 00:50:34,000<br>Yeah, there’s pros and cons of all these different choices, but from the engineering perspective, it’s just each of your pick one. Or should I use two-based locking or OCC?</p>
<p>631<br>00:50:34,000 –&gt; 00:50:41,000<br>Yeah, there’s times you want one versus the other, but everyone does just one. It’s so hard to build the whole system, why add additional complexity?</p>
<p>632<br>00:50:41,000 –&gt; 00:50:42,000<br>Yes.</p>
<p>633<br>00:50:52,000 –&gt; 00:50:57,000<br>This question is why is not having being able to put a rogue group entirely memory of that idea?</p>
<p>634<br>00:50:57,000 –&gt; 00:51:05,000<br>So, I got to go fetch it from somewhere. I bring it to the node that’s going to process it. I don’t have enough memory for it. Where does it got to go?</p>
<p>635<br>00:51:05,000 –&gt; 00:51:07,000<br>To disk.</p>
<p>636<br>00:51:07,000 –&gt; 00:51:21,000<br>Yes, David. Could you break up the rogue group and execute it incrementally? Yes. Then it’s another request that we get more data as they need.</p>
<p>637<br>00:51:21,000 –&gt; 00:51:40,000<br>This is just a diagram from data bricks showing sort of a tutorial what Park A looks like. You can sort of see in all things we talked about here, there’s the footer, there’s these rogue groups that are numbered, and then within them you have a column chunk.</p>
<p>638<br>00:51:40,000 –&gt; 00:51:59,000<br>In the column chunk, you would have additional page metadata for the encoder bags and so forth. The exact distinction of how we’re going to lay out those internal parts versus Park A and Orc, I don’t care that much about, but again, the idea is that there’s this hierarchal nature to the file, and we can store additional metadata as we go along.</p>
<p>639<br>00:51:59,000 –&gt; 00:52:05,000<br>And the right size of each part, again, depends on a bunch of things.</p>
<p>640<br>00:52:05,000 –&gt; 00:52:17,000<br>We’re at CMU and they love type systems in the PL group, so we have to worry about a little bit. So, type systems is going to define in our file format, and you’re shaking your head, yes.</p>
<p>641<br>00:52:17,000 –&gt; 00:52:25,000<br>The type systems is going to define how we actually can store types themselves and what are the bytes going to look like.</p>
<p>642<br>00:52:25,000 –&gt; 00:52:42,000<br>So, there’s the physical type, and that’s the lowest level representation you’d have for a given value, and for this we’re not going to think special, we’re for the most part for integers and floating point numbers, we’re going to use the IEEE 734 standard, and that specifies what how hardware should represent the data.</p>
<p>643<br>00:52:42,000 –&gt; 00:52:50,000<br>You can think of like a regular integer in 32 on C++, that’s 74 standard, that’s what I get, because that’s the hardware given.</p>
<p>644<br>00:52:50,000 –&gt; 00:52:57,000<br>So, we’re talking about strings in a few more lectures, again, there’s some tricks we can do to speed that up.</p>
<p>645<br>00:52:57,000 –&gt; 00:53:04,000<br>And then the logical types will be built on top of the physical types, and it’s basically going to find how we would map some logical type to a physical type.</p>
<p>646<br>00:53:04,000 –&gt; 00:53:14,000<br>So, for example, if I want to store timestamps, what is a timestamp? It’s just the number of seconds or milliseconds, or nanoseconds, from some starting point, well, that’s just a number.</p>
<p>647<br>00:53:14,000 –&gt; 00:53:24,000<br>So, I can just store that as a physical type as an int64, and then I have a notion of a logical type that says how to parse the bits within that physical type.</p>
<p>648<br>00:53:24,000 –&gt; 00:53:38,000<br>So, a parking org have different complexity to their type system, and then that determines how much work you have to do upstream is the thing actually generating data for these file formats or actually consuming it, how much you’re working up to do to interpret the contents.</p>
<p>649<br>00:53:38,000 –&gt; 00:53:53,000<br>So, in parkay, they have the bare minimum of types. They only have int32, int64, int64, and int64, and then the set of 50 or standard, and then the byte arrays, and then the only have strings, because you would interpret that as a byte array.</p>
<p>650<br>00:53:53,000 –&gt; 00:54:04,000<br>So, it’s interesting, they don’t store, you know, an 8-bit int or 16-bit int. So, if you declare I want an 8-bit int or 16-bit int, they’re still going to store it as a 32-bit int.</p>
<p>651<br>00:54:04,000 –&gt; 00:54:10,000<br>And the reason is that, okay, well, yeah, there’s a bunch of zeros in my bits that I’m not using, well, a lot of this compress out.</p>
<p>652<br>00:54:10,000 –&gt; 00:54:19,000<br>And that reduces the complexity of what they have to support. Or, because much larger, they have all these various types.</p>
<p>653<br>00:54:19,000 –&gt; 00:54:28,000<br>Some of these are logical, some of these are physical, they don’t really make a distinction, but you can find way more things than you can with in parkay.</p>
<p>654<br>00:54:29,000 –&gt; 00:54:34,000<br>I’m not saying one is bad versus the other, this is how they chose the implement things.</p>
<p>655<br>00:54:35,000 –&gt; 00:54:40,000<br>All right, so now the encoding team is going to specify, for a given physical or logical type, yes.</p>
<p>656<br>00:54:40,000 –&gt; 00:54:44,000<br>Why isn’t org better, because if you have more types, you can do different coding for longer.</p>
<p>657<br>00:54:44,000 –&gt; 00:54:48,000<br>So, I should, why isn’t org better, because if you have more types, you can do more things encoding them.</p>
<p>658<br>00:54:48,000 –&gt; 00:54:54,000<br>So, the parkay will say you just represent those logical types, and you extend the file from that way.</p>
<p>659<br>00:54:55,000 –&gt; 00:54:59,000<br>I’ve never taken a type system class, this is my understanding.</p>
<p>660<br>00:55:01,000 –&gt; 00:55:06,000<br>All right, so again, the coding scheme is going to specify for the physical or logical types, the actual bits themselves.</p>
<p>661<br>00:55:06,000 –&gt; 00:55:14,000<br>How, you know, how can we actually store them for contiguous or related tuples within our column chunk.</p>
<p>662<br>00:55:14,000 –&gt; 00:55:18,000<br>And the paper talks about a bunch of different schemes that we’ve covered in the intro class.</p>
<p>663<br>00:55:18,000 –&gt; 00:55:21,000<br>I don’t think I mentioned Framer reference in the intro class.</p>
<p>664<br>00:55:22,000 –&gt; 00:55:30,000<br>It’s basically like Delta encoding, but instead of having, like in Delta encoding, it’s like what’s the difference between the value before you.</p>
<p>665<br>00:55:30,000 –&gt; 00:55:40,000<br>You pick some starting point, like maybe the min value of a column chunk, and then now you’re just storing the delta from that global value.</p>
<p>666<br>00:55:40,000 –&gt; 00:55:46,000<br>It’s sort of variant of Delta encoding.</p>
<p>667<br>00:55:47,000 –&gt; 00:55:54,000<br>And then there’s partial frame of reference coding, which I think the paper mentions P4, that’s basically for any outliers that would wreck your encoding scheme.</p>
<p>668<br>00:55:54,000 –&gt; 00:55:57,000<br>They have a way to handle those separately, but we can ignore that.</p>
<p>669<br>00:55:57,000 –&gt; 00:56:03,000<br>The one I spent time talking about is actually dictionary encoding, because one, this is the most common coding scheme that most data systems support.</p>
<p>670<br>00:56:03,000 –&gt; 00:56:07,000<br>This is where you get most of the win for getting compression.</p>
<p>671<br>00:56:07,000 –&gt; 00:56:15,000<br>And you know, for these, the different schemes here vary not in how they implemented, but more like when it’s triggered.</p>
<p>672<br>00:56:15,000 –&gt; 00:56:23,000<br>Like in ORC, they’re very aggressive using RLE, like if they see three or more particular values, then RLE kicks in.</p>
<p>673<br>00:56:23,000 –&gt; 00:56:28,000<br>If you’re in parquet, it has to be eight or more, and you can’t change that.</p>
<p>674<br>00:56:28,000 –&gt; 00:56:39,000<br>And so, you know, probably dictionary encoding, because you can then take the results of the dictionary encoding, the compressed column, and then apply all these other things on top of it, and get it compressed even more.</p>
<p>675<br>00:56:39,000 –&gt; 00:56:53,000<br>But dictionary encoding, again, this is the same thing with interclass, the basic idea is that we’re going to replace values that occur often in our column with some smaller fixed length dictionary code from a smaller domain.</p>
<p>676<br>00:56:53,000 –&gt; 00:57:00,000<br>And then we use that at runtime to figure out, okay, if I see this dictionary code in this column, I can do reference the dictionary to figure out what the actual value should be.</p>
<p>677<br>00:57:00,000 –&gt; 00:57:09,000<br>Again, this is how we convert variable length strings, variable length data into fixed length values that we can store in our columns.</p>
<p>678<br>00:57:09,000 –&gt; 00:57:20,000<br>It means the meta data is going to be now arbitrary length, because all the strings that were variable length in our column are now being stored in the dictionary, which is stored in the header of the row group.</p>
<p>679<br>00:57:20,000 –&gt; 00:57:38,000<br>So, the code dictionary code could either be positions within the dictionary, and therefore you have to maintain a hash table to figure out how to find that position in the dictionary, or if you do offsets within the dictionary, assuming it’s like everything is because it’s getting those bytes.</p>
<p>680<br>00:57:38,000 –&gt; 00:57:44,000<br>We can also, also, sort the values in the dictionary, and that’ll help us get some additional benefits for compression in some cases.</p>
<p>681<br>00:57:44,000 –&gt; 00:57:52,000<br>And then we can further compress the dictionary or encode in columns to reduce the name further, and use RLE and other techniques.</p>
<p>682<br>00:57:52,000 –&gt; 00:58:01,000<br>So, in the different formats, they handle the case when the dictionary becomes too large, because there’s too many unique values, and therefore I’m losing all the benefit of dictionary coding.</p>
<p>683<br>00:58:01,000 –&gt; 00:58:15,000<br>Like, if I have my column is just monitoring increasing values from one to a billion, then it’s kind of stupid to store dictionary code for a billion two-poles that are all unique, because I’m going to have a billion dictionary codes.</p>
<p>684<br>00:58:15,000 –&gt; 00:58:23,000<br>So, now I have the original column that’s dictionary coded with a billion unique values, and I’m storing the billion unique values in the dictionary, so I’m double the size of it.</p>
<p>685<br>00:58:23,000 –&gt; 00:58:28,000<br>So, they have various techniques that figure out, okay, this is not working out, I don’t want to do dictionary encoding.</p>
<p>686<br>00:58:28,000 –&gt; 00:58:39,000<br>So, in case of par k, if the dictionary gets lower to the one megabyte, then they just stop, and then everything comes out, comes after that point, it’s just stored as regular the plain encoding, the original values.</p>
<p>687<br>00:58:39,000 –&gt; 00:58:51,000<br>In the case of ORC, they compute the number distinct values ahead of time, but basically they have a look ahead buffer where they can say, let me look at the, when I’m starting to write out chunk of data, let me go look ahead like 512 values,</p>
<p>688<br>00:58:51,000 –&gt; 00:59:00,000<br>or 500 values, go figure out whether there’s enough distinct values, and I think the rest of the data is going to look like that, and doesn’t make sense to do dictionary coding or not.</p>
<p>689<br>00:59:00,000 –&gt; 00:59:09,000<br>If they get it wrong, then they do the same thing as par k, they basically stop encoding, and just store data in this native format.</p>
<p>690<br>00:59:09,000 –&gt; 00:59:20,000<br>So, here’s a really simple example, I have some column with a bunch of strings in it. If I’m doing an un-sworded dictionary, then the values of the strings, then I’m trying to compress,</p>
<p>691<br>00:59:20,000 –&gt; 00:59:40,000<br>and the order that they appear as I’m scanning through the column. I can either store the position in the dictionary, again, so the first one here was William, so at offset 1, then I would find the actual string that I wanted.</p>
<p>692<br>00:59:40,000 –&gt; 00:59:54,000<br>Or I can store this as an offset, so if I take the treatises as bytes, so I know that if I look at my dictionary, I’d need to jump to the seventh byte, and that’s going to tell me where the starting point for my entry will be.</p>
<p>693<br>00:59:54,000 –&gt; 00:59:57,000<br>And I don’t need to maintain a hash table.</p>
<p>694<br>00:59:57,000 –&gt; 00:59:58,000<br>Yes.</p>
<p>695<br>00:59:58,000 –&gt; 01:00:03,000<br>Any other questions? What are the advantages of this advantage? Hash table.</p>
<p>696<br>01:00:03,000 –&gt; 01:00:11,000<br>So, we’ll see Arrow later on. Arrow does it this way, because then you don’t have to serialize the hash table, you pre-sort everything, and then jump into it more easily.</p>
<p>697<br>01:00:11,000 –&gt; 01:00:18,000<br>I was thinking if you just had a look at the right, that’s an offset of the right, as it’s indexed index, or not, but the more it’s a variable one.</p>
<p>698<br>01:00:19,000 –&gt; 01:00:35,000<br>Any of us sort of dictionary? Again, you get all the dictionary bags ahead of time, and then you sort them, and then you, again, it’s just like before you have position into the dictionary or an offset, the byte offset.</p>
<p>699<br>01:00:35,000 –&gt; 01:00:46,000<br>But you can kind of see now, here in this example here, I have Andy repeated a bunch of times, well one is now I’ve heard things to integers in my dictionary encodes,</p>
<p>700<br>01:00:47,000 –&gt; 01:01:08,000<br>but now I have repeated values. So then I can take this and say, oh, I have four twos in a row, let me compress that with RLE, or I can do, you know, delta encoding or sort of frame reference, where now I just say, okay, these are all integers, they’re all in a small domain, because I only have four different unique values in my dictionary, and I can then compress these things.</p>
<p>701<br>01:01:08,000 –&gt; 01:01:09,000<br>Yes.</p>
<p>702<br>01:01:09,000 –&gt; 01:01:25,000<br>When we talked about doing a single column, we talked about the number of distinct values, extremely high, very very strange, to indirect something.</p>
<p>703<br>01:01:26,000 –&gt; 01:01:42,000<br>Yes, question is, okay, so if I recognize that I have a bunch of random strings, and I can’t do dictionary encoding, but now I have a bunch of very long data, how do I handle that? You have auxiliary data, and then now you’re just throwing the offsets into that blob.</p>
<p>704<br>01:01:43,000 –&gt; 01:01:57,000<br>Okay, so a couple of design decisions we have to make is what data we actually want to compress with dictionary encoding? I mean, we’re running short on time, sorry, you know, it’s all I’m going to get to, sorry.</p>
<p>705<br>01:01:58,000 –&gt; 01:02:05,000<br>So, as I said before, parquet, they compress everything. Flows, integers, strings, dates, they have again small number of pinnets types, they compress all that.</p>
<p>706<br>01:02:05,000 –&gt; 01:02:26,000<br>In ORC, they only compress strings. And this seems like common sense, because most of the variability you can see, the randomness you would see in values are going to be mostly, sorry, the repeated values will be in strings, not in integers and floats.</p>
<p>707<br>01:02:27,000 –&gt; 01:02:32,000<br>But when we did our analysis, it actually turned out to be the work way, sorry, the parquet way is better.</p>
<p>708<br>01:02:33,000 –&gt; 01:02:41,000<br>Next question is, what do you do, can you compress the encoded data? So parquet is pretty simplistic, they’ll just do RLE and bitpacking.</p>
<p>709<br>01:02:41,000 –&gt; 01:02:53,000<br>Again, bitpacking is just saying, oh, I recognize that my dictionary codes are 33-bit integers, but my values are within 0 to 20, so I can use 8-bit integers for that, or even some smaller amount.</p>
<p>710<br>01:02:53,000 –&gt; 01:03:02,000<br>And now my comm gets compressed even further. And then I’ve repeated values, I can do RLE on that. But again, they only kick RLE, only kicks in if you have 8 or more values.</p>
<p>711<br>01:03:02,000 –&gt; 01:03:18,000<br>In ORC, they have a bunch of different things you could do, RLE, delta encoding, bitpacking, frame of reference. And they basically have a greedy algorithm, they look ahead and the buffer, they’re trying to figure out what the data looks like, run some heuristics to figure out which of these approaches is the best.</p>
<p>712<br>01:03:19,000 –&gt; 01:03:27,000<br>It always tries to use RLE first, and then if you can’t, then it drives delta, and then if you can’t do that, it uses either bitpacking or frame of reference.</p>
<p>713<br>01:03:27,000 –&gt; 01:03:40,000<br>And then another design station is the dictionary that’s being generated for this encoded values, do you expose that to the outside of the file format, or the library that’s processing the file?</p>
<p>714<br>01:03:41,000 –&gt; 01:03:43,000<br>Let me take us why you’d want to do that.</p>
<p>715<br>01:03:48,000 –&gt; 01:04:04,000<br>So we’ll go back here. So say I want to look up, find all the, my query is, my query is, select count from the table, where name equals Andy.</p>
<p>716<br>01:04:05,000 –&gt; 01:04:14,000<br>So if I can then look at the dictionary, I can, it’s almost like a ZOMA, I can see whether it’s any, is even there or not.</p>
<p>717<br>01:04:14,000 –&gt; 01:04:18,000<br>Or, select more.</p>
<p>718<br>01:04:18,000 –&gt; 01:04:20,000<br>What’s that?</p>
<p>719<br>01:04:21,000 –&gt; 01:04:24,000<br>Is this a query?</p>
<p>720<br>01:04:24,000 –&gt; 01:04:29,000<br>It seems to be a range dictionary, it’s a range query, maybe I’ll say, is there any sort of dictionary.</p>
<p>721<br>01:04:29,000 –&gt; 01:04:36,000<br>Yeah, you want to count all the, count all the unique values within a given range, I can do that by looking at the dictionary.</p>
<p>722<br>01:04:36,000 –&gt; 01:04:38,000<br>That’s another example.</p>
<p>723<br>01:04:38,000 –&gt; 01:04:41,000<br>So, parking or do not do this.</p>
<p>724<br>01:04:41,000 –&gt; 01:04:52,000<br>So in the library implementations, when you say, scan some column, you basically get an iterator through their library, and that’s going to give you back the columns that you asked for in their original form.</p>
<p>725<br>01:04:52,000 –&gt; 01:04:58,000<br>So on the cover is the library is doing all the decoding, depressing for you.</p>
<p>726<br>01:04:58,000 –&gt; 01:05:09,000<br>So again, that means that you can’t really push down predicates all the way down to the lowest level of looking at the file itself, you do have to do whatever, whatever the library spits out back to you.</p>
<p>727<br>01:05:10,000 –&gt; 01:05:12,000<br>So this is, in recognize, it’s a problem.</p>
<p>728<br>01:05:12,000 –&gt; 01:05:17,000<br>So there’s a paper we’re not going to cover from Google called, for a system called Prasella.</p>
<p>729<br>01:05:17,000 –&gt; 01:05:27,000<br>This was developed at, in house, for YouTube to process, do analytics, but also serving data online manner, which we don’t want to care about right now.</p>
<p>730<br>01:05:27,000 –&gt; 01:05:34,000<br>But there’s this little blurb here, they developed their own file system, or so file format, instead of using OrkapparK called Artis.</p>
<p>731<br>01:05:34,000 –&gt; 01:05:44,000<br>And one advantage they talk about in Artis is that, oh, they actually expose the dictionary to the query engine, so that you can do the predicate push down that you want to do.</p>
<p>732<br>01:05:44,000 –&gt; 01:05:55,000<br>So this is something I said meant before. This is a known problem, Parkane, Ork, and then the newer stuff that people are looking at, once the solve the problems that expose to you what the dictionary is.</p>
<p>733<br>01:05:55,000 –&gt; 01:06:06,000<br>Because then now you can do, you know, you can do evaluation directly in compressed data by compressing your predicate, and then comparing your predicate versus the compressed data rather than decompressing everything first.</p>
<p>734<br>01:06:08,000 –&gt; 01:06:16,000<br>All right, next thing is do block compression, and this is basically taking off the shelf, naive, general purpose, compression algorithm,</p>
<p>735<br>01:06:16,000 –&gt; 01:06:27,000<br>that just take the blocks, the row groups, and just run that and compress it. And the paper talks about, Parkane, Ork, I mean the default is snappy.</p>
<p>736<br>01:06:27,000 –&gt; 01:06:36,000<br>The best compression algorithm you want to use right now is actually Z standard from Facebook. There’s a newer version that’s not out yet, it’s called something different.</p>
<p>737<br>01:06:36,000 –&gt; 01:06:46,000<br>That supposedly is better, but Parkane Ork can make come with snappy because that was the thing back in the day, when those fault parts are invented.</p>
<p>738<br>01:06:46,000 –&gt; 01:07:02,000<br>So there’s this, you know, the things that consider whether you actually want to do this or not is whether you, you know, you’re willing to pay the computational overhead of compress, or is that decompression of the data, the blocks when it comes back, even though it’s already been encoded with one addiction and coding other schemes.</p>
<p>739<br>01:07:02,000 –&gt; 01:07:10,000<br>And you can still get some compression benefits, but now it’s going to make processing the data much slower because you have to do this extra step to decompress it.</p>
<p>740<br>01:07:10,000 –&gt; 01:07:19,000<br>Because these are opaque compression schemes, meaning if I run something through snappy or Z standard, the bytes come out, the data system doesn’t know what those bytes mean.</p>
<p>741<br>01:07:19,000 –&gt; 01:07:25,000<br>And I can’t jump to arbitrary offsets with them and to go find data I’m looking for, I got to decompress the whole block.</p>
<p>742<br>01:07:25,000 –&gt; 01:07:33,000<br>Again, this made sense in 2013, 2012, when these fault formats were designed because disk was slow, network slow.</p>
<p>743<br>01:07:33,000 –&gt; 01:07:45,000<br>So if I can reduce the amount of data at the go read, you know, from some local source, and then bring it to my memory, then I’m willing to pay that CPU cost.</p>
<p>744<br>01:07:45,000 –&gt; 01:07:51,000<br>But things have changed a lot now. The CPU is actually one of the slower things.</p>
<p>745<br>01:07:51,000 –&gt; 01:07:56,000<br>So this actually doesn’t make sense anymore.</p>
<p>746<br>01:07:56,000 –&gt; 01:08:04,000<br>So the additional metadata we can keep track of are the filters. So the only two types of filters that they would have are zone maps and blue filters.</p>
<p>747<br>01:08:04,000 –&gt; 01:08:13,000<br>Again, I remember, even though the paper calls it a page index, what’s the difference between that index and a filter?</p>
<p>748<br>01:08:13,000 –&gt; 01:08:20,000<br>Index tells you where something is and what if it exists, a filter tells you something could exist or does exist. Doesn’t tell you where it is though.</p>
<p>749<br>01:08:20,000 –&gt; 01:08:29,000<br>So zone maps can say here’s my minimax values, I’m trying to find something within the given range. If it’s in that minimax range, then it exists, but I don’t know where it is.</p>
<p>750<br>01:08:29,000 –&gt; 01:08:37,000<br>I got to go to a central scan to find it. Whereas a B plus G would say, hey, you’re at this offset, right? And we don’t care about.</p>
<p>751<br>01:08:37,000 –&gt; 01:08:48,000<br>So we’re ready to write zone maps. And again, my default parking or going to store in the zone maps in the header, each group, you can store it in the file level, but I don’t think that’s on my default.</p>
<p>752<br>01:08:48,000 –&gt; 01:08:57,000<br>And then for blue filters within each row group, they can keep track of whether a value could exist for a given column.</p>
<p>753<br>01:08:57,000 –&gt; 01:09:06,000<br>Again, a blue filter is a probabilistic data structure. It can tell you definitely that something does not exist, but it can tell you what to tell you that something may exist.</p>
<p>754<br>01:09:06,000 –&gt; 01:09:10,000<br>You can get false positives, but not false negatives. Yes.</p>
<p>755<br>01:09:10,000 –&gt; 01:09:13,000<br>Why does it matter whether the value is a cluster for a blue filter?</p>
<p>756<br>01:09:13,000 –&gt; 01:09:24,000<br>So why does it matter whether values are closer for a blue filter? Because the, how do you say this?</p>
<p>757<br>01:09:24,000 –&gt; 01:09:27,000<br>I don’t know why I wrote that.</p>
<p>758<br>01:09:27,000 –&gt; 01:09:30,000<br>It doesn’t matter what order you reserve.</p>
<p>759<br>01:09:30,000 –&gt; 01:09:33,000<br>Because you hash it in the scattered.</p>
<p>760<br>01:09:33,000 –&gt; 01:09:41,000<br>You have a cluster that you can store on with one part.</p>
<p>761<br>01:09:41,000 –&gt; 01:09:47,000<br>This might be for this. This should be for this.</p>
<p>762<br>01:09:47,000 –&gt; 01:09:52,000<br>The reason why it matters is because the range is smaller and I can throw things out.</p>
<p>763<br>01:09:52,000 –&gt; 01:09:56,000<br>If it’s zero to infinity, that’s the usual zone map.</p>
<p>764<br>01:09:56,000 –&gt; 01:09:58,000<br>This should be for this.</p>
<p>765<br>01:09:58,000 –&gt; 01:10:12,000<br>The split block blue filter is basically a way to, if your blue filter is so many bits, instead of having your hash function look at any possible number of bits, you basically narrow down to a block or subset of it.</p>
<p>766<br>01:10:12,000 –&gt; 01:10:15,000<br>And so, and that means you can bring it in within a single cache line.</p>
<p>767<br>01:10:15,000 –&gt; 01:10:23,000<br>We’ll cover cache line stuff later on, but like, the way it is reduced the, keep everything in like L1 to run as fast as possible.</p>
<p>768<br>01:10:23,000 –&gt; 01:10:31,000<br>This part is a bit tricky. I’ll do what I can on the time, but the nested data structure is also really important.</p>
<p>769<br>01:10:31,000 –&gt; 01:10:40,000<br>We’ll cover the Dremel system later on, but there’s this paper from, I think, 2011, 2010, about the system called Dremel,</p>
<p>770<br>01:10:40,000 –&gt; 01:10:45,000<br>which is precursor to what, or it is the Dremel’s internal name for BigQuery.</p>
<p>771<br>01:10:45,000 –&gt; 01:10:55,000<br>So we’ll read those papers, but they talk about how back then that like I’ve Google when they were building Dremel, they had all these applications generating this protocol buffer data.</p>
<p>772<br>01:10:55,000 –&gt; 01:11:01,000<br>That’s all nested and semi-structured and inconsistent schemas, and they needed the way to efficiently process them.</p>
<p>773<br>01:11:01,000 –&gt; 01:11:07,000<br>So the Dremel paper talks about using this technique called a record shredding, which is mentioned in the paper you guys read.</p>
<p>774<br>01:11:07,000 –&gt; 01:11:16,000<br>And this is an alternative, instead of a better approach to the, what Ork does, and other, other systems do, we call length of presence encoding.</p>
<p>775<br>01:11:16,000 –&gt; 01:11:26,000<br>So we can cover this again later if necessary, but the basic idea with shredding is that, again, instead of storing the semi-structured data that I have,</p>
<p>776<br>01:11:26,000 –&gt; 01:11:42,000<br>as a single blob column, that I then have a parse every single time I want to be processing on it, I’m going to split it up so that every level in a path is now treated as a separate column.</p>
<p>777<br>01:11:43,000 –&gt; 01:11:57,000<br>Now I can rip through those columns to say, if I need to find like, for a given field in my JSON file, does it have this attribute set to a certain value, like I can rip through that column and find it without having to parse everything every single time.</p>
<p>778<br>01:11:58,000 –&gt; 01:12:22,000<br>So the idea with shredding is that, instead of keeping track of the explicit hierarchy of a document for a given tuple, I just, I stored some repetition and a definition column that tells me whether this thing exists for some tuple or some offset for, as I’m scanning along.</p>
<p>779<br>01:12:23,000 –&gt; 01:12:34,000<br>So the basic idea is that, say I have some perturbation definition like this, there’s always a document ID at the top level, so I have a separate column for that, and that’s always going to be there, and therefore the repetition and definition is always 0.</p>
<p>780<br>01:12:34,000 –&gt; 01:12:45,000<br>Because there’s only one integer, one doc ID per document, there’s no repetition, and then there’s no definition saying that there’s other things I need to look at from the other nested columns.</p>
<p>781<br>01:12:46,000 –&gt; 01:13:10,000<br>But then you see here that within the name field, I can have a language, I have a code, I always have this optional string URL, and then I can define now separate column for all these things, and this is telling me the repetition is like, for how many tuples after the run you’re looking at, am I, do I belong to the original tuple that, or the original tuple that was created at the top of the hierarchy.</p>
<p>782<br>01:13:10,000 –&gt; 01:13:23,000<br>I’m going this way fast, we can cover this next class. The basic idea is again, I have some additional metadata of these additional columns I’m using to keep track of that I can use to reverse and figure out where I’m at in my hierarchy.</p>
<p>783<br>01:13:23,000 –&gt; 01:13:39,000<br>The easier one is the length and presence, this one basically says that, as I’m scanning through my document, I’m generating the data, that if a tuple doesn’t have an attribute, then at a given level, then I’ll just set its presence to the bottom level.</p>
<p>784<br>01:13:39,000 –&gt; 01:13:51,000<br>And I’ll just set its presence to false and leave it blank space. So I’m always putting in blank spaces for optional data to know whether this, this, this, the even tuple as I reconstruct things exists or not.</p>
<p>785<br>01:13:51,000 –&gt; 01:14:04,000<br>And that way I can just do the offsets to say, okay, well, I know as I’m processing along, as I scan through, how to reverse, use the presence of time we reverse back to where I’m looking for, because the offsets are going to match up.</p>
<p>786<br>01:14:04,000 –&gt; 01:14:18,000<br>Again, I’m, I’m, I’m butchering that’s going me through fast, but basically again, the main, main takeaway here is that I can split things up and, based on the past in my JSON document and then run all the encoding compression stuff that we did before.</p>
<p>787<br>01:14:18,000 –&gt; 01:14:32,000<br>I’m going to show one experiment from the paper you guys read. We basically did was rather than just looking at synthetic data like from TPCDS or these other benchmarks you guys are going to see in a bunch of papers and the Microsoft people guys used.</p>
<p>788<br>01:14:32,000 –&gt; 01:14:47,000<br>We said, okay, let’s go find out some real data. Let’s go find random parkay files or random data sets, let’s just up in parkay and org and then understand like are the design decisions that these file formats make are these good for real data.</p>
<p>789<br>01:14:47,000 –&gt; 01:14:57,000<br>So, and then I said there’s a bunch of parkay or file or sorry, not work. A bunch of parkay files you can find on GitHub and the internet through HuggingFace. We basically download a bunch of these things and then load it up.</p>
<p>790<br>01:14:57,000 –&gt; 01:15:15,000<br>And so for our value age for the most part we’re going to use arrow C++ invitation of parkay and org. Even though parkay and org, the original file library, the support libraries to process those files and create them, it’s all written in Java because the Hadoop world wrote everything in Java from 10 years ago.</p>
<p>791<br>01:15:15,000 –&gt; 01:15:30,000<br>We wanted to have the best performing implementations you could have so we use C++ for the problem is like the file spec for these parkay and org, they have all these new things that they’ve added over the years that are defined as optional.</p>
<p>792<br>01:15:30,000 –&gt; 01:15:42,000<br>Like the page index for the the zone map in the footer is optional. And so the various implementations of these formats, sort of these processing libraries, some of them implemented things, something didn’t.</p>
<p>793<br>01:15:42,000 –&gt; 01:15:58,000<br>And then there’s just the implementation of things that are required, how high performance were those. And so in the case of parkay and org when we looked at like, you know, stuff in rust and other implementations, they didn’t have Cindy support or like parkay was really good but org was really crappy.</p>
<p>794<br>01:15:58,000 –&gt; 01:16:07,000<br>It was really hard for us to find like a sort of true apples apples comparison between these different formats. Because there’s everyone just sort of writes their own thing over the years.</p>
<p>795<br>01:16:08,000 –&gt; 01:16:21,000<br>All right, so the first question we did is, okay, well, how well do these things actually compress. And the x-axis is just showing a bunch of different workloads that we generated for based on real data sets.</p>
<p>796<br>01:16:21,000 –&gt; 01:16:31,000<br>And see, you see that for the most part, parkay is better for the logging and the ML workload than over parkay because you want lowers better.</p>
<p>797<br>01:16:32,000 –&gt; 01:16:43,000<br>Because these are, these data sets are mostly comprised of floating point numbers, right? All the weights from some ML model, or all floating point numbers.</p>
<p>798<br>01:16:43,000 –&gt; 01:16:49,000<br>And because parkay does dictionary encoding for floating point numbers, you actually get a big win.</p>
<p>799<br>01:16:49,000 –&gt; 01:16:56,000<br>Again, that seems counter-intuitive. This was surprising to me that like floating point data actually compresses really well through dictionary encoding.</p>
<p>800<br>01:16:56,000 –&gt; 01:17:10,000<br>And then the org is going to do better for the classic workload and the geospatial workload because they mostly contain strings and parkay, sorry, org is more aggressive in compressing the dictionary codes.</p>
<p>801<br>01:17:10,000 –&gt; 01:17:15,000<br>It has that four different schemes that can use after you’ve already done the dictionary code.</p>
<p>802<br>01:17:15,000 –&gt; 01:17:23,000<br>All right, so the file size, you know, these aren’t huge wins, right? These aren’t mind-blowingly different, right? These are my opinion, the margin of error.</p>
<p>803<br>01:17:23,000 –&gt; 01:17:29,000<br>And not like one’s 10x larger than another, right? And these are, as I’ve said, storage is infinite.</p>
<p>804<br>01:17:29,000 –&gt; 01:17:49,000<br>But now when you actually run the queries on these things, right, simulating what an actual query engine would actually do for scans and range scans, or sorry, full of bunch of scans, and then partial selects and point queries, you see parkay is going to be faster because it’s going to mostly use bitpacking.</p>
<p>805<br>01:17:49,000 –&gt; 01:18:00,000<br>And for some of these workloads, there isn’t a lot of repetition that are for contiguous values. There’s repetition within the column, but it’s not like you see 1-1-1-1 over and over again.</p>
<p>806<br>01:18:00,000 –&gt; 01:18:09,000<br>So RLE doesn’t have a big meaning for this, right? Because again, the only RLE only kicks in in parkay if there’s eight or more repeated values.</p>
<p>807<br>01:18:09,000 –&gt; 01:18:19,000<br>And so ORC is more aggressive for RLE, again, three or more, but the problem is when you use RUN-like encoding, you can’t vectorize that very easily with SIMD.</p>
<p>808<br>01:18:19,000 –&gt; 01:18:32,000<br>And I don’t think the error implementation of the ORC processing library that we had, none of that was vectorized. So basically, we left doing SISD operations or SISD instructions on columnar vectorized data.</p>
<p>809<br>01:18:33,000 –&gt; 01:18:54,000<br>The other thing probably saw with the case ORC, and again, this would be a reoccurring theme throughout the entire semester, is that the additional complexity of supporting four different encoding schemes for the dictionary codes, or dictionary compressed columns, is that at runtime, as you’re trying to rip through the column, you’ve got to keep checking, okay, for this column chunk, how is it actually being encoded?</p>
<p>810<br>01:18:55,000 –&gt; 01:19:03,000<br>And then now you have this branching in your code to say, if I look at my header, it says encoded this way, then I want to use this function decompressant.</p>
<p>811<br>01:19:03,000 –&gt; 01:19:18,000<br>If it’s this way, use this other function. And all that indirection or conditional clauses, calls as branch mis prediction in the CPU, which in and again, a modern architecture with a super scalar CPU architecture is terrible.</p>
<p>812<br>01:19:18,000 –&gt; 01:19:26,000<br>You’ve got to flush the pipeline and pause and all that garbage, right? So for that reason, parquet, because it’s much more simple, works way better.</p>
<p>813<br>01:19:27,000 –&gt; 01:19:35,000<br>And we’ll see in some cases, when we talk about sweatshirt scans and actually applying predicates, the stupid thing of like just always applying the predicate,</p>
<p>814<br>01:19:36,000 –&gt; 01:19:48,000<br>I start always copying the two pool, as like always copying the two pool into your output buffer, and then apply the predicate, actually works faster in some cases, then checking the predicate, and then if it matches, then putting the output buffer.</p>
<p>815<br>01:19:48,000 –&gt; 01:19:56,000<br>So always copying seems like the wrong thing to do, but on super scalability to use, when everything’s in memory, you try to rip through as fast as possible, turns out to be the better choice.</p>
<p>816<br>01:19:57,000 –&gt; 01:20:11,000<br>And this is why the compilation, the specialization stuff we’ll see later on is going to help us as well, because now we don’t think the giant switch calls that says, if I’m in 32 through this, if I’m, you know, floating point to that, which again, if you overlook the bus stop code or post-codes and all that, that’s basically what it looks like when they process types.</p>
<p>817<br>01:20:12,000 –&gt; 01:20:18,000<br>So if you can specialize all of that, then you avoid that indirection. So that’s why parquet simplicity is going to help with here.</p>
<p>818<br>01:20:19,000 –&gt; 01:20:27,000<br>I’m well, let me finish quickly. So main takeaways of this, dictionary encoding is effective for all types, not just strings. And again, to me, this is surprising.</p>
<p>819<br>01:20:27,000 –&gt; 01:20:44,000<br>The simple plus decoding scheme is actually is better for modern hardware, as I just said. And then because the hardware landscape has changed so much, network has gotten so much faster, then, then, and this has got much faster too, then we just want to avoid using snappy and z standard entirely.</p>
<p>820<br>01:20:45,000 –&gt; 01:20:51,000<br>The native encoding schemes or dictionary coding, RLE and all that, that’s always me better.</p>
<p>821<br>01:20:53,000 –&gt; 01:21:01,000<br>I’ve already said this, hardware has changed. And then, even though there’s been widely successful, they’re used everywhere.</p>
<p>822<br>01:21:01,000 –&gt; 01:21:07,000<br>There’s a lot of things that are missing in parquet and work they didn’t consider when they first designed these things. That would help us if we want to process OLAP queries.</p>
<p>823<br>01:21:08,000 –&gt; 01:21:21,000<br>So there’s no statistics. There’s a zone maps and tuple counts and bloom filters. No histograms, no sketches, nothing about what’s inside those columns that I could use for cardinality estimations and my query optimizer.</p>
<p>824<br>01:21:21,000 –&gt; 01:21:30,000<br>I can’t increment, can’t increment, decirilize the schema. If I have a 10,000 columns, I’ve got to decirilize that protobuff thing at the very beginning, all the ones.</p>
<p>825<br>01:21:31,000 –&gt; 01:21:39,000<br>And then, as I said, there’s a bunch of different limitations and pick whatever query language, sorry, whatever programming language you want. There’s a parquet library for it.</p>
<p>826<br>01:21:39,000 –&gt; 01:21:48,000<br>But it doesn’t, they’re all, I’ll say they’re all garbage, we’re like, none of them supports exactly what’s in the spec. Even the job one doesn’t.</p>
<p>827<br>01:21:48,000 –&gt; 01:21:58,000<br>Okay, so next class, you’re going to read fast lanes, we’ll cover better blocks. These are going to be sort of modern encoding schemes that go beyond the things we talked about today.</p>
<p>828<br>01:21:58,000 –&gt; 01:22:13,000<br>That’s going to be really designed for modern hardware. And the fast lanes one is what is basically saying, hey, if you don’t put things in the order that they, in memory, in the way that they’re found somebody inserted them, you know, not even sorting it.</p>
<p>829<br>01:22:13,000 –&gt; 01:22:23,000<br>The store things a certain way that, that, because you know it’s been processed with SIMD, then you get much better performance. It’s pretty well. Okay? Any last questions?</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CMU15721 P3S202402 DataFormatsEncodingPart1CMUAdvancedDatabaseSystems</div>
      <div>http://example.com/2025/10/25/CMU15721 P3S202402-DataFormatsEncodingPart1CMUAdvancedDatabaseSystems/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月25日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/25/CMU15721%20P4S202403-DataFormatsEncodingPart2CMUAdvancedDatabaseSystems/" title="CMU15721 P4S202403 DataFormatsEncodingPart2CMUAdvancedDatabaseSystems">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CMU15721 P4S202403 DataFormatsEncodingPart2CMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/25/CMU15721%20P2S202401-ModernOLAPDatabaseSystemsCMUAdvancedDatabaseSystems/" title="CMU15721 P2S202401 ModernOLAPDatabaseSystemsCMUAdvancedDatabaseSystems">
                        <span class="hidden-mobile">CMU15721 P2S202401 ModernOLAPDatabaseSystemsCMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
