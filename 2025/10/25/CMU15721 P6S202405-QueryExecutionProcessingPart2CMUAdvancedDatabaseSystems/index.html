

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="100:00:00,000 –&gt; 00:00:06,000Can’t Niki Melon University’s advanced database systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio audience? 300:00:09,000 –&gt; 00:00:20,">
<meta property="og:type" content="article">
<meta property="og:title" content="CMU15721 P6S202405 QueryExecutionProcessingPart2CMUAdvancedDatabaseSystems">
<meta property="og:url" content="http://example.com/2025/10/25/CMU15721%20P6S202405-QueryExecutionProcessingPart2CMUAdvancedDatabaseSystems/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="100:00:00,000 –&gt; 00:00:06,000Can’t Niki Melon University’s advanced database systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio audience? 300:00:09,000 –&gt; 00:00:20,">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-25T05:03:39.756Z">
<meta property="article:modified_time" content="2025-10-25T05:03:39.756Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>CMU15721 P6S202405 QueryExecutionProcessingPart2CMUAdvancedDatabaseSystems - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CMU15721 P6S202405 QueryExecutionProcessingPart2CMUAdvancedDatabaseSystems"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-25 13:03" pubdate>
          2025年10月25日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.4k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          62 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CMU15721 P6S202405 QueryExecutionProcessingPart2CMUAdvancedDatabaseSystems</h1>
            
            
              <div class="markdown-body">
                
                <p>1<br>00:00:00,000 –&gt; 00:00:06,000<br>Can’t Niki Melon University’s advanced database systems courses</p>
<p>2<br>00:00:06,000 –&gt; 00:00:09,000<br>filming front of the live studio audience?</p>
<p>3<br>00:00:09,000 –&gt; 00:00:20,000<br>So today we’re going to pick up where we left off last class and talk about how we’re actually going to execute queries.</p>
<p>4<br>00:00:20,000 –&gt; 00:00:27,000<br>And so in last class we discussed, we mostly discussed query processing models</p>
<p>5<br>00:00:27,000 –&gt; 00:00:35,000<br>and it’s part of this is deciding what is the shape and the amount of data we’re sending from one operator to the next in the query plan.</p>
<p>6<br>00:00:35,000 –&gt; 00:00:42,000<br>And then what is the control mechanism that we’re going to use to tell one operator to execute?</p>
<p>7<br>00:00:42,000 –&gt; 00:00:45,000<br>And the distinction here was the push versus pull.</p>
<p>8<br>00:00:45,000 –&gt; 00:00:50,000<br>And BUSTUB is a pull-based system. We’re primarily focusing on push-based systems.</p>
<p>9<br>00:00:50,000 –&gt; 00:00:55,000<br>And then we talked about how do you actually represent the output of predicates and filters,</p>
<p>10<br>00:00:55,000 –&gt; 00:01:01,000<br>either a selection vector of offsets or position lists or bitmap saying what two-plus-a-gotts satisfied?</p>
<p>11<br>00:01:01,000 –&gt; 00:01:08,000<br>So again, going forward in this semester, we’re going to assume that in our conceptual system we’re building as we go along,</p>
<p>12<br>00:01:08,000 –&gt; 00:01:10,000<br>it’s going to be a vectorized push-based system.</p>
<p>13<br>00:01:10,000 –&gt; 00:01:15,000<br>Now vectorized is pretty much every other system now does vectorized.</p>
<p>14<br>00:01:15,000 –&gt; 00:01:20,000<br>Push-based, I don’t actually know the number of maybe less than half is push-based.</p>
<p>15<br>00:01:20,000 –&gt; 00:01:23,000<br>Still a lot of systems are still pull-based.</p>
<p>16<br>00:01:23,000 –&gt; 00:01:36,000<br>But the one-to-key advantages that we talked a little bit at the end about push-based systems that you essentially have this centralized scheduler that has complete control of what task on an execute in when.</p>
<p>17<br>00:01:36,000 –&gt; 00:01:43,000<br>And this allows you to do, have fine-grained control over when things actually are executing.</p>
<p>18<br>00:01:43,000 –&gt; 00:01:55,000<br>And so there may be situations where if you’re running out of memory because too many queries are running, you can just pause in a push-based system because you just stop executing tasks.</p>
<p>19<br>00:01:55,000 –&gt; 00:02:04,000<br>Or if you have another operator that needs to go get something over the network, that’s a blocking call.</p>
<p>20<br>00:02:04,000 –&gt; 00:02:11,000<br>You just can deschedule that task and then fire it up again when it’s available.</p>
<p>21<br>00:02:11,000 –&gt; 00:02:16,000<br>So push-based model allows more control over exactly what’s going on than a pull-based approach.</p>
<p>22<br>00:02:16,000 –&gt; 00:02:23,000<br>So not every system is the push-based, but almost every other system today will be vectorized.</p>
<p>23<br>00:02:23,000 –&gt; 00:02:31,000<br>So today’s class, we’re going to continue where we left off last time talking about how you’re actually going to run queries in parallel, the different types of parallelism that we have.</p>
<p>24<br>00:02:31,000 –&gt; 00:02:40,000<br>And essentially how we’re going to architect the system so that we can allow multiple-oper instances to run at the same time, either within the same query or multiple queries concurrently.</p>
<p>25<br>00:02:41,000 –&gt; 00:02:49,000<br>And then going forward starting next week, we’ll see how we actually start parallelizing the individual operators within the query plan.</p>
<p>26<br>00:02:49,000 –&gt; 00:03:02,000<br>Then we’ll talk about what the operator output looks like, in terms of how do we reference other parts of a tuple, other columns, whether we put everything together or do late materialization.</p>
<p>27<br>00:03:02,000 –&gt; 00:03:08,000<br>We’ll talk about what the data is going to look like going from one operator to the next, and the spoilers are going to be used arrow for this.</p>
<p>28<br>00:03:08,000 –&gt; 00:03:15,000<br>How we are evaluating expressions, and then we’ll just do a quick preview of what adaptive query execution looks like.</p>
<p>29<br>00:03:15,000 –&gt; 00:03:22,000<br>What we focus on the, in the context of the Veloc paper you guys read, we’ll focus on how we do predicate evaluation and let that be adaptive.</p>
<p>30<br>00:03:22,000 –&gt; 00:03:29,000<br>But then going forward throughout the rest of the semester, and then when we talk about the query optimization, we’ll see how we do query add that activity.</p>
<p>31<br>00:03:30,000 –&gt; 00:03:40,000<br>And this is the hardest one, this is one of the hardest things you can do in a database system, and you can’t need to design these data at the very beginning to be able to support this rather than go retrofit it.</p>
<p>32<br>00:03:40,000 –&gt; 00:03:47,000<br>Mostly on the query plan side, for the expression stuff you can add this stuff later on.</p>
<p>33<br>00:03:48,000 –&gt; 00:03:54,000<br>So the parallel execution, the idea is pretty straightforward. Again, some of this will be a review of what we discussed in the intro class.</p>
<p>34<br>00:03:54,000 –&gt; 00:04:03,000<br>But the idea is that we want to be able to have our database system run multiple things at the same time to take advantage of the hardware that’s available to us.</p>
<p>35<br>00:04:03,000 –&gt; 00:04:13,000<br>No longer are database systems running on these monolithic machines that have one CPU with one CPU core, and no way to talk to other machines.</p>
<p>36<br>00:04:14,000 –&gt; 00:04:22,000<br>At the very least, even if you’re single socket, it’s going to have dozens of cores, maybe a multi-sug machine, and then now that’s basically a distributed system.</p>
<p>37<br>00:04:22,000 –&gt; 00:04:37,000<br>Let alone scaling out to multiple nodes. So in that environment, we want to be able to have the system be able to take one query and fan it out across multiple machines to paralyze the various operations.</p>
<p>38<br>00:04:38,000 –&gt; 00:04:49,000<br>Does it matter? Yeah. So this question is multi-sug machine’s a multiple cores. For all the things we’re going to talk about today, it doesn’t matter where it’s a multi-threaded, multi-process, multi-node system.</p>
<p>39<br>00:04:49,000 –&gt; 00:04:58,000<br>It doesn’t matter. Because think about what is a multi-sug at CPU? You have to communicate something. It may or may not even in the same address space whether it’s a multi-process multi-threaded.</p>
<p>40<br>00:04:58,000 –&gt; 00:05:12,000<br>So if you have the communication channels set up to distribute things, assuming you’re not going to call it low-level pipes to do IPCs, to different processes, you can extend that easily to multi-node.</p>
<p>41<br>00:05:12,000 –&gt; 00:05:18,000<br>And we’re going to talk about what kind of frameworks you could use, but there’s frameworks that hide that abstraction.</p>
<p>42<br>00:05:18,000 –&gt; 00:05:24,000<br>Does it though? Nowadays? Okay.</p>
<p>43<br>00:05:24,000 –&gt; 00:05:36,000<br>Never was really fast. Especially in the same rack. Yeah, the CPU is often going to be a lot of luck. The things in the last two or three years, things have flipped.</p>
<p>44<br>00:05:36,000 –&gt; 00:05:48,000<br>So again, this is basically what I just told him. The high level approaches that we’re talking about here today aren’t going to matter whether it’s multi-threaded, multi-process, or multi-node.</p>
<p>45<br>00:05:48,000 –&gt; 00:05:57,000<br>So at a high level, there’s two types of parallelism, inter query parallelism, inter query parallelism. Again, I think we’ve covered some of this in the interclass. We’ll go more detail on these.</p>
<p>46<br>00:05:57,000 –&gt; 00:06:12,000<br>So inter query parallelism basically means that can I have multiple queries running at the same time in my system, and there’s something up above that a coordinator or a scheduler that’s responsible for figuring out who runs where and what time.</p>
<p>47<br>00:06:12,000 –&gt; 00:06:26,000<br>I did some previous work in the space, and when we looked at existing systems, most of them are using a really basic first-come-first serve policy, meaning when a query shows up, it’s assigned some priority based on that, and then it’s allowed to run on whatever resources that it needs.</p>
<p>48<br>00:06:26,000 –&gt; 00:06:33,000<br>And then other queries that come out later in time will get resources when they’re available when the first query finishes.</p>
<p>49<br>00:06:33,000 –&gt; 00:06:49,000<br>The enterprise systems can do more sophisticated things, like have priorities for individual users or connection strings. So if you’re a user, if you log in with some user credentials, you’ll be given high priority to other queries.</p>
<p>50<br>00:06:49,000 –&gt; 00:07:00,000<br>But a high-low would still doing more or less first-come-first serve. So in OLAP queries, they’re going to have both parallelizable and non-parallel phases, and we’ll see what that looks like in a second.</p>
<p>51<br>00:07:00,000 –&gt; 00:07:14,000<br>But the main idea is that we want to keep all our recours, all our resources, actually running something. So if there’s a point where a query has to call us data to a single node, like an exchange operator, that change operator may be running on a single core.</p>
<p>52<br>00:07:15,000 –&gt; 00:07:24,000<br>So we have other cores available to do other things. And again, this scheduler component is responsible for figuring out how to take advantage of all those available resources.</p>
<p>53<br>00:07:24,000 –&gt; 00:07:29,000<br>So we won’t go into much detail in this class about how we want to do this for inter query parallelism.</p>
<p>54<br>00:07:29,000 –&gt; 00:07:39,000<br>There’ll be a separate lecture on scheduling queries, and then we’ll see things like, for joint algorithms and other things, how do we handle the cases where we’re distributing tasks across different nodes.</p>
<p>55<br>00:07:40,000 –&gt; 00:07:49,000<br>So different workers, whether they’re in the same box or not, and we need to send data around. So we’ll cover this a bit more detail later in the semester.</p>
<p>56<br>00:07:52,000 –&gt; 00:08:04,000<br>The thing that most people think about when they say query parallelism is intro query parallelism. The idea is, again, taking a single query and then distributing across multiple resources and multiple workers running at the same time.</p>
<p>57<br>00:08:05,000 –&gt; 00:08:18,000<br>And this is the beauty of a declarative language like SQL, where ideally you don’t let them know or care what the system architecture looks like, whether it has one node or four nodes or 10 nodes or 100 nodes, it doesn’t matter.</p>
<p>58<br>00:08:18,000 –&gt; 00:08:25,000<br>You can the same SQL query can then be chopped up into different plan fragments or tasks, and then let the system distribute it for you.</p>
<p>59<br>00:08:25,000 –&gt; 00:08:36,000<br>So it’s not like you have to debug things locally, when you maybe debug things locally on the subset of the data and then submit it to the larger machine to run on all the data, you have to rewrite everything.</p>
<p>60<br>00:08:36,000 –&gt; 00:08:45,000<br>The SQL in theory should just be able to get translated into a distributed plan or parallel plan on the system without any changes.</p>
<p>61<br>00:08:46,000 –&gt; 00:08:52,000<br>So the two types of parallelism, the most common one is intro operator parallelism through what’s called horror than a parallelism.</p>
<p>62<br>00:08:52,000 –&gt; 00:09:01,000<br>I think the one of the execution engine teams was asking about asynchronous IO and doing a technique called vertical parallelism, so we’ll quickly talk what that looks like.</p>
<p>63<br>00:09:01,000 –&gt; 00:09:08,000<br>Say else the two all of these things are not machine exclusive. You can combine these things together and various systems are going to do this in different ways.</p>
<p>64<br>00:09:09,000 –&gt; 00:09:18,000<br>But again, the intro operator parallelism is way more common. And then for each of the individual relational operators that are out there, there’s parallel versions of all of them.</p>
<p>65<br>00:09:18,000 –&gt; 00:09:26,000<br>And oftentimes that whether or not the operator itself is aware that it’s being paralyzed or not depends on the implementation.</p>
<p>66<br>00:09:26,000 –&gt; 00:09:37,000<br>And if it’s not aware, then you will see an exchange operator in the second where you put that above and allow you to combine results with again not being aware that it may have got to end up across multiple tasks.</p>
<p>67<br>00:09:37,000 –&gt; 00:10:01,000<br>So horror’s not parallelism that says in those common one, the idea is that we’re going to break up a an operator in our query plan into independent instances of them operator instances that are basically going to do the exact same function or exact same computation on its input as all the other operators instances that it’s a copy of, but it’s going to meet operating on different pieces of data, different chunks of data.</p>
<p>68<br>00:10:01,000 –&gt; 00:10:12,000<br>Again, just think of like a sequential scan. If I have a 10 parquet files in my table that I want to scan, then I could have each operator instance be responsible for scanning just one file.</p>
<p>69<br>00:10:12,000 –&gt; 00:10:23,000<br>And they run in parallel. I don’t need to coordinate the intermediate results across while the running between them. They’re both independent, independent computations.</p>
<p>70<br>00:10:23,000 –&gt; 00:10:31,000<br>But again, at some point we need to put the results back together for other parts of the query plan or the final output to give to the application.</p>
<p>71<br>00:10:31,000 –&gt; 00:10:42,000<br>So we’re going to use an exchange operator that allow us to coalesce these results from the different operator instances and to combine them together and then send it off to whoever needs it next.</p>
<p>72<br>00:10:43,000 –&gt; 00:10:48,000<br>So we’ll see this technique used in the morsels paper when we talk about scheduling in a few weeks.</p>
<p>73<br>00:10:48,000 –&gt; 00:10:59,000<br>This idea of an exchange operator dates back to the 1990s. So it’s not new, but this is basically how a lot of distributed systems or parallel systems are going to operate.</p>
<p>74<br>00:10:59,000 –&gt; 00:11:05,000<br>Even if they may not call it exchange operator, it still will be some variation of it. I’ll show what I mean in a second.</p>
<p>75<br>00:11:06,000 –&gt; 00:11:13,000<br>So here’s the most basic version of doing exchange. So we have our table here. We want to join a and b and we have a really simple predicate.</p>
<p>76<br>00:11:13,000 –&gt; 00:11:25,000<br>So the first thing we’re going to do is convert this logical plan into a physical plan. And so say we have there’s three chunks of data, three partitions on table A that we want to scan.</p>
<p>77<br>00:11:25,000 –&gt; 00:11:34,000<br>So we could have an operator instance for each of those that it gets assigned to an individual core worker. And again, whether it’s a worker thread worker process worker node, we don’t care.</p>
<p>78<br>00:11:34,000 –&gt; 00:11:37,000<br>Random function, it doesn’t matter.</p>
<p>79<br>00:11:37,000 –&gt; 00:11:48,000<br>And so now we’re going to be starting building up the pipeline. So we know we want to scan a and then immediately after scanning some some you know some vector a row group out of A, we want to apply the filter on it.</p>
<p>80<br>00:11:48,000 –&gt; 00:11:56,000<br>And so again, we don’t need to coordinate that filter operation across different different different workers or different operative instances. So that can run in parallel.</p>
<p>81<br>00:11:56,000 –&gt; 00:12:06,000<br>Right. And maybe we want to do predicate. Sorry projection push down. So we’ll push that down above here in our pipeline. And then now we then feed that into the build side of our hash joined.</p>
<p>82<br>00:12:06,000 –&gt; 00:12:26,000<br>Right. So the now the output of these three operator instances are not going to feed into this exchange operator. Think of this as like the pipeline breaker for multiple operator instances. So I can’t do the the probe side of the join until the exchange operator gets all of the results from the individual operator instances.</p>
<p>83<br>00:12:26,000 –&gt; 00:12:34,000<br>What I’m also not showing is how we’re actually building the hash table here is that one hash table three hash tables. Right. Again, we’ll cover that when we talk about joins later on.</p>
<p>84<br>00:12:34,000 –&gt; 00:12:43,000<br>Right. Sometimes it’s faster just to build three separate ones and then do another passive combined together, which sounds crazy, but sometimes it is faster.</p>
<p>85<br>00:12:43,000 –&gt; 00:12:50,000<br>Most systems are they’re on the same box. They’ll probably single build a single one. So you need latching inside of that thing to protect it.</p>
<p>86<br>00:12:50,000 –&gt; 00:12:57,000<br>So you think each of these is now these these boundaries here. This is the operator instance. This is like the pipeline.</p>
<p>87<br>00:12:57,000 –&gt; 00:13:14,000<br>Right. So once we have that, then we go now on the on the other side of the other query plan tree. Again, say we have three, three files or B. So we have three different instances that are going to scan B, do the filter, do the projection, and then probe the hash table and produce the output.</p>
<p>88<br>00:13:14,000 –&gt; 00:13:26,000<br>Right. But now in this case here, assuming we have a single hash table for for for our query plan, each of these individual output instances in real pipelines could probe the hash table separately.</p>
<p>89<br>00:13:26,000 –&gt; 00:13:41,000<br>Right. So you’re going to have three sort of three threads or three workers producing outputs from from this join here. So I need another exchange operator to take their individual results, combine them together, and then produce the final output to to the user.</p>
<p>90<br>00:13:42,000 –&gt; 00:13:51,000<br>Right. So again, just the pipeline boundary for each operator says it’s going to look something like this. Yes.</p>
<p>91<br>00:13:52,000 –&gt; 00:14:06,000<br>Is question is where they change the running on these one is course. It’ll be yeah, think of this as like an operator instance that’s running and keeping track of the inputs that it’s getting.</p>
<p>92<br>00:14:06,000 –&gt; 00:14:18,000<br>And then depending on whether or not it actually does any work. Right. Like in this case here, the exchange operator could just be all I got a notification that we’ve populated the hash table from three operating instances.</p>
<p>93<br>00:14:19,000 –&gt; 00:14:30,000<br>So it doesn’t do any computation. It’s just a barrier to say, OK, now now we can start running on the side here. In the case of here this exchange operator, it actually wants to call us the results.</p>
<p>94<br>00:14:30,000 –&gt; 00:14:46,000<br>And again, whether or not that’s a physical operator that is just taking buffers and putting them to a final output, or somehow we’ve staged the output buffers in such a way that three threads can write into at the same time, it doesn’t matter.</p>
<p>95<br>00:14:46,000 –&gt; 00:14:56,000<br>So the exchange operator I just showed you is what is known as a gather operator. Again, so this is like the these are like the variations of an exchange that you can have.</p>
<p>96<br>00:14:56,000 –&gt; 00:15:07,000<br>And it’s that the gather one is most common one you’ll see in the systems. And so this terminology I’m showing here, this is coming from SQL server from a blog article from I think 2006.</p>
<p>97<br>00:15:07,000 –&gt; 00:15:25,000<br>So this is their terminology, but I think it’s it clearly defines the different ways they’re going to take the data and send data out. So not again, not every data system is going to follow this taxonomy, but I like it because it’s again to me, it seems easier reason about just saying, oh, yeah, they’re all exchanges, but sometimes things come in one way versus another.</p>
<p>98<br>00:15:25,000 –&gt; 00:15:32,000<br>So again, the gather operator is just combining results to multiple workers and producing a single output stream that it sends to somewhere else in the query plan.</p>
<p>99<br>00:15:32,000 –&gt; 00:15:43,000<br>The distribute exchange operator takes a single input and then fans it out to multiple workers to whoever actually needs it.</p>
<p>100<br>00:15:43,000 –&gt; 00:16:06,000<br>So you can think of this like when we talk about enemy results, but like we’ve talked about before like, okay, what if I want to have the output of one query operator be then used as for multiple query operators, like if I’m doing a like a sub query that appears multiple times the query plan, I want to be to reuse the result for different parts of the query plan in the DAG.</p>
<p>101<br>00:16:06,000 –&gt; 00:16:21,000<br>So distribute operator would allow you to do this. And then the last one is to re partition basically a multiple, multiple operators producing input to the exchange operator and then it’s going to produce multiple outputs to different channels.</p>
<p>102<br>00:16:21,000 –&gt; 00:16:22,000<br>Yes.</p>
<p>103<br>00:16:22,000 –&gt; 00:16:29,000<br>So it seems like map reduce, right? That’s the part that’s just reducing the map again.</p>
<p>104<br>00:16:29,000 –&gt; 00:16:37,000<br>So it seems like map reduce, sure, what came first parallel to this is map reduce.</p>
<p>105<br>00:16:37,000 –&gt; 00:16:46,000<br>So when would you need the, so the galaxy wants to be the only one that I think is going to be useful.</p>
<p>106<br>00:16:46,000 –&gt; 00:16:53,000<br>No. So is that every partition, the query is going to cause us shuffle, right?</p>
<p>107<br>00:16:53,000 –&gt; 00:17:04,000<br>And so after every pipeline, they’re going to do this shuffle thing and they have to have a separate service, take the inputs and stages them and then the operators can, the operators upstream can then pull from them.</p>
<p>108<br>00:17:04,000 –&gt; 00:17:14,000<br>And it may be the case that the number of operators, the number of workers that are on the input side might be different than on the output side.</p>
<p>109<br>00:17:14,000 –&gt; 00:17:22,000<br>So having this like natural staging point, barrier point, you can then scale out a scale in number of execution engines or opportunities you want to bubble.</p>
<p>110<br>00:17:22,000 –&gt; 00:17:26,000<br>So this is more of a line of play. If you’re over there, it would want to increase in the query.</p>
<p>111<br>00:17:26,000 –&gt; 00:17:28,000<br>Doesn’t have to be in that work, but yeah, yes.</p>
<p>112<br>00:17:28,000 –&gt; 00:17:31,000<br>But it won’t really have no single node.</p>
<p>113<br>00:17:31,000 –&gt; 00:17:39,000<br>No, so it won’t happen in single node. No, it may be the case that the, like, you have a physical plan.</p>
<p>114<br>00:17:39,000 –&gt; 00:17:49,000<br>And again, we’ll talk about it at a two to ten second. I’m like, like, I have a physical plan and I have some optimizer estimates of like, here’s the amount of data I expect to get from these operators below me.</p>
<p>115<br>00:17:49,000 –&gt; 00:17:53,000<br>And therefore, I know how many operators I’m going to need to process it above the exchange operator.</p>
<p>116<br>00:17:53,000 –&gt; 00:17:59,000<br>But what if those estimates are wrong and you, like, woefully underestimate it?</p>
<p>117<br>00:17:59,000 –&gt; 00:18:02,000<br>So even on the same box, you may want to scale this in and out up above.</p>
<p>118<br>00:18:02,000 –&gt; 00:18:06,000<br>And the repartitioned shop operator allows you to do that.</p>
<p>119<br>00:18:06,000 –&gt; 00:18:14,000<br>Okay. So the other question I have is, other than table scans, right?</p>
<p>120<br>00:18:14,000 –&gt; 00:18:22,000<br>Maybe your style of this, which other function, which other operators could we take advantage of this?</p>
<p>121<br>00:18:22,000 –&gt; 00:18:27,000<br>Because it doesn’t seem trivial doing this, but it will be open.</p>
<p>122<br>00:18:27,000 –&gt; 00:18:34,000<br>So, so, so, so, it’s saving us other other, what type of operators would you have? Could these be anything?</p>
<p>123<br>00:18:34,000 –&gt; 00:18:42,000<br>I mean, my last slide, in this case here, yeah, this is the, this is coming from the scan, you don’t want to put stuff into it.</p>
<p>124<br>00:18:42,000 –&gt; 00:18:45,000<br>And then I’m showing the output of the join goes into exchange operator.</p>
<p>125<br>00:18:45,000 –&gt; 00:18:49,000<br>But what if there was something above above? Like what if this is like a nested subquery?</p>
<p>126<br>00:18:49,000 –&gt; 00:18:52,000<br>And then there’s something else that wants to use it above, right?</p>
<p>127<br>00:18:52,000 –&gt; 00:18:56,000<br>Again, the relational, the relational, the relational operator allows us to be composed these things in any way as you want.</p>
<p>128<br>00:18:56,000 –&gt; 00:19:04,000<br>Right. So, so, so, for the later, like, all of these, is the, the amount of data in the same, the, the algorithm is like copy-a to some kind of,</p>
<p>129<br>00:19:04,000 –&gt; 00:19:15,000<br>is there any more semantics than, it seems like for a re-predition, can it sometimes be like, like, making like, disjoint subsets going out or copying out?</p>
<p>130<br>00:19:15,000 –&gt; 00:19:19,000<br>So, this question is, is it always a, maybe, in other words, is it always one to one inputs to outputs?</p>
<p>131<br>00:19:19,000 –&gt; 00:19:23,000<br>Or could you make duplicate of the output? You could duplicate the output, right?</p>
<p>132<br>00:19:23,000 –&gt; 00:19:28,000<br>As I examined it said before, what if I want to re-use some, some, some nested query computation?</p>
<p>133<br>00:19:28,000 –&gt; 00:19:38,000<br>Well, you know, I have a thousand tuples coming in, logically, I have a thousand tuples coming out, but physically, I maybe have, you know, double that, because it’s going to do two inputs in the query.</p>
<p>134<br>00:19:38,000 –&gt; 00:19:43,000<br>Does that change, like, the station of log? Is it, is it, is it all, is it all, is it a re-partition?</p>
<p>135<br>00:19:43,000 –&gt; 00:19:46,000<br>All, is it a re-partition if you do that?</p>
<p>136<br>00:19:46,000 –&gt; 00:19:51,000<br>Because it, like, it seems like a theological difference between, on cloning the data to different places.</p>
<p>137<br>00:19:51,000 –&gt; 00:19:54,000<br>I mean, you can do that with this one too, right?</p>
<p>138<br>00:19:55,000 –&gt; 00:20:00,000<br>There’s another question is, like, how, how much computation is actually going on here?</p>
<p>139<br>00:20:00,000 –&gt; 00:20:07,000<br>Like, could you, like, is it, like, sorting? No. Could it do, um…</p>
<p>140<br>00:20:07,000 –&gt; 00:20:16,000<br>Well, actually, it, it, it, it, it, it, it, it, it, it, it, it, like, maps like this specific, like, amount of, like, the third operator to, like, the first thing thing out.</p>
<p>141<br>00:20:16,000 –&gt; 00:20:18,000<br>Sort of, good idea. Yeah, that, doing that would be a bad idea.</p>
<p>142<br>00:20:18,000 –&gt; 00:20:23,000<br>So, it’s just taking a bunch of things in, like, okay, I’m just going to randomly decide what person was like really.</p>
<p>143<br>00:20:23,000 –&gt; 00:20:26,000<br>You’re not randomly deciding. This would be cool.</p>
<p>144<br>00:20:26,000 –&gt; 00:20:29,000<br>I’m sorry. It needs to be cool, right?</p>
<p>145<br>00:20:29,000 –&gt; 00:20:34,000<br>I mean, the, I mean, the amount of work that goes to each one, ideally, but you can’t always, you can’t guarantee that, right?</p>
<p>146<br>00:20:34,000 –&gt; 00:20:42,000<br>So, like, again, think of this case here, it’s thinking like, I take something and put in and then I’m gonna hash it from some key and then a redistribut based on that hash key.</p>
<p>147<br>00:20:42,000 –&gt; 00:20:50,000<br>So, in that case, again, if I recognize that everything skewed and it’s all going to this operator, then maybe I want to do it with another round of hashing and redistribut.</p>
<p>148<br>00:20:51,000 –&gt; 00:20:54,000<br>And then, refonditioner, operator is running on one single node.</p>
<p>149<br>00:20:54,000 –&gt; 00:20:56,000<br>No. No, not necessarily.</p>
<p>150<br>00:20:56,000 –&gt; 00:21:01,000<br>Oh, that’s the, that’s the, again, BigQuery, this is like, we’ll leave it in the semester.</p>
<p>151<br>00:21:01,000 –&gt; 00:21:06,000<br>BigQuery is going to run a whole separate, like, service that just does shuffle, that just does this.</p>
<p>152<br>00:21:06,000 –&gt; 00:21:14,000<br>It’s the specific implementation of this specific refonditioner that determines it. So, there’s no general rule.</p>
<p>153<br>00:21:14,000 –&gt; 00:21:18,000<br>Yeah, so, its question is, like, okay, this is PowerPoint.</p>
<p>154<br>00:21:18,000 –&gt; 00:21:27,000<br>This boxes with arrows is going in, right? I’m just giving you the idea that, like, you could have three inputs in and then two inputs coming out, that logically could build this, right?</p>
<p>155<br>00:21:27,000 –&gt; 00:21:34,000<br>I’m sorry, you could build this based on this logical idea and the sophistication of the different limitations are going to depend on the various systems.</p>
<p>156<br>00:21:34,000 –&gt; 00:21:41,000<br>BigQuery, they fabricate hardware to make this go as fast as possible, right?</p>
<p>157<br>00:21:42,000 –&gt; 00:21:48,000<br>Microsoft doesn’t do that, other people don’t do that. Other people don’t actually, you know, BigQuery is always going to do this.</p>
<p>158<br>00:21:48,000 –&gt; 00:21:55,000<br>And it gives them, you know, it seems like it would be expensive, but it opens up a lot of opportunities to add activity later on.</p>
<p>159<br>00:21:58,000 –&gt; 00:22:05,000<br>Okay, so, again, you can obviously see how this is just, this is just generalization of the other two.</p>
<p>160<br>00:22:06,000 –&gt; 00:22:12,000<br>And I’m saying, re-partition, again, this is Microsoft’s terminology. Google is going to call it shuffle.</p>
<p>161<br>00:22:12,000 –&gt; 00:22:19,000<br>Benjamin, I was going to, BigQuery is going to call it shuffle. I forget, with Databricks, I think Databricks might call it shuffle too.</p>
<p>162<br>00:22:19,000 –&gt; 00:22:25,000<br>From Matt Reduce, the Hadoop world, they would call this shuffle, right? But the idea is more or less the same.</p>
<p>163<br>00:22:25,000 –&gt; 00:22:32,000<br>And the question is, again, statement is, isn’t just copying Matt Reduce, although around Matt Reduce is copying databases.</p>
<p>164<br>00:22:32,000 –&gt; 00:22:38,000<br>And I have a paper that talks about that. Yes. We can take that one off one.</p>
<p>165<br>00:22:39,000 –&gt; 00:22:55,000<br>Okay, so the last type of parallelism is interoperator parallelism. And the idea here is that within a single query plan itself, we could have multiple pipelines running at the same time.</p>
<p>166<br>00:22:56,000 –&gt; 00:23:07,000<br>And you sort of think it is like an asynchronous model where you have one pipeline always running, but it’s producing two pulls, and that two pulls get fed into some other operator, sorry, some other pipeline, that’s always running as well.</p>
<p>167<br>00:23:07,000 –&gt; 00:23:21,000<br>And so you may still need to exchange operators to combine any results from different segments. But if your query plan allows this, and not all of them do, you could get better speed up, because now you don’t want to run one pipeline, and when that’s done, that’s switch over to another one.</p>
<p>168<br>00:23:21,000 –&gt; 00:23:27,000<br>You can start running higher parts of the query plan as soon as you have two pulls. And when it’s allowed.</p>
<p>169<br>00:23:27,000 –&gt; 00:23:33,000<br>This is sometimes called pipeline parallelism. You’ll see this mostly in the stream processing systems, which are not going to talk about this semester.</p>
<p>170<br>00:23:33,000 –&gt; 00:23:44,000<br>Examples of that would be with a spark streaming, not sans a pulsars, another one.</p>
<p>171<br>00:23:45,000 –&gt; 00:23:56,000<br>There’s that one guy who came here for a year, rising wave, right? There’s a bunch of these systems that are done is Kafka is probably the biggest example of this.</p>
<p>172<br>00:23:56,000 –&gt; 00:24:09,000<br>I blanked out on Kafka Kafka is the big one. So say we have this again, this unusual query. It’s basically Cartesian product across four tables with no no where clause.</p>
<p>173<br>00:24:10,000 –&gt; 00:24:25,000<br>And so in each of these, I can run the, I can run each of the pipelines just at the same time. This one appear and the two ones up the here are going to be blocked waiting for whatever comes out of these two down here.</p>
<p>174<br>00:24:26,000 –&gt; 00:24:34,000<br>So as this thing is running, it’s sending two pulls up and then it can merely start doing the join on other things once it’s available.</p>
<p>175<br>00:24:34,000 –&gt; 00:24:47,000<br>Cartesian product, you’re trying to match everybody with everybody. So there’s no, you won’t have any false, you won’t have any false negatives because you’re just waiting for things to come up and soon as you can start matching and producing the output.</p>
<p>176<br>00:24:47,000 –&gt; 00:24:58,000<br>Again, this is a stupid example, but it just gives the idea that you could run these things at the same time and you’re just, it’s a producer consumer model. This thing’s producing two pulls that it sends up and then that guy’s waiting for for those results.</p>
<p>177<br>00:24:58,000 –&gt; 00:25:03,000<br>But again, this is not that common.</p>
<p>178<br>00:25:03,000 –&gt; 00:25:31,000<br>So now let’s talk about the paper that I had you guys read. And the reason I had you guys read the VeloX papers because as I sort of mentioned in the second lecture for the first lecture, that there’s this trend now where people recognize that the, I look at OLAP execution engine, sort of become a commodity and that there really hasn’t been groundbreaking.</p>
<p>179<br>00:25:34,000 –&gt; 00:25:44,000<br>And so, the things that we’ve been doing, you know, changes in hardware that requires to rethink how we want to build everything certainly as optimizations and improvements over the years, but at a high level, like it’s a vectorized push based system.</p>
<p>180<br>00:25:44,000 –&gt; 00:25:55,000<br>Well, that’s what snowflake, you know, invented in 2013, 2012, and that’s actually what the, the money, the money, the money, the VX 100 system was doing back in the day. They weren’t doing push based.</p>
<p>181<br>00:25:55,000 –&gt; 00:26:14,000<br>So a bunch of these different products that are out there that are all more or less doing the same thing. And you can sort of think of now, instead of, if you’re going to build a new system scratch, although we are doing that here, as part of the class, like instead of having to build all the low level details you would need or functionality for an execution engine.</p>
<p>182<br>00:26:14,000 –&gt; 00:26:27,000<br>It’d be nice if there was just something off the shelf we could use that could provide this basic functionality for us. Right? And so that’s the idea of these sort of these composable libraries for building database systems.</p>
<p>183<br>00:26:27,000 –&gt; 00:26:40,000<br>And that you couldn’t theory instead of building everything from scratch for every new environment, every new workload, whatever, you could just use these things and modify them and extend them for whatever the hardware, whatever whatever the application wants to support.</p>
<p>184<br>00:26:40,000 –&gt; 00:26:50,000<br>So the VLox one is probably the most famous one, again, at a meta, which we’ll cover in a second. Data fusion, we’re also aware of, which is part of the arrow project, which is, I don’t feel like it should be a separate thing, but that’s another story.</p>
<p>185<br>00:26:50,000 –&gt; 00:26:58,000<br>There’s this OAP thing from Intel, Polar is another one where, again, they’re all trying to be these libraries to go plop down the execution engines. Yes.</p>
<p>186<br>00:26:58,000 –&gt; 00:27:12,000<br>Data fusion in Polar is a front end. We’ll get that in a second. Yes. It’s statement as data fusion includes a front end, Velox doesn’t. Yes. But you don’t have to use the data fusion front end. You could just use the execution ended. Right?</p>
<p>187<br>00:27:12,000 –&gt; 00:27:33,000<br>OK. So Velox is, is again, came out of a meta. And when I liked about the paper, they talked about how the original motivation for this was they looked across the entire organization of meta and they saw people just reinventing the wheel over and ever again building these custom execution engines that are at a high level all doing the same thing.</p>
<p>188<br>00:27:33,000 –&gt; 00:27:55,000<br>And it was a bunch of wasted, wasted effort where everyone’s trying to do the same snowflake style optimizations to all these execution engines. And they would end up having different semantics and understanding and data types and other capabilities and be really hard to sort of transfer any any one from one workload running on one system to another system.</p>
<p>189<br>00:27:55,000 –&gt; 00:28:10,000<br>Or the one example he mentions in the paper is that they they did a survey and they found 12 different implementations of the substring function in call all meta where sometimes they they started with zero sometimes I started one. Sometimes they took null sometimes they threw exceptions like everything was completely different.</p>
<p>190<br>00:28:10,000 –&gt; 00:28:17,000<br>And so they were trying to unify this or having sort of Velox be this again this low level.</p>
<p>191<br>00:28:17,000 –&gt; 00:28:33,000<br>This you know, execution engine library that could you could extend for the different categories of workloads or different environments that he wants to put across meta and then have you know small team make that one really, really fast instead of random people can try to re re reinvent the wheel.</p>
<p>192<br>00:28:33,000 –&gt; 00:28:48,000<br>So as he said in Velox it’s not a you know it’s not something you just plot down it immediately use there’s no front end does there’s no SQL parser there’s no query optimizer there’s no metadata catalog there’s nothing to keep track what tables you have.</p>
<p>193<br>00:28:48,000 –&gt; 00:29:04,000<br>Literally is just just the execution engine that implements the operators they have a little bit more things I can do the they have some memory management thread management they have connectors to read data from different from different sources.</p>
<p>194<br>00:29:04,000 –&gt; 00:29:11,000<br>But I mean it’s a core that the really thing that try not to lies essentially running these operators in parallel in a vectorized manner.</p>
<p>195<br>00:29:12,000 –&gt; 00:29:25,000<br>So the input is going to be a physical plan of data operators and then it’s going to produce some output wherever you tell it to go and again it’s not like you just you know it’s an excuse we’ll get run you have to build this in the context of a larger system.</p>
<p>196<br>00:29:26,000 –&gt; 00:29:49,000<br>So for all the systems we’re going to cover throughout the semester we’ll try to do this quick summary to say here’s the core features that they have again and then you have this mental check lesson say okay I have a high high level understanding of what they’re actually doing so when we talk about the Databricks photon and Snowflake and other things we’ll try to do the same things like here’s the key things that they have based on everything we talk about this semester.</p>
<p>197<br>00:29:49,000 –&gt; 00:30:02,000<br>So that locks us me a push based vectorized query processing engine they’re going to use pre compile primitives and cogent for expressions in C++ again we haven’t discussed with this is that’ll be the paper you guys read on Monday next week.</p>
<p>198<br>00:30:02,000 –&gt; 00:30:15,000<br>But again just think of like the thing of like the for the you know the scanning a table where you know a column is going to be in 32 they have a compiled version of that you know compiled version for floats and all the other data types.</p>
<p>199<br>00:30:16,000 –&gt; 00:30:34,000<br>There’s a lot of data with error which will cover what error looks like in a second they have some support for adaptive query optimization not the full plan query plan re optimization that we’ll see later this semester but they can use some little tricks as you’re running the skin long data to modify how they’re going to buy expressions and so forth.</p>
<p>200<br>00:30:34,000 –&gt; 00:30:55,000<br>And then as far as I know at least when I look at this before the only support certainers joins and has joins again there’s no query optimizer there’s no there’s no networking layer all this is has to be provided by whatever the encompassing system that wants to include include Bellox again as he said data fusion has more of this stuff.</p>
<p>201<br>00:30:55,000 –&gt; 00:30:59,000<br>But even that’s not meant to be a full complete system.</p>
<p>202<br>00:31:00,000 –&gt; 00:31:15,000<br>So as part of this again because it’s not a full complete system Bellox is not doesn’t have its own proprietary data format doesn’t own any of the data similar to this data lake architecture we’ve been talking about where we assume that’s a bunch of files somewhere on s3 whatever object store you want.</p>
<p>203<br>00:31:15,000 –&gt; 00:31:30,000<br>And they’ll have connectors allow the scan operators go retrieve that data and process it but it’s it’s not meant to be the the library by itself can’t control you know the ingestion of data and so far like something else has to manage all that.</p>
<p>204<br>00:31:31,000 –&gt; 00:31:44,000<br>So they’re going to the the API is going to that Bellox provides exposed have to keep you wanting to define connectors that allow you to retrieve data from other systems so connectors are very common term that’s using different systems like you type in your favorite database type can.</p>
<p>205<br>00:31:45,000 –&gt; 00:32:01,000<br>So if you’re a connector you’ll see different you know we’ll see what kind of kind of capabilities in support to talk to other things so the one that’s probably the most extensible is like Treno or Presto Presto came out of that as well but they have connections to talk to any possible database you would want.</p>
<p>206<br>00:32:01,000 –&gt; 00:32:21,000<br>The idea is like it’s an API within the system allowed you go talk to some other system you want to database our file system and then pull data in so you sort of basically are federated database we make one logical database talk to multiple physical databases and connectors is how they’ll do that for first class the foreign data rapper API is the same thing.</p>
<p>207<br>00:32:21,000 –&gt; 00:32:50,000<br>Duck DB calls my extensions but the term usually called is called connector and then they have adapter that allow them to decode and encode the different storage formats that we talked about so far so parquet or dwarf is the extended version eternal version of orc from from meta and then alpha I think I mentioned before this is something that the Bellox guys are building the next version of orc.</p>
<p>208<br>00:32:52,000 –&gt; 00:33:18,000<br>So there’s a bunch of components that he talked about in the paper some of these divided cover some of these I want to spend on on today right so the type system the sport scalers sort of complex nested types the vectors that they’re going to operate on are going to be arrow compatible with some extensions although some of the things that they’ve added are now in the main line arrow codes I don’t I don’t know how to overlap there is today or how much distinction there is between two of them.</p>
<p>209<br>00:33:19,000 –&gt; 00:33:39,000<br>They have a ability to find functions if you want to have like your own version of substring or whatever you want that’s not in the built-in Bellox library you can send that then there’s the engine that actually runs the operators which we’ve covered a lot connectors you talked about and the resource management would be like memory pools basically a buffer pool to keep things in memory and pull things out as needed.</p>
<p>210<br>00:33:40,000 –&gt; 00:34:02,000<br>So today’s class I want to talk about these three things because we already thought the type systems and context of a parquet function API again it’s just it’s like a user-defined function allows you to say here’s here’s this new capability built built in capability connecting different connecting different from stories devices or whatnot that’s not really new and the resource management stuff will cover it.</p>
<p>211<br>00:34:02,000 –&gt; 00:34:05,000<br>Yes. I was going to ask you.</p>
<p>212<br>00:34:05,000 –&gt; 00:34:10,000<br>What’s your question? Is that the other entry also in the window?</p>
<p>213<br>00:34:11,000 –&gt; 00:34:20,000<br>Or is there always a place to have a separate your memory into something that you can pull and give back?</p>
<p>214<br>00:34:21,000 –&gt; 00:34:28,000<br>Your question is is that a good idea to manage memory in the way they do with the rena pools?</p>
<p>215<br>00:34:28,000 –&gt; 00:34:31,000<br>Yeah. Absolutely. Yeah because what’s the alternative?</p>
<p>216<br>00:34:31,000 –&gt; 00:34:34,000<br>Well I don’t think Rust has something like that so that’s the last one.</p>
<p>217<br>00:34:34,000 –&gt; 00:34:35,000<br>You sound Rust?</p>
<p>218<br>00:34:35,000 –&gt; 00:34:42,000<br>No, not for spilling it for the way you approach the whole entire IO.</p>
<p>219<br>00:34:42,000 –&gt; 00:34:44,000<br>No, not IO.</p>
<p>220<br>00:34:44,000 –&gt; 00:34:46,000<br>You said Rust like the programming language?</p>
<p>221<br>00:34:46,000 –&gt; 00:34:50,000<br>Yeah, it doesn’t let you do with something similar to the way they have.</p>
<p>222<br>00:34:50,000 –&gt; 00:34:52,000<br>That’s what it does.</p>
<p>223<br>00:34:53,000 –&gt; 00:34:55,000<br>I mean this is like a this is a layer above the programming language.</p>
<p>224<br>00:34:55,000 –&gt; 00:34:58,000<br>It’s like it’s a system right they have to manage memory.</p>
<p>225<br>00:34:58,000 –&gt; 00:35:01,000<br>It’s also like a C++.</p>
<p>226<br>00:35:01,000 –&gt; 00:35:03,000<br>It’s a memory.</p>
<p>227<br>00:35:03,000 –&gt; 00:35:05,000<br>It’s memory.</p>
<p>228<br>00:35:05,000 –&gt; 00:35:13,000<br>I mean the answer is yes.</p>
<p>229<br>00:35:13,000 –&gt; 00:35:18,000<br>So in the Rust has this.</p>
<p>230<br>00:35:18,000 –&gt; 00:35:23,000<br>Rust is going to prevent you from like handing off memory without keeping track of it.</p>
<p>231<br>00:35:23,000 –&gt; 00:35:28,000<br>But like that chunk of memory you don’t want to go malic every single time.</p>
<p>232<br>00:35:28,000 –&gt; 00:35:31,000<br>You want to you want a memory pool?</p>
<p>233<br>00:35:33,000 –&gt; 00:35:36,000<br>So this is a layer above the programming language.</p>
<p>234<br>00:35:37,000 –&gt; 00:35:44,000<br>Again if it was like Java like a memory manage, memory manage a programming language.</p>
<p>235<br>00:35:44,000 –&gt; 00:35:49,000<br>Even then you want to avoid like in the GVM, you want to avoid their garbage collector.</p>
<p>236<br>00:35:49,000 –&gt; 00:35:51,000<br>You want to allocate your own memory.</p>
<p>237<br>00:35:51,000 –&gt; 00:35:53,000<br>Absolutely yes.</p>
<p>238<br>00:35:53,000 –&gt; 00:35:54,000<br>Where the data is here.</p>
<p>239<br>00:35:54,000 –&gt; 00:35:59,000<br>We can do anything better than what the OS can do or what the GVM or whatever can do.</p>
<p>240<br>00:35:59,000 –&gt; 00:36:02,000<br>So you want to manage everything ourselves.</p>
<p>241<br>00:36:03,000 –&gt; 00:36:04,000<br>Okay.</p>
<p>242<br>00:36:04,000 –&gt; 00:36:14,000<br>So all right the first thing I’m going to talk about is like what is the what is the what do two pulls look like not in terms of the physical level but like that’s not true either.</p>
<p>243<br>00:36:14,000 –&gt; 00:36:20,000<br>But like how are we putting together and sending data from one operator to the next.</p>
<p>244<br>00:36:20,000 –&gt; 00:36:28,000<br>Not in terms of what the encoding is just in terms of like including all the data we need or some of the data right.</p>
<p>245<br>00:36:28,000 –&gt; 00:36:33,000<br>So the operator alpha is going to define what is actually being passed up from one operator to the next.</p>
<p>246<br>00:36:33,000 –&gt; 00:36:35,000<br>Whether it’s in the same pipeline or not.</p>
<p>247<br>00:36:35,000 –&gt; 00:36:39,000<br>Again it doesn’t it matters but like the.</p>
<p>248<br>00:36:39,000 –&gt; 00:36:44,000<br>The Davidson will be able to decide okay at what point do I want to want to materialize everything.</p>
<p>249<br>00:36:44,000 –&gt; 00:36:49,000<br>And the what how we’re going to put things together it’s going to depend on what the processing model is.</p>
<p>250<br>00:36:49,000 –&gt; 00:36:52,000<br>So in the vectorized case because we want to operate on columns.</p>
<p>251<br>00:36:52,000 –&gt; 00:37:07,000<br>We we don’t want to materialize maybe all the columns all at once because we know we may not need that up in the query plan depends on what you know with the row store column store in our case as column stores and we want to lay materialization and depends on what the data requirements are meaning.</p>
<p>252<br>00:37:07,000 –&gt; 00:37:20,000<br>At what point at what point do I actually need certain pieces of data in the query plan and how expensive is it going to need to go get it again versus materialize it at the point I’m actually scanning the data file.</p>
<p>253<br>00:37:20,000 –&gt; 00:37:30,000<br>Right so let’s say in the case here we want to do this join on our nest and there’s this you know the joint causes RID equals this ID.</p>
<p>254<br>00:37:30,000 –&gt; 00:37:37,000<br>So we know that while we’re scanning our nest depending on the number of columns that they have and how wide the table actually is.</p>
<p>255<br>00:37:37,000 –&gt; 00:37:48,000<br>We know that we only need certain attributes from those tables at this point here so what point do we actually materialize the two pull so we can do the join or do the projection above it.</p>
<p>256<br>00:37:48,000 –&gt; 00:37:58,000<br>So to do early materialization the idea is that you put you you at the moment you’re scanning the data the leaf knows and the scan operators or whatever part in the query plan you’re going to put the entire two pull together.</p>
<p>257<br>00:37:58,000 –&gt; 00:38:09,000<br>Right and this is easy to do in a row store because when you scan the two pull right everything’s right there and even the past model it’s it’s it’s all going to be within the sort of same broke group and close by.</p>
<p>258<br>00:38:09,000 –&gt; 00:38:16,000<br>But depending on how wide the table is you may not actually want to put it all together right because that that’s not that’s not free.</p>
<p>259<br>00:38:16,000 –&gt; 00:38:29,000<br>Right so if I’m doing this join again on our earlier materialization then I’m going to stitch together the entire two pull of the two join of the join operator and have that be my output that’s going to be sent from this operator to the next operator.</p>
<p>260<br>00:38:29,000 –&gt; 00:38:38,000<br>Right so again the idea here is that all of the sub-contin operators in the query plan never need to go back to get more data because we’re passing the entire material.</p>
<p>261<br>00:38:38,000 –&gt; 00:39:01,000<br>So we’re passing the entire materialized two pull up above in late materialization which is what the VLS people do and most of that system do the idea is that you only want to pass up the the bare middle number attributes you actually need in the query plan and at any point you need to get more data that operator has the ability to go down and get it.</p>
<p>262<br>00:39:01,000 –&gt; 00:39:13,000<br>So if you’re doing the join I only materialize the record ID for R and S and then like a two pull ID or an offset that says to allow me to go find the other attributes within the columns below it.</p>
<p>263<br>00:39:13,000 –&gt; 00:39:30,000<br>And so that’s what gets passed up here in the in the query plan. So in case of R I’m just feeding up the two pull ID and RID because that’s all I need to do this out of the join and the same thing here once I get past the filter on value I only need to pass up the SID.</p>
<p>264<br>00:39:30,000 –&gt; 00:39:41,000<br>So I do the join but now up above here after I do my join I have RID because I was passed up to me but then I don’t have the the creation date on the S table.</p>
<p>265<br>00:39:41,000 –&gt; 00:39:50,000<br>So in this case here the operator has the ability says okay I need this other two I need this other attribute and it has the capability to ask the system to go fetch it for you and then I can stitch it together.</p>
<p>266<br>00:39:50,000 –&gt; 00:40:18,000<br>So in this case you know the you may not always want to do it this right maybe the case that the cost of going fetching things from disk again or from from the other store could be expensive again if it’s cash you avoid that that I want to I want to materialize everything early but again if I have a billion two pulls and the join produces one two pull then I’m passing along a billion two pulls a bunch of columns I don’t actually need and it’s better better do late materialization.</p>
<p>267<br>00:40:18,000 –&gt; 00:40:47,000<br>So this is an old idea that was backed about 15 years ago I don’t know whether the first column stores stuff did this in the 90s from Siby’s IQ but I know money money be did DB to this vertical do this look this is this is an old idea that pretty much everyone does today right again we did do in the pack model or the in the pack model or the column store because we can go fetch individual columns as needed and put things together.</p>
<p>268<br>00:40:47,000 –&gt; 00:41:16,000<br>Yes. So this question is if you have a say really high query plan a lot of a lot of steps going up and there’s a there’s a there’s an attribute that’s used at the top of the bottom I know where in between would you drop an add back you could but I don’t think anybody does that just too much bookkeeping I think also here depends on like</p>
<p>269<br>00:41:16,000 –&gt; 00:41:31,000<br>they yeah they get to me estimate things it’s it’s hard to get it right yes</p>
<p>270<br>00:41:31,000 –&gt; 00:41:53,000<br>do like systems like just one or the other so this question is there’s pros and cons of both of these I’m rephrasing this perfect pros and I both these do most systems just pick one just use that or they try to be clever and do a hybrid approach so similar to what he was saying can you try to figure out like can you</p>
<p>271<br>00:41:53,000 –&gt; 00:42:11,000<br>well he’s an extreme example like can you try to figure out for any query plan should I use early materialization or late materialization I think some systems do the enterprise ones I don’t know about the open source ones I like I don’t know what ducty be does I think probably what is most common is just pick like late materialization and do that because there’s less engineering</p>
<p>272<br>00:42:11,000 –&gt; 00:42:33,000<br>okay so now so now we know we’re assuming we’re doing late materialization and we know that what we’re sending up is going to be you know columns of values it’s the bare minimum what we actually need to process each operator as we go up</p>
<p>273<br>00:42:33,000 –&gt; 00:43:02,000<br>now need to talk about what the encoding of that data those columns actually look like right and the challenge here is going to be for these lake house systems these overlap systems that want to read data from a bunch of files on s3 all those files can be encoded in different ways like parquet or whatever CSVs and that means that you need a way to transform whatever the on disk representation of that data that you’re reading</p>
<p>274<br>00:43:02,000 –&gt; 00:43:25,000<br>into some common format that the execution engine can actually understand and support like you you would not want to say here’s my scunchle scan operator or my joint operator for parquet data my scunchle scan joint operator for for for or data like that just be so much you know so much code to maintain it wouldn’t be worth it.</p>
<p>275<br>00:43:25,000 –&gt; 00:43:36,000<br>So all these systems are going to translate or transform the the dis format into some internal representation right and that’s what’s going to get propagated from one operative the next as we go up in the query plan.</p>
<p>276<br>00:43:38,000 –&gt; 00:43:54,000<br>So the so the internalization or internal coding again it’s just as I said is well how the system is going to represent data on on the inside this may not be what could then is then exposed to you as the user as the result of the query like it may come out as arrow may come out of the parquet file whatever.</p>
<p>277<br>00:43:55,000 –&gt; 00:43:59,000<br>But as it’s going from one operative the next it’s going to be it’s going to be different.</p>
<p>278<br>00:44:00,000 –&gt; 00:44:11,000<br>And so just like when it was on desk we want our columns to be the values within each colony fixed length because then we can find the corresponding matches across tuples very easily.</p>
<p>279<br>00:44:12,000 –&gt; 00:44:23,000<br>Ideally you want to be able to move data and use data structures and move them from one operative the next without serializing right without having to run like in better blocks of an algorithm to do like to do three passes to try to encode it.</p>
<p>280<br>00:44:23,000 –&gt; 00:44:34,000<br>That’s super slow to do that while we’re actually running a query. So we want something that that can ideally natively operate on the encoded data and move that along without having to do to deserialize it first.</p>
<p>281<br>00:44:35,000 –&gt; 00:44:48,000<br>Similarly we also nice to have zero copy memory access because that would then potentially allow us to take our internal representation of our data and be able to share that with external processes.</p>
<p>282<br>00:44:48,000 –&gt; 00:44:58,000<br>Now this is a new idea this is actually what arrow is is really predicated on but think of something like actually duck DB does this really well.</p>
<p>283<br>00:44:59,000 –&gt; 00:45:12,000<br>So like when you can use duck DB inside like Python code right and it’s like a data frames API and you can you can access data that’s running in memory in duck DB and all it’s doing is passing a pointer because it’s the same process.</p>
<p>284<br>00:45:12,000 –&gt; 00:45:17,000<br>You don’t need to to serialize it and deserialize it to transfer data.</p>
<p>285<br>00:45:18,000 –&gt; 00:45:26,000<br>And so if you have the ability to have this common representation that everyone potentially support then you can do that zero copy memory access.</p>
<p>286<br>00:45:27,000 –&gt; 00:45:41,000<br>That’s sort of an extreme example but think of like I could send data from one node to another node over the network and if my transfer scheme or transfer encoding scheme is the same as the internal representation of data.</p>
<p>287<br>00:45:42,000 –&gt; 00:45:49,000<br>So if you have the data that you want to use and you can use it as a data operator then I don’t need to serialize and deserialize it as it would approach offers.</p>
<p>288<br>00:45:50,000 –&gt; 00:45:55,000<br>I just send over that data and then soon as the bytes arrive on the other node it just starts crunching them.</p>
<p>289<br>00:45:56,000 –&gt; 00:46:02,000<br>You can crazier you can put things down the FBGA and start on the Nick and that can start crunching on without having to decompress it first.</p>
<p>290<br>00:46:03,000 –&gt; 00:46:05,000<br>And there’s some systems that do that.</p>
<p>291<br>00:46:05,000 –&gt; 00:46:12,000<br>And so in the same way Parquet and Ork are defined as open source formats for storing data on files on disk.</p>
<p>292<br>00:46:13,000 –&gt; 00:46:21,000<br>Arrow is a open source format to describe how to store data in memory in a common art fashion.</p>
<p>293<br>00:46:22,000 –&gt; 00:46:34,000<br>And the idea is that it’s going to set the sort of encode data in such a way that an implementation of something that operates on arrow data will be able to do all the cache efficient vectorized stuff that we’re talking about throughout the entire semester.</p>
<p>294<br>00:46:35,000 –&gt; 00:46:42,000<br>The one thing that is going to be much different than arrow sorry Parquet and Ork is that they’re also going to be able to support random access more efficiently.</p>
<p>295<br>00:46:43,000 –&gt; 00:46:51,000<br>I can jump to an offset and get the exact value for that tuple without having to be decode using RLE or delta encoding all the stuff that we talked about before.</p>
<p>296<br>00:46:52,000 –&gt; 00:46:56,000<br>And then this Crunch will access into ripping through the columns and processing things.</p>
<p>297<br>00:46:57,000 –&gt; 00:47:08,000<br>So arrows a lot of stuff like it’s meant to be sort of this framework that comes with memory management, thread management, RPC mechanisms.</p>
<p>298<br>00:47:09,000 –&gt; 00:47:12,000<br>They have a SQL parser and a SQL transport layer as well.</p>
<p>299<br>00:47:13,000 –&gt; 00:47:19,000<br>But I’m talking about here today is just going to be the encoding scheme for the columns for the data we’re passing from an operative annex.</p>
<p>300<br>00:47:20,000 –&gt; 00:47:23,000<br>Again, whether it’s on the same box or a different box, it doesn’t matter.</p>
<p>301<br>00:47:24,000 –&gt; 00:47:35,000<br>You also have a sort of a not simplistic but a basic expression, expression engine for evaluating expression trees, like similar filters projections.</p>
<p>302<br>00:47:36,000 –&gt; 00:47:41,000<br>So you can’t do all of the complicated functions that you would want or could do in Velox.</p>
<p>303<br>00:47:42,000 –&gt; 00:47:45,000<br>You can do really basic things like this is this column equal to this and so forth.</p>
<p>304<br>00:47:46,000 –&gt; 00:47:50,000<br>And they actually compile it down into LLVM using this engine they have called Gindiba.</p>
<p>305<br>00:47:52,000 –&gt; 00:47:54,000<br>So arrows mean much different than a parking in work.</p>
<p>306<br>00:47:55,000 –&gt; 00:47:58,000<br>And again, because they want to be lightweight, they only support two encoding schemes.</p>
<p>307<br>00:47:59,000 –&gt; 00:48:02,000<br>They have dictionary encoding and some variation of RLE.</p>
<p>308<br>00:48:03,000 –&gt; 00:48:08,000<br>If they’re not doing delta encoding, they’re not doing all the bitmap stuff that better box is doing.</p>
<p>309<br>00:48:09,000 –&gt; 00:48:11,000<br>It’s just going to be these two things.</p>
<p>310<br>00:48:12,000 –&gt; 00:48:29,000<br>And so the way they’re going to dictionary encoding, at least the original version, was just taking all the strings that are in a column, sorting them, and then the offsets are the codes you’re going to embed in the column itself.</p>
<p>311<br>00:48:31,000 –&gt; 00:48:32,000<br>Yes.</p>
<p>312<br>00:48:37,000 –&gt; 00:48:39,000<br>Why do they have expression in their vibration?</p>
<p>313<br>00:48:40,000 –&gt; 00:48:44,000<br>There’s a specification of what the encoding looks like.</p>
<p>314<br>00:48:45,000 –&gt; 00:48:50,000<br>And then there’s an implementation of being operated on it in C++.</p>
<p>315<br>00:48:51,000 –&gt; 00:48:59,000<br>And so as part of that implementation, they include the ability to find the matching two-boats or something equals something.</p>
<p>316<br>00:49:00,000 –&gt; 00:49:06,000<br>And the way they’re going to execute it, we’ll see in a second, instead of traversing the tree, they’re going to compile it down to LLVM.</p>
<p>317<br>00:49:07,000 –&gt; 00:49:13,000<br>Again, it’s hard to say, like, I was debating whether you were going to mention it because we haven’t talked about query compilation, which is next week.</p>
<p>318<br>00:49:14,000 –&gt; 00:49:16,000<br>And we’ll see how you can do this for expressions in a second.</p>
<p>319<br>00:49:17,000 –&gt; 00:49:22,000<br>Again, it’s more than the spec, there’s an invitation library for it as well.</p>
<p>320<br>00:49:26,000 –&gt; 00:49:33,000<br>All right, so I should have talked about this earlier when we talked about data encoding of how you actually want to represent strings.</p>
<p>321<br>00:49:34,000 –&gt; 00:49:40,000<br>Again, strings are variable length, you want to convert them into fixed length values, to store them in your columns.</p>
<p>322<br>00:49:41,000 –&gt; 00:49:48,000<br>But then what are you actually storing for that value in the fixed length column, and then what are you actually storing the variable length string?</p>
<p>323<br>00:49:49,000 –&gt; 00:49:57,000<br>So what Error originally did was the fixed length data would just be a size and a pointer, so 12 bytes.</p>
<p>324<br>00:49:58,000 –&gt; 00:50:07,000<br>And then the pointer would just be an offset in some blob region in memory, and it would just point to what byte offset you would find the string that you’re looking for.</p>
<p>325<br>00:50:08,000 –&gt; 00:50:16,000<br>So the column would be this data up here, always 12 bytes, and then these other variable length things that has the actual strings itself.</p>
<p>326<br>00:50:17,000 –&gt; 00:50:20,000<br>So it didn’t matter what the string was, you were always using this scheme.</p>
<p>327<br>00:50:21,000 –&gt; 00:50:35,000<br>So in Velox, in the paper, they talk about how they extended Error to support this additional storage scheme that’s based on something that the Germans did, and what is called that German string storage.</p>
<p>328<br>00:50:36,000 –&gt; 00:50:49,000<br>So the idea is that if a string is small, so it’s always going to be 16 bytes instead of 12 bytes, you always have the header you have the size, and then if the string is small, you store a 4x4.</p>
<p>329<br>00:50:50,000 –&gt; 00:50:58,000<br>So the 4x4 is the first 4x4, and then whatever the remaining data is in the remaining 8 bytes, it’s padded out of the 0s.</p>
<p>330<br>00:50:59,000 –&gt; 00:51:10,000<br>But the string is larger than this, and the prefix, you show out the size, you show out the prefix, but then you have the pointer now to that blob area where you have the actual full string itself.</p>
<p>331<br>00:51:11,000 –&gt; 00:51:19,000<br>So even though, in this case here, you have the prefix here, you’re still going to store it down here because you don’t want to have to like, you know, stitch things together when you’re scanning things along, you just jump to here and get down to everything you need.</p>
<p>332<br>00:51:20,000 –&gt; 00:51:28,000<br>And so this is a really simple idea, but it makes a huge difference because now in some cases or some queries, I actually don’t need to follow the pointers.</p>
<p>333<br>00:51:29,000 –&gt; 00:51:42,000<br>Like if I’m just trying to find all the strings with the letters A-N, I can rip through my column that are always going to be 16 bytes and just do the pattern matching on the prefix, not even follow the pointer.</p>
<p>334<br>00:51:42,000 –&gt; 00:51:49,000<br>Right? Where in the case of a bob, I don’t have anything in my fixed length data, I always got to follow the pointer.</p>
<p>335<br>00:51:50,000 –&gt; 00:51:58,000<br>Right? So again, another way to think about the pointer, like that’s the, sorry, like this thing here, that’s the dictionary code, the pointer is the dictionary code.</p>
<p>336<br>00:51:59,000 –&gt; 00:52:11,000<br>Right? And there’s, in some cases here, like, you do, times where you don’t even need to follow the full string to do some computations, whether it’s filtering, aggregations and stuff, you just look at the prefix.</p>
<p>337<br>00:52:13,000 –&gt; 00:52:20,000<br>And so I talked to this last year and there is no name for this technique. I’ve just been calling it German, German style string storage.</p>
<p>338<br>00:52:21,000 –&gt; 00:52:35,000<br>And then other people start using that term too. Right? So this is from the arrow people, which says, you know, last year or six months ago, this is in October, that says arrow can’t do German style string storage, literally some of the same slides, and then he shows the PR where this comes out.</p>
<p>339<br>00:52:36,000 –&gt; 00:52:45,000<br>But then someone messaged in afterwards, like, hey, what is German style string storage, right? And this, the paper from the Germans from Ambra, where they invented the technique.</p>
<p>340<br>00:52:46,000 –&gt; 00:53:02,000<br>The Germans do amazing stuff, like, well, I said this before, like, they love to pack, they love to pack, like, bloom filters and other things and left over bits in different parts of the system, like for hash tables, even though there’s 64 bit pointers in x86, until actually only uses 48 bits.</p>
<p>341<br>00:53:03,000 –&gt; 00:53:13,000<br>So they take the remaining 16 bits and they put a bloom filter in for your hash table. So that just checks whether the key is getting going to exist and the chain that comes after that. So you can look at the bloom filter, not even follow the pointer.</p>
<p>342<br>00:53:14,000 –&gt; 00:53:23,000<br>So similar idea here, they do amazing stuff. And then this actually came out two days ago, the Polars guy, they have a new blog articles now, they’re using this format.</p>
<p>343<br>00:53:24,000 –&gt; 00:53:29,000<br>And of course, somebody’s asking again, what is German style storage actually meet, right? So this is my fault.</p>
<p>344<br>00:53:33,000 –&gt; 00:53:46,000<br>So this is the Ambra style string storage or norm install string storage, okay? Dr. B does this, Velox does this, Polars does this, Ambra obviously does this. I don’t know what the hyper did, right?</p>
<p>345<br>00:53:47,000 –&gt; 00:53:51,000<br>This is a really simple technique and it’s a huge, huge win.</p>
<p>346<br>00:53:53,000 –&gt; 00:53:54,000<br>Yes.</p>
<p>347<br>00:54:03,000 –&gt; 00:54:20,000<br>The question is, if you need to match, see all five bytes, the first five bytes instead of four, would you do a first pass, look at the prefix, figure out what matches then within that, do a pass on this.</p>
<p>348<br>00:54:21,000 –&gt; 00:54:33,000<br>Probably, yes, I don’t know what Polars does in the other systems do. It would make sense because in that case, you could rip it off a column, figure out what you actually need to look at.</p>
<p>349<br>00:54:34,000 –&gt; 00:54:47,000<br>And then in that case, also, you would avoid duplicates, right? Because like, again, think of this as like, I could have multiple, if multiple tuples are sharing the same string, then the pointer would be the same. So I could just call less them.</p>
<p>350<br>00:54:47,000 –&gt; 00:54:56,000<br>I had to do some bookkeeping to figure out, okay, which one is actually matched with the same thing when I pass up the results. But yeah, that would probably be a faster way to do that.</p>
<p>351<br>00:54:56,000 –&gt; 00:55:03,000<br>Again, assuming that there’s enough overlap, but I don’t know what people actually implement. But it’s a good optimization. Yes.</p>
<p>352<br>00:55:03,000 –&gt; 00:55:05,000<br>Is pointer like at all?</p>
<p>353<br>00:55:06,000 –&gt; 00:55:17,000<br>So, question is this pointer an offset? Yeah, so it could be an offset within some blob region memory.</p>
<p>354<br>00:55:17,000 –&gt; 00:55:31,000<br>I don’t know what error is, I don’t think it’s a, yeah, in error, I know it’s like a bucket number, then it’s offset within the bucket because they have different buckets for the very linked data.</p>
<p>355<br>00:55:31,000 –&gt; 00:55:36,000<br>Okay, yes. I think that looks like just a pointer.</p>
<p>356<br>00:55:36,000 –&gt; 00:55:41,000<br>Yeah, yeah. And it was looking at the arrows back. It’s a bucket.</p>
<p>357<br>00:55:41,000 –&gt; 00:55:49,000<br>Because again, think of like, you’re sending stuff over the network. You don’t want to deserialize it.</p>
<p>358<br>00:55:49,000 –&gt; 00:55:58,000<br>So like, if it’s just a logical pointer, then the physical memory just doesn’t matter on the other side.</p>
<p>359<br>00:55:59,000 –&gt; 00:56:03,000<br>Okay, so German style string storage is really unversal string storage.</p>
<p>360<br>00:56:03,000 –&gt; 00:56:08,000<br>Okay, so I don’t want to talk too much about this. I just want to mention it real quickly.</p>
<p>361<br>00:56:08,000 –&gt; 00:56:25,000<br>We mentioned substrate before. This is a, is it not directly related to arrow, but think of this as like in the same way that arrows is meant to be the universal file format or universal encoding scheme for transferring data across different systems.</p>
<p>362<br>00:56:26,000 –&gt; 00:56:34,000<br>Substrate is meant to be a open source specification for how to represent query plans, relational, algebra query plans.</p>
<p>363<br>00:56:34,000 –&gt; 00:56:43,000<br>So in, in like, I think data fusion and ducty B, I think they have the ability to take these substrate query plans and actually execute them.</p>
<p>364<br>00:56:43,000 –&gt; 00:56:54,000<br>So the idea, you know, the idea seems great. Like it’d be something like I can take a something like calcite or some, some query out of the server service, have produced substrate.</p>
<p>365<br>00:56:55,000 –&gt; 00:57:03,000<br>And then if my system can then operate on that, and that query plan, that’s the, you know, I can just take it and run it. I have these things a bit more decoupled.</p>
<p>366<br>00:57:03,000 –&gt; 00:57:12,000<br>So this was created by the guy that did a patchy drill, which is a, a open source clone of BigQuery from Google.</p>
<p>367<br>00:57:12,000 –&gt; 00:57:19,000<br>And then I think he also could have found a Dremio as well. But now he’s focusing entirely on substrate question. Sorry.</p>
<p>368<br>00:57:20,000 –&gt; 00:57:33,000<br>Okay. Again, I, so when I talk to some of the arrow people at Volchandata, the problem with substrate is that it’s for really simple things, it’s okay.</p>
<p>369<br>00:57:33,000 –&gt; 00:57:39,000<br>But it’s sort of, you’re sort of extending it going beyond what they expect and it causes problems.</p>
<p>370<br>00:57:40,000 –&gt; 00:57:48,000<br>And I think it’s just one dude. There’s like doing it. Whereas like arrows, you know, rather large consortium, I think substrate is mostly, at least it was at the time.</p>
<p>371<br>00:57:48,000 –&gt; 00:57:53,000<br>It’s just one dude doing as like, as like, is hobby now.</p>
<p>372<br>00:57:53,000 –&gt; 00:58:01,000<br>I did it, we mentioned multiple times. And again, the branding seems kind of weird because like it’s a part of the patchy arrow project.</p>
<p>373<br>00:58:01,000 –&gt; 00:58:12,000<br>But it’s almost like his own entire separate thing. And so I wouldn’t be surprised if this thing gets forked out underneath the arrow umbrella and comes to its own standalone top level project in a patchy.</p>
<p>374<br>00:58:12,000 –&gt; 00:58:20,000<br>But the basically like VeloX, it’s a vectorized execution library for operating, directly on patchy arrow data.</p>
<p>375<br>00:58:20,000 –&gt; 00:58:26,000<br>It does include a query optimizer. It does include the, you know, SQL front end.</p>
<p>376<br>00:58:26,000 –&gt; 00:58:34,000<br>So it’s a bit more, it has a bit more features than VeloX, but it’s core. If you don’t want any of that, you could just take their query engine.</p>
<p>377<br>00:58:34,000 –&gt; 00:58:40,000<br>So there’s already a bunch of systems that are based on this. Inflex DB is probably the biggest one out of all these.</p>
<p>378<br>00:58:40,000 –&gt; 00:58:47,000<br>This is their third, Inflex DB’s third rewrite. And now there’s a long story.</p>
<p>379<br>00:58:47,000 –&gt; 00:58:54,000<br>They had the, they had the, it was version 1.0 and then they switched, they got off SQL when I told them the bad idea and they added MAP, 10,000, the bad idea.</p>
<p>380<br>00:58:54,000 –&gt; 00:59:01,000<br>And that was version two and then version three, they got rid of MAP, they added back SQL and now they’re based entirely on data fusion.</p>
<p>381<br>00:59:01,000 –&gt; 00:59:12,000<br>Serious DB, Knaw CV, I think they’re out of China and then CFAL. I think these guys are, oh, they just got acquired by enterprise DB.</p>
<p>382<br>00:59:12,000 –&gt; 00:59:20,000<br>I think two or three weeks ago, I think this is a data fusion, execution engine that’s designed to run inside of Postgres.</p>
<p>383<br>00:59:20,000 –&gt; 00:59:26,000<br>So, so, so Arrow is, you know, Arrow has sort of become the de facto standard, like Snowflake supports it.</p>
<p>384<br>00:59:26,000 –&gt; 00:59:34,000<br>Like this, this is a lot of systems are based on this and then data fusion is one implementation of an engine that can operate on it.</p>
<p>385<br>00:59:34,000 –&gt; 00:59:45,000<br>All right, so I want to finish up talking about expression evaluation and then sprinkle a little bit of that execution and then that’ll then set us up for talking about vectorized execution next week.</p>
<p>386<br>00:59:45,000 –&gt; 00:59:57,000<br>So, expression evaluation basically says how do you actually take within the wear clause or join clause or the predicates and evaluate them on the data as we’re scanning or doing our, our, our execute operator.</p>
<p>387<br>00:59:58,000 –&gt; 01:00:10,000<br>And so, at a high level, you can think of like the, the, the parses are going to, you know, the SQL parses are going to take whatever’s in the SQL query and convert it into a tree, the expression tree that looks like this.</p>
<p>388<br>01:00:10,000 –&gt; 01:00:15,000<br>And every node’s going to represent some operator or operand within that expression tree.</p>
<p>389<br>01:00:16,000 –&gt; 01:00:27,000<br>Like so you have for your, your conjunction, disjunctions less than greater than not equals so forth, all your arithmetic operators, constant values, references to two pulls within or column, and then additional functions.</p>
<p>390<br>01:00:27,000 –&gt; 01:00:33,000<br>And you basically translate the, you know, within the SQL query into a tree like this.</p>
<p>391<br>01:00:33,000 –&gt; 01:00:44,000<br>Now, whether or not you break up like the join clause into, in the wear clause to separate trees or put them together or how you break up within the, you have nested queries, like all of that is left up the implementation.</p>
<p>392<br>01:00:45,000 –&gt; 01:00:49,000<br>But it’s always going to end up looking like a tree structure like this. Right.</p>
<p>393<br>01:00:50,000 –&gt; 01:00:59,000<br>And so how we actually execute this, well, in the naive ways that for every single two pull as I’m scanning my, my data, I’m going to traverse the tree.</p>
<p>394<br>01:01:00,000 –&gt; 01:01:08,000<br>Right, to do this, you know, I would start with the and go down this side, evaluate these things pulled up, see whether it’s equal is good on the other side and do that comparison.</p>
<p>395<br>01:01:09,000 –&gt; 01:01:26,000<br>But obviously that’s going to be super, super slow because it’s a bunch of in direction now. I’m, I mean, jumps and falling, you know, pointers in, you know, virtual function tables, if it’s a sleep of the laws, like running this, it’s sort of a naive case, which bus stop does use, would be super slow, if trying to ski through a billion, billion, two pulls.</p>
<p>396<br>01:01:27,000 –&gt; 01:01:54,000<br>So what we want to try to do instead is represent the expression in, and as if it was like a function call for, you know, within a programming language, because now we can hand that off to a potentially compiler, and whether or not that’s the chore optimizer or a traditional programming language compiler like GCC, you know, you can do all the standard tricks that we know how to do in the past 50 years.</p>
<p>397<br>01:01:55,000 –&gt; 01:02:15,000<br>Right. So say we have a real simple query, we’re, you know, select start from table or missing the from calls, or espow equals one, right. Again, we have an expression tree with the Ego calls, espow and constant. Well, ideally, we want to have a function that just takes in value as an input and checks to see whether it equals one.</p>
<p>398<br>01:02:15,000 –&gt; 01:02:33,000<br>And have just run that function in every single two pull. And so we could then compile this now again using our favorite compiler into actual machine code, and then now as we’re scanning along the two pull, we just invoke that function set a traversing the tree.</p>
<p>399<br>01:02:34,000 –&gt; 01:02:40,000<br>So that’s what that gandeeva thing I mentioned in error. That’s what they do is what postgres does when you turn on the jit compiler.</p>
<p>400<br>01:02:40,000 –&gt; 01:02:45,000<br>This is what a lot of systems actually actually do. Yes.</p>
<p>401<br>01:02:46,000 –&gt; 01:02:52,000<br>Yes. Why isn’t that the reason that it’s like.</p>
<p>402<br>01:02:54,000 –&gt; 01:03:09,000<br>This question is go back to the show. Why would traversing this tree so slow? Yeah. So think of like I have a, in my operator, my scan operator, that all I have is a pointer to some root of an expression tree.</p>
<p>403<br>01:03:10,000 –&gt; 01:03:32,000<br>And to make it composable or generic, it’s going to be a, you know, it’s going to be an abstract class. So I’m going to invoke like run function on this expression operand again, sum is C++, that’s a virtual function table to look up to see what’s the actual function I’m calling.</p>
<p>404<br>01:03:32,000 –&gt; 01:03:42,000<br>Then inside of this operand, I’m going to have two pointers left and right. I had to call those pointers. Same thing virtual function table look up. Then you know, execute this thing and go down and now I’m copying data up.</p>
<p>405<br>01:03:43,000 –&gt; 01:03:47,000<br>That’s how it’s thinking that you have like so much in direction. Absolutely. Yes.</p>
<p>406<br>01:03:48,000 –&gt; 01:03:54,000<br>But what, okay, so what if you did something like a plus tree, you had everything stored in one big chunk.</p>
<p>407<br>01:03:54,000 –&gt; 01:03:59,000<br>Can you potentially get over this cost? I mean, it’s not. It’s just compilation.</p>
<p>408<br>01:03:59,000 –&gt; 01:04:03,000<br>Stay with me. What do you do something like a B plus tree and store everything as one big chunk? What do you mean?</p>
<p>409<br>01:04:03,000 –&gt; 01:04:19,000<br>Instead of having to do, instead of having the full 100 direction, I know that it’s not going to be the entire tree as the same way of B plus tree is faster than that.</p>
<p>410<br>01:04:20,000 –&gt; 01:04:25,000<br>I get re-leveled the trees. You could store it if you’re working with one chunk of memory.</p>
<p>411<br>01:04:25,000 –&gt; 01:04:28,000<br>Or flatten it. That’s what Velox does.</p>
<p>412<br>01:04:29,000 –&gt; 01:04:42,000<br>They flatten it and then they come. Well, see in a second. Velox has an experimental compiler that will then convert it like the Kindeva and arrow.</p>
<p>413<br>01:04:43,000 –&gt; 01:04:50,000<br>The Kindeva thing and the Velox one are separate. Velox is going to flatten it as you say. Then you run along sequentially along the leakness.</p>
<p>414<br>01:04:50,000 –&gt; 01:05:02,000<br>Postgres, again, if you’re not doing the jit compiler, it’ll traverse the tree. My SQL traverses the tree. A lot of systems do that. And obviously that’s terrible for overlap.</p>
<p>415<br>01:05:03,000 –&gt; 01:05:10,000<br>Again, in their world, they’re operating on a single tool for most of the time. In our world, we went operating on columns, ton of data.</p>
<p>416<br>01:05:10,000 –&gt; 01:05:22,000<br>So, not only do we not want to traverse the tree, we want to make this whatever the thing we convert it to, the actual function call, instead of passing a single value, we want to vector values.</p>
<p>417<br>01:05:23,000 –&gt; 01:05:25,000<br>You do what, sorry?</p>
<p>418<br>01:05:26,000 –&gt; 01:05:33,000<br>Oh, yeah, so we want to work this function at a billion times, yes. We can vectorize this.</p>
<p>419<br>01:05:34,000 –&gt; 01:05:39,000<br>And then it goes back to, again, we’ll talk about next week, we talked about a little bit on Monday this week.</p>
<p>420<br>01:05:40,000 –&gt; 01:05:49,000<br>Because, you know, I only have so many data types in my columns and my constants, like instead of passing, having one hard code, I could pass one as a constant value here.</p>
<p>421<br>01:05:50,000 –&gt; 01:06:00,000<br>And I would have a version of this check function, a quality check for in 32, in 64 floats and so forth. And I’m not just picking it runtime, which is this pre-compiled function I want to use.</p>
<p>422<br>01:06:00,000 –&gt; 01:06:05,000<br>Given a vector, given some constant, you know, return whether it’s something equals something.</p>
<p>423<br>01:06:05,000 –&gt; 01:06:07,000<br>And this is different from query compilation.</p>
<p>424<br>01:06:08,000 –&gt; 01:06:15,000<br>Sabine is, is this different query compilation? Yes, query compilation, we’ll differentiate between holistic query compilation where I’m compiling the entire query plan.</p>
<p>425<br>01:06:16,000 –&gt; 01:06:26,000<br>This is just compiling the expressions. So you can still have an attributed engine, which, which Vellac stars and what most systems do.</p>
<p>426<br>01:06:27,000 –&gt; 01:06:31,000<br>But then within that, when I call expressions, those are pre-compiled.</p>
<p>427<br>01:06:31,000 –&gt; 01:06:33,000<br>Pre-compiled primitives.</p>
<p>428<br>01:06:35,000 –&gt; 01:06:37,000<br>As I said, we want to vectorize everything.</p>
<p>429<br>01:06:38,000 –&gt; 01:06:46,000<br>So, in the case of Vellac’s, it’s an example that he said, I wouldn’t call it the B-blust tree, but I’m sure there’s a compiler term to this.</p>
<p>430<br>01:06:46,000 –&gt; 01:06:49,000<br>You’re basically flattening down the tree so that you just actually things sequentially.</p>
<p>431<br>01:06:50,000 –&gt; 01:06:59,000<br>And then you can think of what you’re actually executing now, function, you’re making calls to function pointers that are pre-compiled, as I said.</p>
<p>432<br>01:06:59,000 –&gt; 01:07:08,000<br>And because we’re operating on vectors of data rather than single attributes within a single tuple, then that advertises the jump cost going to that function.</p>
<p>433<br>01:07:09,000 –&gt; 01:07:14,000<br>And there is an experimental branch that is being actively maintained in Vellac’s, but I don’t think it’s turned on by default.</p>
<p>434<br>01:07:15,000 –&gt; 01:07:28,000<br>That can convert the flattening expression from the query plan into some IR that they then convert to C++ and do a fork exec to compile that in GCC.</p>
<p>435<br>01:07:29,000 –&gt; 01:07:35,000<br>Which is slow because it’s synthesizing C++ code in the file.</p>
<p>436<br>01:07:36,000 –&gt; 01:07:38,000<br>Then I forked it as that GCC, which fires up all the processes.</p>
<p>437<br>01:07:38,000 –&gt; 01:07:41,000<br>And GCC is going to bunch of other initialization stuff on its own.</p>
<p>438<br>01:07:41,000 –&gt; 01:07:45,000<br>Like, it’s going to check config files and other stuff, right? Then in Compositor code.</p>
<p>439<br>01:07:47,000 –&gt; 01:07:51,000<br>That’s what the first version of single store MemSQL back 10 years ago, they would do the same thing.</p>
<p>440<br>01:07:51,000 –&gt; 01:07:56,000<br>And of course, in some cases, the compilation calls for the query could be like seconds.</p>
<p>441<br>01:07:57,000 –&gt; 01:08:04,000<br>So, you would only want to do this kind of compilation stuff if you know that the query is going to run for a long, long time, and that will negate the compilation code.</p>
<p>442<br>01:08:05,000 –&gt; 01:08:10,000<br>Like, if your query is going to run for 10 milliseconds, your compilation times 500 milliseconds, that’s not a fair trade-off.</p>
<p>443<br>01:08:10,000 –&gt; 01:08:12,000<br>So, you’ve got to be careful when you actually do this.</p>
<p>444<br>01:08:12,000 –&gt; 01:08:22,000<br>In sending a process, they’ll have an estimate for how long it’s going to take the query plan to run, how long it’s going to take to actually compile something, and they don’t always compile everything.</p>
<p>445<br>01:08:24,000 –&gt; 01:08:32,000<br>So, even without this compilation step, though, into machine code, Vellac does some additional optimizations that I think are kind of cool in what we’re discussing.</p>
<p>446<br>01:08:33,000 –&gt; 01:08:36,000<br>So, the one they’re going to do is constant folding.</p>
<p>447<br>01:08:37,000 –&gt; 01:08:47,000<br>See, the idea here is that if you know that there’s some operation doing over and over again on some constant value, then to do it once, memorize it, or retain it, and don’t repeat it.</p>
<p>448<br>01:08:47,000 –&gt; 01:08:48,000<br>Question? Yes?</p>
<p>449<br>01:08:48,000 –&gt; 01:08:49,000<br>Yes.</p>
<p>450<br>01:08:49,000 –&gt; 01:09:06,000<br>I was going to ask, because like, borrowing the user media, with these common expressions, there are some more operations that you know, like anyone looks really really like a lightweight computer on the media, but this like, whole makeup like has the people’s expressions.</p>
<p>451<br>01:09:06,000 –&gt; 01:09:10,000<br>Because I don’t know if you can do this like, no one, it’s a priority.</p>
<p>452<br>01:09:10,000 –&gt; 01:09:20,000<br>So, his question is, has anybody considered basically embedding or compiling in the data system itself, and using that to compile like everything?</p>
<p>453<br>01:09:20,000 –&gt; 01:09:27,000<br>Or, it would be like limited to just the simple expressions that are like, no you’d be asking, you’d be asking, you’d be asking, like, making really awkward words.</p>
<p>454<br>01:09:27,000 –&gt; 01:09:28,000<br>Yes, ignore you.</p>
<p>455<br>01:09:28,000 –&gt; 01:09:30,000<br>I think we’re saying the announcement.</p>
<p>456<br>01:09:30,000 –&gt; 01:09:31,000<br>It’s consequential to the announcement.</p>
<p>457<br>01:09:31,000 –&gt; 01:09:32,000<br>Yeah, yeah, yeah.</p>
<p>458<br>01:09:32,000 –&gt; 01:09:34,000<br>So, this is where I’m getting at.</p>
<p>459<br>01:09:34,000 –&gt; 01:09:42,000<br>So, the thing about describe you, again, you learn in a compiler class, but in the case of Velox, they’re re-implementing that.</p>
<p>460<br>01:09:42,000 –&gt; 01:09:47,000<br>And then the lines get blurry of who’s actually should be doing the optimization, should it be Velox?</p>
<p>461<br>01:09:47,000 –&gt; 01:09:52,000<br>Because they don’t have a query optimizer, should it be the query optimizer above, or in your case, could it be the compiler itself?</p>
<p>462<br>01:09:52,000 –&gt; 01:10:03,000<br>So, there is a system at a Germany, same school as Ambra, she’s not German though, but she’s at the German school, called LingoDB, where they convert a query plan to this thing called MLIR,</p>
<p>463<br>01:10:03,000 –&gt; 01:10:09,000<br>which is similar to substrate, but think of like, you know, meant for more general purpose programming.</p>
<p>464<br>01:10:09,000 –&gt; 01:10:14,000<br>And then they run Clang or LLVM compiler on that, who then does all this stuff as well.</p>
<p>465<br>01:10:14,000 –&gt; 01:10:16,000<br>So, yeah, people have done this.</p>
<p>466<br>01:10:16,000 –&gt; 01:10:21,000<br>But I think, I don’t think any commercialism is doing exactly what LingoDB does.</p>
<p>467<br>01:10:21,000 –&gt; 01:10:22,000<br>Yes.</p>
<p>468<br>01:10:22,000 –&gt; 01:10:36,000<br>So, this question is, is there a difference between generating LLVM IR versus generating C++ code?</p>
<p>469<br>01:10:36,000 –&gt; 01:10:38,000<br>Let’s punch on that until next week.</p>
<p>470<br>01:10:38,000 –&gt; 01:10:46,000<br>So, we’ll look at an early system called IQ that did generate C++ code, said MimSeco did that.</p>
<p>471<br>01:10:46,000 –&gt; 01:10:51,000<br>There’s pros and cons of this. Obviously, it’s going to be slower than just generating LLVM IR.</p>
<p>472<br>01:10:51,000 –&gt; 01:10:56,000<br>The other hand, if it crashes, you have C code you can look at and figure out what went wrong.</p>
<p>473<br>01:10:56,000 –&gt; 01:10:58,000<br>So, we’ll come to that later.</p>
<p>474<br>01:10:58,000 –&gt; 01:11:04,000<br>But to his point here, here’s how to do a constant voting expression tree, that’s compiler 101.</p>
<p>475<br>01:11:04,000 –&gt; 01:11:11,000<br>So, I’m calling this upper function on the constant Wutang to do my comparison against the column,</p>
<p>476<br>01:11:11,000 –&gt; 01:11:14,000<br>but I’m doing that over and over again for every single tuple.</p>
<p>477<br>01:11:15,000 –&gt; 01:11:23,000<br>So, obviously, I can just do this once and replace it with Wutang and then I don’t want to run it again.</p>
<p>478<br>01:11:23,000 –&gt; 01:11:28,000<br>Another common thing they do is, or at least in the Vellocs, they do common sub tree elimination.</p>
<p>479<br>01:11:28,000 –&gt; 01:11:35,000<br>So, if you recognize that you have the same subtree in your expression plan or expression tree that you’re running over and over again,</p>
<p>480<br>01:11:36,000 –&gt; 01:11:47,000<br>so run string position on a column for a given constant, then I can just do it once and then just link up this operator here to get the result from whatever comes out of this thing.</p>
<p>481<br>01:11:47,000 –&gt; 01:11:53,000<br>Again, as we said, the lines are blur between who’s actually doing this.</p>
<p>482<br>01:11:53,000 –&gt; 01:11:59,000<br>Should it be the execution in something like Vellocs? Should it be the query optimizer? Should it be doing this?</p>
<p>483<br>01:12:00,000 –&gt; 01:12:13,000<br>In the case of, if you have a standalone query optimizer, it might be kind of hard to do the constant folding thing for this function because now you need the implementation of the upper function.</p>
<p>484<br>01:12:13,000 –&gt; 01:12:20,000<br>So, here you call the system that you’re running on and say, hey, here’s upper, give me what the result of this, but that could be slow.</p>
<p>485<br>01:12:20,000 –&gt; 01:12:22,000<br>Or you’d re-implement it yourself.</p>
<p>486<br>01:12:23,000 –&gt; 01:12:30,000<br>Calcy re-implement some of the stuff themselves. Data fusion is called data fusion. I don’t know what work it does from Green Plum.</p>
<p>487<br>01:12:30,000 –&gt; 01:12:41,000<br>In case of my SQL, I know that for these kind of things, within the optimizer itself, if they see that something they want to do this constant folding thing or other, they want to know what’s the value for this operation in a constant,</p>
<p>488<br>01:12:41,000 –&gt; 01:12:53,000<br>they’ll convert some subset of the where clause into a table of select statement. Within the optimizer, go run that query, get back the result, and then inject the value into the query plan.</p>
<p>489<br>01:12:53,000 –&gt; 01:13:05,000<br>And you can do that because my SQL can do that because it’s tightly coupled. But again, if you’re trying to build these things as separate standalone services, that’d be hard to do. You’d have to have an API to support it.</p>
<p>490<br>01:13:06,000 –&gt; 01:13:18,000<br>The last thing we’re going to cover is the ad-activity. The idea here is that, although we haven’t talked about query optimizers yet, from the interclass, you should be aware of what they’re actually doing.</p>
<p>491<br>01:13:18,000 –&gt; 01:13:25,000<br>They’re taking a SQL query plan, sorry, SQL query, converting to a physical plan, we can actually then run on a system.</p>
<p>492<br>01:13:25,000 –&gt; 01:13:37,000<br>And the way a good query optimizer is going to do this is through cost model estimates. I try to predict the selectivity of every single operator to determine how much data they’re shoving up from one operator to the next.</p>
<p>493<br>01:13:37,000 –&gt; 01:13:50,000<br>They can use this to determine join ordering. And so for all the optimizations that we’ve talked about in the semester, like all that is a complete waste of time if we’re given crappy query plans.</p>
<p>494<br>01:13:51,000 –&gt; 01:14:03,000<br>If we choose the worst join order that we possibly have, then it doesn’t matter whether we’re vectorized, we’re in compilation, we’re running on fancy GPUs, hardware, whatever, all that can turn out the window because we have the crappy query plan.</p>
<p>495<br>01:14:03,000 –&gt; 01:14:18,000<br>So we need a way to be resilient or robust. Have the extension be robust enough that like even if we are giving a crappy query plan, we can tweak things and make changes as we go along to try to improve our situation.</p>
<p>496<br>01:14:18,000 –&gt; 01:14:31,000<br>Challenger, you may say, okay, well, okay, well, just make sure I don’t have crappy query plans. The problem is, again, in a lake house environment or a data lake environment, you may not have any any statistics about your data.</p>
<p>497<br>01:14:31,000 –&gt; 01:14:41,000<br>Like someone uploaded a bunch of parquet files in S3, and then you’re told to go run select queries on top of that data. You’ve never seen it before. How can you actually start producing estimates?</p>
<p>498<br>01:14:42,000 –&gt; 01:14:53,000<br>Or if you use one of these connector things that talk to, you know, have your lake house system talk to Postgres, Postgres may not expose to you, it’s internal statistics, so you have no idea what you’re actually reading.</p>
<p>499<br>01:14:53,000 –&gt; 01:15:00,000<br>So we need a way to adapt the query plan while it’s running based on the data that we’re seeing while we’re going along.</p>
<p>500<br>01:15:00,000 –&gt; 01:15:09,000<br>And this is what adaptive query processing allows us to do. We’re allowed the execution and to make decisions on its own without maybe consulting with the query optimizer.</p>
<p>501<br>01:15:09,000 –&gt; 01:15:23,000<br>In some cases, you do some cases you don’t to either modify the query plan structure itself, potentially changing join ordering, moving things, projections up and down, or changing the expression tree while the queries running based on the data we’re seeing.</p>
<p>502<br>01:15:23,000 –&gt; 01:15:32,000<br>So the idea is like similar to in better blocks, they did a little sampling to figure out here’s the data about to encode, then use that to make a decision of what the best compression scheme to use was.</p>
<p>503<br>01:15:32,000 –&gt; 01:15:46,000<br>While the queries running were seeing, you were seeing the data, so we can start making estimates of what the real selectivity of our predicates or whatever we’re doing is on that data and then decide whether how to modify our execution plan.</p>
<p>504<br>01:15:46,000 –&gt; 01:15:58,000<br>In the extreme case, we just say this is all garbage, these estimates are way off, give up, throw away all the results, go back to the query after I’m saying try again, you did it wrong, here’s some new results.</p>
<p>505<br>01:15:58,000 –&gt; 01:16:10,000<br>And it seems crazy, why would I stop a query, throw away everything and go back and try to run the again with the new query plan, well again the worst query plan can be ordered to magnitude difference between the best query plan.</p>
<p>506<br>01:16:10,000 –&gt; 01:16:19,000<br>So it is actually worth it to go do that. Very many few systems do that, most of them just take whatever you’re giving and run it.</p>
<p>507<br>01:16:19,000 –&gt; 01:16:29,000<br>And I think only in academic systems will they be this, just give up.</p>
<p>508<br>01:16:29,000 –&gt; 01:16:38,000<br>So we’ll discuss how to modify query plans later in the semester. Today I just want to focus on what tricks we can do to make the expressions evaluation go faster.</p>
<p>509<br>01:16:38,000 –&gt; 01:16:51,000<br>And so again, all the major O-lap systems that are out there, snowflakes, the data bricks, the dremels, the big queries, I think redshift as well, they’re all going to do bits and pieces of this.</p>
<p>510<br>01:16:51,000 –&gt; 01:17:03,000<br>I don’t think anybody really does this one, throw it away and go back and try again, but they’ll do some of the things that we’ll talk about here. But I’m going to focus on the Velox ones.</p>
<p>511<br>01:17:03,000 –&gt; 01:17:16,000<br>So the first trick Velox is going to use is called predicate reordering. This is an old idea that goes back to the 1990s as the paper that did this in Postgres from a long time ago, actually from Joe Hellerstein who’s now the data professor at Berkeley.</p>
<p>512<br>01:17:16,000 –&gt; 01:17:25,000<br>The basic idea is that if I have say two functions in my wear clause that I need to run, I can decide in what order I actually want to run them.</p>
<p>513<br>01:17:25,000 –&gt; 01:17:34,000<br>And there may be the trade-offs between how long it takes to compute one function versus how long it versus the selectivity of its operation.</p>
<p>514<br>01:17:34,000 –&gt; 01:17:40,000<br>So you decide maybe I want to run the slow function first because that’s going to be able to fill throughout more tuples than the faster function.</p>
<p>515<br>01:17:40,000 –&gt; 01:17:48,000<br>And then as you’re running, you can decide how to change the order as you go along based on what the computational time would be and the selectivity would be.</p>
<p>516<br>01:17:48,000 –&gt; 01:17:59,000<br>Common prefetching, the idea here is that if within my operator itself, I need say two columns.</p>
<p>517<br>01:17:59,000 –&gt; 01:18:05,000<br>But so I can start scanning one column that maybe do the first half of my expression tree and rip through that.</p>
<p>518<br>01:18:05,000 –&gt; 01:18:11,000<br>But then I make an asynchronous IOPE fetch call to the storage service to go fetch the second column.</p>
<p>519<br>01:18:11,000 –&gt; 01:18:20,000<br>So I can start ripping through the first column. Then in the background, it’s fetching the second column. By the time I finish the first column, then I can then start scanning the second one.</p>
<p>520<br>01:18:20,000 –&gt; 01:18:34,000<br>We talked about how to prefetching before in the intro class. But that was prefetching individual pages. This is within for integer columns within my, you know, within a single operator itself.</p>
<p>521<br>01:18:34,000 –&gt; 01:18:45,000<br>The not null fast path, this technique basically says if I recognize that the null bitmap or the thing passed up from is my input to my operator.</p>
<p>522<br>01:18:45,000 –&gt; 01:18:55,000<br>If I do maybe a quick pop count and identify that there’s zero nulls in my column, then I don’t need to do null checks.</p>
<p>523<br>01:18:55,000 –&gt; 01:19:03,000<br>And I just allied that process entirely and I have a faster version of whatever the operator that I want to run or expression operator and I want to run.</p>
<p>524<br>01:19:03,000 –&gt; 01:19:10,000<br>I’m going to show the post-crisis numeric code. There’s a bunch of null checks in there if null then do this, if not null do that.</p>
<p>525<br>01:19:10,000 –&gt; 01:19:19,000<br>That’s all going to be in direction. That’s all going to be calls branch misprediction. So if I know there’s no nulls, throw that away entirely and run a faster version.</p>
<p>526<br>01:19:19,000 –&gt; 01:19:27,000<br>Similarly, you can also do, you can avoid additional checks for string data if you know everything’s asking.</p>
<p>527<br>01:19:28,000 –&gt; 01:19:43,000<br>So again, ASCII is the original coding scheme for how we represent strings. But obviously that’s been extended to UTF-8 to support different international languages and larger character schemes like poop emojis and things like that.</p>
<p>528<br>01:19:43,000 –&gt; 01:19:53,000<br>But if you know all your data within a column, it’s going to be ASCII. You don’t have to do the more expensive UTH, UTF-8 check.</p>
<p>529<br>01:19:54,000 –&gt; 01:20:09,000<br>So in the VALX paper, they just show that if you compare the cost of running different string functions on the UTF-8 version versus the ASCII version, it’s ordered by two different between some of these functions.</p>
<p>530<br>01:20:09,000 –&gt; 01:20:22,000<br>This is a really simple check. So the idea is that you always run with the UTF-8 one first. Then if you recognize that you’re scanning, hey, this is all ASCII, then you just run the ASCII version.</p>
<p>531<br>01:20:22,000 –&gt; 01:20:25,000<br>And if you get it wrong, or a board and roll back, yes.</p>
<p>532<br>01:20:25,000 –&gt; 01:20:33,000<br>So other than the predicate, we all think all of three of these are something that you would do while you’re running the query, but something that you would know about the query itself, right?</p>
<p>533<br>01:20:33,000 –&gt; 01:20:36,000<br>There are no knowledge in this one or that you’ve been going to ask.</p>
<p>534<br>01:20:36,000 –&gt; 01:20:38,000<br>No, no, no, you don’t know anything.</p>
<p>535<br>01:20:38,000 –&gt; 01:20:44,000<br>So while I running it, I’m just asking ASCII characters by assuming that only ASCII is that how good.</p>
<p>536<br>01:20:45,000 –&gt; 01:20:49,000<br>For this one here, I’m running, so I’m giving, hey, here’s your string column.</p>
<p>537<br>01:20:49,000 –&gt; 01:20:54,000<br>And I don’t think Park A or tell you whether it’s UTF-8 or ASCII.</p>
<p>538<br>01:20:54,000 –&gt; 01:20:59,000<br>See, it’s not even Park A file. You’re parsing a log file or CSV, right?</p>
<p>539<br>01:20:59,000 –&gt; 01:21:07,000<br>So if you recognize that, hey, this is all ASCII, I can run the fast-for-asky version, but I need to see some of it a little bit first to make that decision.</p>
<p>540<br>01:21:07,000 –&gt; 01:21:09,000<br>That’s the basic idea.</p>
<p>541<br>01:21:10,000 –&gt; 01:21:11,000<br>Yeah.</p>
<p>542<br>01:21:11,000 –&gt; 01:21:14,000<br>So I think they have a fallback mechanism for this. I think if you’re wrong.</p>
<p>543<br>01:21:18,000 –&gt; 01:21:20,000<br>I think for ASCII, uh…</p>
<p>544<br>01:21:22,000 –&gt; 01:21:27,000<br>So if you see something with it for the one-day set, yeah, yes, yes, yes, yes.</p>
<p>545<br>01:21:27,000 –&gt; 01:21:29,000<br>If it’s one, then for them, like…</p>
<p>546<br>01:21:29,000 –&gt; 01:21:32,000<br>ASCII says to be backwards compatible with UTF-8.</p>
<p>547<br>01:21:32,000 –&gt; 01:21:34,000<br>But with the NUS still, you’ve checked that out.</p>
<p>548<br>01:21:34,000 –&gt; 01:21:35,000<br>But what?</p>
<p>549<br>01:21:35,000 –&gt; 01:21:38,000<br>I think if you have another, you’re going to know another part of the file.</p>
<p>550<br>01:21:38,000 –&gt; 01:21:41,000<br>Yeah, but like, you have the null vector ahead of time.</p>
<p>551<br>01:21:41,000 –&gt; 01:21:42,000<br>Right?</p>
<p>552<br>01:21:42,000 –&gt; 01:21:47,000<br>So you just do pop count as any bit set to one, if no, run the null.</p>
<p>553<br>01:21:47,000 –&gt; 01:21:50,000<br>We’re over time here, sorry.</p>
<p>554<br>01:21:50,000 –&gt; 01:21:55,000<br>One last trick I want to show is they also have the ability to, which I like,</p>
<p>555<br>01:21:55,000 –&gt; 01:21:57,000<br>is if you recognize that the…</p>
<p>556<br>01:21:57,000 –&gt; 01:22:01,000<br>Where operation you’re trying to do in the expression tree on a string data,</p>
<p>557<br>01:22:01,000 –&gt; 01:22:04,000<br>is can just overwrite whatever data you’re given.</p>
<p>558<br>01:22:04,000 –&gt; 01:22:08,000<br>Again, think of like, I have my original data files I’ve learned from Parquet to Arrow.</p>
<p>559<br>01:22:08,000 –&gt; 01:22:10,000<br>Now I’m just passing them on memory chunks from one over to the next.</p>
<p>560<br>01:22:10,000 –&gt; 01:22:16,000<br>If I recognize that, oh, I can just overwrite the memory of this column,</p>
<p>561<br>01:22:16,000 –&gt; 01:22:20,000<br>of this data, with whatever the operation I want to do on it,</p>
<p>562<br>01:22:20,000 –&gt; 01:22:23,000<br>I don’t have to… I don’t call them malic or allocate memory to put new output in.</p>
<p>563<br>01:22:23,000 –&gt; 01:22:27,000<br>So again, if you’re running that upper function, all that’s doing is taking the characters</p>
<p>564<br>01:22:27,000 –&gt; 01:22:29,000<br>to give an uppercasing them.</p>
<p>565<br>01:22:29,000 –&gt; 01:22:33,000<br>You can write that back over the original value and reuse the same buffer</p>
<p>566<br>01:22:33,000 –&gt; 01:22:36,000<br>that you then sent up to the next operate.</p>
<p>567<br>01:22:36,000 –&gt; 01:22:43,000<br>So they show like if you do that trick, you shave off another 25%,</p>
<p>568<br>01:22:43,000 –&gt; 01:22:48,000<br>nearly 25% to 40%, on your runtime, by even reusing the buffers.</p>
<p>569<br>01:22:48,000 –&gt; 01:22:51,000<br>So this is the baseline using the UTF version, there’s the ASCII version,</p>
<p>570<br>01:22:51,000 –&gt; 01:22:55,000<br>and then you get even more using…</p>
<p>571<br>01:22:55,000 –&gt; 01:22:57,000<br>using reusing buffers.</p>
<p>572<br>01:22:57,000 –&gt; 01:22:59,000<br>That’s a nice little trick I like that.</p>
<p>573<br>01:22:59,000 –&gt; 01:23:02,000<br>Just to finish up, today’s lecture was basically a quick overview.</p>
<p>574<br>01:23:02,000 –&gt; 01:23:06,000<br>Here’s a bunch of stuff of how to design your execution engine,</p>
<p>575<br>01:23:06,000 –&gt; 01:23:10,000<br>and I’m trying to show you… Here’s the broad categories of things you have to think about.</p>
<p>576<br>01:23:10,000 –&gt; 01:23:13,000<br>What kind of payloads are you going to sport,</p>
<p>577<br>01:23:13,000 –&gt; 01:23:15,000<br>how are you going to move data from one over to the next,</p>
<p>578<br>01:23:15,000 –&gt; 01:23:17,000<br>what that data is actually going to look like,</p>
<p>579<br>01:23:17,000 –&gt; 01:23:20,000<br>how you got to evaluate expressions, and what tricks you can do to optimize them.</p>
<p>580<br>01:23:20,000 –&gt; 01:23:24,000<br>So I’m not saying that everything I shoot today is the complete menu</p>
<p>581<br>01:23:24,000 –&gt; 01:23:26,000<br>that we’re going to be able to do, but now you need to understand.</p>
<p>582<br>01:23:26,000 –&gt; 01:23:28,000<br>Here’s the things that I think about when you build one of these engines.</p>
<p>583<br>01:23:28,000 –&gt; 01:23:34,000<br>And again, my opinion that arrow is going to be the best choice for internal representation,</p>
<p>584<br>01:23:34,000 –&gt; 01:23:37,000<br>is it perfect? No, are there improvements to it? Yes.</p>
<p>585<br>01:23:37,000 –&gt; 01:23:42,000<br>But it is actually involving, like they added the umbra style string storage</p>
<p>586<br>01:23:42,000 –&gt; 01:23:45,000<br>they added that last year, right? So it’s not stagnant.</p>
<p>587<br>01:23:45,000 –&gt; 01:23:49,000<br>Okay, so next class on Monday, we’re going to talk now</p>
<p>588<br>01:23:49,000 –&gt; 01:23:54,000<br>how do you actually run the operators themselves in a vectorized manner using SIMD?</p>
<p>589<br>01:23:54,000 –&gt; 01:23:56,000<br>So the paper I signed to you is from the Germans.</p>
<p>590<br>01:23:56,000 –&gt; 01:24:04,000<br>It’s basically a deep dive into how to use ADX512’s features to optimize and vectorize,</p>
<p>591<br>01:24:04,000 –&gt; 01:24:07,000<br>you know, relational operators. Okay?</p>
<p>592<br>01:24:07,000 –&gt; 01:24:10,000<br>All right guys, have a good weekend, enjoy the unusually warm weather.</p>
<p>593<br>01:24:11,000 –&gt; 01:24:14,000<br>You know, you’ve got a belt to get the 40-ounce bar.</p>
<p>594<br>01:24:14,000 –&gt; 01:24:17,000<br>Get a grip, take a sip, and you’ll be picking up bottles.</p>
<p>595<br>01:24:17,000 –&gt; 01:24:21,000<br>Ain’t ain’t no puzzle, I’m just a cousin, I’m more a man, I’m telling the 40-ounce, I’m a 40-ounce guy,</p>
<p>596<br>01:24:21,000 –&gt; 01:24:24,000<br>four cans, stack and six packs on a table.</p>
<p>597<br>01:24:24,000 –&gt; 01:24:28,000<br>And I’m able to see St. Izz on a label, no short, put the cloth, you know what got them.</p>
<p>598<br>01:24:28,000 –&gt; 01:24:33,000<br>I take off the cap, my first attack on a bottle, throw my green and three, so I can kill it.</p>
<p>599<br>01:24:33,000 –&gt; 01:24:37,000<br>Careful with the bottle, baby, we’ll just throw a pill in, cause St. Izz and say the pain I’ve wet,</p>
<p>600<br>01:24:37,000 –&gt; 01:24:41,000<br>you drink it down with the gauze, little box head, take back the pack of drugs.</p>
<p>601<br>01:24:41,000 –&gt; 01:24:44,000<br>It won’t get you so safe now, so drink it till it’s flush.</p>
<p>602<br>01:24:44,000 –&gt; 01:24:47,000<br>Billie Danes, it’s only tasty, down with the weak glass.</p>
<p>603<br>01:24:47,000 –&gt; 01:24:49,000<br>Be a man of gith, I can’t say hard.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CMU15721 P6S202405 QueryExecutionProcessingPart2CMUAdvancedDatabaseSystems</div>
      <div>http://example.com/2025/10/25/CMU15721 P6S202405-QueryExecutionProcessingPart2CMUAdvancedDatabaseSystems/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月25日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/25/CMU15721%20P5S202404-QueryExecutionProcessingPart1CMUAdvancedDatabaseSystems/" title="CMU15721 P5S202404 QueryExecutionProcessingPart1CMUAdvancedDatabaseSystems">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CMU15721 P5S202404 QueryExecutionProcessingPart1CMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/25/CMU15721%20P4S202403-DataFormatsEncodingPart2CMUAdvancedDatabaseSystems/" title="CMU15721 P4S202403 DataFormatsEncodingPart2CMUAdvancedDatabaseSystems">
                        <span class="hidden-mobile">CMU15721 P4S202403 DataFormatsEncodingPart2CMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
