

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="100:00:00,000 –&gt; 00:00:06,000Canneke Mellon University’s advanced database systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio audience. 300:00:14,000 –&gt; 00:00:16,00">
<meta property="og:type" content="article">
<meta property="og:title" content="CMU15721 P9S202408 QuerySchedulingCoordinationCMUAdvancedDatabaseSystems">
<meta property="og:url" content="http://example.com/2025/10/25/CMU15721%20P9S202408-QuerySchedulingCoordinationCMUAdvancedDatabaseSystems/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="100:00:00,000 –&gt; 00:00:06,000Canneke Mellon University’s advanced database systems courses 200:00:06,000 –&gt; 00:00:09,000filming front of the live studio audience. 300:00:14,000 –&gt; 00:00:16,00">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-25T05:03:39.758Z">
<meta property="article:modified_time" content="2025-10-25T05:03:39.758Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>CMU15721 P9S202408 QuerySchedulingCoordinationCMUAdvancedDatabaseSystems - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CMU15721 P9S202408 QuerySchedulingCoordinationCMUAdvancedDatabaseSystems"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-25 13:03" pubdate>
          2025年10月25日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          9k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          76 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CMU15721 P9S202408 QuerySchedulingCoordinationCMUAdvancedDatabaseSystems</h1>
            
            
              <div class="markdown-body">
                
                <p>1<br>00:00:00,000 –&gt; 00:00:06,000<br>Canneke Mellon University’s advanced database systems courses</p>
<p>2<br>00:00:06,000 –&gt; 00:00:09,000<br>filming front of the live studio audience.</p>
<p>3<br>00:00:14,000 –&gt; 00:00:16,000<br>It’s too hot now.</p>
<p>4<br>00:00:16,000 –&gt; 00:00:19,000<br>We covered Bristol scale, we covered Diarrhea, we covered</p>
<p>5<br>00:00:19,000 –&gt; 00:00:22,000<br>what else, behavioral interviews.</p>
<p>6<br>00:00:22,000 –&gt; 00:00:25,000<br>Okay, let’s talk about databases.</p>
<p>7<br>00:00:25,000 –&gt; 00:00:28,000<br>So today we’re going to talk about now how do we take the query</p>
<p>8<br>00:00:28,000 –&gt; 00:00:32,000<br>plans that we’ve been given and actually start running them on our</p>
<p>9<br>00:00:32,000 –&gt; 00:00:33,000<br>system.</p>
<p>10<br>00:00:33,000 –&gt; 00:00:37,000<br>So recall that the last couple lectures we’ve had were focusing</p>
<p>11<br>00:00:37,000 –&gt; 00:00:42,000<br>on how to actually optimize the or build an optimized</p>
<p>12<br>00:00:42,000 –&gt; 00:00:45,000<br>execution engine so that we can run Scenroscan queries as fast as</p>
<p>13<br>00:00:45,000 –&gt; 00:00:46,000<br>possible.</p>
<p>14<br>00:00:46,000 –&gt; 00:00:49,000<br>And again, the major two camps are going to be the vectorization</p>
<p>15<br>00:00:49,000 –&gt; 00:00:52,000<br>people using SIMD and then the query compilation stuff that we</p>
<p>16<br>00:00:52,000 –&gt; 00:00:53,000<br>talked about last class.</p>
<p>17<br>00:00:53,000 –&gt; 00:00:56,000<br>In the case of query compilation, there was two high level</p>
<p>18<br>00:00:56,000 –&gt; 00:00:57,000<br>approaches.</p>
<p>19<br>00:00:57,000 –&gt; 00:00:59,000<br>There’s translation or source to source compilation.</p>
<p>20<br>00:00:59,000 –&gt; 00:01:03,000<br>That’s like you have your C++ code and MIT C++ code that then gets</p>
<p>21<br>00:01:03,000 –&gt; 00:01:04,000<br>compiled.</p>
<p>22<br>00:01:04,000 –&gt; 00:01:08,000<br>And then the alternative was from the hyper paper that you guys read was</p>
<p>23<br>00:01:08,000 –&gt; 00:01:12,000<br>compilation by generating like a load level IR for the actual</p>
<p>24<br>00:01:12,000 –&gt; 00:01:15,000<br>instructions you want to execute for that query plan and then using</p>
<p>25<br>00:01:15,000 –&gt; 00:01:19,000<br>something like ASMG or LLVM to compile it.</p>
<p>26<br>00:01:19,000 –&gt; 00:01:23,000<br>And again, as I said, the main takeaway that from, you know,</p>
<p>27<br>00:01:23,000 –&gt; 00:01:27,000<br>from the, since the seminal papers and vectorization and</p>
<p>28<br>00:01:27,000 –&gt; 00:01:30,000<br>compilation have come out over the last decade is that most of</p>
<p>29<br>00:01:30,000 –&gt; 00:01:33,000<br>the systems that we’re going to read about in the near the end of</p>
<p>30<br>00:01:33,000 –&gt; 00:01:37,000<br>the semester are going to choose vectorization with SIMD and</p>
<p>31<br>00:01:37,000 –&gt; 00:01:40,000<br>often kind of, it’s combination of auto vectorization and,</p>
<p>32<br>00:01:40,000 –&gt; 00:01:43,000<br>with the Transix, but they’re going to choose that over compilation</p>
<p>33<br>00:01:43,000 –&gt; 00:01:46,000<br>just because the engineering overhead to maintain, to build a</p>
<p>34<br>00:01:46,000 –&gt; 00:01:50,000<br>maintain a jick compiled database engine is super high.</p>
<p>35<br>00:01:51,000 –&gt; 00:01:53,000<br>They will read this in the photon paper from data bricks.</p>
<p>36<br>00:01:53,000 –&gt; 00:01:56,000<br>They explicitly call out it’s better off having a bunch of</p>
<p>37<br>00:01:56,000 –&gt; 00:01:59,000<br>people try to optimize the SIMD stuff because then you can reach</p>
<p>38<br>00:01:59,000 –&gt; 00:02:05,000<br>parity to the compilation implementation versus like if you go</p>
<p>39<br>00:02:05,000 –&gt; 00:02:09,000<br>down the jit path, there’s a small number of people that actually</p>
<p>40<br>00:02:09,000 –&gt; 00:02:10,000<br>can work on it.</p>
<p>41<br>00:02:10,000 –&gt; 00:02:13,000<br>All right.</p>
<p>42<br>00:02:13,000 –&gt; 00:02:16,000<br>So again, today we’re talking about scheduling of how do you take a</p>
<p>43<br>00:02:16,000 –&gt; 00:02:19,000<br>query plan and divide it up amongst the different workers in our</p>
<p>44<br>00:02:20,000 –&gt; 00:02:21,000<br>system.</p>
<p>45<br>00:02:21,000 –&gt; 00:02:23,000<br>And so again, just make sure that we’re using the right terminology.</p>
<p>46<br>00:02:23,000 –&gt; 00:02:26,000<br>We’re going to say that a query plan is going to be a</p>
<p>47<br>00:02:26,000 –&gt; 00:02:29,000<br>DAG of operators and then, you know, relational operators and then the</p>
<p>48<br>00:02:29,000 –&gt; 00:02:32,000<br>operator instance is going to be a new location of that operator on</p>
<p>49<br>00:02:32,000 –&gt; 00:02:36,000<br>some portion of the data that we’re trying to scan so we’re trying to</p>
<p>50<br>00:02:36,000 –&gt; 00:02:37,000<br>read a table.</p>
<p>51<br>00:02:37,000 –&gt; 00:02:40,000<br>You know, if it’s broken up to row groups, we would have an operator</p>
<p>52<br>00:02:40,000 –&gt; 00:02:43,000<br>instance be responsible for scanning a single row group and</p>
<p>53<br>00:02:43,000 –&gt; 00:02:45,000<br>processing that.</p>
<p>54<br>00:02:45,000 –&gt; 00:02:48,000<br>And then a task is going to be some computational piece of work that’s</p>
<p>55<br>00:02:49,000 –&gt; 00:02:53,000<br>going to contain multiple operator instances typically in the same</p>
<p>56<br>00:02:53,000 –&gt; 00:02:57,000<br>pipeline that we want to then hand out to our workers to execute.</p>
<p>57<br>00:02:57,000 –&gt; 00:03:02,000<br>And then a task set, sometimes called a resource set and some of</p>
<p>58<br>00:03:02,000 –&gt; 00:03:06,000<br>the papers, this is going to be the collection of the tasks that we</p>
<p>59<br>00:03:06,000 –&gt; 00:03:09,000<br>need, or the collection of the tasks that we have for given query that</p>
<p>60<br>00:03:09,000 –&gt; 00:03:11,000<br>we need to execute.</p>
<p>61<br>00:03:11,000 –&gt; 00:03:14,000<br>And the idea is that we know where the pipeline breakers are because</p>
<p>62<br>00:03:14,000 –&gt; 00:03:17,000<br>where the data system, where the one building the query plan.</p>
<p>63<br>00:03:17,000 –&gt; 00:03:22,000<br>So the idea is to convert these pipelines into individual tasks that we can</p>
<p>64<br>00:03:22,000 –&gt; 00:03:23,000<br>then farm out and execute.</p>
<p>65<br>00:03:23,000 –&gt; 00:03:27,000<br>And so today’s class is really discussing figuring out how do we</p>
<p>66<br>00:03:27,000 –&gt; 00:03:31,000<br>assign these tasks to workers in our system.</p>
<p>67<br>00:03:31,000 –&gt; 00:03:34,000<br>I’m loosely defined, you know, reason to work term worker generically,</p>
<p>68<br>00:03:34,000 –&gt; 00:03:38,000<br>but you can think of it almost as either a core or thread or process.</p>
<p>69<br>00:03:38,000 –&gt; 00:03:41,000<br>It doesn’t matter, or a node.</p>
<p>70<br>00:03:41,000 –&gt; 00:03:45,000<br>And then keeping track of where the data they need access is coming</p>
<p>71<br>00:03:45,000 –&gt; 00:03:51,000<br>from, and where is any intermediate results at the generating, where is that going to go?</p>
<p>72<br>00:03:51,000 –&gt; 00:03:53,000<br>So this is basically what I said.</p>
<p>73<br>00:03:53,000 –&gt; 00:03:57,000<br>The idea of the schedule in our system is that we want to know, for</p>
<p>74<br>00:03:57,000 –&gt; 00:04:00,000<br>giving query, how many tasks should we use?</p>
<p>75<br>00:04:00,000 –&gt; 00:04:03,000<br>Because if you want to take advantage of all the parallel</p>
<p>76<br>00:04:03,000 –&gt; 00:04:06,000<br>cores that we have available to us, also the parallel operations within</p>
<p>77<br>00:04:06,000 –&gt; 00:04:11,000<br>SIMD, or that’s usually below then what we’re actually going to schedule for.</p>
<p>78<br>00:04:11,000 –&gt; 00:04:14,000<br>But keeping track of how many tasks we want to use, how many CPU cores we want</p>
<p>79<br>00:04:14,000 –&gt; 00:04:18,000<br>to farm them out on.</p>
<p>80<br>00:04:18,000 –&gt; 00:04:24,000<br>And then when a task generates some kind of output that needs to go to the next</p>
<p>81<br>00:04:24,000 –&gt; 00:04:27,000<br>task, where should we actually store that?</p>
<p>82<br>00:04:27,000 –&gt; 00:04:30,000<br>Because in some cases, if it’s stored at local loss, it may be the test that’s going to</p>
<p>83<br>00:04:30,000 –&gt; 00:04:32,000<br>read it, it may be remote.</p>
<p>84<br>00:04:32,000 –&gt; 00:04:35,000<br>So it might be better to push the data to where the next task is going to need it.</p>
<p>85<br>00:04:35,000 –&gt; 00:04:39,000<br>But you might not know when that task is going to be.</p>
<p>86<br>00:04:39,000 –&gt; 00:04:41,000<br>So we’ll see this as we go along.</p>
<p>87<br>00:04:41,000 –&gt; 00:04:47,000<br>But the paper I had you guys read from Hyper, it’s about single node execution.</p>
<p>88<br>00:04:47,000 –&gt; 00:04:50,000<br>And we’ll see how we tie this all later at the end of the class.</p>
<p>89<br>00:04:50,000 –&gt; 00:04:54,000<br>But the three implementation of a schedule we’re going to look at, they’re all</p>
<p>90<br>00:04:54,000 –&gt; 00:04:57,000<br>also going to be all single node systems.</p>
<p>91<br>00:04:57,000 –&gt; 00:05:01,000<br>And so the reason why I’m focusing on this rather than the distributed system,</p>
<p>92<br>00:05:01,000 –&gt; 00:05:03,000<br>because they think the problem is the same.</p>
<p>93<br>00:05:03,000 –&gt; 00:05:08,000<br>It doesn’t matter whether it’s a single node or multi threaded or multiple nodes that</p>
<p>94<br>00:05:08,000 –&gt; 00:05:09,000<br>are each single thread.</p>
<p>95<br>00:05:09,000 –&gt; 00:05:14,000<br>It really is, the high little problem trying to solve is what task we want to run where,</p>
<p>96<br>00:05:14,000 –&gt; 00:05:17,000<br>and where should the output results go.</p>
<p>97<br>00:05:17,000 –&gt; 00:05:22,000<br>And the main takeaway is going to be that we’re always going to want to do this ourselves,</p>
<p>98<br>00:05:22,000 –&gt; 00:05:26,000<br>especially on a single node and not the operating system.</p>
<p>99<br>00:05:26,000 –&gt; 00:05:30,000<br>I think in the Hyper, paper, guys read, I think the distributed between Postgres,</p>
<p>100<br>00:05:30,000 –&gt; 00:05:33,000<br>Postgres is just letting the OS do all the scheduling, because it’s just, you know,</p>
<p>101<br>00:05:33,000 –&gt; 00:05:36,000<br>for full processes.</p>
<p>102<br>00:05:36,000 –&gt; 00:05:41,000<br>And I don’t think they even play games like process priority and so forth.</p>
<p>103<br>00:05:41,000 –&gt; 00:05:45,000<br>But instead, exception of Postgres, every data system is going to want to do all the</p>
<p>104<br>00:05:45,000 –&gt; 00:05:47,000<br>scheduling stuff itself.</p>
<p>105<br>00:05:47,000 –&gt; 00:05:51,000<br>So we can talk about how to do the single node, and then you’ll see how that maps to a</p>
<p>106<br>00:05:51,000 –&gt; 00:05:53,000<br>distributed environment.</p>
<p>107<br>00:05:53,000 –&gt; 00:05:55,000<br>Although there’ll be some things we can do in a distributed environment that we’re not going to</p>
<p>108<br>00:05:55,000 –&gt; 00:05:57,000<br>cover today, but we’ll see this later on.</p>
<p>109<br>00:05:57,000 –&gt; 00:06:02,000<br>Like in BigQuery Dremel, they’re going to do a shuffle stage after every pipeline breaker.</p>
<p>110<br>00:06:02,000 –&gt; 00:06:07,000<br>And that allows them to reorganize and recalibrate the workers later on.</p>
<p>111<br>00:06:07,000 –&gt; 00:06:10,000<br>But we’ll cover that later.</p>
<p>112<br>00:06:10,000 –&gt; 00:06:14,000<br>All right, so what are our goals for building a high performance schedule for a data system?</p>
<p>113<br>00:06:14,000 –&gt; 00:06:16,000<br>So obviously we want to maximize throughput.</p>
<p>114<br>00:06:16,000 –&gt; 00:06:21,000<br>We want to be able to process as many queries as possible in our system,</p>
<p>115<br>00:06:21,000 –&gt; 00:06:24,000<br>you know, just sort of keep the thing always running and always consuming</p>
<p>116<br>00:06:24,000 –&gt; 00:06:27,000<br>of results and producing a pretty thing output.</p>
<p>117<br>00:06:27,000 –&gt; 00:06:31,000<br>We’re going to maintain some notion of fairness.</p>
<p>118<br>00:06:31,000 –&gt; 00:06:35,000<br>And again, this is subjective of what fairness means to sort of one query to another query.</p>
<p>119<br>00:06:35,000 –&gt; 00:06:40,000<br>But at a high level is that at the end of the day, we need to make sure that no query gets</p>
<p>120<br>00:06:40,000 –&gt; 00:06:41,000<br>started for resources.</p>
<p>121<br>00:06:41,000 –&gt; 00:06:44,000<br>So even though we may delay the priority, and we’ll talk about what that means,</p>
<p>122<br>00:06:44,000 –&gt; 00:06:48,000<br>why we want to do that as we go along, but we want to, even if you get a lower priority,</p>
<p>123<br>00:06:48,000 –&gt; 00:06:55,000<br>or you’re a long running query, the end of the day, we still want you to complete.</p>
<p>124<br>00:06:55,000 –&gt; 00:07:00,000<br>And then the flip side of this is that we want to make sure that the system seems to be responsive.</p>
<p>125<br>00:07:00,000 –&gt; 00:07:06,000<br>That’s reducing the tail latency, like the P999 latency of queries if we can.</p>
<p>126<br>00:07:06,000 –&gt; 00:07:10,000<br>But this will matter a lot for short queries.</p>
<p>127<br>00:07:10,000 –&gt; 00:07:14,000<br>And so the idea here is that we want our short queries to complete as fast as possible,</p>
<p>128<br>00:07:14,000 –&gt; 00:07:17,000<br>because that’s something someone’s going to notice.</p>
<p>129<br>00:07:17,000 –&gt; 00:07:21,000<br>At the shortest scales of a query execution, like if your query goes from, you know,</p>
<p>130<br>00:07:21,000 –&gt; 00:07:26,000<br>100 milliseconds to a thousand milliseconds, then you would notice that.</p>
<p>131<br>00:07:26,000 –&gt; 00:07:29,000<br>So you want to get these short queries out as quickly as possible.</p>
<p>132<br>00:07:29,000 –&gt; 00:07:34,000<br>But if you’re like your query is running for 10 minutes, and it takes 20 seconds longer,</p>
<p>133<br>00:07:34,000 –&gt; 00:07:36,000<br>no one’s going to notice that.</p>
<p>134<br>00:07:36,000 –&gt; 00:07:40,000<br>So the system’s going to appear by responsive if you get the shorter queries out more quickly.</p>
<p>135<br>00:07:40,000 –&gt; 00:07:48,000<br>And the case of the morsel stuff, they don’t have a specific way to actually handle that.</p>
<p>136<br>00:07:48,000 –&gt; 00:07:50,000<br>They’re sort of treating everyone rough.</p>
<p>137<br>00:07:50,000 –&gt; 00:07:56,000<br>They had a few other things they were suggesting that might be in the subsequent paper.</p>
<p>138<br>00:07:56,000 –&gt; 00:07:58,000<br>In the umbra one.</p>
<p>139<br>00:07:58,000 –&gt; 00:08:00,000<br>It wasn’t the hyperbale, the one that we did.</p>
<p>140<br>00:08:00,000 –&gt; 00:08:05,000<br>There was something in the conclusion that they would like to do it.</p>
<p>141<br>00:08:05,000 –&gt; 00:08:07,000<br>I don’t know why they didn’t hyper.</p>
<p>142<br>00:08:07,000 –&gt; 00:08:08,000<br>They do it in umbra.</p>
<p>143<br>00:08:08,000 –&gt; 00:08:13,000<br>We’ll cover that in a second.</p>
<p>144<br>00:08:13,000 –&gt; 00:08:19,000<br>Yeah, so I mean, but the morsel’s one is again, that sets the foundation for this idea of how to divide up the work.</p>
<p>145<br>00:08:19,000 –&gt; 00:08:24,000<br>And they’re going to do a static assignment of tasks to morsels, and we’ll see in umbra how they can break that.</p>
<p>146<br>00:08:24,000 –&gt; 00:08:27,000<br>The umbra one is more sophisticated.</p>
<p>147<br>00:08:27,000 –&gt; 00:08:31,000<br>And the last one, of course, is that we want our scheduler to have low overhead.</p>
<p>148<br>00:08:31,000 –&gt; 00:08:39,000<br>Like it doesn’t help us ever running this super complex computation to figure out the optimal schedule for all our tasks.</p>
<p>149<br>00:08:39,000 –&gt; 00:08:43,000<br>That takes 20 minutes, and a query can finish up in a few milliseconds.</p>
<p>150<br>00:08:43,000 –&gt; 00:08:53,000<br>So we want to have most of our system spending time, uh, you’re computing queries, because that’s what people, you know, end of the day really care about.</p>
<p>151<br>00:08:53,000 –&gt; 00:08:56,000<br>So I’m going to just talk about some quick background in the beginning.</p>
<p>152<br>00:08:56,000 –&gt; 00:09:02,000<br>Again, this will be a, uh, quick reminder of the things we talked about in the intro class.</p>
<p>153<br>00:09:02,000 –&gt; 00:09:04,000<br>But first I’m about like, what is actually a worker?</p>
<p>154<br>00:09:04,000 –&gt; 00:09:09,000<br>Like how are we defining the scope of a competition, you know, where is it actually located in our system?</p>
<p>155<br>00:09:09,000 –&gt; 00:09:13,000<br>What briefly talk about what data placement actually means in the context of partitioning?</p>
<p>156<br>00:09:13,000 –&gt; 00:09:18,000<br>And that basically just, if I’ve already divided the data up in some way, where should I put that data?</p>
<p>157<br>00:09:18,000 –&gt; 00:09:22,000<br>And the two are linked together, but we’ll discuss that.</p>
<p>158<br>00:09:22,000 –&gt; 00:09:30,000<br>And then that’s the one of the things that the morsels paper spent a lot of time talking about was this notion between, uh, you know, local memory and remote memory in a new architecture.</p>
<p>159<br>00:09:30,000 –&gt; 00:09:35,000<br>And they were trying to schedule things so that you’re always processing stuff that was local to you.</p>
<p>160<br>00:09:35,000 –&gt; 00:09:47,000<br>Again, the same idea applies in an distributed system, ideally, we want our, our workers, we want to know processing data that’s local to it, rather than having to go over the network, you know, to some far stories.</p>
<p>161<br>00:09:47,000 –&gt; 00:09:50,000<br>And then we’ll talk about three implications of schedulers.</p>
<p>162<br>00:09:50,000 –&gt; 00:09:53,000<br>We’ll talk about the morsels and hyper, we’ll talk about the follow up and umbra.</p>
<p>163<br>00:09:53,000 –&gt; 00:09:56,000<br>And then we’ll talk about an alternative from the SAP HANA guys.</p>
<p>164<br>00:09:56,000 –&gt; 00:10:02,000<br>Um, and then we’ll finish off again just putting in the context of an distribute architecture.</p>
<p>165<br>00:10:03,000 –&gt; 00:10:04,000<br>Okay?</p>
<p>166<br>00:10:05,000 –&gt; 00:10:13,000<br>And so the, the, what will be interesting about the, the, we’re talking about schedule implementations, we’ll see an umbra and hyper.</p>
<p>167<br>00:10:13,000 –&gt; 00:10:30,000<br>These are, these are going to have like, sort of dedicated worker pools that are just like, uh, crunching through all the tasks as they can, whereas, uh, in the HANA one, they’re trying to be a bit more sophisticated and have this notion of some worker threads can be asleep, some worker threads can be parked.</p>
<p>168<br>00:10:30,000 –&gt; 00:10:37,000<br>Uh, and we’ll also see this trade up between work stealing and not work stealing, which is another dynamic we have to consider as well.</p>
<p>169<br>00:10:37,000 –&gt; 00:10:46,000<br>Alright, so, and this is just a reminder of, of from the interclass that there’s a, there’s this notion of a process model and any data is minute system.</p>
<p>170<br>00:10:46,000 –&gt; 00:10:54,000<br>And this specifies what the, uh, what a worker actually is going to be in our, in our system.</p>
<p>171<br>00:10:54,000 –&gt; 00:11:06,000<br>Right? So the, the sort of earlier day, early database systems in the 1980s, early 1990s, these were a process based system, meaning like every worker was a separate OS process.</p>
<p>172<br>00:11:06,000 –&gt; 00:11:11,000<br>Because back then they didn’t have, uh, p threads like we have now that, you know, they weren’t really portable.</p>
<p>173<br>00:11:11,000 –&gt; 00:11:22,000<br>So if you wanted to support one unit versus another unit, you had basically hit, you know, the positive CPI specified how to do fork to spawn processes, then maybe not, uh, didn’t have threading.</p>
<p>174<br>00:11:22,000 –&gt; 00:11:28,000<br>Every modern system now today is going to be multi threaded. So we’ll assume in our system we’re conceptually building that we multi threaded.</p>
<p>175<br>00:11:28,000 –&gt; 00:11:36,000<br>The only ones that are not multi threaded are ones that fork postgres, because postgres is a, uses a process per worker.</p>
<p>176<br>00:11:36,000 –&gt; 00:11:48,000<br>And the worker is going to be this generic term that means it’s just the computational resource that can be assigned a task, uh, to execute, you know, for some query or something for the internal data, something for the database system.</p>
<p>177<br>00:11:48,000 –&gt; 00:11:54,000<br>And that it can take some data in crunch on it, uh, in our operative instances and then produce output.</p>
<p>178<br>00:11:54,000 –&gt; 00:12:02,000<br>Now, as I said, for, for our purposes going forward, consume every system, unless they fork postgres is going to be multi threaded.</p>
<p>179<br>00:12:02,000 –&gt; 00:12:12,000<br>For some reason in, uh, in the early days when I first started CMU, we took postgres and we decided to, uh, make it multi threaded instead of multi process.</p>
<p>180<br>00:12:12,000 –&gt; 00:12:26,000<br>Um, I forget why we did that. Uh, well, the interesting about it is, if you’re looking at postgres code, there’s a bunch of pound of fines for the different CPU for different OSs, they support, like Linux and Windows and Hpux and BSD.</p>
<p>181<br>00:12:26,000 –&gt; 00:12:35,000<br>And we end up going and using like the win 32, uh, code. And that was, that was the least starting point for us to then convert everything to P threads.</p>
<p>182<br>00:12:35,000 –&gt; 00:12:44,000<br>And we’re not converted to C++ 11, which I don’t know why we did that one either. Um, yeah, we did it. We shouldn’t have done it, but whatever. Yes.</p>
<p>183<br>00:12:44,000 –&gt; 00:12:47,000<br>You said postgres is single credit, but I thought you also said that there’s processes and then there’s a process.</p>
<p>184<br>00:12:47,000 –&gt; 00:12:56,000<br>Postgres, it’s a process per worker. So the, the worker is going to be a whole entire OS process.</p>
<p>185<br>00:12:56,000 –&gt; 00:13:12,000<br>So we can do some parallel execution of queries, but that’s going to cross multiple processes. And they use shared memory to communicate, but no, but like you wouldn’t build a system like that today. Yes.</p>
<p>186<br>00:13:12,000 –&gt; 00:13:21,000<br>So the question is, if it’s all, fall moderating systems are multi threaded, why, why was it a bad idea for us to try to do that in postgres?</p>
<p>187<br>00:13:21,000 –&gt; 00:13:30,000<br>Looking back on it now, I fail to see what the, what the research, uh, research contribution would have been. Right. We.</p>
<p>188<br>00:13:30,000 –&gt; 00:13:45,000<br>Well, we had this execution that was written in C++ that was faster than postgres, which is not hard, always hard to do. I mean, I said not that hard to do. And then rather than sliding it in as an extension using extension hooks that postgres supports, which is a times go does and slightest does.</p>
<p>189<br>00:13:45,000 –&gt; 00:13:53,000<br>C fouls another one. There’s a bunch of these data systems that use extension hooks to get over that bench inside of postgres. We decided to like fork everything.</p>
<p>190<br>00:13:53,000 –&gt; 00:14:03,000<br>And then we the top half of postgres, we have the top half kept the top half, rewrote the bottom half inside of the end and just scrap the whole top and rewrite everything because everything was sort of slow for what we wanted to do.</p>
<p>191<br>00:14:03,000 –&gt; 00:14:10,000<br>But again, I had to do a lot of again. I would have just use extensions. Yes.</p>
<p>192<br>00:14:10,000 –&gt; 00:14:19,000<br>So this process model is just like multiple.</p>
<p>193<br>00:14:19,000 –&gt; 00:14:28,000<br>It’s question is the process model determine the, the new into policy stuff, or is it just like, is it a thread per worker? Is it a process for? It’s just a process for worker.</p>
<p>194<br>00:14:28,000 –&gt; 00:14:34,000<br>It’s just like, like, is it a, what is the, is it a thread? Is it a process is the process pool?</p>
<p>195<br>00:14:34,000 –&gt; 00:14:49,000<br>The new most stuff is is, is it’s low? Okay. So the other thing we can account for is how do you want to sign the workers to CPU cores?</p>
<p>196<br>00:14:49,000 –&gt; 00:15:00,000<br>And the, the basic two approaches are you could have a single dedicated thread or single dedicated worker be the, the only thing that can run on a single core, single CPU core.</p>
<p>197<br>00:15:00,000 –&gt; 00:15:15,000<br>And this prevents like, this prevents contention on that core where like two threads are trying to run at the same time, the workers trying to run at the same core and they’re trashing each other’s, you know, L3 caches and so forth. Yes.</p>
<p>198<br>00:15:15,000 –&gt; 00:15:27,000<br>So one worker is working on one task or one task set or one partition of the task set?</p>
<p>199<br>00:15:27,000 –&gt; 00:15:42,000<br>We’re not there yet. Right. But it’s going to be one task. Yes. And again, and the thing about like at the scan this table, the table has 10 chunks or morsels, you have one worker for each of those, those 10 morsels.</p>
<p>200<br>00:15:43,000 –&gt; 00:16:01,000<br>Right. The other one is going to be, you know, multiple workers on a, on a CPU core. And the idea here is that with when one, one core gets, or thread gets stalled because, a one worker thread gets stalled because the thing it needs is on, is that on disk, maybe a fashion of memory or even, there’s like a low level L3 cache miss.</p>
<p>201<br>00:16:01,000 –&gt; 00:16:23,000<br>Right. You could have other threads run at the same time. For maximum performance, this is probably the right way to go though. Right. And actually, if it’s have, you also want to turn off hyper threading is run, you know, bare metal harbor threads. Because the, because we’re going to be CPU bound and most of the computations we’re going to do in our database system, we don’t want any contention on, on the actual harbor itself.</p>
<p>202<br>00:16:23,000 –&gt; 00:16:37,000<br>Right. There’s other advantages in this for, if you do like transaction processing, where thing you can’t, you don’t want any stools, but for OLAP, for both OLAP and OTP, this could be the better way to go.</p>
<p>203<br>00:16:37,000 –&gt; 00:16:44,000<br>The Honda guys are going to claim this is better because they’re going to have really, they’re going to try to do fine grade control, what threads are actually awake and running at a given time.</p>
<p>204<br>00:16:44,000 –&gt; 00:16:52,000<br>But again, we’ll, we’ll cover that later and they claim that’s me better for a machine with a lot, lot, lot of corpse, I’m sorry, a lot of sockets. Yes.</p>
<p>205<br>00:16:52,000 –&gt; 00:16:55,000<br>You see what you want to spend it? Yes.</p>
<p>206<br>00:16:55,000 –&gt; 00:16:58,000<br>Why would you want the course to be?</p>
<p>207<br>00:16:58,000 –&gt; 00:17:07,000<br>So if, if you’re compute bound and you’re careful about what you’re putting in your CPU cache is and you’re prefetching things ahead of time, the course should never be stalled.</p>
<p>208<br>00:17:08,000 –&gt; 00:17:21,000<br>Again, we saw this with the branchless stuff. Like if you, if you designed the, the system such a way that like, you avoid branch mis predictions and having to flush the pipeline of the CPU, then you should just be crunching through data as fast as possible.</p>
<p>209<br>00:17:21,000 –&gt; 00:17:24,000<br>And you should never have installs for branches. Yes.</p>
<p>210<br>00:17:24,000 –&gt; 00:17:27,000<br>It is for CPU core or for hardware thread.</p>
<p>211<br>00:17:27,000 –&gt; 00:17:34,000<br>For core, but within that you turn off hyper threading so it’s one-hubber thread.</p>
<p>212<br>00:17:36,000 –&gt; 00:17:41,000<br>It’s like a socket can have four cores and each core can have two threads because of hyper threading.</p>
<p>213<br>00:17:41,000 –&gt; 00:17:45,000<br>So you turn off hyper threading and now it’s one core equals one-hubber thread.</p>
<p>214<br>00:17:45,000 –&gt; 00:17:50,000<br>So turn off hyper threading because the boiling contains more value? Yes.</p>
<p>215<br>00:17:50,000 –&gt; 00:17:52,000<br>You’re running basically a bare metal.</p>
<p>216<br>00:17:53,000 –&gt; 00:17:59,000<br>What, wait, why turn off hyper threading on the whole system? Like wouldn’t you be able to then run a little like give a cron job running?</p>
<p>217<br>00:17:59,000 –&gt; 00:18:03,000<br>So you have that run on the, what is that cron job? What is this for?</p>
<p>218<br>00:18:03,000 –&gt; 00:18:06,000<br>I don’t like give a server run. You have a database on a server. Yes.</p>
<p>219<br>00:18:06,000 –&gt; 00:18:21,000<br>It’s dominating the CPU. Yes. Right? You can have that. It can handle all of the, all of the, it can sort of own all of the actual real threads that correspond to real core and maybe then the back-on-past system can run on the unutilized structure for it.</p>
<p>220<br>00:18:21,000 –&gt; 00:18:27,000<br>So these back, like, like, like, garbage collection and stats collection or what, what, what are these background tasks? Like not even prior to the data.</p>
<p>221<br>00:18:27,000 –&gt; 00:18:32,000<br>Oh, random, like random stuff? Why would you run them in your database server?</p>
<p>222<br>00:18:32,000 –&gt; 00:18:36,000<br>Well, I mean, presumably it’s running on an electrical stage. Yes?</p>
<p>223<br>00:18:36,000 –&gt; 00:18:41,000<br>We’re doing something. Yeah. Turn all that off. You don’t want any of that. No, no.</p>
<p>224<br>00:18:41,000 –&gt; 00:18:46,000<br>The core of the question is why not these have the threading on? Is there an advantage to turn it off?</p>
<p>225<br>00:18:46,000 –&gt; 00:18:53,000<br>Yeah. You don’t have, you don’t have two threads contending for the same hardware resource.</p>
<p>226<br>00:18:53,000 –&gt; 00:19:01,000<br>Yeah. I do have a graph.</p>
<p>227<br>00:19:01,000 –&gt; 00:19:12,000<br>My impression was that when you have a type of threading on the other left, the thread is formed between core. It’s going to only use the unutilized part of the core.</p>
<p>228<br>00:19:12,000 –&gt; 00:19:21,000<br>What is the unutilized part of the core? You have a pipeline of instructions.</p>
<p>229<br>00:19:21,000 –&gt; 00:19:32,000<br>So when one gets stalled, the idea is you swap out all the registers for this other logical thread, hyperthread, and then they come in and pick up where you left off.</p>
<p>230<br>00:19:32,000 –&gt; 00:19:39,000<br>But, like, again, if you’re CPU bound, there’s not going to be stalled like that.</p>
<p>231<br>00:19:39,000 –&gt; 00:19:49,000<br>Yes. So we don’t want contention just as some of the threads. We don’t want contention because when we have two threads, if we have a pf-fending, then we end up…</p>
<p>232<br>00:19:49,000 –&gt; 00:19:52,000<br>…is because they find the same hardware, we don’t even have cash in this.</p>
<p>233<br>00:19:52,000 –&gt; 00:19:54,000<br>At the other level, or is…</p>
<p>234<br>00:19:54,000 –&gt; 00:19:56,000<br>Yes. The same thing is…</p>
<p>235<br>00:19:56,000 –&gt; 00:20:05,000<br>If there’s more contention, if there’s more things running on a single core, saying two hyperthreads, then they’re both trying to do something to amount of work,</p>
<p>236<br>00:20:05,000 –&gt; 00:20:15,000<br>and they want things that need bring in the L3 cash, or the brand in your caches, and that’s going to pollute it, and then where has it been better off just letting one thread run to completion?</p>
<p>237<br>00:20:15,000 –&gt; 00:20:22,000<br>Yes. For your desktop, sure, because you’re browsing the web, listening to music, watching videos, there’s a ton of stalls of that. Who cares?</p>
<p>238<br>00:20:22,000 –&gt; 00:20:34,000<br>But for the deabysism, you’re not running random bitcoin mining on it. There’s no cron jobs. If you really care about your deabysism, if it’s a blog, you’re running my SQL Postgres,</p>
<p>239<br>00:20:34,000 –&gt; 00:20:43,000<br>just to service the blog, no, who cares? Sure. But if it’s like a high-end deabysism, like, you’re not letting anybody’s ssc into and run random stuff.</p>
<p>240<br>00:20:43,000 –&gt; 00:21:02,000<br>So this is an older experiment that students did here at CMU, where it’s just sort of toy-in-memory engine, and it’s just damaging between letting the database system decide where it actually plays data in a new architecture.</p>
<p>241<br>00:21:02,000 –&gt; 00:21:18,000<br>Does anyone know what Numa is? Who doesn’t know what Numa is? Perfect. Excellent. Yeah. So the idea is, do you let the deabysism figure out, okay, this piece of data is going to go this court at this location, or this Numa region, or you let the OS figure it out for you.</p>
<p>242<br>00:21:18,000 –&gt; 00:21:32,000<br>And so what you see here is that before you get to the hyper-threading line, you’re going to get better scalable performance, better performance, when the deabysism controls exactly where the data is relocated, because now, as we’ve added data, that’s local to it.</p>
<p>243<br>00:21:32,000 –&gt; 00:21:43,000<br>You don’t have to go over the interconnect, which in some cases can be 2x latency. So that’s why this thing is going to happen. Now, to your point, why not leave hyper-threading on? This is when hyper-threading kicks in.</p>
<p>244<br>00:21:43,000 –&gt; 00:22:03,000<br>And now you see complete flatlines, because for either one, because it’s CPU bound computation. And in the cases when it has to go to memory to go fetch something to fill out the cache, well, if I’m running another hyper-thread, it’s going to do the exact same thing, and now I’m not going to get any benefit.</p>
<p>245<br>00:22:03,000 –&gt; 00:22:12,000<br>So here, I’m throwing more threads at it, but performance is plateaued. Yes?</p>
<p>246<br>00:22:12,000 –&gt; 00:22:30,000<br>Yes? All of these are, right? Like, all of these are fetching from memory. But like, if I’m fetching from memory is so expensive, if I’m fetching from memory, and I’m waiting for that, then you start running, well, what are you going to do?</p>
<p>247<br>00:22:30,000 –&gt; 00:22:42,000<br>What are you going to do? fetch from memories? Is it the exact same thing I am? So, and now you’re blocked on the bandwidth of getting things from the dims to the CPU. Plus you’re polluting your cache.</p>
<p>248<br>00:22:42,000 –&gt; 00:22:59,000<br>Furthermore, you might, I mean, this is, it’s hard to talk to the architecture people, but now you like, I don’t know how well the hardware prefetching is going to be, because now, like, I would have been better off having one thread ripped through a larger chunk of data versus having two threads sort of different spots and try to prefetch those.</p>
<p>249<br>00:22:59,000 –&gt; 00:23:12,000<br>Again, I think the hardware prefetching stuff could probably handle that, but it’s just making things more complicated. Whereas if you keep the system more simple, you know, we can get better utilization of what we have.</p>
<p>250<br>00:23:12,000 –&gt; 00:23:24,000<br>Okay, so, hyper threading, nice, in general, but not for databases. Okay, so we were.</p>
<p>251<br>00:23:24,000 –&gt; 00:23:33,000<br>So, the next thing we got to consider is how we actually are going to get our tasks to our workers. And there’s basically two approaches either push versus the pool.</p>
<p>252<br>00:23:33,000 –&gt; 00:23:47,000<br>And the, in the, the push approach, there’s some kind of centralized dispatch or schedule component that has a global view or a view of what the workers that it’s, that’s that are on its purview or administrative control.</p>
<p>253<br>00:23:47,000 –&gt; 00:23:59,000<br>It knows what tasks they’re doing. And then as, as new tasks arrive, it’s pushing the, those requests, those tasks, requests to the different, different workers, always give them something to do.</p>
<p>254<br>00:23:59,000 –&gt; 00:24:07,000<br>Right? And then when the worker notifies if batch is finished, you know, it’s immediately going to be given here, here’s the next thing to do.</p>
<p>255<br>00:24:07,000 –&gt; 00:24:21,000<br>The pool based approach, which is going to be the better approach, which everyone’s going to do, is going to be that they’re the, the, the sum’s schedule component that’s maintaining the, the queue of all possible tasks that could be executed at any given time.</p>
<p>256<br>00:24:21,000 –&gt; 00:24:35,000<br>With additional metadata of maybe about what data they’re trying to access and where’s that located. And then now the workers went, when they need something to do, they, they come to this, this, this queue and get the next thing to do.</p>
<p>257<br>00:24:35,000 –&gt; 00:24:46,000<br>Right? And this is just easier because it’s, it’s less coordination of like, or maintaining state about, you know, where, where each worker is in, you know, in this computation.</p>
<p>258<br>00:24:46,000 –&gt; 00:24:55,000<br>It just says, hey, here’s much stuff to do. Here’s, you know, it’s all a cart and people can come, come and pull things off the buffet tray when they’re ready for it.</p>
<p>259<br>00:24:55,000 –&gt; 00:24:56,000<br>Yes.</p>
<p>260<br>00:24:56,000 –&gt; 00:24:58,000<br>Which one has a lower overhead?</p>
<p>261<br>00:24:58,000 –&gt; 00:25:01,000<br>Questions? Which one has lower overhead overhead from what?</p>
<p>262<br>00:25:01,000 –&gt; 00:25:11,000<br>We don’t want to share any of the things with you much time. And so, naturally, if you, if you’re a worker who has to go pull from the queue, that probably is, you’re eating up the cycle of them doing that.</p>
<p>263<br>00:25:11,000 –&gt; 00:25:25,000<br>So, yeah, so the statement is, I think there’s actually two parts of it. The statement is, questions, which one has lower overhead? If it’s the pool based approach, then you have every worker thread going to say, what should I be doing next on their own?</p>
<p>264<br>00:25:25,000 –&gt; 00:25:29,000<br>And won’t that incur a penalty for, you know, when they could be running queries?</p>
<p>265<br>00:25:29,000 –&gt; 00:25:41,000<br>Sure. Yes. In some ways, and there’s the second aspect also too, is like, this queue thing, which you’ll see in the hyperpaper, is a global data structure, which now you have to protect with latches or locks.</p>
<p>266<br>00:25:41,000 –&gt; 00:25:44,000<br>And then now everybody could be potentially contended on that.</p>
<p>267<br>00:25:45,000 –&gt; 00:25:57,000<br>So, everyone’s still going to choose this just because it’s, you can build this, this schedule as a separate service, and not worry about exact control, exact knowledge of what every single worker is actually doing.</p>
<p>268<br>00:25:57,000 –&gt; 00:26:06,000<br>Because the worker may die, right? And then now you’ve got to figure out, like, you know, that I told a bunch of, two bunch of stuff ahead of time, and now I can’t do it.</p>
<p>269<br>00:26:06,000 –&gt; 00:26:12,000<br>Where you just say, here’s everything I need to do, and then now each worker thread that then figure out on the run, what’s the best thing for them to do?</p>
<p>270<br>00:26:12,000 –&gt; 00:26:15,000<br>They’re all sort of working globally to solve the problem.</p>
<p>271<br>00:26:15,000 –&gt; 00:26:18,000<br>Also, it’s less intelligent than the pool based approach.</p>
<p>272<br>00:26:18,000 –&gt; 00:26:20,000<br>Yeah.</p>
<p>273<br>00:26:20,000 –&gt; 00:26:22,000<br>You can make the difference.</p>
<p>274<br>00:26:22,000 –&gt; 00:26:26,000<br>He says it’s less intelligent, but like, relative to what? To this?</p>
<p>275<br>00:26:26,000 –&gt; 00:26:29,000<br>To that, because you can have that out even in the first one.</p>
<p>276<br>00:26:29,000 –&gt; 00:26:33,000<br>You can have the priorities in pool one, just because hyper does and doesn’t mean you can’t.</p>
<p>277<br>00:26:34,000 –&gt; 00:26:36,000<br>We’ll see it in a second.</p>
<p>278<br>00:26:36,000 –&gt; 00:26:39,000<br>You mentioned like, the push based approach, like, workers die even in some issues.</p>
<p>279<br>00:26:39,000 –&gt; 00:26:41,000<br>How does it also not make a issue with a pool based approach?</p>
<p>280<br>00:26:41,000 –&gt; 00:26:45,000<br>Yeah, he’s right. If a push based approach has workers die, you’ve got to figure it out.</p>
<p>281<br>00:26:45,000 –&gt; 00:26:49,000<br>Actually, that would be a approach to not approach one.</p>
<p>282<br>00:26:49,000 –&gt; 00:26:51,000<br>You sort of have to deal with it in the pool one.</p>
<p>283<br>00:26:51,000 –&gt; 00:26:58,000<br>Yes, all right. Because you basically need a hard peat to figure out who they didn’t come back.</p>
<p>284<br>00:26:58,000 –&gt; 00:27:08,000<br>Another way to think about it in the pool based approach is that you can have the logic to figure out what task it the worker needs to execute itself.</p>
<p>285<br>00:27:08,000 –&gt; 00:27:13,000<br>Next is basically that logic is being distributed across multiple workers.</p>
<p>286<br>00:27:13,000 –&gt; 00:27:20,000<br>Whereas, if it’s a single centralized service, then it’s one beefy box or whatever it has to then figure this out.</p>
<p>287<br>00:27:20,000 –&gt; 00:27:22,000<br>I’d probably say that’s the main distinction.</p>
<p>288<br>00:27:22,000 –&gt; 00:27:23,000<br>Yes.</p>
<p>289<br>00:27:23,000 –&gt; 00:27:28,000<br>So, you can use this approach for the customer to maybe be talking about how long you have to take.</p>
<p>290<br>00:27:28,000 –&gt; 00:27:33,000<br>The question is, in the pool based approach, is the schedule need to keep track of how long each asset is going to take?</p>
<p>291<br>00:27:33,000 –&gt; 00:27:37,000<br>Yes. Hyper doesn’t do that. We’ll see you in a second.</p>
<p>292<br>00:27:37,000 –&gt; 00:27:41,000<br>Can you use the same cost model as the query optimizer?</p>
<p>293<br>00:27:41,000 –&gt; 00:27:44,000<br>The question is, can you use the same cost model as the query optimizer?</p>
<p>294<br>00:27:44,000 –&gt; 00:28:00,000<br>So, the challenge there is that some cost models in some systems, you can’t map whatever the cost estimate to a wall clock time.</p>
<p>295<br>00:28:00,000 –&gt; 00:28:04,000<br>The error looked at postgres, it’s some number.</p>
<p>296<br>00:28:04,000 –&gt; 00:28:07,000<br>It’s a combination of like, is it?</p>
<p>297<br>00:28:07,000 –&gt; 00:28:12,000<br>I don’t think it’s going to take that number.</p>
<p>298<br>00:28:12,000 –&gt; 00:28:14,000<br>Oh, it’s 10 milliseconds.</p>
<p>299<br>00:28:14,000 –&gt; 00:28:25,000<br>The high end enterprise data system is trying to give you cost estimates to say, here’s the relative cost of the query, which is an internal value that you can use to compare different query plans.</p>
<p>300<br>00:28:25,000 –&gt; 00:28:27,000<br>They also give it spit out.</p>
<p>301<br>00:28:27,000 –&gt; 00:28:29,000<br>I think it’s going to take this amount of time.</p>
<p>302<br>00:28:29,000 –&gt; 00:28:32,000<br>You could do that.</p>
<p>303<br>00:28:32,000 –&gt; 00:28:34,000<br>But again, we’ll see this in a few weeks.</p>
<p>304<br>00:28:34,000 –&gt; 00:28:36,000<br>Cost models are always widely off.</p>
<p>305<br>00:28:36,000 –&gt; 00:28:45,000<br>So, in the umbraith, which I keep, the umbraith scheduling paper, which I keep alluding to, they’re actually going to watch how long it has to taste.</p>
<p>306<br>00:28:45,000 –&gt; 00:28:56,000<br>And then use that to figure out, you know, to get a rough estimate of like what the schedule time should be for certain things.</p>
<p>307<br>00:28:56,000 –&gt; 00:28:58,000<br>Okay.</p>
<p>308<br>00:28:58,000 –&gt; 00:29:06,000<br>So, regardless of how we’re going to allocate workers or tasks to our resources and our system.</p>
<p>309<br>00:29:06,000 –&gt; 00:29:15,000<br>As we said already, that it’s important to make sure that the data they’re going to process ideally is going to be local to whatever that worker is.</p>
<p>310<br>00:29:15,000 –&gt; 00:29:20,000<br>And in the case of the hyperpaper, it’s an n-memory database.</p>
<p>311<br>00:29:20,000 –&gt; 00:29:23,000<br>So, local means it’s in the same Numa region.</p>
<p>312<br>00:29:23,000 –&gt; 00:29:36,000<br>Obviously, in a distributed system, especially within a shared disk architecture, while the cost of going to get data from S3 could be basically the same for every single worker node, assuming you’re in the same data center in the same region.</p>
<p>313<br>00:29:36,000 –&gt; 00:29:47,000<br>But once we start caching things, which we’ll see later in the semester, like every compute node could have its own local copy of, you know, follows red from S3.</p>
<p>314<br>00:29:47,000 –&gt; 00:29:54,000<br>And then now, when I want to sign tasks, I want to make sure that the task is assigned to the node that’s going to have a local copy of that data.</p>
<p>315<br>00:29:54,000 –&gt; 00:29:55,000<br>Yes.</p>
<p>316<br>00:29:55,000 –&gt; 00:29:59,000<br>This is also not a region where it’s full of these slightly numbers because it doesn’t know what part of what.</p>
<p>317<br>00:29:59,000 –&gt; 00:30:03,000<br>If it just looks at a global queue and like, give me the next task.</p>
<p>318<br>00:30:03,000 –&gt; 00:30:04,000<br>Yes.</p>
<p>319<br>00:30:04,000 –&gt; 00:30:10,000<br>It’s not a story that the data that is needed for that next task is there a nice global story.</p>
<p>320<br>00:30:10,000 –&gt; 00:30:20,000<br>And, David, is the pool seems to be done because if the worker is trying to maximize the locality of the data and needs access, it can’t do that in a pool waste approach. Why not?</p>
<p>321<br>00:30:20,000 –&gt; 00:30:23,000<br>Because it’s just popping the top one through the top.</p>
<p>322<br>00:30:23,000 –&gt; 00:30:25,000<br>You know, you’re not the pop from the top.</p>
<p>323<br>00:30:25,000 –&gt; 00:30:26,000<br>What do we do?</p>
<p>324<br>00:30:26,000 –&gt; 00:30:30,000<br>Well, the hyper does it one way. We’ll see in other ways, in other systems.</p>
<p>325<br>00:30:30,000 –&gt; 00:30:31,000<br>All right.</p>
<p>326<br>00:30:31,000 –&gt; 00:30:35,000<br>I mean, you have a priority queue. You don’t always have to pop from the top.</p>
<p>327<br>00:30:35,000 –&gt; 00:30:47,000<br>Now, if you’re doing work-stealing, you may recognize that the thing at the head of the queue is not local to you, but you may want to go ahead and run it anyway because you’re available.</p>
<p>328<br>00:30:47,000 –&gt; 00:30:49,000<br>Hyper does that.</p>
<p>329<br>00:30:49,000 –&gt; 00:30:53,000<br>I don’t think Umber doesn’t, Hanna doesn’t.</p>
<p>330<br>00:30:53,000 –&gt; 00:30:56,000<br>It’s only the priority would be differently for a greater understanding of the problem.</p>
<p>331<br>00:30:56,000 –&gt; 00:30:57,000<br>Yeah, absolutely.</p>
<p>332<br>00:30:57,000 –&gt; 00:31:03,000<br>Yes, so it’s not so it’s a global queue in that.</p>
<p>333<br>00:31:03,000 –&gt; 00:31:11,000<br>In that everyone can see it and manipulate it, but it’s not going to guarantee fight for ordering of the elements of the unit.</p>
<p>334<br>00:31:11,000 –&gt; 00:31:14,000<br>Right. So it’s a priority queue.</p>
<p>335<br>00:31:14,000 –&gt; 00:31:19,000<br>In hyper, it’s a hash table, right?</p>
<p>336<br>00:31:19,000 –&gt; 00:31:26,000<br>Okay. So again, I’ve already said this. Like you could have some data systems will have locally attached storage as a cache.</p>
<p>337<br>00:31:26,000 –&gt; 00:31:32,000<br>Like again, think of like you spin up an EC2 node, you can get ones with NVMe drives that are local to that are really fast.</p>
<p>338<br>00:31:32,000 –&gt; 00:31:34,000<br>And so you’d use that as a local cache.</p>
<p>339<br>00:31:34,000 –&gt; 00:31:38,000<br>It’s a femoral, so if the node crashes, you don’t need to retain anything in there.</p>
<p>340<br>00:31:38,000 –&gt; 00:31:43,000<br>But again, while it’s available, you could use that set of having to go to S3.</p>
<p>341<br>00:31:44,000 –&gt; 00:31:51,000<br>In some systems, again, Snowflake is probably the most aggressive on this, because again, they don’t want to pay Amazon S3 costs.</p>
<p>342<br>00:31:51,000 –&gt; 00:31:53,000<br>They can also use other nodes as nearby cache.</p>
<p>343<br>00:31:53,000 –&gt; 00:31:58,000<br>So if you know this other node is run for the data and you’re running the task, it’s set to go into S3.</p>
<p>344<br>00:31:58,000 –&gt; 00:32:07,000<br>You could go directly to that node and get it, but they’re actually not going to do that because they don’t want to interfere with the other node because they might be going slow.</p>
<p>345<br>00:32:08,000 –&gt; 00:32:12,000<br>If you’re stealing work that was meant for this other node, it’s probably because they’re slow.</p>
<p>346<br>00:32:12,000 –&gt; 00:32:15,000<br>So why go start making requests of them to make them even slower?</p>
<p>347<br>00:32:15,000 –&gt; 00:32:17,000<br>It’s sort of the logic there.</p>
<p>348<br>00:32:17,000 –&gt; 00:32:24,000<br>And then the Numa versus not uniform memory access stuff we’ve already talked about, like local versus remote memory.</p>
<p>349<br>00:32:24,000 –&gt; 00:32:26,000<br>Sorry.</p>
<p>350<br>00:32:26,000 –&gt; 00:32:28,000<br>Oh yeah, so partitioning placement.</p>
<p>351<br>00:32:29,000 –&gt; 00:32:44,000<br>So in the intro class, we talked about partitioning this idea of how to take a data set and pick some set of columns or some keys and then divide it up based on the values of these keys across different files.</p>
<p>352<br>00:32:44,000 –&gt; 00:32:57,000<br>And that would allow you to spread out the data evenly, ideally across resources, so that when a query arrives that can run in parallel, each worker can have the same amount of work.</p>
<p>353<br>00:32:57,000 –&gt; 00:33:00,000<br>So they’re all sort of processing things uniformly.</p>
<p>354<br>00:33:00,000 –&gt; 00:33:07,000<br>So there’s going to be some policy that you’re going to use to say, here’s how I want to split things up, hash partitioning, round, round, range partitioning, and so forth.</p>
<p>355<br>00:33:07,000 –&gt; 00:33:14,000<br>And then there’ll be some target objective you’re trying to have for deciding how I want the reason why I want to partition things a certain way.</p>
<p>356<br>00:33:14,000 –&gt; 00:33:19,000<br>So one thing to be I want to reduce the amount of network traffic when I do a join.</p>
<p>357<br>00:33:20,000 –&gt; 00:33:30,000<br>Maybe I want to partition my tables on the things that the join keys so that the joins can always be repeated locally rather than have to do a shuffle or broadcast join.</p>
<p>358<br>00:33:30,000 –&gt; 00:33:38,000<br>In our world, in the shared disk, you know, the lake house environment, we’re typically going to be doing round, round, and based on files.</p>
<p>359<br>00:33:38,000 –&gt; 00:33:41,000<br>Because we’re not the ones generating these files.</p>
<p>360<br>00:33:41,000 –&gt; 00:33:44,000<br>Right, someone loaded a bunch of stuff in S3, a bunch of parquet, org files.</p>
<p>361<br>00:33:44,000 –&gt; 00:33:52,000<br>We’re not going to have time to go fix them up and put them partition them and rebalance them according to, again, some target objective.</p>
<p>362<br>00:33:52,000 –&gt; 00:33:57,000<br>Snowflake will do this, they call micro partitions, but I think they only do it for their internal data format.</p>
<p>363<br>00:33:57,000 –&gt; 00:34:03,000<br>Meaning like if you do insert queries to put data into snowflake, then they can rebalance stuff later on.</p>
<p>364<br>00:34:03,000 –&gt; 00:34:08,000<br>Repartition later on. But again, if it’s not your files in S3, you can’t rewrite them.</p>
<p>365<br>00:34:08,000 –&gt; 00:34:27,000<br>So if you’re doing round, round, and partitioning at the file level, what is the data that you’re doing?</p>
<p>366<br>00:34:27,000 –&gt; 00:34:32,000<br>It literally is like file one, go to you, file two, go to him, file three, go to him, that’s it.</p>
<p>367<br>00:34:32,000 –&gt; 00:34:35,000<br>Question, they shouldn’t be uniform size.</p>
<p>368<br>00:34:35,000 –&gt; 00:34:40,000<br>Typically, yeah, I think so, yeah.</p>
<p>369<br>00:34:40,000 –&gt; 00:34:50,000<br>I mean, obviously you can imagine, to generate cases where I have a bunch of one gay white files and I have one terabyte file, then that was screw things up.</p>
<p>370<br>00:34:50,000 –&gt; 00:34:53,000<br>I don’t know what they might break that up, so far, right?</p>
<p>371<br>00:34:53,000 –&gt; 00:35:07,000<br>I mean, the way to handle that one also, you could assign the same file to different nodes, but then you just within that file you say, okay, 0 to 5 goes to this one, 6 to 10 goes to that one.</p>
<p>372<br>00:35:07,000 –&gt; 00:35:09,000<br>You could divide that further, but I don’t think they do that.</p>
<p>373<br>00:35:09,000 –&gt; 00:35:14,000<br>Yeah, originally on Ruby, it plays more sense, so those are similar in size.</p>
<p>374<br>00:35:14,000 –&gt; 00:35:23,000<br>Yes, I could walk again, in parquet based on number of tuples, or because based on the data size, but in the end, roughly about the same, yes.</p>
<p>375<br>00:35:23,000 –&gt; 00:35:32,000<br>Sorry, because it’s roughly the same question, but I’m trying, like, in the main reason that we would split at the file level, when doing round the Robin.</p>
<p>376<br>00:35:32,000 –&gt; 00:35:34,000<br>Is it like, further within a file?</p>
<p>377<br>00:35:34,000 –&gt; 00:35:42,000<br>No, no, I like, if we have 5 files, why is it then 1, 2, 3, 4 files? Why is that split? If some advantage to it beyond it’s like, if you…</p>
<p>378<br>00:35:42,000 –&gt; 00:35:51,000<br>His question is, if, if, why do it split based on the file, is there any performance advantage to it? It’s easy.</p>
<p>379<br>00:35:51,000 –&gt; 00:35:52,000<br>It’s easy.</p>
<p>380<br>00:35:52,000 –&gt; 00:35:57,000<br>I mean, you maintain less metadata. I have 5 files, I need 5 entries in my catalog to say where these files are.</p>
<p>381<br>00:35:57,000 –&gt; 00:36:06,000<br>If I start doing some division within that, then I have to maintain more metadata, which some systems can do.</p>
<p>382<br>00:36:06,000 –&gt; 00:36:15,000<br>If you’re doing range partitioning, yeah, you had to keep track where the ranges are. If it’s hash partitioning, you actually don’t need to do any of that. It’s even cheaper.</p>
<p>383<br>00:36:15,000 –&gt; 00:36:21,000<br>You could just say, here’s the column, here’s the hash key, you’re whatever, you know, I want to hash on, and decide where it goes.</p>
<p>384<br>00:36:21,000 –&gt; 00:36:30,000<br>Now, if you’re doing consistent hashing, which snowflake does, where on the file level, then you’ve got to maintain that data structure to do consistent hashing, but that we’ll cover that later.</p>
<p>385<br>00:36:30,000 –&gt; 00:36:38,000<br>So partitioning tells you how to split things up. The placement policy determines where those partitions are actually going to go.</p>
<p>386<br>00:36:38,000 –&gt; 00:36:45,000<br>And again, the simplest thing to do is basically, I got 5 machines, then you just get 1 file, it’s around robbing that.</p>
<p>387<br>00:36:46,000 –&gt; 00:36:55,000<br>You could try to be clever of breaking things up in more sophisticated ways. We can ignore that. This is the easiest way to do it.</p>
<p>388<br>00:36:55,000 –&gt; 00:37:12,000<br>So now in our catalog, we say, I have these sizes and this and this and that, whatever I was able to clean when it was imported into my system, where I was notified that it existed in S3, and then I keep track of like, this worker is responsible for it.</p>
<p>389<br>00:37:12,000 –&gt; 00:37:27,000<br>So any query that shows up, then has a task that wants to process that file, again, which we would determine in the optimized for the catalog, ideally we want the worker that’s been responsible for that file to process that data.</p>
<p>390<br>00:37:28,000 –&gt; 00:37:41,000<br>And whether or not that worker has a local cache or not is a matter, it’s just, we’re just saying that rather than everybody read everything, we can get more structure and say, this worker is going to read this file.</p>
<p>391<br>00:37:43,000 –&gt; 00:37:55,000<br>So far we have a task assignment model, basically how to assign workers to threads or processes and so forth, and whether we’re going to push versus pool from the scheduler.</p>
<p>392<br>00:37:55,000 –&gt; 00:38:07,000<br>There are data plays in policy, again, for our purposes, I’m going to share a disc architecture and a modern lake house or a data lake system, it’s going to be at the file level and round robbing distribution.</p>
<p>393<br>00:38:08,000 –&gt; 00:38:14,000<br>Then now we’re going to say, how do we take a logical query plan and convert it to something that we can then execute.</p>
<p>394<br>00:38:15,000 –&gt; 00:38:22,000<br>And I’ve sort of said this, looting us a bit along, like, you know, we know where the pipeline breakers are, that’s going to be the boundary for our task within this part.</p>
<p>395<br>00:38:23,000 –&gt; 00:38:26,000<br>But then now how do we take those tasks and run them?</p>
<p>396<br>00:38:27,000 –&gt; 00:38:32,000<br>So if it’s an OLITN query, this is super easy to do because these queries typically only have one pipeline.</p>
<p>397<br>00:38:33,000 –&gt; 00:38:39,000<br>Do an index scan, maybe a projection, and that’s it, right? There’s not many operators in it, and there’s no dependencies between pipelines, there’s only one.</p>
<p>398<br>00:38:40,000 –&gt; 00:38:43,000<br>So that’s easy, we just pin that out and let them run as fast as possible.</p>
<p>399<br>00:38:44,000 –&gt; 00:38:57,000<br>But for OLITN query, it’s more complicated because we know there’s dependencies between these pipelines, so we can’t, to avoid false negatives, we can’t have one pipeline start running if the pipelines that it’s dependent on have been completed to produce whatever the intermediate results that are needed.</p>
<p>400<br>00:38:58,000 –&gt; 00:39:00,000<br>So you can’t always paralyze them.</p>
<p>401<br>00:39:01,000 –&gt; 00:39:02,000<br>Yes.</p>
<p>402<br>00:39:02,000 –&gt; 00:39:06,000<br>Why does the 50-side slide say logical query plan is that a bit of a bit?</p>
<p>403<br>00:39:07,000 –&gt; 00:39:09,000<br>How does it say it?</p>
<p>404<br>00:39:10,000 –&gt; 00:39:15,000<br>Like, because the law, yeah, you can say physical yes. Yes.</p>
<p>405<br>00:39:16,000 –&gt; 00:39:19,000<br>If you just get rid of logical, then it’s fine. Yes, that’s a title.</p>
<p>406<br>00:39:20,000 –&gt; 00:39:32,000<br>Right, again, the logical query plan says, scan this, I want to read this table. It doesn’t tell you how to do it, it doesn’t tell you how to do a joint, it says join a and b.</p>
<p>407<br>00:39:33,000 –&gt; 00:39:45,000<br>The physical query plan is the exact algorithms you want to use, so when we create a query plan, when we convert a query plan to much a task, we’re going to be doing that on the physical operators, not the logical ones.</p>
<p>408<br>00:39:46,000 –&gt; 00:39:48,000<br>Thank you. How will it fix that?</p>
<p>409<br>00:39:49,000 –&gt; 00:39:52,000<br>So the easiest type of scheduling to do is called static scheduling.</p>
<p>410<br>00:39:52,000 –&gt; 00:40:07,000<br>And this is where the optimizer figures out, or the scheduler figures out in the very beginning, I have this query plan, I have these workers, and I have this data, and it does a static assignment of tasks to those individual workers.</p>
<p>411<br>00:40:08,000 –&gt; 00:40:16,000<br>And it doesn’t, you know, the simplest way to think of it is I have one task per core or per worker, and they just all run at the same time.</p>
<p>412<br>00:40:17,000 –&gt; 00:40:22,000<br>You still can assign the workers to, or the task to workers that based on the data is local to them.</p>
<p>413<br>00:40:23,000 –&gt; 00:40:31,000<br>But again, there’s no dynamicism, there’s no adapting to the behavior, the performance of the workers as they actually process processes the data.</p>
<p>414<br>00:40:33,000 –&gt; 00:40:37,000<br>Right, you can think of this generic postgres does this.</p>
<p>415<br>00:40:38,000 –&gt; 00:40:47,000<br>So now the problem with this is that there’s going to be some tasks that are going to be slower, either because the data that the processing, it just takes longer to execute whatever the operators that they have on it.</p>
<p>416<br>00:40:48,000 –&gt; 00:40:59,000<br>Like you think of like, I have a complicated wear clause where there’s some predicate that is that is really fast compute, and but can be very selective on some of the data.</p>
<p>417<br>00:41:00,000 –&gt; 00:41:07,000<br>And then the remaining predicates in that wear clause are slow to compute. So 9 out of the 10 files, all the data gets filtered out by that fast predicate.</p>
<p>418<br>00:41:08,000 –&gt; 00:41:19,000<br>So those tasks run really fast. But the one unlucky worker that has all the data that does satisfy the predicate then has to run the more expensive predicates, and then it’s just going to be way slower than the other ones.</p>
<p>419<br>00:41:20,000 –&gt; 00:41:29,000<br>So now all the other workers have to wait until that task actually finishes before they can move on to the next pipeline for that query plan.</p>
<p>420<br>00:41:31,000 –&gt; 00:41:36,000<br>So there’s no dynamicism in any of the decisions that we’re making here. Everything’s figured out ahead of time.</p>
<p>421<br>00:41:39,000 –&gt; 00:41:42,000<br>So what more so is designed to solve is that exact problem.</p>
<p>422<br>00:41:43,000 –&gt; 00:42:03,000<br>How do we figure out how do we on the fly dynamically adjust how we’re executing or assigning tasks to workers so that if there’s unexpected variations in the run time of tasks, we can have other workers fill in and start computing things rather than waiting for the slow straggler.</p>
<p>423<br>00:42:04,000 –&gt; 00:42:14,000<br>So the morsels term comes from the hyper guys because they were just looking for another term to mean chunk of data. They didn’t want to use partition, they didn’t want to use shard, they couldn’t use block.</p>
<p>424<br>00:42:15,000 –&gt; 00:42:25,000<br>Because the morsels meant to be something bigger. But the high level it’s still the same thing. You know, missing the same thing as a row group. But I think this paper came up with the row group stuff.</p>
<p>425<br>00:42:26,000 –&gt; 00:42:33,000<br>So in their architecture, they’re going to have one worker per core, they’re going to turn up hyper threading. They’re going to have an assignment of one morsel per task.</p>
<p>426<br>00:42:34,000 –&gt; 00:42:43,000<br>So tasks is going to be your sponsor for processing one morsel of data. They’re going to do a pool based task assignment. They’re going to do this global queue that they’re all going to try to pull from and figure out what to do next.</p>
<p>427<br>00:42:44,000 –&gt; 00:42:46,000<br>And they’re going to do really simple round ramen data placement.</p>
<p>428<br>00:42:47,000 –&gt; 00:42:57,000<br>And so again, they keep track of what new morsel is going to be in. And they would annotate the task and say this task is going to act to this morsel on this morsel that’s located in this new morsel.</p>
<p>429<br>00:42:58,000 –&gt; 00:43:04,000<br>Then each worker, the candidate side, whether they want to run a task that’s not processing data, it’s not local to it.</p>
<p>430<br>00:43:05,000 –&gt; 00:43:18,000<br>So they’ll have all the operators, we parallel a new morsel, that we can basically ignore. But it’s in thinking of having the exchange operators keep track of what inputs I’m waiting on before I can coalesce things and move on to the next pipeline.</p>
<p>431<br>00:43:20,000 –&gt; 00:43:30,000<br>So this morsel paper came out in 2014. And it’s fairly influential. And this is actually what DuckDB does as well. And they’re very upfront about this.</p>
<p>432<br>00:43:31,000 –&gt; 00:43:41,000<br>They basically took this paper and re-implement it in DuckDB. And we had Mark give a guest lecture for us last year in the spring semester last year. And they basically said they’re doing morsel driven parallelism.</p>
<p>433<br>00:43:42,000 –&gt; 00:43:53,000<br>So again, this is not just for hyper DuckDB is widely used in this basements as well. I meant to look at I think there’s a couple other systems out there that are using a similar approach.</p>
<p>434<br>00:43:54,000 –&gt; 00:44:04,000<br>So in hyper, there’s not going to be a separate dispatcher thread. Every worker is going to be responsible for figuring out what’s the next task I need to execute.</p>
<p>435<br>00:44:05,000 –&gt; 00:44:21,000<br>So you sort of think this is a cooperative scheduling where everyone’s working together for this common cause, this global, you know, trying to achieve the best performance of the system, they’re working together to try to do that. But then the logic to figure out what’s the next best thing for me to run is going to be distributed across the different workers.</p>
<p>436<br>00:44:23,000 –&gt; 00:44:40,000<br>So in the ideal scenario, they’re going to go look in the task queue and try to choose a task that again, that’s going to process a morsel that’s local to it. If there are no local tasks that are available for the current query, then they’ll go find what’s the next task.</p>
<p>437<br>00:44:41,000 –&gt; 00:44:52,000<br>What’s the very next task, even if that date is not local to it. Because again, that’s going to be able to do to help mitigate the issue of stragglers slowing everybody, everybody behind.</p>
<p>438<br>00:44:54,000 –&gt; 00:45:04,000<br>So now in the paper, they’re going to ignore a very key problem, which I think I’ve already talked about in their approach. And that’s going to be the synchronization cost of this global hash table.</p>
<p>439<br>00:45:05,000 –&gt; 00:45:14,000<br>It’s a bit hand-wavy saying that it’s not a big deal, but when we see the umbra paper next, they basically throw it away and they switch to a more distributed scalable approach.</p>
<p>440<br>00:45:15,000 –&gt; 00:45:25,000<br>And in the case of the HANA paper, which I covered in a few minutes, they closely call out the hyper approach of having this global task queue when you have a large number of cores is going to be a problem.</p>
<p>441<br>00:45:26,000 –&gt; 00:45:29,000<br>What is the log number of the customer’s thing?</p>
<p>442<br>00:45:29,000 –&gt; 00:45:31,000<br>But they do in the paper, right?</p>
<p>443<br>00:45:31,000 –&gt; 00:45:41,000<br>Yeah, but I think it’s a four socket machine, right? I mean, for the HANA guys, they’re talking like 128 sockets, no more larger.</p>
<p>444<br>00:45:42,000 –&gt; 00:45:51,000<br>They even told me they had this like before Sigma, they would have this like the invite from David’s faculty come season presentations from people working on HANA.</p>
<p>445<br>00:45:52,000 –&gt; 00:46:04,000<br>And they had one of their customers come in and talk about how they were running on some beefy box where they were running out of address space on X64.</p>
<p>446<br>00:46:05,000 –&gt; 00:46:13,000<br>You have 64-bit pointers, but Intel only uses 48 bits. They were running out of space. They’re addressing 48 bits and it was running on.</p>
<p>447<br>00:46:13,000 –&gt; 00:46:18,000<br>And that was a few years ago. I’m sure people are easily exceeding that now.</p>
<p>448<br>00:46:18,000 –&gt; 00:46:19,000<br>So, okay.</p>
<p>449<br>00:46:20,000 –&gt; 00:46:24,000<br>So, again, the high, the high-level idea is going to be that we have some data table here.</p>
<p>450<br>00:46:24,000 –&gt; 00:46:31,000<br>And there’s going to be some arbitrary slicing it up horizontally into different morsels.</p>
<p>451<br>00:46:31,000 –&gt; 00:46:36,000<br>And then each of these morsels will be assigned to one socket, one numerator region.</p>
<p>452<br>00:46:36,000 –&gt; 00:46:40,000<br>So, what does that look like? What we already talked about with Pax.</p>
<p>453<br>00:46:40,000 –&gt; 00:46:43,000<br>Rogue Groups. Basically the same thing, just a different name.</p>
<p>454<br>00:46:44,000 –&gt; 00:46:57,000<br>So, in there, this paper, they claim 100,000 tuples per morsel was the right size because that gave them the right amount of parallelism across all the cores.</p>
<p>455<br>00:46:57,000 –&gt; 00:47:01,000<br>If you set it too small, then you’re always going to the task you and that becomes the bottleneck.</p>
<p>456<br>00:47:01,000 –&gt; 00:47:10,000<br>If you set it too big, then you have the problem of, again, the straggler, it’s the only way to process some giant morsel and everyone gets stalled for that.</p>
<p>457<br>00:47:11,000 –&gt; 00:47:18,000<br>When we were building our system peloton here, we did 1,000 tuples per morsel.</p>
<p>458<br>00:47:18,000 –&gt; 00:47:29,000<br>And then I think in the follow-up system, with noise page, we were doing 10 megabyte morsels because you could play some trick with InstaPelSaus to do 20-bit pointers, 20-bit offsets.</p>
<p>459<br>00:47:29,000 –&gt; 00:47:31,000<br>But we can ignore that.</p>
<p>460<br>00:47:32,000 –&gt; 00:47:49,000<br>All right, so now we have a query plan, we convert it into a bunch of tasks, we have our task queue, and sort of thinking in the task queue, again, it’s the, what the computation of the operators want to do within the pipeline, and then it’s tagged with what morsels they want to operate on.</p>
<p>461<br>00:47:49,000 –&gt; 00:47:58,000<br>So, on each core now, that each going to have a memory region that corresponds to the morsels, again, this is just the table space for the tables.</p>
<p>462<br>00:47:58,000 –&gt; 00:48:05,000<br>Then there’s some local buffer they’re going to use to write out intermediate results. Again, this is an in-memory database, so everything’s all in memory here.</p>
<p>463<br>00:48:05,000 –&gt; 00:48:11,000<br>And then you have whatever your local, it’s all in your local memory, then you have your single core.</p>
<p>464<br>00:48:11,000 –&gt; 00:48:19,000<br>So, to get started, each of these guys are going to go into the queue and pull out things that are, that are going to process the data as local to it.</p>
<p>465<br>00:48:19,000 –&gt; 00:48:29,000<br>And then when now it runs, again, it’s just computing or crunching on the data that’s local to it, so it doesn’t have to go to the interconnect on the CPU, everything can run really fast.</p>
<p>466<br>00:48:29,000 –&gt; 00:48:40,000<br>And then they’re always going to produce the output back into their local buffer, again, because they want to avoid the traffic over the interconnect, having to write to some, to some remote memory.</p>
<p>467<br>00:48:40,000 –&gt; 00:48:59,000<br>So now, say this on CPU3 for whatever reason, it’s just running slower. In this case here, because there’s no tasks that are available for this query, to execute, because the next stage we have to wait for the output of the first stage, the first pipeline, so these guys essentially have to stall.</p>
<p>468<br>00:48:59,000 –&gt; 00:49:13,000<br>Now, if there was another query in our queue, you could start processing that, but then you get in this contention of like, okay, well, I only have so much space for my buffers, do I go into the start processing another query that I can then interfere with the data I want to store in my buffers?</p>
<p>469<br>00:49:13,000 –&gt; 00:49:15,000<br>Because then I have to start swapping things in and out.</p>
<p>470<br>00:49:15,000 –&gt; 00:49:21,000<br>These are CPUs, or in individual cores?</p>
<p>471<br>00:49:21,000 –&gt; 00:49:23,000<br>Individual cores.</p>
<p>472<br>00:49:23,000 –&gt; 00:49:27,000<br>Like, does it all in one CPU? They have their own memory?</p>
<p>473<br>00:49:27,000 –&gt; 00:49:39,000<br>It’s a real quick. One core, they have L1, L2, and then on the same socket, all the cores share L3, and then they have local memory.</p>
<p>474<br>00:49:39,000 –&gt; 00:49:48,000<br>So I’m drawing the CPU symbol. I could put red, whatever. But they have one hardware core.</p>
<p>475<br>00:49:48,000 –&gt; 00:49:57,000<br>So when this guy then finishes, this then frees up all the other guys, then go back to the queue, and pull out more tasks.</p>
<p>476<br>00:49:57,000 –&gt; 00:50:08,000<br>And again, because the last task they executed for this query wrote data to their local buffer, we want to then have the affinity of making sure that the next task is going to process that data that we just did.</p>
<p>477<br>00:50:08,000 –&gt; 00:50:13,000<br>And the task that’s that data that we just generated in the previous pipeline is going to run on the same core.</p>
<p>478<br>00:50:13,000 –&gt; 00:50:15,000<br>Again, avoiding that interconnect traffic.</p>
<p>479<br>00:50:15,000 –&gt; 00:50:22,000<br>So now in this case here, say this one finishes up first, so we do have actually a task we could execute.</p>
<p>480<br>00:50:22,000 –&gt; 00:50:37,000<br>And so, so hyper says that in this case here, when you do work, work stealing, it’s okay for you to go across the new Maria region to go get the data you need, because it’s better, it’s better to do that than having idle resources.</p>
<p>481<br>00:50:37,000 –&gt; 00:50:53,000<br>Is there like a heuristic cost? Obviously costs and you don’t, but you can’t ever predict when that task on like, see for a core three is going to finish.</p>
<p>482<br>00:50:53,000 –&gt; 00:51:01,000<br>So what happens if our core three finishes, the core is like stopped, stopped, we’re going to operate on that further.</p>
<p>483<br>00:51:01,000 –&gt; 00:51:09,000<br>So his statement is, the question is, like, is there a heuristic to figure out when is it actually okay to steal?</p>
<p>484<br>00:51:09,000 –&gt; 00:51:18,000<br>Because it may be the case that right before I steal, or right immediately after I steal, this thing finishes, then it could have processed it.</p>
<p>485<br>00:51:18,000 –&gt; 00:51:23,000<br>And then I could have processed the data locally, and that would have embedded in this guy stealing.</p>
<p>486<br>00:51:23,000 –&gt; 00:51:31,000<br>Again, when it’s on a single node and you’re measuring things like, the more cells are like 100,000 tuples, you’re getting down to like milliseconds here.</p>
<p>487<br>00:51:31,000 –&gt; 00:51:37,000<br>And the additional bookkeeping you have to do to figure that out, you pay a high penalty for that.</p>
<p>488<br>00:51:37,000 –&gt; 00:51:39,000<br>A lot of overhead maintain that.</p>
<p>489<br>00:51:39,000 –&gt; 00:51:41,000<br>Is this better to block so you get this?</p>
<p>490<br>00:51:41,000 –&gt; 00:51:44,000<br>Yeah, and they claim it’s always better to steal.</p>
<p>491<br>00:51:44,000 –&gt; 00:51:51,000<br>In the case of HANA, they’re going to be doing even more bookkeeping, and they say don’t do any of that.</p>
<p>492<br>00:51:51,000 –&gt; 00:51:57,000<br>And it’s better to just, don’t do any stealing because the call system is too high.</p>
<p>493<br>00:51:57,000 –&gt; 00:52:02,000<br>Then what I guess about how stealing is partial work?</p>
<p>494<br>00:52:02,000 –&gt; 00:52:08,000<br>This question is, how can you handle partial work and work stealing?</p>
<p>495<br>00:52:08,000 –&gt; 00:52:17,000<br>Because presumably, the work that is taking is like stopping the machine from like the more cells.</p>
<p>496<br>00:52:17,000 –&gt; 00:52:20,000<br>So every task is one morsel.</p>
<p>497<br>00:52:20,000 –&gt; 00:52:29,000<br>So this guy is processing whatever, this one here, no one can take the same task, because it’s no longer in the queue.</p>
<p>498<br>00:52:29,000 –&gt; 00:52:37,000<br>So when this guy, when one steals the next task, it’s not processing on the same data as this guy, it’s completely disjoint and separate.</p>
<p>499<br>00:52:37,000 –&gt; 00:52:42,000<br>So there’s no concern of synchronizing about partial results.</p>
<p>500<br>00:52:42,000 –&gt; 00:52:44,000<br>Oh, okay.</p>
<p>501<br>00:52:44,000 –&gt; 00:52:47,000<br>Yeah, the morsels are disjoint.</p>
<p>502<br>00:52:47,000 –&gt; 00:52:48,000<br>Yes.</p>
<p>503<br>00:52:48,000 –&gt; 00:52:52,000<br>So in the type where these, these type where they’re compiled, right?</p>
<p>504<br>00:52:52,000 –&gt; 00:52:55,000<br>Which ductubes do you do, like, they can pile up by one?</p>
<p>505<br>00:52:55,000 –&gt; 00:52:57,000<br>This question is, in hyper, these are compiled pipelines.</p>
<p>506<br>00:52:57,000 –&gt; 00:52:59,000<br>And ductubes do they have compiled pipelines? No.</p>
<p>507<br>00:52:59,000 –&gt; 00:53:03,000<br>So they just do it normally, how to chain up up the data?</p>
<p>508<br>00:53:03,000 –&gt; 00:53:06,000<br>What’s normal?</p>
<p>509<br>00:53:06,000 –&gt; 00:53:14,000<br>Like, it’s a push-based model, but they’re doing the vectorized approach of pre-compiled primitives.</p>
<p>510<br>00:53:14,000 –&gt; 00:53:18,000<br>Again, this is completely independent of the query processing model here.</p>
<p>511<br>00:53:18,000 –&gt; 00:53:22,000<br>Does it matter whether it’s compiled or vectorized?</p>
<p>512<br>00:53:22,000 –&gt; 00:53:26,000<br>Back, yes.</p>
<p>513<br>00:53:26,000 –&gt; 00:53:29,000<br>This question is, do you rebalance the enemy results?</p>
<p>514<br>00:53:29,000 –&gt; 00:53:32,000<br>Meaning, like, oh, yeah.</p>
<p>515<br>00:53:32,000 –&gt; 00:53:36,000<br>So if this guy steals a bunch of stuff and he keeps writing to the buffer,</p>
<p>516<br>00:53:36,000 –&gt; 00:53:39,000<br>it’s a local buffer, and then is it going to run out of space?</p>
<p>517<br>00:53:39,000 –&gt; 00:53:41,000<br>I don’t know what they do.</p>
<p>518<br>00:53:41,000 –&gt; 00:53:44,000<br>At some point, I suspect he would, yes.</p>
<p>519<br>00:53:49,000 –&gt; 00:53:56,000<br>Yeah, I don’t know how to handle, but you can imagine identifying that, oh, I can’t…</p>
<p>520<br>00:53:57,000 –&gt; 00:54:02,000<br>I’m running out of space. I can’t process anything else, so either you don’t process anything else until the query finishes.</p>
<p>521<br>00:54:02,000 –&gt; 00:54:09,000<br>Or you can introduce an internal task that then moves things around,</p>
<p>522<br>00:54:09,000 –&gt; 00:54:13,000<br>but then booking for that would be expensive.</p>
<p>523<br>00:54:17,000 –&gt; 00:54:21,000<br>Yeah, I mean, that’s one of the challenges in MMB databases is that you can run out of memory.</p>
<p>524<br>00:54:21,000 –&gt; 00:54:24,000<br>I think they just assume that you don’t. Yes.</p>
<p>525<br>00:54:24,000 –&gt; 00:54:29,000<br>So at some point, the enemy results have to be…</p>
<p>526<br>00:54:29,000 –&gt; 00:54:33,000<br>The same question is, at some point you have to aggregate the enemy results? Yes.</p>
<p>527<br>00:54:33,000 –&gt; 00:54:36,000<br>But that’s the exchange operator we saw before.</p>
<p>528<br>00:54:36,000 –&gt; 00:54:41,000<br>And then in that case, that you can’t really paralyze, because this is one thread that has to be pulled in the data</p>
<p>529<br>00:54:41,000 –&gt; 00:54:47,000<br>from all the children below the exchange operator, and then co-lesting that to produce the final output.</p>
<p>530<br>00:54:48,000 –&gt; 00:55:02,000<br>Okay. So, one of the key problems with Umbra, or sorry, with Hyper, is that because one worker is assigned…</p>
<p>531<br>00:55:02,000 –&gt; 00:55:12,000<br>There’s only one worker per core and one morsel per task, they had to do work stealing because it’s almost like this…</p>
<p>532<br>00:55:13,000 –&gt; 00:55:19,000<br>It’s not exactly static scheduling because they are allowing things to pull data as they go along,</p>
<p>533<br>00:55:19,000 –&gt; 00:55:24,000<br>but they can’t rebalance the amount of work that each task is doing.</p>
<p>534<br>00:55:24,000 –&gt; 00:55:29,000<br>So in the cases where you’re blocked on waiting this last task, you need for this pipeline,</p>
<p>535<br>00:55:29,000 –&gt; 00:55:33,000<br>everyone has to stall until that thing finishes.</p>
<p>536<br>00:55:35,000 –&gt; 00:55:40,000<br>The other challenge, as I already said, is they don’t really talk about how they built their lock free hash table.</p>
<p>537<br>00:55:40,000 –&gt; 00:55:46,000<br>That part’s a bit hand-waving in the paper, but again, as we know, that’s going to be…</p>
<p>538<br>00:55:46,000 –&gt; 00:55:50,000<br>Always going to be content-apport. Lock free, it doesn’t mean it’s magically scalable.</p>
<p>539<br>00:55:50,000 –&gt; 00:55:54,000<br>It just means that you’re never going to stall waiting for…</p>
<p>540<br>00:55:54,000 –&gt; 00:55:57,000<br>Or waiting to acquire something in the lock.</p>
<p>541<br>00:55:57,000 –&gt; 00:56:00,000<br>You have to spin until you can acquire something though.</p>
<p>542<br>00:56:00,000 –&gt; 00:56:03,000<br>Right, and now you’re burning cycles.</p>
<p>543<br>00:56:04,000 –&gt; 00:56:14,000<br>The other two problems are going to have in hypers that they’re going to treat the execution cost of every tuple in a morsel to be the same.</p>
<p>544<br>00:56:14,000 –&gt; 00:56:22,000<br>And as I said before, you can easily come up with examples where that may not be the case based on what the query is or what the predicates are.</p>
<p>545<br>00:56:23,000 –&gt; 00:56:27,000<br>The other issue is going to be…</p>
<p>546<br>00:56:27,000 –&gt; 00:56:35,000<br>He’s mentioned in the conclusion, say, what would be nice to have quality of service or priorities to keep track of these things, but they simply can’t do that.</p>
<p>547<br>00:56:35,000 –&gt; 00:56:39,000<br>It’s almost a free-fraud. Here’s whatever’s in the…</p>
<p>548<br>00:56:39,000 –&gt; 00:56:45,000<br>My task cue, and then the workers are trying to pull things as fast as possible and just running it.</p>
<p>549<br>00:56:46,000 –&gt; 00:57:02,000<br>But that means I could have a long running query take up all the resources, all the workers, while it’s processing, and then how much of these short queries showing up, and I have no way to easily interleave them and make sure the short queries get processed.</p>
<p>550<br>00:57:02,000 –&gt; 00:57:09,000<br>And as we said, that’s bad because people are going to notice when the short queries run slow.</p>
<p>551<br>00:57:09,000 –&gt; 00:57:21,000<br>So the follow-up work to the hyperpaper you guys read or extension to morsels is this paper from 2021 on the new system that came after hyper, called Umbra.</p>
<p>552<br>00:57:21,000 –&gt; 00:57:27,000<br>The background is that hyper was built by Thomas and his team at TU Minic.</p>
<p>553<br>00:57:27,000 –&gt; 00:57:38,000<br>They formed a little mini startup based on it, then they got acquired by Tableau, and it was being used as the internal memory query cache for Tableau.</p>
<p>554<br>00:57:38,000 –&gt; 00:57:44,000<br>The app, and in time you used it. And then Tableau got bought by Salesforce and so forth.</p>
<p>555<br>00:57:44,000 –&gt; 00:57:49,000<br>Thomas then lost control of hyper because Tableau now owned it.</p>
<p>556<br>00:57:49,000 –&gt; 00:57:53,000<br>So we started building a new system called Umbra.</p>
<p>557<br>00:57:53,000 –&gt; 00:57:59,000<br>And you couldn’t use any of the source code we had from hyper. Everything’s written from scratch because it’s a free-fraud.</p>
<p>558<br>00:57:59,000 –&gt; 00:58:10,000<br>So this is the new scheduler that they built in Umbra that is meant to overcome the deficiencies that they had in the hyper morsel scheduler.</p>
<p>559<br>00:58:10,000 –&gt; 00:58:20,000<br>So the key things are that the tasks are not going to be created statically at runtime, and they’re not going to have a one-to-one relation between a task and a morsel.</p>
<p>560<br>00:58:20,000 –&gt; 00:58:29,000<br>One task can process potentially multiple morsels if it still has time available to compute things.</p>
<p>561<br>00:58:29,000 –&gt; 00:58:38,000<br>So another way to think about this is that they’re basically going to be slicing up the computational resource based on time.</p>
<p>562<br>00:58:38,000 –&gt; 00:58:47,000<br>So within your quantum, you can keep processing of many tuples as you can. And then when you run out of time, then you have to give the CPU back.</p>
<p>563<br>00:58:47,000 –&gt; 00:58:53,000<br>But I still think even though you give it back, you’re still tied to the morsel of your processing, so nobody else can take it.</p>
<p>564<br>00:58:53,000 –&gt; 00:59:06,000<br>The other thing we’re going to be able to do to handle make sure that short queries aren’t blocked by the longer run queries, they’re going to do automatic exponential priority decay for queries.</p>
<p>565<br>00:59:06,000 –&gt; 00:59:11,000<br>So that the longer a query is running in the system, the lower priority gets over time.</p>
<p>566<br>00:59:11,000 –&gt; 00:59:24,000<br>And so again, it’ll still be scheduled eventually, but it’s not going to get, you know, it’s not going to be able to execute as many resources and to give them time slice as a short run query, who just arrived in the system.</p>
<p>567<br>00:59:24,000 –&gt; 00:59:30,000<br>So at a high level, this is a variation of striped scheduling, which I think came out of the 80s or 90s.</p>
<p>568<br>00:59:30,000 –&gt; 00:59:36,000<br>Do they teach that here or no? I don’t think so. Did you get an OS? No.</p>
<p>569<br>00:59:36,000 –&gt; 00:59:51,000<br>Okay. Think of it’s a primitive way to do scheduling in an operating system for tasks and processes where you keep track of how long things have been running and how much work they’re going to do every time they run.</p>
<p>570<br>00:59:51,000 –&gt; 01:00:02,000<br>But in the original implementation, like there’s a global, there’s a global, you know, global priority list, there’s global information that you have to maintain, and you assume that the workload is fixed.</p>
<p>571<br>01:00:02,000 –&gt; 01:00:10,000<br>But obviously in a database system query that coming and going all the time, and so we can’t make that assumption. So they have ways to fix that.</p>
<p>572<br>01:00:10,000 –&gt; 01:00:11,000<br>Yes.</p>
<p>573<br>01:00:11,000 –&gt; 01:00:27,000<br>So, for sure, the second fourth point, the exponentially growing the more precisely to get a lower value of how long it takes to execute the query for the XQP of the task, how do you know how long it has to execute the way executed?</p>
<p>574<br>01:00:27,000 –&gt; 01:00:37,000<br>You don’t. So the question is, how do you know how long the task is going to take to execute it? You don’t. They just turn on monitoring on it and keep track of it over time.</p>
<p>575<br>01:00:37,000 –&gt; 01:00:43,000<br>So when they say XQP is going to be growing the more useful sizes, is it like there’s adding more data to the more so? Yes.</p>
<p>576<br>01:00:43,000 –&gt; 01:00:48,000<br>They have a query? Or is it just giving them more more resources to stack?</p>
<p>577<br>01:00:48,000 –&gt; 01:00:54,000<br>So the thing of the more so concept, it’s just a logical concept of like here’s the divider line of like for one more so ends and stops.</p>
<p>578<br>01:00:54,000 –&gt; 01:01:04,000<br>So like if you recognize that each task is computing each more so really, really fast, then you, and then there’s more bookkeeping you have to go back and get more, you know, give me the next task.</p>
<p>579<br>01:01:04,000 –&gt; 01:01:14,000<br>You sort of increase what the boundary is for the morsels so that eventually you, the amount of work you do, the amount of time it takes to process that morsel is one level second.</p>
<p>580<br>01:01:14,000 –&gt; 01:01:22,000<br>Okay, so you’re not changing like a given morsel efforts created? It’s like for future morsels you’re going to make them bigger to better take a bit.</p>
<p>581<br>01:01:22,000 –&gt; 01:01:31,000<br>It’s the same as you’re not changing morsels after trade and you’re just making future morsels bigger. Yes, but again, make sure we clear what we say creation.</p>
<p>582<br>01:01:31,000 –&gt; 01:01:39,000<br>It’s just a logical boundary. It’s not like I’m copying data and making it bigger. I’m just saying like, you know, like here’s how to cut things off.</p>
<p>583<br>01:01:39,000 –&gt; 01:01:43,000<br>Okay, I’m just saying like you don’t go to, you don’t go back to something’s already running. Hey, here’s 10 more tuples you didn’t have before.</p>
<p>584<br>01:01:43,000 –&gt; 01:01:51,000<br>I actually changed this value because I was asking, we’re never changing the boundary for it, because it’s already running. It’s like we changed the boundary for other things that haven’t been processed.</p>
<p>585<br>01:01:51,000 –&gt; 01:01:54,000<br>For the remaining parts of the data table. Yes.</p>
<p>586<br>01:01:54,000 –&gt; 01:02:02,000<br>So there’s some sort of small size that I start talking about and then it just keeps rolling until one of them takes longer than one of those.</p>
<p>587<br>01:02:02,000 –&gt; 01:02:09,000<br>Yes, saving is correct. Is it just that you start with a small size and say, okay, here’s the amount of work you’re going to do in a task.</p>
<p>588<br>01:02:09,000 –&gt; 01:02:18,000<br>It’s going to be the morsel is to be 100,000 tuples. But then if you complete that in less than one millisecond, then the morsel size for the next thing you’re going to process will be a little bit bigger.</p>
<p>589<br>01:02:18,000 –&gt; 01:02:24,000<br>You keep making it a little bit bigger until you, well, exponentially bigger until you, your task takes one and one millisecond.</p>
<p>590<br>01:02:24,000 –&gt; 01:02:25,000<br>Yes.</p>
<p>591<br>01:02:25,000 –&gt; 01:02:32,000<br>What happens if like your splitting amount takes like, for example, like five milliseconds.</p>
<p>592<br>01:02:32,000 –&gt; 01:02:34,000<br>There’s never going to grow anymore. The total is not tricky.</p>
<p>593<br>01:02:34,000 –&gt; 01:02:39,000<br>For example, it takes like a lot of time. There’s some reason it’s going to start with really big morsel size.</p>
<p>594<br>01:02:39,000 –&gt; 01:02:45,000<br>Its question is, what if the, what do you start with, you know, 1 billion tuples per morsel and you’re really big, could you shrink it?</p>
<p>595<br>01:02:45,000 –&gt; 01:02:47,000<br>Yes. Why not?</p>
<p>596<br>01:02:47,000 –&gt; 01:02:52,000<br>I just like, I didn’t know that it was a part of the system design. Because it seemed like it was only growing.</p>
<p>597<br>01:02:52,000 –&gt; 01:02:54,000<br>Yeah, but that’s, that’s true.</p>
<p>598<br>01:02:54,000 –&gt; 01:02:58,000<br>Why do they want to use these?</p>
<p>599<br>01:02:58,000 –&gt; 01:03:02,000<br>Why do they want to do, why do they want to do one millisecond run?</p>
<p>600<br>01:03:02,000 –&gt; 01:03:12,000<br>Because it allows you to be more dynamic and not have a worker just avoid the straggler problem.</p>
<p>601<br>01:03:12,000 –&gt; 01:03:26,000<br>I think they, it’s, it is doing work ceiling, but not in the, how does it, it is doing work ceiling.</p>
<p>602<br>01:03:26,000 –&gt; 01:03:41,000<br>So it’s not, it’s work ceiling in that, there still would be a global cube, but there would be clever how they maintain it.</p>
<p>603<br>01:03:41,000 –&gt; 01:03:51,000<br>So when I got, when I got to say, what’s the next thing I want I need to go do, I got to go consider the location, the data, plus a priority of what the next thing I need to run.</p>
<p>604<br>01:03:51,000 –&gt; 01:04:01,000<br>Right? So like in, in the morsel’s approach, it was, this morsel has to be processed by this core because that it’s been assigned to that.</p>
<p>605<br>01:04:01,000 –&gt; 01:04:07,000<br>And the work ceiling part is, I’m allowed to run tasks that are for data that, that are not local to me.</p>
<p>606<br>01:04:07,000 –&gt; 01:04:14,000<br>So in this one, they’re doing the same thing, but they’re also not including the priority information about the, you know, about the query itself.</p>
<p>607<br>01:04:14,000 –&gt; 01:04:22,000<br>So it is, it is doing work ceiling, but it’s, it’s, it’s sort of a natural, natural is not the right word either.</p>
<p>608<br>01:04:22,000 –&gt; 01:04:26,000<br>It just, it just happens because the way they’re maintaining the cube.</p>
<p>609<br>01:04:26,000 –&gt; 01:04:32,000<br>So, why is the goal to make it one second, one second, like, different ones, is it like, two thousand, or so?</p>
<p>610<br>01:04:32,000 –&gt; 01:04:37,000<br>Yeah, it says, it says, why is it the goal to make it one millisecond per task? It is to balance it, absolutely, yes.</p>
<p>611<br>01:04:39,000 –&gt; 01:04:44,000<br>You still probably like to sign before the query’s actually a horizontal plane?</p>
<p>612<br>01:04:44,000 –&gt; 01:04:51,000<br>It’s, I mean, what we’ll see prior to the second question is, is the priority assigned when the query, when the query starts? Yes.</p>
<p>613<br>01:04:51,000 –&gt; 01:04:57,000<br>Like, everyone starts with like one, and the long you run, then that decays.</p>
<p>614<br>01:04:57,000 –&gt; 01:05:02,000<br>How do you, how do you, you try to, the short run, like, switch 12 and 11, like, switch 12 and 11, like, switch 12?</p>
<p>615<br>01:05:02,000 –&gt; 01:05:08,000<br>In question, how do you make sure that the short running query is finished more quickly, and the long running query is keep running?</p>
<p>616<br>01:05:08,000 –&gt; 01:05:14,000<br>No, like, it seems like, the short running, like, you, you guarantee the short running, like, switch 15. Yes?</p>
<p>617<br>01:05:14,000 –&gt; 01:05:17,000<br>How do you guarantee that the long run doesn’t get starved?</p>
<p>618<br>01:05:17,000 –&gt; 01:05:24,000<br>How do you guarantee that the long run doesn’t get starved? Because the striped scheduling will handle that. There’s this notion of a pass.</p>
<p>619<br>01:05:24,000 –&gt; 01:05:36,000<br>If I haven’t, if the, this sort of global counter, this watermark keeps ticking forward, and if my query’s below that, then I get an omelet, loud run again.</p>
<p>620<br>01:05:36,000 –&gt; 01:05:45,000<br>And any new query that shows up, the shorter running queries that show up, they’re going to be assigned a watermark that’s above that global one, so that they’ll, they’ll be starved out.</p>
<p>621<br>01:05:45,000 –&gt; 01:05:46,000<br>Yes?</p>
<p>622<br>01:05:46,000 –&gt; 01:06:00,000<br>So, in the most of these, the most of the, the most of the, and buffers, the prioritized, like, if there was, they would actually be brought to with water in the buffer, like, that cost the prioritized for that given input.</p>
<p>623<br>01:06:00,000 –&gt; 01:06:01,000<br>Yes.</p>
<p>624<br>01:06:01,000 –&gt; 01:06:11,000<br>So, if the priority is, you know, really for these workers, the low-category of data in the most of the, about this form, or they’re interested.</p>
<p>625<br>01:06:11,000 –&gt; 01:06:21,000<br>So, the question is, in the morsels case, the priority was, was based on what data, you know, what data I need access in the morsels, and where am I going to write it to?</p>
<p>626<br>01:06:21,000 –&gt; 01:06:34,000<br>But in, in this case, am I not having the notion of locality? You do. And that, that would be, you would have a local priority. We’ll, we’ll cover that in a second.</p>
<p>627<br>01:06:34,000 –&gt; 01:06:56,000<br>So, let’s first describe how they’re going to avoid the global, the global task cube. And so, I mean, it still is a global task cube, but the, the state about whether or not I need to refer to it to figure out what, what actually changed is we maintained in, in thread local storage at every worker.</p>
<p>628<br>01:06:56,000 –&gt; 01:07:08,000<br>Again, assuming we’re running on a single node. So, there’ll be a global task set, but all this is just an array of pointers that tell you to go where to go find the information about the tasks that you have for a given query.</p>
<p>629<br>01:07:08,000 –&gt; 01:07:23,000<br>And then, with any worker, there’s going to be these masks that keep track of which, which slots in my, my slot array of above are active, whether, and then change mask, return mask, tell me whether something has changed above, and whether I should go confer it.</p>
<p>630<br>01:07:23,000 –&gt; 01:07:52,000<br>And so, what they’re going to do is they’re going to have their different workers across different threads are allowed to go right into the memory of these, this information for the other workers as well, but they’re going to be a, to a time of comparing swap operations, just to flip bits, or basically, XORs and in the single instruction, and that, you know, yes, there’s cash line and validation across interconnects, but that’s not, it’s not like you’re copying a bunch of data, you’re just doing one, you know, one comparing swap over the network.</p>
<p>631<br>01:07:52,000 –&gt; 01:08:06,000<br>So, in my example, here, I’m going to show that we have four slots in our, there could be active any time, I think in the paper they talk about, have 128 slots, and that’s just to bound how much work can be run actually running at a given time.</p>
<p>632<br>01:08:06,000 –&gt; 01:08:13,000<br>And again, the classical stride scheduling is you allow, the number of slots is unbounded.</p>
<p>633<br>01:08:13,000 –&gt; 01:08:26,000<br>Right, so again, the global task slot, that’s, task that slots, these are just pointers to where to go find the metadata in memory about what these queries are actually running.</p>
<p>634<br>01:08:26,000 –&gt; 01:08:35,000<br>So, let’s look at an example of when a query finishes and when a new query arrives, or when it’s, sorry, when a task set finishes.</p>
<p>635<br>01:08:35,000 –&gt; 01:08:43,000<br>So, let’s say that worker one here, he’s running Q1, task set one, worker two is running Q2, task set one.</p>
<p>636<br>01:08:43,000 –&gt; 01:08:55,000<br>So, when this thing finishes, we then need to go back up to this task set slot array, follow the pointer to go look and say, okay, is there something else I should be doing for this, this task set for the query.</p>
<p>637<br>01:08:55,000 –&gt; 01:09:03,000<br>And let’s say in this case here, we’ve completed all, we’ve processed all the morsels, so we know that this thing is done.</p>
<p>638<br>01:09:03,000 –&gt; 01:09:10,000<br>So, then now the worker thread is then responsible for them taking the next task set and we’re putting that back in the queue.</p>
<p>639<br>01:09:10,000 –&gt; 01:09:22,000<br>Right, but now we need, we want to notify all the workers that hey, something has changed in this task set queue up here, so let’s go find out what it is, because we want them to pull it and not have to push it.</p>
<p>640<br>01:09:23,000 –&gt; 01:09:30,000<br>Right, because you have to, you’re not pushing things, you have to maintain latches to make sure that you’re not overwriting information inappropriately.</p>
<p>641<br>01:09:30,000 –&gt; 01:09:39,000<br>So, all we need to do now is just update this return mask, we just do a compare and swap at each thread to now say set a one to this slot.</p>
<p>642<br>01:09:40,000 –&gt; 01:09:53,000<br>And then next time the worker comes back around says, okay, I need to go, I need to do something, I need another task to compute on, it knows that he’s going to check the task queue to find out new information that somebody posted about it.</p>
<p>643<br>01:09:53,000 –&gt; 01:10:03,000<br>It’s like a message board saying, hey, by the way, here’s a change, I’m not telling you what it is, it’s like a new email notification, I’m not telling you what it is, but you know where to go look for that information.</p>
<p>644<br>01:10:04,000 –&gt; 01:10:25,000<br>Alright, so now let’s say queue three shows up query three, so it ends up getting put into the global task slot in this position here, and again some other thread, like a scheduled thread or a coordinated thread, is responsible for then flipping a bit in the change mask for all these threads, say hey, by the way, there’s a new query to show up in this slot.</p>
<p>645<br>01:10:25,000 –&gt; 01:10:31,000<br>And the return mask and the change mask, because there’s some bookkeeping reason you have to do this, yes.</p>
<p>646<br>01:10:34,000 –&gt; 01:10:45,000<br>It’s not a special thread like, hey, here’s this task, there’s something about the query that shows up, and somebody’s got to then put that in the queue.</p>
<p>647<br>01:10:45,000 –&gt; 01:11:06,000<br>So whatever that is, you could call that a coordinator or a scheduler thread, but whatever that thread is not responsible for saying you’re going to do this, you’re going to do that, they’re all pooling with themselves, but you just need something to flip the bit and say, by the way, we added something new, make sure you go check it out.</p>
<p>648<br>01:11:06,000 –&gt; 01:11:08,000<br>Back, yes.</p>
<p>649<br>01:11:10,000 –&gt; 01:11:22,000<br>The question is, with the dispatcher, will you be responsible flipping the bits in the return mask? No, as far as I know, the thread that was responsible for putting the data, that computer was all, is responsible flipping the bits in everyone.</p>
<p>650<br>01:11:25,000 –&gt; 01:11:31,000<br>The question is, why not let the dispatcher handle that, because now you have to go tell the dispatcher to go do it. It’s just cheaper to go do it yourself.</p>
<p>651<br>01:11:32,000 –&gt; 01:11:35,000<br>I assume somewhere we’re not going to answer that.</p>
<p>652<br>01:11:36,000 –&gt; 01:11:38,000<br>The question is, why aren’t we doing push versus pull?</p>
<p>653<br>01:11:39,000 –&gt; 01:11:54,000<br>Because when the worker is trying to come from the global task set starts, there is locking, yes, we can’t get it that, but it’s basically cycle to go and ask and look into the global task set, hey, what do I need to do?</p>
<p>654<br>01:11:54,000 –&gt; 01:12:03,000<br>And if there is already a thread that’s running and changing the block, it’s a bit, it might work you track of what you work and do, and then tell it to do something.</p>
<p>655<br>01:12:04,000 –&gt; 01:12:13,000<br>So what’s David is, if something’s already responsible for putting things in the global task queue, why not just have that thing responsible for telling people what to do?</p>
<p>656<br>01:12:13,000 –&gt; 01:12:41,000<br>But again, you’ve got to maintain that, you have to maintain the state somewhere, and they’re arguing that it’s better to distribute it across the different workers in TLS, and have that be, and then to do simple repair and swaps to maintain, to notify them the changes that are occurring, rather than having a more heavy-weight approach of farming out complex messages that they need to process themselves.</p>
<p>657<br>01:12:43,000 –&gt; 01:12:57,000<br>It’s a cooperative scheduling, so rather than having one thread be responsible for everything, and potentially that could be more efficient to do with a swan number of cores, but this approach is definitely more scalable, or a larger number of cores.</p>
<p>658<br>01:12:57,000 –&gt; 01:13:26,000<br>Okay. Right, so then this thing knows that when this finishes the task that was running, goes back up, looks in the queue, decides that, you know, for whatever reason, that, you know, we’ll get the information looking at the change master and return master, what needs update, it updates its active slot now to say, hey, there’s something in one I could go take, and then it decides to run Q3, test at one.</p>
<p>659<br>01:13:27,000 –&gt; 01:13:41,000<br>And notice here now on Worker 2, it doesn’t know about the Q3 yet, it just knows that a bit got flipped in the first slot, and eventually I’m going to go back and look at the test set, a queue, I’ll go learn what that is.</p>
<p>660<br>01:13:41,000 –&gt; 01:13:45,000<br>So these things can run independently of each other, and not have to coordinate across all of them.</p>
<p>661<br>01:13:45,000 –&gt; 01:13:48,000<br>Which arguably always going to be better.</p>
<p>662<br>01:13:49,000 –&gt; 01:13:54,000<br>Right, so we’re not going to be a few minutes.</p>
<p>663<br>01:13:54,000 –&gt; 01:14:17,000<br>I’ll skip the priority to like, but basically think about as like, there’s this notion of the global pass, just thinking that the number of times I’ve passed through are at executed things, and then I have priorities about individual queries, I have local priorities based on how much work I’ve done for this query, and the combination of these things then determines.</p>
<p>664<br>01:14:18,000 –&gt; 01:14:27,000<br>What you want to run, but idea again, the highlight is that as a query runs longer, this priority decays and goes down.</p>
<p>665<br>01:14:27,000 –&gt; 01:14:40,000<br>So I’m quickly zoomed through HANA, again this is just a, this is the other end of the complexity, so I think like Postgres is the easiest one, you just say, let the OS do it.</p>
<p>666<br>01:14:40,000 –&gt; 01:14:55,000<br>The HANA approach here, which again, I think it was a PG dissertation as somebody that worked at SAP, so I don’t think this ever actually made it in the real HANA system, especially at the rewrote it in the late 2010s.</p>
<p>667<br>01:14:55,000 –&gt; 01:15:08,000<br>But just the idea is that like again, I’ve had it just happen to do even more scousing on its own for initial threads and not let the OS do any of that.</p>
<p>668<br>01:15:08,000 –&gt; 01:15:25,000<br>So this is going to support both workload stealing and poor scaling, meaning within a single socket, a single Numer region, I can add more threads dynamically and not have the limitation of having one thread per CPU core.</p>
<p>669<br>01:15:25,000 –&gt; 01:15:32,000<br>I can start adding more and more cores if I think things are going to get stalled on doing, you know, for a variety of reasons.</p>
<p>670<br>01:15:33,000 –&gt; 01:15:48,000<br>And then I’m going to have this notion of a two different kind of cues of work, I’m going to have a soft cue and a hard cue, a hard cue is going to be tasks that you don’t anybody to steal that has to run in that socket or that Numer region.</p>
<p>671<br>01:15:49,000 –&gt; 01:15:59,000<br>So I think of something like garbage collection for data that’s in that Numer region, you don’t want to have that go in or connect or like a networking task that has to run on a given socket.</p>
<p>672<br>01:15:59,000 –&gt; 01:16:07,000<br>But the soft cue will be things that workers are allowed to steal, similar to the hybrid approach when you’re doing scans.</p>
<p>673<br>01:16:07,000 –&gt; 01:16:15,000<br>So everybody said this, we’re going to have the soft and hard priority cues, but then they’re going to have different four different worker poles of threads.</p>
<p>674<br>01:16:15,000 –&gt; 01:16:24,000<br>So you have worker threads that are actively running something in active ones that are blocked in the kernel waiting for some kind of conditional, conditional lock or conditional variable latch.</p>
<p>675<br>01:16:24,000 –&gt; 01:16:27,000<br>Then you have ones that are free that I wake up a little bit, check to see what they’re going to do.</p>
<p>676<br>01:16:27,000 –&gt; 01:16:37,000<br>And then you have a parked threads where you’ve actually disccedured them and you hand them back to the OS kernel like a sleeper yield.</p>
<p>677<br>01:16:37,000 –&gt; 01:16:40,000<br>And they are sitting down there and then if you need them, you can spend them up.</p>
<p>678<br>01:16:40,000 –&gt; 01:16:48,000<br>And the argument here is that it’s cheaper to go put some threads down in the scheduler in the OS and let them to sleep down there.</p>
<p>679<br>01:16:48,000 –&gt; 01:16:56,000<br>So that when I need them, I can pick them up and start running with them compared to having to spin up a whole process of spinning a whole thread if I all said I need more of that.</p>
<p>680<br>01:16:57,000 –&gt; 01:17:01,000<br>So let me skip this real quick.</p>
<p>681<br>01:17:01,000 –&gt; 01:17:06,000<br>Basic idea works the same thing as before that we have a bunch of stuff we want to want to execute for a query.</p>
<p>682<br>01:17:06,000 –&gt; 01:17:12,000<br>But now I’m including some maintenance tasks like garbage collection for multi-version concurrent control because HANA was an MPCC system.</p>
<p>683<br>01:17:12,000 –&gt; 01:17:24,000<br>So these queries here, they have to run, you know, you need to run them right now, but they can go into the soft cue because, again, technically any new Osaka, any region can run them.</p>
<p>684<br>01:17:25,000 –&gt; 01:17:28,000<br>But then the GCC stuff, say we’ll put that in the hard cue.</p>
<p>685<br>01:17:28,000 –&gt; 01:17:32,000<br>And then the working threads are responsible for executing these acts of tasks.</p>
<p>686<br>01:17:32,000 –&gt; 01:17:44,000<br>And then the inactive ones, again, these are things that haven’t, that are, that are, and active is blocked on something that, like in the kernels that we can’t actually start executing them, but we expect them to wake up fairly soon.</p>
<p>687<br>01:17:44,000 –&gt; 01:17:51,000<br>Free is going to be one that is just spinning all the time looking for work to do and the part of the ones that are heavyweight paused down the kernel.</p>
<p>688<br>01:17:52,000 –&gt; 01:17:58,000<br>So again, the free ones are allowed to pull this all the time and they find something that is allowed to execute it.</p>
<p>689<br>01:17:58,000 –&gt; 01:18:12,000<br>So the HANA guys are going to claim that in their experimentation with this approach that they, it was better for the large, large socket machines to turn off all the work stealing.</p>
<p>690<br>01:18:12,000 –&gt; 01:18:22,000<br>So you basically don’t put everything always in the hard cue and that was always better than moving things across different, different, different newer regions.</p>
<p>691<br>01:18:22,000 –&gt; 01:18:30,000<br>I would argue that I think the, I like this. I like having all the stuff manage yourself instead of OS doing it.</p>
<p>692<br>01:18:31,000 –&gt; 01:18:40,000<br>And so for the inactive ones, again, for, for, for, oh, that this maybe this is less of an issue, but for an O2B system this, this makes sense.</p>
<p>693<br>01:18:40,000 –&gt; 01:18:47,000<br>And HANA was trying to support with O lab N O O T B. So it made sense to have the sort of different variations of threading.</p>
<p>694<br>01:18:47,000 –&gt; 01:18:56,000<br>All right, I’m going to run through this really fast. I want to show you one last thing about in SQL Server, because to me this is, this is relevant for high design systems.</p>
<p>695<br>01:18:56,000 –&gt; 01:19:01,000<br>What? Is it OS like these designed for the 90s?</p>
<p>696<br>01:19:01,000 –&gt; 01:19:08,000<br>SQL OS? Yeah. SQL, it’s not a full operating system. Right?</p>
<p>697<br>01:19:08,000 –&gt; 01:19:22,000<br>So SQL OS is an abstraction layer in SQL Server that they built in 2006 that hides the load level details of hardware and the operating system from the upper parts of the database system.</p>
<p>698<br>01:19:22,000 –&gt; 01:19:31,000<br>And the reason why they built this is that Microsoft observed that every time new hardware was coming out, they had to rewrite all their operations to account for whatever the hardware was.</p>
<p>699<br>01:19:31,000 –&gt; 01:19:35,000<br>Like, you have a bunch more cores, you have to rewrite a bunch of stuff, they had a newer regions that rewrite a bunch of stuff.</p>
<p>700<br>01:19:35,000 –&gt; 01:19:45,000<br>And so this abstraction layer allows them to hide those load level details and the scheduling and the, and the, and the movement of, of data can all be managed by this SQL OS thing.</p>
<p>701<br>01:19:45,000 –&gt; 01:19:53,000<br>So it’s not a full operating system like the Linux kernel or Windows or HBox, whatever. Right? It’s just sort of an abstraction layer.</p>
<p>702<br>01:19:53,000 –&gt; 01:19:59,000<br>But the cool thing that they’re going to do is going to do non-parental thread scheduling inside the database system.</p>
<p>703<br>01:19:59,000 –&gt; 01:20:02,000<br>What’s another word for non-parental thread schedule?</p>
<p>704<br>01:20:04,000 –&gt; 01:20:08,000<br>Covertines. Same idea. Right?</p>
<p>705<br>01:20:08,000 –&gt; 01:20:16,000<br>Where you have the, you have a thread that you’re managing threading multi-threading within within the database system itself.</p>
<p>706<br>01:20:16,000 –&gt; 01:20:24,000<br>But since, since it’s not preemptive, like preemptive means the OS can go to steal you take, you know, take your hardware thread and give it to somebody else.</p>
<p>707<br>01:20:24,000 –&gt; 01:20:29,000<br>It’s, we’re all running, all these threads are running within the database themselves, so we can’t do that. We can’t send it interrupt to ourselves.</p>
<p>708<br>01:20:29,000 –&gt; 01:20:37,000<br>Like that, right? So that means that in our code itself, we’re going to have to go put explicit instructions to yield back to the scheduler to say,</p>
<p>709<br>01:20:37,000 –&gt; 01:20:42,000<br>hey, by the way, go check to see whether something else could be running instead of me.</p>
<p>710<br>01:20:42,000 –&gt; 01:20:49,000<br>And they did this on two back in 2006 before seamless loss and go and other programming languages now have, you know, built in support for covertines.</p>
<p>711<br>01:20:49,000 –&gt; 01:21:00,000<br>So there’s a great article here at how they built it. I used to say that SQLS allowed Microsoft to make it to get SQL server to run on Linux.</p>
<p>712<br>01:21:00,000 –&gt; 01:21:09,000<br>And I, because again, abstracted the, abstracted the, the OS layer, the guy that built SQLS then called me and said, that’s not true.</p>
<p>713<br>01:21:09,000 –&gt; 01:21:14,000<br>Actually, their attempt to kind of get SQL server running in Docker turned out to be what got them to be able to support it.</p>
<p>714<br>01:21:14,000 –&gt; 01:21:18,000<br>But there’s a good article that talks about it. Again, real quickly, let me just show you what it looks like.</p>
<p>715<br>01:21:18,000 –&gt; 01:21:21,000<br>So you have some SQL query like this. They can set their quantum to four milliseconds.</p>
<p>716<br>01:21:21,000 –&gt; 01:21:25,000<br>So, so say you want to do a sequential scan of this data.</p>
<p>717<br>01:21:25,000 –&gt; 01:21:28,000<br>Again, approximate query plan would look like this. Evalopratic it, admit it.</p>
<p>718<br>01:21:28,000 –&gt; 01:21:34,000<br>What they’re letting you do is keep track of the time a different parts of the operator while they run.</p>
<p>719<br>01:21:34,000 –&gt; 01:21:40,000<br>And they check to see whether the, the elapsed time since they started running is greater than the quantum for milliseconds.</p>
<p>720<br>01:21:40,000 –&gt; 01:21:42,000<br>If yes, they yield back.</p>
<p>721<br>01:21:42,000 –&gt; 01:21:48,000<br>Now, this is pseudo code. You would not want to do this. It’s like, you know, going check in the system clock is expensive. Don’t do that.</p>
<p>722<br>01:21:48,000 –&gt; 01:21:51,000<br>Right. There’s, there’s, there’s hardware instructions to hide that for you.</p>
<p>723<br>01:21:51,000 –&gt; 01:21:56,000<br>But basically again, if I, if I, if I know I’m running longer than I need to, I’ll go yield.</p>
<p>724<br>01:21:56,000 –&gt; 01:22:02,000<br>You can do this for other things in your data system too. Like if you’re going to go try to acquire a lock, the lock’s not available,</p>
<p>725<br>01:22:02,000 –&gt; 01:22:05,000<br>set it your thread spinning and waiting try to try that lock, you yield back.</p>
<p>726<br>01:22:05,000 –&gt; 01:22:11,000<br>But then again, because of the data systems controls everything, we yield back with not just like, you know, you yield to the OS.</p>
<p>727<br>01:22:11,000 –&gt; 01:22:15,000<br>What do you say yield? That’s it. And the database system, we can say, I’m yielding back to the scheduler.</p>
<p>728<br>01:22:15,000 –&gt; 01:22:20,000<br>And oh, by the way, I need this lock. Don’t schedule me until that lock is now available.</p>
<p>729<br>01:22:20,000 –&gt; 01:22:26,000<br>And when it does, the OS can put you back. Right. So again, complete control of everything if we do this ourselves.</p>
<p>730<br>01:22:26,000 –&gt; 01:22:30,000<br>So SQLS is probably the first one that did this. There’s other systems that do this now.</p>
<p>731<br>01:22:30,000 –&gt; 01:22:32,000<br>CeliaDB does has this framework called C-star.</p>
<p>732<br>01:22:32,000 –&gt; 01:22:37,000<br>Phonodini is a poor man’s version of this. Basically, every time before they read something from disk, they just yield.</p>
<p>733<br>01:22:37,000 –&gt; 01:22:44,000<br>That’s it. Right. And the core base is a experimental system out of silent-fraging university that explicitly does built-in-tired cover teens.</p>
<p>734<br>01:22:44,000 –&gt; 01:22:51,000<br>And there’s a video from the CeliaDB guys from a few years ago that talks about the C-star thing.</p>
<p>735<br>01:22:51,000 –&gt; 01:22:56,000<br>Okay. We’re well over time.</p>
<p>736<br>01:22:56,000 –&gt; 01:23:01,000<br>Distribute scheduling, basically, all the same problems. We’re now over the network. Right.</p>
<p>737<br>01:23:01,000 –&gt; 01:23:07,000<br>I mean, you laugh. That’s really what it is. And then we’ll cover work list, like dynamic scaling later.</p>
<p>738<br>01:23:07,000 –&gt; 01:23:12,000<br>But we’ll see this snowflake can do both. So systems can do ones or the other.</p>
<p>739<br>01:23:12,000 –&gt; 01:23:16,000<br>All right. May take away. Data data systems are beautiful.</p>
<p>740<br>01:23:16,000 –&gt; 01:23:20,000<br>Don’t let the Aubrey system tell you, bossy around. Don’t let the Aubrey system try to do anything.</p>
<p>741<br>01:23:20,000 –&gt; 01:23:24,000<br>You want to do everything yourself. I think the C-go-S approach is probably the right way to do it.</p>
<p>742<br>01:23:24,000 –&gt; 01:23:29,000<br>I think it’s overkill for what we’re doing in this class. But, you know, it’s beautiful.</p>
<p>743<br>01:23:29,000 –&gt; 01:23:32,000<br>Okay. What’s that? Strong will, too, yes.</p>
<p>744<br>01:23:32,000 –&gt; 01:23:37,000<br>Next class, we’ll do another one, hash joins. And then on Monday next week, we’ll do multi-late joins.</p>
<p>745<br>01:23:37,000 –&gt; 01:23:41,000<br>We’ll do a quick overview of performance counters that we talked about.</p>
<p>746<br>01:23:41,000 –&gt; 01:23:47,000<br>Somebody had a question about, we’ll do that next week on Monday. And then Wednesday next week will be the project status updates.</p>
<p>747<br>01:23:47,000 –&gt; 01:23:48,000<br>Okay.</p>
<p>748<br>01:24:11,000 –&gt; 01:24:13,000<br>Okay.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CMU15721 P9S202408 QuerySchedulingCoordinationCMUAdvancedDatabaseSystems</div>
      <div>http://example.com/2025/10/25/CMU15721 P9S202408-QuerySchedulingCoordinationCMUAdvancedDatabaseSystems/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月25日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/25/CS144-networkP1007-6PhysicalandLink/" title="CS144 NetworkP1007 6PhysicalandLink">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CS144 NetworkP1007 6PhysicalandLink</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/25/CMU15721%20P7S202406-VectorizedQueryExecutionUsingSIMDCMUAdvancedDatabaseSystems/" title="CMU15721 P7S202406 VectorizedQueryExecutionUsingSIMDCMUAdvancedDatabaseSystems">
                        <span class="hidden-mobile">CMU15721 P7S202406 VectorizedQueryExecutionUsingSIMDCMUAdvancedDatabaseSystems</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
