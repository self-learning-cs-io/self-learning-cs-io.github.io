

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="100:00:00,000 –&gt; 00:00:04,000we’re talking about is Spark. 200:00:04,000 –&gt; 00:00:08,480And so this goes back to almost the beginning of the semester. 300:00:08,480 –&gt; 00:00:11,200We talked a">
<meta property="og:type" content="article">
<meta property="og:title" content="MIT6824 P17Lecture16 BigData Spark">
<meta property="og:url" content="http://example.com/2025/10/25/MIT6824%20P17Lecture16-BigData-Spark/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="100:00:00,000 –&gt; 00:00:04,000we’re talking about is Spark. 200:00:04,000 –&gt; 00:00:08,480And so this goes back to almost the beginning of the semester. 300:00:08,480 –&gt; 00:00:11,200We talked a">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-25T05:03:39.802Z">
<meta property="article:modified_time" content="2025-10-25T05:03:39.802Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>MIT6824 P17Lecture16 BigData Spark - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="MIT6824 P17Lecture16 BigData Spark"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-25 13:03" pubdate>
          2025年10月25日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          9.4k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          79 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">MIT6824 P17Lecture16 BigData Spark</h1>
            
            
              <div class="markdown-body">
                
                <p>1<br>00:00:00,000 –&gt; 00:00:04,000<br>we’re talking about is Spark.</p>
<p>2<br>00:00:04,000 –&gt; 00:00:08,480<br>And so this goes back to almost the beginning of the semester.</p>
<p>3<br>00:00:08,480 –&gt; 00:00:11,200<br>We talked a quite a bit about MapReduce.</p>
<p>4<br>00:00:11,200 –&gt; 00:00:15,839<br>In fact, you implement MapReduce.</p>
<p>5<br>00:00:15,839 –&gt; 00:00:21,359<br>So really, what’s informally Spark is basically</p>
<p>6<br>00:00:21,359 –&gt; 00:00:28,879<br>the successor to Hadub.</p>
<p>7<br>00:00:28,879 –&gt; 00:00:38,719<br>And Hadub is the sort of open-source version of MapReduce.</p>
<p>8<br>00:00:38,719 –&gt; 00:00:46,799<br>So I think today people typically will use Spark instead of Hadub.</p>
<p>9<br>00:00:46,799 –&gt; 00:00:48,320<br>And so it’s really widely used.</p>
<p>10<br>00:00:52,799 –&gt; 00:00:56,039<br>And it’s widely used for data science computation.</p>
<p>11<br>00:00:56,039 –&gt; 00:00:58,679<br>So people that have lots and lots of data</p>
<p>12<br>00:00:58,679 –&gt; 00:01:01,479<br>that need to run some computation over it, require</p>
<p>13<br>00:01:01,479 –&gt; 00:01:03,320<br>ton of machines.</p>
<p>14<br>00:01:03,320 –&gt; 00:01:08,039<br>Spark is designed for that particular case.</p>
<p>15<br>00:01:08,039 –&gt; 00:01:10,280<br>It is commercialized by a company called Databricks.</p>
<p>16<br>00:01:14,039 –&gt; 00:01:17,480<br>Matei Sahariya, who is the main offer of this paper,</p>
<p>17<br>00:01:17,480 –&gt; 00:01:21,079<br>his PhD thesis started with a number of other people</p>
<p>18<br>00:01:21,079 –&gt; 00:01:25,879<br>with company Databricks, which commercializes Spark.</p>
<p>19<br>00:01:25,879 –&gt; 00:01:33,079<br>But it also supports the Apache open-source Spark version.</p>
<p>20<br>00:01:33,079 –&gt; 00:01:36,359<br>There’s a pretty popular open-source project,</p>
<p>21<br>00:01:36,359 –&gt; 00:01:39,480<br>or very popular open-source project.</p>
<p>22<br>00:01:39,480 –&gt; 00:01:44,680<br>It is, one reason has replaced the use of Adub is because it’s</p>
<p>23<br>00:01:44,680 –&gt; 00:01:49,400<br>actually supports a wider range of applications</p>
<p>24<br>00:01:49,400 –&gt; 00:01:52,520<br>that is MapReduce.</p>
<p>25<br>00:01:52,520 –&gt; 00:01:57,160<br>And the particular is going to be very good at these iterative</p>
<p>26<br>00:01:57,160 –&gt; 00:01:57,800<br>what happened there.</p>
<p>27<br>00:02:01,000 –&gt; 00:02:04,360<br>Let me see something that will be next year or no crash.</p>
<p>28<br>00:02:04,360 –&gt; 00:02:05,160<br>Hold on a second.</p>
<p>29<br>00:02:19,400 –&gt; 00:02:35,400<br>Okay.</p>
<p>30<br>00:02:35,400 –&gt; 00:02:37,400<br>Okay.</p>
<p>31<br>00:02:37,400 –&gt; 00:02:41,400<br>Fortunately, I have a good check.</p>
<p>32<br>00:02:41,400 –&gt; 00:02:45,400<br>Okay, so it supports a wider range of applications.</p>
<p>33<br>00:02:45,400 –&gt; 00:02:47,400<br>And the particular is good at these iterative applications.</p>
<p>34<br>00:02:47,400 –&gt; 00:02:51,400<br>So applications were multiple rounds of MapReduce operations.</p>
<p>35<br>00:02:51,400 –&gt; 00:02:55,400<br>So if you have an application that requires sort of one set of MapReduce,</p>
<p>36<br>00:02:55,400 –&gt; 00:02:57,400<br>followed by another set of MapReduce, followed by another MapReduce,</p>
<p>37<br>00:02:57,400 –&gt; 00:02:59,400<br>followed by another series of competition, Spark is really good at it.</p>
<p>38<br>00:02:59,400 –&gt; 00:03:03,400<br>And the reason it’s so good at it is because basically it keeps the intermediate results in memory.</p>
<p>39<br>00:03:03,400 –&gt; 00:03:07,400<br>And that’s really good support, programming support for doing so.</p>
<p>40<br>00:03:07,400 –&gt; 00:03:15,400<br>And in some ways, if there’s any connection at all between the previous paper and this paper,</p>
<p>41<br>00:03:15,400 –&gt; 00:03:25,400<br>which basically is not, but they’re all both targeted to sort of in memory competitions.</p>
<p>42<br>00:03:25,400 –&gt; 00:03:31,400<br>For data sets that basically fit in memory, in the previous paper,</p>
<p>43<br>00:03:31,400 –&gt; 00:03:35,400<br>and the farm paper is all about the database fitting in memory.</p>
<p>44<br>00:03:35,400 –&gt; 00:03:41,400<br>Here is the data set of the data science competition for the data science competition that you want to do.</p>
<p>45<br>00:03:41,400 –&gt; 00:03:47,400<br>Of course, since 2012, when you were this paper was published, a lot of things didn’t happen.</p>
<p>46<br>00:03:47,400 –&gt; 00:03:55,400<br>The Spark is not really tight to Scala, as I sort of described in this paper,</p>
<p>47<br>00:03:55,400 –&gt; 00:03:57,400<br>but there are other language frontends, for example.</p>
<p>48<br>00:03:57,400 –&gt; 00:04:10,400<br>But probably more importantly, the RDDs, as defined in this paper, are slightly deprecated and replaced by data frames.</p>
<p>49<br>00:04:10,400 –&gt; 00:04:18,399<br>But data frames, the way to think about it, the way I think about it, is basically column is an RDD with explicit columns.</p>
<p>50<br>00:04:18,399 –&gt; 00:04:24,399<br>And all the good ideas of RDDs are also true for data frames.</p>
<p>51<br>00:04:24,399 –&gt; 00:04:28,399<br>And so from the rest of the lecture, I’m just going to talk about RDDs,</p>
<p>52<br>00:04:28,399 –&gt; 00:04:36,399<br>and think about them as equivalently to data frames.</p>
<p>53<br>00:04:36,399 –&gt; 00:04:46,399<br>Any questions before I proceed?</p>
<p>54<br>00:04:46,399 –&gt; 00:04:50,399<br>And then quick out of point, maybe this research is really quite successful.</p>
<p>55<br>00:04:50,399 –&gt; 00:05:00,399<br>And you’re certainly going to use, also, but they’re going to receive the ACM Doctoral Feuses award for it.</p>
<p>56<br>00:05:00,399 –&gt; 00:05:04,399<br>Or it is feces that basically is all about Spark.</p>
<p>57<br>00:05:04,399 –&gt; 00:05:16,399<br>So it’s quite unusual, actually, for, you know, doctoral feces that have that kind of impact.</p>
<p>58<br>00:05:16,399 –&gt; 00:05:18,399<br>Okay.</p>
<p>59<br>00:05:18,399 –&gt; 00:05:28,399<br>So the way I want to talk about Spark is by just looking at the, so many examples, because I think you get the best.</p>
<p>60<br>00:05:28,399 –&gt; 00:05:32,399<br>You understand the programming model.</p>
<p>61<br>00:05:32,399 –&gt; 00:05:40,399<br>And that is based on RDDs, best by just, I think, looking at examples.</p>
<p>62<br>00:05:40,399 –&gt; 00:05:44,399<br>And you get the best idea of what actually an RDD is.</p>
<p>63<br>00:05:44,399 –&gt; 00:05:48,399<br>So let me pull up some of the examples that are in the paper.</p>
<p>64<br>00:05:48,399 –&gt; 00:05:52,399<br>And then we’ll walk through those.</p>
<p>65<br>00:05:52,399 –&gt; 00:05:56,399<br>So let’s start here.</p>
<p>66<br>00:05:56,399 –&gt; 00:05:58,399<br>Very simple example.</p>
<p>67<br>00:05:58,399 –&gt; 00:06:02,399<br>And so, you know, the idea is that.</p>
<p>68<br>00:06:02,399 –&gt; 00:06:08,399<br>In this example is, you know, first of all, you can sort of use.</p>
<p>69<br>00:06:08,399 –&gt; 00:06:14,399<br>Spark interactively, you can see that they start up Spark in your workstation, where you’re locked up.</p>
<p>70<br>00:06:14,399 –&gt; 00:06:18,399<br>And start interacting with Spark.</p>
<p>71<br>00:06:18,399 –&gt; 00:06:22,399<br>And then the way, you know, you can type in commands like this.</p>
<p>72<br>00:06:22,399 –&gt; 00:06:24,399<br>And so, what does this command do?</p>
<p>73<br>00:06:24,399 –&gt; 00:06:32,399<br>You know, this basically creates an RDD, you know, called, this is RDDs, lines is an RDD.</p>
<p>74<br>00:06:32,399 –&gt; 00:06:36,399<br>And it just represents the RDD that actually stored in HTFS.</p>
<p>75<br>00:06:36,399 –&gt; 00:06:44,399<br>And you know, the HTFS, you know, might be have money, partitions, you know, for this particular file.</p>
<p>76<br>00:06:44,399 –&gt; 00:06:53,399<br>And then, you know, the first, you know, thousands of million records, they’re on like partition one, the next million living partition two and the next male live to in partition three.</p>
<p>77<br>00:06:53,399 –&gt; 00:07:00,399<br>And this RDD, lines basically represents, you know, that, that set of petitions.</p>
<p>78<br>00:07:00,399 –&gt; 00:07:07,399<br>When you run this line, where you type in this line, and you return, hit return, basically nothing really happens.</p>
<p>79<br>00:07:07,399 –&gt; 00:07:11,399<br>And so this is what the paper refers to as lazy computations.</p>
<p>80<br>00:07:11,399 –&gt; 00:07:15,399<br>In fact, the computation is executed some point later and see in the second one.</p>
<p>81<br>00:07:15,399 –&gt; 00:07:24,399<br>But at this particular point, the only thing that actually happened is that there is a line, lines object that happens to be an RDD.</p>
<p>82<br>00:07:24,399 –&gt; 00:07:30,399<br>And an RDD, you know, supports sort of a wide range of operations.</p>
<p>83<br>00:07:30,399 –&gt; 00:07:34,399<br>We can actually look in a little bit at some of the operations that supports.</p>
<p>84<br>00:07:34,399 –&gt; 00:07:41,399<br>Yeah, so it just as an RDD has an API.</p>
<p>85<br>00:07:41,399 –&gt; 00:07:47,399<br>And it turns out that the methods on the API or the methods on the RDD follow to two classes.</p>
<p>86<br>00:07:47,399 –&gt; 00:07:56,399<br>One, RDD, our actions and actions are really operations that will actually cause computation to happen.</p>
<p>87<br>00:07:56,399 –&gt; 00:08:01,399<br>And so all the lazily sort of build up computation really happens at the point that you run an action.</p>
<p>88<br>00:08:01,399 –&gt; 00:08:09,399<br>And that’s an example you run count to collect, then the, the, the, the spark computation action is executed.</p>
<p>89<br>00:08:09,399 –&gt; 00:08:14,399<br>All the other API or methods are transformations.</p>
<p>90<br>00:08:14,399 –&gt; 00:08:18,399<br>And they basically take one RDD and turn it into another RDD.</p>
<p>91<br>00:08:18,399 –&gt; 00:08:23,399<br>It turns out that every RDD is actually read only or immutable.</p>
<p>92<br>00:08:23,399 –&gt; 00:08:26,399<br>So you can’t modify an RDD.</p>
<p>93<br>00:08:26,399 –&gt; 00:08:31,399<br>You can only basically generate new RDDs for an existing one.</p>
<p>94<br>00:08:31,399 –&gt; 00:08:37,399<br>And so if we look at the second line, this basically creates a second RDD, the RDD errors.</p>
<p>95<br>00:08:37,399 –&gt; 00:08:44,399<br>And that one is created by running a filter on the lines RDD.</p>
<p>96<br>00:08:44,399 –&gt; 00:08:52,399<br>So the lines are the reason we don’t need, if we don’t need the, you can run this method to filter on it, in which in this case basically filters out.</p>
<p>97<br>00:08:52,399 –&gt; 00:08:58,399<br>All the records that start with, or all the lines that start with the message error with a string error.</p>
<p>98<br>00:08:58,399 –&gt; 00:09:01,399<br>And that really represents a new RDD.</p>
<p>99<br>00:09:01,399 –&gt; 00:09:04,399<br>And again, at this point, nothing actually is really being computed.</p>
<p>100<br>00:09:04,399 –&gt; 00:09:20,399<br>It’s usually like a recipe or a built over like almost like a data flow or what the paper calls a linear graph off the computation.</p>
<p>101<br>00:09:21,399 –&gt; 00:09:30,399<br>Also, when the computation actually starts running, it hasn’t run yet, but when it will start running, these operations are pipeline.</p>
<p>102<br>00:09:30,399 –&gt; 00:09:44,399<br>And with that, they mean that, so for example, in stage one, you know, the, this computation of the lines, the stage one will read some set of records, you know, from this first partition.</p>
<p>103<br>00:09:45,399 –&gt; 00:09:54,399<br>And then do its processing on it, if there’s anything and then the hand it off to stage two, you know, which is stage two is basically doing this filter.</p>
<p>104<br>00:09:54,399 –&gt; 00:10:06,399<br>And so in stage two, you know, the, the, this filter will run and so grab out the lines that actually are that match.</p>
<p>105<br>00:10:07,399 –&gt; 00:10:17,399<br>And then you know, basically produce with that new RDD that just contains, you know, the lines, you know, strings with that start with lines that start with the string error.</p>
<p>106<br>00:10:17,399 –&gt; 00:10:25,399<br>And while the sort of second RDD or second stage runs, you know, the first stage, you know, grabs the next set of records from the file system.</p>
<p>107<br>00:10:25,399 –&gt; 00:10:27,399<br>And then you know, feeds them again to states to.</p>
<p>108<br>00:10:27,399 –&gt; 00:10:33,399<br>And so as you go further further, you have more and more stages in your pipeline or your, your.</p>
<p>109<br>00:10:33,399 –&gt; 00:10:45,399<br>Linus graph, you know, all those stages are going to be running in sort of parallel and that’s what I mean with like sort of pipelining the transformations.</p>
<p>110<br>00:10:46,399 –&gt; 00:10:47,399<br>Okay, so.</p>
<p>111<br>00:10:48,399 –&gt; 00:10:57,399<br>So here this, so this, you know, this line basically describes how to create the RDD error, the RDD errors, RDD.</p>
<p>112<br>00:10:57,399 –&gt; 00:11:08,399<br>And then this line basically says like, tell Spark to basically keep a copy of this RDD in memory. So a subsequent.</p>
<p>113<br>00:11:08,399 –&gt; 00:11:11,399<br>Compotations run that you know, do more stuff with errors.</p>
<p>114<br>00:11:11,399 –&gt; 00:11:16,399<br>Spark will actually keep the original RDD actually in memory.</p>
<p>115<br>00:11:16,399 –&gt; 00:11:22,399<br>So that it can be shared with later computation. So, for example, we wanted to reuse the error file.</p>
<p>116<br>00:11:22,399 –&gt; 00:11:34,399<br>Then, you know, that error file will be in memory. It doesn’t have to be reconstructed from the files that represent the native HDFS and allows you to run the second computation.</p>
<p>117<br>00:11:34,399 –&gt; 00:11:43,399<br>And here, for example, one already, you know, even in this simple example, you see those are sort of a big difference between this and reproduce where can the macraduce job.</p>
<p>118<br>00:11:43,399 –&gt; 00:11:55,399<br>You know, you run the computation, it ends. And then if you want to read, you know, redo something with the data, you have to re-read it in from the file system and using this sort of persistent method.</p>
<p>119<br>00:11:55,399 –&gt; 00:12:02,399<br>Spark can avoid, you know, having to re-read that data from the disk and you know, save a lot of time.</p>
<p>120<br>00:12:05,399 –&gt; 00:12:07,399<br>Any questions so far?</p>
<p>121<br>00:12:07,399 –&gt; 00:12:16,399<br>So when the error file gets extracted from p1, let’s say, and then there’s another error file that gets extracted from p2.</p>
<p>122<br>00:12:16,399 –&gt; 00:12:18,399<br>So my understanding is that this happened in parallel.</p>
<p>123<br>00:12:18,399 –&gt; 00:12:19,399<br>Yes.</p>
<p>124<br>00:12:19,399 –&gt; 00:12:24,399<br>So you can think about it like, you know, there’s going to be many worker, like in the macraduce.</p>
<p>125<br>00:12:24,399 –&gt; 00:12:27,399<br>And the workers work on each partition.</p>
<p>126<br>00:12:27,399 –&gt; 00:12:33,399<br>So basically, you know, the scheduler will send a job, you know, through each of the workers.</p>
<p>127<br>00:12:33,399 –&gt; 00:12:38,399<br>And a job is sort of task, you know, pertains to a particular partition.</p>
<p>128<br>00:12:38,399 –&gt; 00:12:45,399<br>And the worker start working on one of these tasks and basically start running.</p>
<p>129<br>00:12:45,399 –&gt; 00:12:51,399<br>So you get parallelism between the petitions and you get parallelism between the stages into pipeline.</p>
<p>130<br>00:12:51,399 –&gt; 00:12:53,399<br>Hi, Steve.</p>
<p>131<br>00:12:53,399 –&gt; 00:12:55,399<br>Thank you.</p>
<p>132<br>00:12:55,399 –&gt; 00:12:58,399<br>So, what’s the.</p>
<p>133<br>00:12:58,399 –&gt; 00:12:59,399<br>So, can you hear me?</p>
<p>134<br>00:12:59,399 –&gt; 00:13:05,399<br>What’s the difference between the lineage and the like, just the log of transactions that we’ve seen before?</p>
<p>135<br>00:13:05,399 –&gt; 00:13:09,399<br>Like, is it just the granularity of the operations?</p>
<p>136<br>00:13:09,399 –&gt; 00:13:12,399<br>So we’ll see lineage, you know, log is strictly linear.</p>
<p>137<br>00:13:12,399 –&gt; 00:13:13,399<br>Right.</p>
<p>138<br>00:13:13,399 –&gt; 00:13:21,399<br>And the examples that we’ve seen so far, the, the, the, the lineage is also linear, but we’ll see later examples where, you know, we have four.</p>
<p>139<br>00:13:21,399 –&gt; 00:13:25,399<br>And we’re one stage depends on multiple.</p>
<p>140<br>00:13:25,399 –&gt; 00:13:31,399<br>Different RDDs and, you know, not just not represented in a log, right?</p>
<p>141<br>00:13:31,399 –&gt; 00:13:38,399<br>You know, there’s share some similarities in the sense that like you started in the beginning state, only operations are deterministic.</p>
<p>142<br>00:13:38,399 –&gt; 00:13:44,399<br>And then you will end up in some, and if you apply all those operations, you will end in some deterministic and state.</p>
<p>143<br>00:13:44,399 –&gt; 00:13:50,399<br>So in that sense, you know, there’s some similarity, but I think the, you know, they’re quite different.</p>
<p>144<br>00:13:51,399 –&gt; 00:13:53,399<br>I also have a question.</p>
<p>145<br>00:13:53,399 –&gt; 00:13:54,399<br>Yeah.</p>
<p>146<br>00:13:54,399 –&gt; 00:13:55,399<br>Filter.</p>
<p>147<br>00:13:55,399 –&gt; 00:14:04,399<br>It just, so it works by just the blind filter on each partition, but sometimes like I see the transformations also contain join or sort.</p>
<p>148<br>00:14:04,399 –&gt; 00:14:08,399<br>Yeah, yeah, let’s talk about this a little bit. I’ll talk about sort and join in a second.</p>
<p>149<br>00:14:08,399 –&gt; 00:14:09,399<br>Yeah.</p>
<p>150<br>00:14:09,399 –&gt; 00:14:13,399<br>They’re clear much more complicated.</p>
<p>151<br>00:14:13,399 –&gt; 00:14:19,399<br>So is, do we, like, this persists is when we start like computing?</p>
<p>152<br>00:14:19,399 –&gt; 00:14:25,399<br>No, nothing has been computed yet. You’re still all the descriptions. So let’s talk a little bit further.</p>
<p>153<br>00:14:25,399 –&gt; 00:14:30,399<br>Let’s look at actually something that actually generates a computation.</p>
<p>154<br>00:14:30,399 –&gt; 00:14:32,399<br>So.</p>
<p>155<br>00:14:32,399 –&gt; 00:14:38,399<br>So here are two commands that actually result in computation.</p>
<p>156<br>00:14:38,399 –&gt; 00:14:44,399<br>So this commands will result in computation because it contains count, which is an action.</p>
<p>157<br>00:14:44,399 –&gt; 00:14:50,399<br>And this command will result in a competition, collect is an action.</p>
<p>158<br>00:14:50,399 –&gt; 00:14:52,399<br>And so.</p>
<p>159<br>00:14:52,399 –&gt; 00:15:01,399<br>And so you can, and so really look at literally, and so the reason that they show two commands is because to demonstrate that they can reuse errors.</p>
<p>160<br>00:15:01,399 –&gt; 00:15:04,399<br>And so if you look at this.</p>
<p>161<br>00:15:04,399 –&gt; 00:15:13,399<br>A computation, then you can draw the lineage graph, right? So the start with lines.</p>
<p>162<br>00:15:13,399 –&gt; 00:15:19,399<br>That was a filter that we ran.</p>
<p>163<br>00:15:19,399 –&gt; 00:15:28,399<br>Oops, sorry, let me write this like you differently. There was a filter in lines that we just saw that produces errors.</p>
<p>164<br>00:15:28,399 –&gt; 00:15:34,399<br>Or that’s the description how to get errors. Then there’s an in this case, there’s another filter.</p>
<p>165<br>00:15:34,399 –&gt; 00:15:37,399<br>It’s the filter on HTFS.</p>
<p>166<br>00:15:37,399 –&gt; 00:15:42,399<br>And that basically produces another RDD, you know, that RDD is not explicitly named here.</p>
<p>167<br>00:15:42,399 –&gt; 00:15:47,399<br>But you know, it does produce another RDD. So I’m just going to call it HTFS.</p>
<p>168<br>00:15:47,399 –&gt; 00:15:50,399<br>Because it filters on HTFS.</p>
<p>169<br>00:15:50,399 –&gt; 00:15:54,399<br>And then we see there’s a map.</p>
<p>170<br>00:15:54,399 –&gt; 00:16:02,399<br>And that produces yet another RDD. Again, this RDD doesn’t really have a name here, but so anonymous.</p>
<p>171<br>00:16:02,399 –&gt; 00:16:06,399<br>But I’m just going to give it the name. That’s time.</p>
<p>172<br>00:16:06,399 –&gt; 00:16:16,399<br>Because it basically splits the wider line into three pieces and graphs the first piece out of that line. And that happens to be the time.</p>
<p>173<br>00:16:16,399 –&gt; 00:16:21,399<br>And then there’s a final operation.</p>
<p>174<br>00:16:21,399 –&gt; 00:16:27,399<br>So this is actually a result of the RDD produced in the time RDD.</p>
<p>175<br>00:16:27,399 –&gt; 00:16:39,399<br>Okay. So this. So this is actually at this point when you know the return to return here in the user interface or in the interactive user interface.</p>
<p>176<br>00:16:39,399 –&gt; 00:16:45,399<br>It is an at that point, basically, spark.</p>
<p>177<br>00:16:45,399 –&gt; 00:16:51,399<br>Now we’ll collect a lot of a set of workers.</p>
<p>178<br>00:16:51,399 –&gt; 00:16:58,399<br>Split, send them the jobs or basically inform the scheduler that the job needs to be executed.</p>
<p>179<br>00:16:58,399 –&gt; 00:17:03,399<br>And the description of the task that needs to be executed is this linear scrap.</p>
<p>180<br>00:17:03,399 –&gt; 00:17:22,400<br>And so we can sort of think a little bit about exactly how the execution happens. So let me draw a picture.</p>
<p>181<br>00:17:22,400 –&gt; 00:17:27,400<br>And so the picture is just follows because there’s what’s called the driver does the user thing.</p>
<p>182<br>00:17:27,400 –&gt; 00:17:31,400<br>And the program that the user typed into.</p>
<p>183<br>00:17:31,400 –&gt; 00:17:38,400<br>It starts off collecting a bunch of workers, you know, a bunch of machines.</p>
<p>184<br>00:17:38,400 –&gt; 00:17:43,400<br>One with the same way, like as in map produce.</p>
<p>185<br>00:17:43,400 –&gt; 00:17:55,400<br>And you know, there’s going to be an HDVS. There’s this lines file that actually has partitions, so p1, p2, whatever.</p>
<p>186<br>00:17:55,400 –&gt; 00:17:59,400<br>And typically the number of petitions is larger than the number of workers.</p>
<p>187<br>00:17:59,400 –&gt; 00:18:03,400<br>I’m sorry, the number of yes, the number of petitions is larger than the number of workers.</p>
<p>188<br>00:18:03,400 –&gt; 00:18:06,400<br>You know, you know, gets low balance.</p>
<p>189<br>00:18:06,400 –&gt; 00:18:12,400<br>If like one petition small and the other one is big, you know, you don’t want to have workers lying sitting around idle.</p>
<p>190<br>00:18:12,400 –&gt; 00:18:18,400<br>And basically the scheduler, you know, the scheduler.</p>
<p>191<br>00:18:18,400 –&gt; 00:18:25,400<br>That runs, you know, basically the computation that X has the information that has the lineage graph.</p>
<p>192<br>00:18:25,400 –&gt; 00:18:31,400<br>And so work is basically checking the driver X sensing the code.</p>
<p>193<br>00:18:31,400 –&gt; 00:18:34,400<br>The spark program, and I would just construct it.</p>
<p>194<br>00:18:34,400 –&gt; 00:18:39,400<br>And then the work is going to go basically to the schedule and say, please, you know, which petition should I work.</p>
<p>195<br>00:18:39,400 –&gt; 00:18:46,400<br>And then they run basically a part of the pipelines.</p>
<p>196<br>00:18:46,400 –&gt; 00:18:49,400<br>And so we look at this, let me actually draw this slightly differently.</p>
<p>197<br>00:18:49,400 –&gt; 00:18:52,400<br>So a little bit more space here.</p>
<p>198<br>00:18:52,400 –&gt; 00:18:55,400<br>So we saw there’s a whole bunch of stages.</p>
<p>199<br>00:18:55,400 –&gt; 00:19:02,400<br>And then the last stage was the, the last operation was to collect stage.</p>
<p>200<br>00:19:02,400 –&gt; 00:19:14,400<br>So in this, in this scenario that we just looked at the collect stage, of course, needs to collect data from all the petitions.</p>
<p>201<br>00:19:14,400 –&gt; 00:19:24,400<br>So we, in principle, you know, draw the green line, you know, basically everything of this, you know, sort of executed on an independent partition.</p>
<p>202<br>00:19:24,400 –&gt; 00:19:35,400<br>So every worker gets one of these tasks from the scheduler runs, you know, the thing that produces in the end on time.</p>
<p>203<br>00:19:35,400 –&gt; 00:19:48,400<br>RDD, and when this kind of scalar, you know, determines that basically all the time, you know, like all these stages, this is called the stage.</p>
<p>204<br>00:19:48,400 –&gt; 00:19:59,400<br>If all the stages have completed, and so all the time our petitions have reproduced, then it actually will run, you know, the collect action to basically do the addition.</p>
<p>205<br>00:19:59,400 –&gt; 00:20:10,400<br>And you know, retrieve information from every petition to actually complete the collect or actually this is not collect into its account.</p>
<p>206<br>00:20:10,400 –&gt; 00:20:15,400<br>Sorry about that.</p>
<p>207<br>00:20:15,400 –&gt; 00:20:28,400<br>And so one sort of thing to think about this is that this is sort of like, almost like a map reduce where you have the map phase and then you have a sort of a shuffle and then you run the reduced phase and.</p>
<p>208<br>00:20:28,400 –&gt; 00:20:39,400<br>The count is almost similar in that fashion and in the paper, the way they refer to this is to this dependency before there’s a white dependency.</p>
<p>209<br>00:20:39,400 –&gt; 00:21:06,400<br>Because the action or the transformation is dependent on a number of petitions and these are called narrow particular narrow dependencies because this RDD to make that RDD is only dependent on one other only at the only dependent on the parent partition only one parent partition to actually be able to compute it.</p>
<p>210<br>00:21:06,400 –&gt; 00:21:17,400<br>And generally, you would prefer a competition that can have narrow dependencies because they can just run locally without any communication before white dependency, you might have to collect.</p>
<p>211<br>00:21:17,400 –&gt; 00:21:26,400<br>You might have to collect petitions from the parent or you may have to collect petitions from the parent RDD from all the machines.</p>
<p>212<br>00:21:26,400 –&gt; 00:21:44,400<br>Professor, yeah, I had a question. So in the paper, it says narrow dependencies where each partition of the parent RDD is used by the most one partition of the child RDD.</p>
<p>213<br>00:21:44,400 –&gt; 00:22:01,400<br>But it doesn’t say anything about the contrary, like the reverse, like it doesn’t say like a child partition needs to use it most one parent.</p>
<p>214<br>00:22:01,400 –&gt; 00:22:08,400<br>That’s correct because then partition and then yeah, if a child uses multiple parent partition, then it’s a white dependency.</p>
<p>215<br>00:22:08,400 –&gt; 00:22:21,400<br>If a parent, sorry, if if the child needs the petitions of if the petition for multiple multiple parent petitions, then it’s a white dependency.</p>
<p>216<br>00:22:21,400 –&gt; 00:22:31,400<br>So for example, in the count case, correct, you know, you have time, time, petitions, right.</p>
<p>217<br>00:22:31,400 –&gt; 00:22:37,400<br>So if the count operation is going to collect data from all of them, right.</p>
<p>218<br>00:22:37,400 –&gt; 00:22:47,400<br>And so if count were an RDD, it isn’t, but like it’s just an action, but even where an RDD then basically, you know, that would require interaction with all the parents.</p>
<p>219<br>00:22:47,400 –&gt; 00:23:00,400<br>So what I’m saying is I think, I think it’s as the opposite, right. I think this might like, I mean, I, I was actually confused.</p>
<p>220<br>00:23:00,400 –&gt; 00:23:12,400<br>Like with the paper on this specific issue, but like it says, like each partition of the parent RDD is used by most one partition of the child.</p>
<p>221<br>00:23:12,400 –&gt; 00:23:17,400<br>But it doesn’t say one partition of the child uses it most.</p>
<p>222<br>00:23:17,400 –&gt; 00:23:25,400<br>I’m not going to censure exactly why you’re accused of it, but this so let me give you a can we could postpone this and come back to it.</p>
<p>223<br>00:23:25,400 –&gt; 00:23:26,400<br>Sure.</p>
<p>224<br>00:23:26,400 –&gt; 00:23:27,400<br>Sure.</p>
<p>225<br>00:23:27,400 –&gt; 00:23:38,400<br>I think the key thing to the question is, the basically two types of dependency white ones and there are once and white ones basically, you know, basically involve communication because they have to talk to the</p>
<p>226<br>00:23:38,400 –&gt; 00:23:47,400<br>community that collected the information from the day from the parents that traditions.</p>
<p>227<br>00:23:47,400 –&gt; 00:23:48,400<br>Okay.</p>
<p>228<br>00:23:48,400 –&gt; 00:23:50,400<br>Thanks.</p>
<p>229<br>00:23:50,400 –&gt; 00:23:53,400<br>I actually have a question on the interface.</p>
<p>230<br>00:23:53,400 –&gt; 00:23:54,400<br>Yes.</p>
<p>231<br>00:23:54,400 –&gt; 00:24:00,400<br>And like the previous like one or two slides, what happens if you don’t call errors stop persist?</p>
<p>232<br>00:24:00,400 –&gt; 00:24:12,400<br>If you do not, then the you would the second computation like this computation would recompute errors from the beginning.</p>
<p>233<br>00:24:12,400 –&gt; 00:24:22,400<br>So if you run this workflow, this spark in a computation, it would recompute errors from the starting file.</p>
<p>234<br>00:24:22,400 –&gt; 00:24:25,400<br>Got it. Thank you.</p>
<p>235<br>00:24:25,400 –&gt; 00:24:44,400<br>So I actually have a question about this point. So for the partitions that we don’t call persist on in the map reduce case, we basically stored them in intermediate files, but we nonetheless stored them in a local file system, let’s say, in the case of</p>
<p>236<br>00:24:44,400 –&gt; 00:24:55,400<br>a do we actually store intermediate files here that we don’t persist in some persistent storage or we just keep the whole flow in memory throughout the whole by default.</p>
<p>237<br>00:24:55,400 –&gt; 00:25:01,400<br>The whole flow is in memory, except you can provide to you.</p>
<p>238<br>00:25:01,400 –&gt; 00:25:08,400<br>There’s one exception. I’ll talk about a little bit more in detail in a second hopefully, which is.</p>
<p>239<br>00:25:08,400 –&gt; 00:25:15,400<br>You see this persist here. This persist can take another flag thing is called reliable.</p>
<p>240<br>00:25:15,400 –&gt; 00:25:22,400<br>And then that said is actually stored in HTFS and basically called as a checkpoint.</p>
<p>241<br>00:25:22,400 –&gt; 00:25:26,400<br>I see. Thank you.</p>
<p>242<br>00:25:26,400 –&gt; 00:25:45,400<br>I have a quick question about the partitioning that with partitioning the RDDs initially, is it initially HTFS who partitions them for each worker to operate on or is is spark handling all of them.</p>
<p>243<br>00:25:45,400 –&gt; 00:25:54,400<br>So this lines are you best, you know, the petition is defined basically by the files that are actually in HTFS.</p>
<p>244<br>00:25:54,400 –&gt; 00:26:03,400<br>You can re partition and we’ll see in a second that actually might be then it takes you to do so, for example, using this hash partition trick.</p>
<p>245<br>00:26:03,400 –&gt; 00:26:12,400<br>And you can define also your own petitioner, there’s a petitioner object or extraction that you can supply.</p>
<p>246<br>00:26:12,400 –&gt; 00:26:18,400<br>So it’s already handled by HTFS, but if you want to do it again through spark, then you can.</p>
<p>247<br>00:26:18,400 –&gt; 00:26:33,400<br>Yeah, and in some sense, of course, these files are also created by this this files, presumably, you’re created by some logging system that sits on the side and just produce different petitions or you know, you can reshovel if you want to make sense. Thank you.</p>
<p>248<br>00:26:33,400 –&gt; 00:26:39,400<br>Okay.</p>
<p>249<br>00:26:39,400 –&gt; 00:26:48,400<br>So, so that’s the execution model in I want to talk a little bit about fault tolerance.</p>
<p>250<br>00:26:48,400 –&gt; 00:26:57,400<br>And so let’s go back to this. So this is sort of fault tolerance and the thing that we worry about in fault tolerance is that maybe one of these workers.</p>
<p>251<br>00:26:57,400 –&gt; 00:26:59,400<br>You know, my crash.</p>
<p>252<br>00:26:59,400 –&gt; 00:27:05,400<br>The worker is executing some petition and so we need to re execute that.</p>
<p>253<br>00:27:05,400 –&gt; 00:27:11,400<br>And that is basically this plan is sort of the same as in MapReduce, right?</p>
<p>254<br>00:27:11,400 –&gt; 00:27:21,400<br>We’re if a worker crashes, we need to in MapReduce, you know, the map tasks need to be re executed and perhaps maybe a reduced task has to be re executed.</p>
<p>255<br>00:27:21,400 –&gt; 00:27:25,400<br>And here the task is slightly more complicated because they’re basically like these stages.</p>
<p>256<br>00:27:25,400 –&gt; 00:27:31,400<br>And so it means that it won worker fails. We may have to re-compute the stage.</p>
<p>257<br>00:27:31,400 –&gt; 00:27:34,400<br>So let’s talk a little bit more about that.</p>
<p>258<br>00:27:34,400 –&gt; 00:27:39,400<br>That’s sort of the perspective of fault tolerance. That’s what we’re trying to achieve.</p>
<p>259<br>00:27:39,400 –&gt; 00:27:48,400<br>This is really different than like the fault homes that were you implemented in lab, you can to free or taxes and you know, stable storage and all that kind of stuff.</p>
<p>260<br>00:27:48,400 –&gt; 00:27:56,400<br>And here really what we’re worried about is the crash of the worker.</p>
<p>261<br>00:27:56,400 –&gt; 00:28:01,400<br>The worker loses its memory.</p>
<p>262<br>00:28:01,400 –&gt; 00:28:04,400<br>Lost memory.</p>
<p>263<br>00:28:04,400 –&gt; 00:28:11,400<br>The needs of losses partition.</p>
<p>264<br>00:28:11,400 –&gt; 00:28:16,400<br>And later parts of the competition are probably dependent on that partition.</p>
<p>265<br>00:28:16,400 –&gt; 00:28:21,400<br>And so we need to re re read or re compute this partition.</p>
<p>266<br>00:28:21,400 –&gt; 00:28:29,400<br>And so the solution is like exactly like in MapReduce, you know, basically the scalar notice at some point that doesn’t get an answer.</p>
<p>267<br>00:28:29,400 –&gt; 00:28:44,400<br>And then re runs the stage for that partition.</p>
<p>268<br>00:28:44,400 –&gt; 00:28:57,400<br>And you know, and what is the cool part exactly as in MapReduce, all of you look at all these transformations that are sitting here in the API, all these transformations are functional.</p>
<p>269<br>00:28:57,400 –&gt; 00:29:03,400<br>And so they basically take one input. They take an RDD as an input. They produce another RDD as output.</p>
<p>270<br>00:29:03,400 –&gt; 00:29:06,400<br>And just completely deterministic.</p>
<p>271<br>00:29:06,400 –&gt; 00:29:12,400<br>And so like with MapReduce, you know, these maps and the reduced were functional operations.</p>
<p>272<br>00:29:12,400 –&gt; 00:29:20,400<br>If you restart a stage for a sequence of transformations from the same input, then you will produce the same output.</p>
<p>273<br>00:29:20,400 –&gt; 00:29:26,400<br>And so you recreate, you know, the same partition.</p>
<p>274<br>00:29:26,400 –&gt; 00:29:34,400<br>And so you can recreate the same partition.</p>
<p>275<br>00:29:34,400 –&gt; 00:29:38,400<br>We can recreate the partition.</p>
<p>276<br>00:29:38,400 –&gt; 00:29:41,400<br>Okay.</p>
<p>277<br>00:29:41,400 –&gt; 00:29:44,400<br>Sorry, is this why they’re immutable.</p>
<p>278<br>00:29:44,400 –&gt; 00:29:49,400<br>There’s also the reason I think they were randomly immutable. Yes.</p>
<p>279<br>00:29:49,400 –&gt; 00:29:54,400<br>So the tricky case though, which I want to talk about.</p>
<p>280<br>00:29:54,400 –&gt; 00:29:58,400<br>So the full tone space before the narrow case.</p>
<p>281<br>00:29:58,400 –&gt; 00:30:13,400<br>It’s the same sort of as we sell it before in MapReduce, but you know, the tricky case is actually the white dependencies.</p>
<p>282<br>00:30:13,400 –&gt; 00:30:20,400<br>Let’s say, you know, we have some transformations.</p>
<p>283<br>00:30:20,400 –&gt; 00:30:31,400<br>And one of these transformations is dependent on a, you know, uses like sort of you have one worker, we have another worker, we have another worker.</p>
<p>284<br>00:30:31,400 –&gt; 00:30:43,400<br>And one of these stages is actually dependent on a number of parent partitions.</p>
<p>285<br>00:30:43,400 –&gt; 00:30:49,400<br>So let’s say whatever maybe this is a join or we’ll see later other operations.</p>
<p>286<br>00:30:49,400 –&gt; 00:31:00,400<br>We’re going to we’re actually collecting information for lots of partitions and, you know, create a new RDD from that, from that RDD that might be used again by maps or whatever.</p>
<p>287<br>00:31:00,400 –&gt; 00:31:08,400<br>So, let’s say, you know, we’re, you know, we’re a worker and, you know, we crash here.</p>
<p>288<br>00:31:08,400 –&gt; 00:31:16,400<br>And we need to reconstruct this RDD.</p>
<p>289<br>00:31:16,400 –&gt; 00:31:24,400<br>And, you know, we sort of followed that means that we have to, you know, re also re compute this RDD to re compute this RDD on this worker.</p>
<p>290<br>00:31:24,400 –&gt; 00:31:28,400<br>So we also need the partitions on the other workers.</p>
<p>291<br>00:31:28,400 –&gt; 00:31:47,400<br>And so the, and the reconstructed the re execution of a computation on a particular worker, a particular petition may actually result that actually these ones also need to be re computed.</p>
<p>292<br>00:31:47,400 –&gt; 00:31:58,400<br>Now, of course you can do this partially in parallel. You can just ask, you know, please, you know, start to get a re compute this guy, we can see that guy and, you know, produce and the final RDD again.</p>
<p>293<br>00:31:58,400 –&gt; 00:32:09,400<br>But, you know, certainly, you know, a, a family of one worker might result in the re computation of many, many partitions.</p>
<p>294<br>00:32:09,400 –&gt; 00:32:14,400<br>So, you know, slightly, and that could be slightly costly.</p>
<p>295<br>00:32:14,400 –&gt; 00:32:23,400<br>And so the solution is that as a programmer, you can say, you can actually check point or persist.</p>
<p>296<br>00:32:23,400 –&gt; 00:32:27,400<br>RDDs on stable storage.</p>
<p>297<br>00:32:27,400 –&gt; 00:32:32,400<br>And so you might decide, you know, for example, this is an RDD that,</p>
<p>298<br>00:32:32,400 –&gt; 00:32:48,400<br>that you don’t want to re compute in the case of a failure because it requires, you know, re computing all the different partitions, you know, you may want to check point this RDD.</p>
<p>299<br>00:32:48,400 –&gt; 00:33:03,400<br>And then, you know, this kind of stage when it actually, when this, you know, computation needs to be re executed, then we’re going to actually read, you know, the result of the partitions from the checkpoint instead of actually having to re compute them from scratch.</p>
<p>300<br>00:33:03,400 –&gt; 00:33:17,400<br>And so this is why spark supports checkpoints and other sort of their fault on the story for white dependencies.</p>
<p>301<br>00:33:17,400 –&gt; 00:33:20,400<br>Any questions about this?</p>
<p>302<br>00:33:20,400 –&gt; 00:33:25,400<br>I had one question.</p>
<p>303<br>00:33:25,400 –&gt; 00:33:35,400<br>So there was, so you persist, right, just in general, but they also mentioned the reliable flag.</p>
<p>304<br>00:33:35,400 –&gt; 00:33:41,400<br>So I was wondering, like, what’s the difference between just persisting and using a reliable flag?</p>
<p>305<br>00:33:41,400 –&gt; 00:33:47,400<br>Persist just means you’re going to keep that RDD in memory and you’re not going to throw it away.</p>
<p>306<br>00:33:47,400 –&gt; 00:34:05,400<br>So that you can reuse it in later computations in, and then just in memory, the checkpoint or the liability flag basically means you actually write a copy of the whole RDD to HDFS.</p>
<p>307<br>00:34:05,400 –&gt; 00:34:11,400<br>And HDFS is a persistent or stable storage file system.</p>
<p>308<br>00:34:11,400 –&gt; 00:34:12,400<br>Okay.</p>
<p>309<br>00:34:12,400 –&gt; 00:34:16,400<br>Is there a way to tell spark to unpersist something?</p>
<p>310<br>00:34:16,400 –&gt; 00:34:29,400<br>Because otherwise, like, for example, if you persist an RDD, and you do a lot of computations, but like those later computations never use the RDD, you might just have it sticking around in memory forever.</p>
<p>311<br>00:34:29,400 –&gt; 00:34:30,400<br>Yeah.</p>
<p>312<br>00:34:30,400 –&gt; 00:34:34,400<br>So I presume you can.</p>
<p>313<br>00:34:34,400 –&gt; 00:34:42,400<br>There’s a general strategy that spark uses, and they talk a little bit about this, is that they have really no space anymore.</p>
<p>314<br>00:34:42,400 –&gt; 00:34:48,400<br>They might spill some RDDs to HDFS or remove them.</p>
<p>315<br>00:34:48,400 –&gt; 00:34:54,400<br>The papers like you dig in the exactly what their plan for that is.</p>
<p>316<br>00:34:54,400 –&gt; 00:35:11,400<br>And you, of course, when the computation ends and as a user, you log out, or you stop your driver, then I think those RDDs are definitely gone from memory.</p>
<p>317<br>00:35:11,400 –&gt; 00:35:14,400<br>Okay.</p>
<p>318<br>00:35:14,400 –&gt; 00:35:21,400<br>So that is almost the story of spark.</p>
<p>319<br>00:35:21,400 –&gt; 00:35:23,400<br>We’ve seen what an RDD is.</p>
<p>320<br>00:35:23,400 –&gt; 00:35:29,400<br>We’ve seen how for the execution works, and we’ve seen how the fault homes plan works.</p>
<p>321<br>00:35:29,400 –&gt; 00:35:37,400<br>The thing that I want to really want to talk about is another example to really show off where spark shines.</p>
<p>322<br>00:35:37,400 –&gt; 00:35:40,400<br>And that is an iterative example.</p>
<p>323<br>00:35:40,400 –&gt; 00:35:46,400<br>So in computation that has an iterative structure.</p>
<p>324<br>00:35:46,400 –&gt; 00:35:54,400<br>And the particular one I want to talk about is page rank.</p>
<p>325<br>00:35:54,400 –&gt; 00:36:01,400<br>I assume that most of you are familiar with page ranking some form, basically, and it’s a plan to.</p>
<p>326<br>00:36:01,400 –&gt; 00:36:06,400<br>Now we’re going to give weight or importance to web pages.</p>
<p>327<br>00:36:06,400 –&gt; 00:36:10,400<br>And it’s dependent on the number of links that put point to a particular webpage.</p>
<p>328<br>00:36:10,400 –&gt; 00:36:14,400<br>So for example, if you have a webpage you want.</p>
<p>329<br>00:36:14,400 –&gt; 00:36:16,400<br>I may point to itself.</p>
<p>330<br>00:36:16,400 –&gt; 00:36:19,400<br>You know, maybe a webpage you free.</p>
<p>331<br>00:36:19,400 –&gt; 00:36:27,400<br>So I’ll use an example and a web page you to you to has a link to itself and to you free.</p>
<p>332<br>00:36:27,400 –&gt; 00:36:30,400<br>You know, you free has a link to you want.</p>
<p>333<br>00:36:30,400 –&gt; 00:36:35,400<br>And then basically page rank is an algorithm that you know, based on these connectivities.</p>
<p>334<br>00:36:35,400 –&gt; 00:36:38,400<br>Our computer is the importance of a webpage.</p>
<p>335<br>00:36:38,400 –&gt; 00:36:46,400<br>And page rank for sort of the early out of the algorithms that really drove the Google search machine.</p>
<p>336<br>00:36:46,400 –&gt; 00:36:53,400<br>In the sense that if you had a search result, the way you rent search results is that you have a search results.</p>
<p>337<br>00:36:53,400 –&gt; 00:36:56,400<br>And then you have a search results on a more important webpage.</p>
<p>338<br>00:36:56,400 –&gt; 00:36:59,400<br>And that results gets promoted to higher in the list.</p>
<p>339<br>00:36:59,400 –&gt; 00:37:12,400<br>And I’m running the reason that in the early days of Google, the Google search machine actually produced better search results where I made the more important information actually or the more important web pages were at top.</p>
<p>340<br>00:37:12,400 –&gt; 00:37:27,400<br>And so the paper talks about shows off the implementation of page rank in spark.</p>
<p>341<br>00:37:27,400 –&gt; 00:37:35,400<br>So here’s the implementation of the spark implementation of the page rank.</p>
<p>342<br>00:37:35,400 –&gt; 00:37:41,400<br>And as before, you know, this is just a description. So we look at the individual lines.</p>
<p>343<br>00:37:41,400 –&gt; 00:37:48,400<br>You know, these are just the sort of the recipe to actually how to compute page rank.</p>
<p>344<br>00:37:48,400 –&gt; 00:37:54,400<br>And only when like, you know, this particular case, if you do say ranks collect at the very end.</p>
<p>345<br>00:37:54,400 –&gt; 00:37:59,400<br>Then actually the computation would run on, you know, the cluster of machines.</p>
<p>346<br>00:37:59,400 –&gt; 00:38:04,400<br>And using sort of the execution pattern that we have seen so far.</p>
<p>347<br>00:38:04,400 –&gt; 00:38:16,400<br>And so I want to walk through this example in a little bit more detail to get a sense, you know, what the get a better sense why sparks are shines in the iterative case.</p>
<p>348<br>00:38:16,400 –&gt; 00:38:20,400<br>So, so there are two RDDs here.</p>
<p>349<br>00:38:20,400 –&gt; 00:38:30,400<br>And also going to talk about some optimization that are cool in spark one is this links are these and links is basically you know represents the connection of the graphs.</p>
<p>350<br>00:38:30,400 –&gt; 00:38:34,400<br>And so presumably it probably has a line.</p>
<p>351<br>00:38:34,400 –&gt; 00:38:42,400<br>I was going to write it like that is line per URL. So here you want and it has two outgoing links.</p>
<p>352<br>00:38:42,400 –&gt; 00:38:52,400<br>And you want you to you free.</p>
<p>353<br>00:38:52,400 –&gt; 00:38:54,400<br>Actually I miss one link here.</p>
<p>354<br>00:38:54,400 –&gt; 00:39:05,400<br>And then there’s a you to entry for you to which has now gone link to you to you to you free.</p>
<p>355<br>00:39:05,400 –&gt; 00:39:13,400<br>And entry you know you free going to you want.</p>
<p>356<br>00:39:13,400 –&gt; 00:39:17,400<br>So this is basically description of you will the you know the worldwide web.</p>
<p>357<br>00:39:17,400 –&gt; 00:39:21,400<br>And so of course, you know, my tiny little example I have free web ages.</p>
<p>358<br>00:39:21,400 –&gt; 00:39:26,400<br>But if you know we’re running this at the scale of Google you would have a billion web agents right.</p>
<p>359<br>00:39:26,400 –&gt; 00:39:33,400<br>And so this follows gigantic and is partitioned in the predictions.</p>
<p>360<br>00:39:33,400 –&gt; 00:39:35,400<br>So that’s links.</p>
<p>361<br>00:39:35,400 –&gt; 00:39:43,400<br>And then ranks is a sort of similar file that contains the current ranks with these web pages.</p>
<p>362<br>00:39:43,400 –&gt; 00:39:46,400<br>And so you can think about this is going to you won.</p>
<p>363<br>00:39:46,400 –&gt; 00:39:48,400<br>Now comma, you know, it’s rank.</p>
<p>364<br>00:39:48,400 –&gt; 00:39:52,400<br>And let’s assume that the ranks are initialized at 1.0.</p>
<p>365<br>00:39:52,400 –&gt; 00:39:55,400<br>And then you know, years 1.0.</p>
<p>366<br>00:39:55,400 –&gt; 00:40:00,400<br>And you know, you two 1.0.</p>
<p>367<br>00:40:00,400 –&gt; 00:40:05,400<br>You you three 1.0.</p>
<p>368<br>00:40:05,400 –&gt; 00:40:11,400<br>And we see actually that the links that the links are these is actually persistent in memory.</p>
<p>369<br>00:40:11,400 –&gt; 00:40:17,400<br>That’s presumably like in the same way as the error file that we saw before or the error and already.</p>
<p>370<br>00:40:17,400 –&gt; 00:40:20,400<br>And then ranks.</p>
<p>371<br>00:40:20,400 –&gt; 00:40:25,400<br>You know, say initialized you know through something and then basically there’s this sort of.</p>
<p>372<br>00:40:25,400 –&gt; 00:40:32,400<br>A description of number of iterations to produce a new ranks.</p>
<p>373<br>00:40:32,400 –&gt; 00:40:33,400<br>RDD.</p>
<p>374<br>00:40:33,400 –&gt; 00:40:37,400<br>And you can see a little bit you know how this actually plays out.</p>
<p>375<br>00:40:37,400 –&gt; 00:40:43,400<br>And one of the things to notice is that links gets reused in every iteration.</p>
<p>376<br>00:40:43,400 –&gt; 00:40:47,400<br>And links actually gets joined with ranks.</p>
<p>377<br>00:40:47,400 –&gt; 00:40:55,400<br>What means what does mean to actually do this join up this this this operation this this operation basically creates an RDD.</p>
<p>378<br>00:40:55,400 –&gt; 00:41:00,400<br>Right. And what is that RDD look like? Well that RDD is going to look like you won.</p>
<p>379<br>00:41:00,400 –&gt; 00:41:04,400<br>And then the join of the ranks in the.</p>
<p>380<br>00:41:04,400 –&gt; 00:41:05,400<br>And the links file.</p>
<p>381<br>00:41:05,400 –&gt; 00:41:07,400<br>So it’s going to be you won.</p>
<p>382<br>00:41:07,400 –&gt; 00:41:12,400<br>You won you two because those are the are you three outgoing links plus the rank.</p>
<p>383<br>00:41:12,400 –&gt; 00:41:17,400<br>For you one, which I’m just going to go right as rank one.</p>
<p>384<br>00:41:17,400 –&gt; 00:41:27,400<br>And that’s sort of the RDD that’s being produced here. And so same thing for you know whatever you do and the one for you free is whatever you want.</p>
<p>385<br>00:41:27,400 –&gt; 00:41:30,400<br>And you know.</p>
<p>386<br>00:41:30,400 –&gt; 00:41:32,400<br>Or free rank free.</p>
<p>387<br>00:41:32,400 –&gt; 00:41:38,400<br>Basically it’s merges you to literally joins the two files based on key.</p>
<p>388<br>00:41:38,400 –&gt; 00:41:40,400<br>Okay.</p>
<p>389<br>00:41:40,400 –&gt; 00:41:47,400<br>And then you know it runs a computation of flat map on this and that flat map itself internally has a map overlinks.</p>
<p>390<br>00:41:47,400 –&gt; 00:42:00,400<br>So basically it’s going to run over like this group is going to run over these lists and basically a partition or divide up the rank, you know, to the outgoing URLs.</p>
<p>391<br>00:42:00,400 –&gt; 00:42:06,400<br>And so it will you know create triples off the form.</p>
<p>392<br>00:42:06,400 –&gt; 00:42:10,400<br>You know, let me write this in green and you know you won.</p>
<p>393<br>00:42:10,400 –&gt; 00:42:15,400<br>R1 divided by two.</p>
<p>394<br>00:42:15,400 –&gt; 00:42:27,400<br>And you won or you were free over the outgoing link. So it gave one to you one one to you three here you three are one divided over two.</p>
<p>395<br>00:42:27,400 –&gt; 00:42:40,400<br>So it creates triples of this kind of form. So basically you know compute the way you get it to divides the rank across the outgoing edges and gives you know the values of these ranks, you know, to the outgoing edges.</p>
<p>396<br>00:42:40,400 –&gt; 00:42:46,400<br>So the outgoing edges. So we’re going to get a big you know RDD that has no this form.</p>
<p>397<br>00:42:46,400 –&gt; 00:42:50,400<br>And that’s produced basically not as the contributions.</p>
<p>398<br>00:42:50,400 –&gt; 00:42:53,400<br>RDD.</p>
<p>399<br>00:42:53,400 –&gt; 00:42:58,400<br>Then there’s a final step in the thing at first, we’re reduced by key.</p>
<p>400<br>00:42:58,400 –&gt; 00:43:12,400<br>So basically graph all the you once you know together and it’s some so much so basically this will result is that you know all the weight or the the fractional weights, you know that you want is going to receive are being added up.</p>
<p>401<br>00:43:12,400 –&gt; 00:43:20,400<br>So you want of course going to receive weights from itself this one is going to relate to you free.</p>
<p>402<br>00:43:20,400 –&gt; 00:43:29,400<br>And so you know, we’ll sum them up. It’s going to be R1 divided by two and R3 divided by one.</p>
<p>403<br>00:43:29,400 –&gt; 00:43:33,400<br>And that is basically the sum has been computed.</p>
<p>404<br>00:43:33,400 –&gt; 00:43:42,400<br>And so that gives you a list of sums and then actually they’re added up and computed into a final value.</p>
<p>405<br>00:43:42,400 –&gt; 00:43:54,400<br>And that produces the new ranks RDD which has this same shape as the one that we saw before mainly this shape for every web page.</p>
<p>406<br>00:43:54,400 –&gt; 00:44:00,400<br>Does that make sense?</p>
<p>407<br>00:44:00,400 –&gt; 00:44:10,400<br>And so it’s interesting to you know that’s sort of the description. So first of all, you can see that the actually the description of page rank is quite precise.</p>
<p>408<br>00:44:10,400 –&gt; 00:44:19,400<br>And there’s one of the examples this is like if you had to run this in a map, you do style, then that would mean at every iteration, you know, this loop.</p>
<p>409<br>00:44:19,400 –&gt; 00:44:29,400<br>At the end of the iteration, you would store the results in the file system and then you re-read the iteration from the result for the next iteration.</p>
<p>410<br>00:44:29,400 –&gt; 00:44:40,400<br>And in this spark system, every iteration runs straight out of memory, it leaves the results in memory so that the next iteration can pick it up right there.</p>
<p>411<br>00:44:40,400 –&gt; 00:44:46,400<br>And for the more, you know, these links file is shared among all the iterations.</p>
<p>412<br>00:44:46,400 –&gt; 00:44:52,400<br>Okay, so to get a little bit more sense of like, you know, why is also cool.</p>
<p>413<br>00:44:52,400 –&gt; 00:45:00,400<br>The way to look at it is to actually look at the lineage graph for this particular complication.</p>
<p>414<br>00:45:00,400 –&gt; 00:45:16,400<br>So here’s the lineage graph for the for patron.</p>
<p>415<br>00:45:16,400 –&gt; 00:45:26,400<br>And so the couple things I want to point out, the first of all, so the lineage graph is like dynamic almost directly looks.</p>
<p>416<br>00:45:26,400 –&gt; 00:45:31,400<br>You know, just keeps growing with the number of iterations.</p>
<p>417<br>00:45:31,400 –&gt; 00:45:42,400<br>And so ask the scheduler, you know, basically, computes and does new stages, then it keeps going. And you know, we can see what the stages are going to be correct because.</p>
<p>418<br>00:45:42,400 –&gt; 00:45:46,400<br>The sort of, you know, every iteration.</p>
<p>419<br>00:45:46,400 –&gt; 00:45:56,400<br>Often look is one stage and will basically append parts of transformations to the lineage graph.</p>
<p>420<br>00:45:56,400 –&gt; 00:46:06,400<br>So we see here’s the input file, we use the links and as we saw, like links are actually created once, then persistent in memory, not on this persistent in memory.</p>
<p>421<br>00:46:06,400 –&gt; 00:46:12,400<br>And it’s being reused many, many times, like in every lib iteration, basically, ranks is being reused.</p>
<p>422<br>00:46:12,400 –&gt; 00:46:17,400<br>And so again, you know, compare to a map reduced if you have to write this into map reduced style, you don’t get that reuse.</p>
<p>423<br>00:46:17,400 –&gt; 00:46:32,400<br>And so that’s of course, and like tremendously preferable performance because links, you know, as we said before, is like a gigantic file that basically corresponds to one line for every web page in the universe.</p>
<p>424<br>00:46:32,400 –&gt; 00:46:41,400<br>Another interesting thing to observe here is that we see your white dependencies here, right, this is a white dependency.</p>
<p>425<br>00:46:41,400 –&gt; 00:46:47,400<br>It could be a white dependency, we a little bit more sophisticated about this in a second.</p>
<p>426<br>00:46:47,400 –&gt; 00:46:56,400<br>Because basically this contributions, you know, when we contribute the intermediate result, it is a join across ranks and links.</p>
<p>427<br>00:46:56,400 –&gt; 00:47:12,400<br>So it needs the partitions from links and it needs partitions for own ranks to basically compute a partition for contribute for the contribute contribute, contribute, contribute, contribute, RDD, sorry.</p>
<p>428<br>00:47:12,400 –&gt; 00:47:17,400<br>And so that might require, you know, from a never communication.</p>
<p>429<br>00:47:17,400 –&gt; 00:47:25,400<br>And so that’s what we’re talking about. They have sort of a clever optimization.</p>
<p>430<br>00:47:25,400 –&gt; 00:47:30,400<br>And this is in response to an earlier question about like partitioning.</p>
<p>431<br>00:47:30,400 –&gt; 00:47:40,400<br>You can specify that you want to partition an RDD using a hash partition.</p>
<p>432<br>00:47:40,400 –&gt; 00:47:48,400<br>The main thing is that the links and ranks file, you know, these two RDDs are going to be partitioned in the same way.</p>
<p>433<br>00:47:48,400 –&gt; 00:48:02,400<br>They’re going to actually partition by key or by the hash of the key. So we go look back, you know, at an RDD picture, you know, the keys for ranks, you know, RDD, you want you to do your free.</p>
<p>434<br>00:48:02,400 –&gt; 00:48:14,400<br>And this is for the links, okay, sorry, the keys for ranks, you want you to free the keys for links are also you want you to free free.</p>
<p>435<br>00:48:14,400 –&gt; 00:48:28,400<br>And basically the clever optimization, and this is a standard optimization from the database literature is that if you partition links and ranks by key, then, you know, this one is going to have you one on one machine.</p>
<p>436<br>00:48:28,400 –&gt; 00:48:32,400<br>That maybe this is you to one machine.</p>
<p>437<br>00:48:32,400 –&gt; 00:48:45,400<br>Then ranks is going to have the same thing is going to have you one on one machine. And in fact, you know, it’s going to have you one on the same machine as the links one and saying for you to and your free.</p>
<p>438<br>00:48:45,400 –&gt; 00:49:12,400<br>So even though this join is exceptionally white dependency, it can be executed like a narrow dependency because basically to compute, you know, the joint of these two for the you one, you know, for the first partition for the p1, you only have to look at the partition p1 of links and p1 of ranks because, you know, that keys are hatched in the same way through the same machine.</p>
<p>439<br>00:49:12,400 –&gt; 00:49:25,400<br>And so the scheduler or the programmer can actually specify these hatch petitions, the scheduler sees, you know, the join actually uses these hatch petitions, the hatch petitions that are the same.</p>
<p>440<br>00:49:25,400 –&gt; 00:49:35,400<br>And therefore, and actually don’t have to do this white dependency, I don’t have to do sort of a complete barrier as you may have to reduce, but I can just treat this as a narrow dependency.</p>
<p>441<br>00:49:35,400 –&gt; 00:49:39,400<br>So that’s pretty cool.</p>
<p>442<br>00:49:39,400 –&gt; 00:49:51,400<br>Then, you know, again, if a machine fails correct, we talked a little bit about this earlier, that might be painful because you may have to re execute many loops or one iterations.</p>
<p>443<br>00:49:51,400 –&gt; 00:50:02,400<br>And so, you know, if you would probably write this real, then the programmer will probably say like maybe after you know 10 iterations.</p>
<p>444<br>00:50:02,400 –&gt; 00:50:11,400<br>You know, basically check point.</p>
<p>445<br>00:50:11,400 –&gt; 00:50:16,400<br>So that you don’t have to re compute the computation all the way from the from the beginning.</p>
<p>446<br>00:50:16,400 –&gt; 00:50:22,400<br>Oh, so we don’t actually re compute links or anything each time, right? Like we don’t persist it. Sorry.</p>
<p>447<br>00:50:22,400 –&gt; 00:50:25,400<br>We don’t persist it. No, not at all.</p>
<p>448<br>00:50:25,400 –&gt; 00:50:31,400<br>The only thing that was persistent is we’re the only ones with links, correct? This is the only thing that was had a persistent call.</p>
<p>449<br>00:50:31,400 –&gt; 00:50:34,400<br>We do persisted links. We do.</p>
<p>450<br>00:50:34,400 –&gt; 00:50:36,400<br>Okay.</p>
<p>451<br>00:50:36,400 –&gt; 00:50:39,400<br>But not the intermediate Rdds.</p>
<p>452<br>00:50:39,400 –&gt; 00:50:46,400<br>Because they’re basically new Rdds every time. Like rings one is a new Rdd, ranks two is a new Rdd, right?</p>
<p>453<br>00:50:46,400 –&gt; 00:50:59,400<br>But you may want to persist them, you know, occasionally, and then store them really nice to you. So that if you have to a failure, you don’t have to go back to iteration, the iteration zero to execute everything.</p>
<p>454<br>00:50:59,400 –&gt; 00:51:03,400<br>Okay. Does this make sense?</p>
<p>455<br>00:51:03,400 –&gt; 00:51:12,400<br>I’m sorry. The different contrips can maybe compute it in parallel on different petitions. Yes.</p>
<p>456<br>00:51:12,400 –&gt; 00:51:16,400<br>Because there, there’s like this line that goes vertically down.</p>
<p>457<br>00:51:16,400 –&gt; 00:51:19,400<br>If it’s going down, it’s pipeline.</p>
<p>458<br>00:51:19,400 –&gt; 00:51:26,400<br>Correct. And then there’s sort of just two types of parallelism. There’s stage parallelism. And there is sort of parallelism between different petitions.</p>
<p>459<br>00:51:26,400 –&gt; 00:51:45,400<br>And we can think about this thing, you know, this whole thing, like running many, many times on different petitions.</p>
<p>460<br>00:51:45,400 –&gt; 00:51:51,400<br>So in this case, the collect at the very end will be the only place where we have a wide.</p>
<p>461<br>00:51:51,400 –&gt; 00:51:59,400<br>Exactly, exactly. The collect is the only one that’s going to, you know, whatever year we have more petitions correct.</p>
<p>462<br>00:51:59,400 –&gt; 00:52:09,400<br>I make a mess of this picture, but that is going to have to get them from everyone.</p>
<p>463<br>00:52:09,400 –&gt; 00:52:14,400<br>Okay. I hope the director was overseas. This is actually pretty cool.</p>
<p>464<br>00:52:14,400 –&gt; 00:52:19,400<br>I’m going to use by expressing these computations and sort of a lineage graph or a data flow computation.</p>
<p>465<br>00:52:19,400 –&gt; 00:52:26,400<br>The scheduler has a bit of room for optimizations like these like exploding has petitions.</p>
<p>466<br>00:52:26,400 –&gt; 00:52:36,400<br>The, you know, we get a lot of parallelism. We get also a lot of reuse. You know, we can keep the results of one RTV and memory so that we can reuse it for the next iteration.</p>
<p>467<br>00:52:36,400 –&gt; 00:52:43,400<br>And you can sort of see that these techniques can buy. You know, I’m going to give you a significant performance optimization.</p>
<p>468<br>00:52:43,400 –&gt; 00:52:52,400<br>And it allows you to expand and express more powerful or more instant computations.</p>
<p>469<br>00:52:52,400 –&gt; 00:53:04,400<br>So maybe with that, I will summarize this lecture.</p>
<p>470<br>00:53:04,400 –&gt; 00:53:07,400<br>So a couple of things.</p>
<p>471<br>00:53:07,400 –&gt; 00:53:20,400<br>You know, some RTVs are made by functional transformations.</p>
<p>472<br>00:53:20,400 –&gt; 00:53:27,400<br>They’re grouped together in sort of a lineage graph, which you can think about as a data flow graph.</p>
<p>473<br>00:53:27,400 –&gt; 00:53:32,400<br>This, the, you know, this allows reuse.</p>
<p>474<br>00:53:32,400 –&gt; 00:53:42,400<br>Those allow some clever optimizations by the scheduler.</p>
<p>475<br>00:53:42,400 –&gt; 00:53:52,400<br>And basically allows also more extra is more expressiveness.</p>
<p>476<br>00:53:52,400 –&gt; 00:53:58,400<br>Then, you know, map reduced by itself.</p>
<p>477<br>00:53:58,400 –&gt; 00:54:14,400<br>And which results basically in good performance because like a lot of the data just stays in memory.</p>
<p>478<br>00:54:14,400 –&gt; 00:54:18,400<br>And so if you actually are excited about this, you can try it out.</p>
<p>479<br>00:54:18,400 –&gt; 00:54:25,400<br>You download, you know, Spark play around in the right programs for you know, go to data bricks.com and you’re creating accounts.</p>
<p>480<br>00:54:25,400 –&gt; 00:54:30,400<br>And then you can run Spark competitions on their on their clusters.</p>
<p>481<br>00:54:30,400 –&gt; 00:54:35,400<br>So you’re excited about this and want to try it out. You know, it’s pretty easy to do so.</p>
<p>482<br>00:54:35,400 –&gt; 00:54:38,400<br>Like, unlike farm, you can just like not play with.</p>
<p>483<br>00:54:38,400 –&gt; 00:54:41,400<br>But this actually you can actually go out and try out.</p>
<p>484<br>00:54:41,400 –&gt; 00:54:46,400<br>Okay, with that, I want to stop for today.</p>
<p>485<br>00:54:46,400 –&gt; 00:54:51,400<br>And the people that want to hang around and ask more questions, please feel free to do so.</p>
<p>486<br>00:54:51,400 –&gt; 00:54:59,400<br>The only thing I want to remind people of is that kind of the deadline for 4B is a little bit away.</p>
<p>487<br>00:54:59,400 –&gt; 00:55:04,400<br>But I just want to remind people that 4B is a pretty tricky, requires a bit of design.</p>
<p>488<br>00:55:04,400 –&gt; 00:55:06,400<br>So don’t start to late.</p>
<p>489<br>00:55:06,400 –&gt; 00:55:09,400<br>And with that, I’ll see you on Tuesday.</p>
<p>490<br>00:55:11,400 –&gt; 00:55:14,400<br>Thank you.</p>
<p>491<br>00:55:14,400 –&gt; 00:55:17,400<br>Thank you.</p>
<p>492<br>00:55:17,400 –&gt; 00:55:20,400<br>Thank you.</p>
<p>493<br>00:55:20,400 –&gt; 00:55:24,400<br>I had a question about the checkpoints.</p>
<p>494<br>00:55:24,400 –&gt; 00:55:33,400<br>I think it’s very mentioned automatic checkpoints using data about how longage computation took.</p>
<p>495<br>00:55:33,400 –&gt; 00:55:37,400<br>And I wasn’t really sure what they mean by this.</p>
<p>496<br>00:55:37,400 –&gt; 00:55:41,400<br>What are they going to be optimizing for?</p>
<p>497<br>00:55:41,400 –&gt; 00:55:46,400<br>I get a little bit of the whole checkpoints is correct. There’s an optimization between.</p>
<p>498<br>00:55:46,400 –&gt; 00:55:49,400<br>Taking checkpoints is expensive.</p>
<p>499<br>00:55:49,400 –&gt; 00:55:51,400<br>And so that takes time.</p>
<p>500<br>00:55:51,400 –&gt; 00:55:57,400<br>But you know, re execution. If there’s a machine failure, also takes a lot of time.</p>
<p>501<br>00:55:57,400 –&gt; 00:56:02,400<br>And so, for example, if you take never check point, then you basically have to re execute the computation from the beginning.</p>
<p>502<br>00:56:02,400 –&gt; 00:56:11,400<br>But if you take periodically checkpoints, you know, you don’t have to repeat, you know, the computation that you did before the checkpoint, but the check taking the checkpoint takes time.</p>
<p>503<br>00:56:11,400 –&gt; 00:56:18,400<br>So if you take very frequent checkpoints, you know, it has to recompute a lot, but you spend only a time taking checkpoints.</p>
<p>504<br>00:56:18,400 –&gt; 00:56:22,400<br>And so there’s sort of an optimization problem here.</p>
<p>505<br>00:56:22,400 –&gt; 00:56:27,400<br>You know, you want to take the checkpoints and some regular interval.</p>
<p>506<br>00:56:27,400 –&gt; 00:56:32,400<br>And you’re willing to take to re compute.</p>
<p>507<br>00:56:32,400 –&gt; 00:56:37,400<br>Okay, so maybe like compute checkpoints only for very large computations.</p>
<p>508<br>00:56:37,400 –&gt; 00:56:45,400<br>Yeah, like an example in the case of page wrong, you know, maybe you know, do it every 10 iterations.</p>
<p>509<br>00:56:45,400 –&gt; 00:56:46,400<br>Thank you.</p>
<p>510<br>00:56:46,400 –&gt; 00:56:52,400<br>It depends of course, of the size of the check point, right? But the size of the check point is small. You can check more frequently.</p>
<p>511<br>00:56:52,400 –&gt; 00:56:57,400<br>But in the case of this page rank, you know, that check one’s going to be pretty big.</p>
<p>512<br>00:56:57,400 –&gt; 00:57:03,400<br>There’s going to be a line or record, you know, per web page.</p>
<p>513<br>00:57:03,400 –&gt; 00:57:05,400<br>That makes sense. Thank you.</p>
<p>514<br>00:57:05,400 –&gt; 00:57:07,400<br>You want to.</p>
<p>515<br>00:57:07,400 –&gt; 00:57:10,400<br>I have a question about the driver.</p>
<p>516<br>00:57:10,400 –&gt; 00:57:17,400<br>Yes, the application is like, does the driver is the driver on the client side or is this.</p>
<p>517<br>00:57:17,400 –&gt; 00:57:25,400<br>If it’s conscious, we lose like the whole graph. And that’s fine because that’s like application.</p>
<p>518<br>00:57:25,400 –&gt; 00:57:33,400<br>Yeah, I don’t exactly know what happens because the, the, the, the schedule has a two.</p>
<p>519<br>00:57:33,400 –&gt; 00:57:35,400<br>And the schedule is full powered.</p>
<p>520<br>00:57:35,400 –&gt; 00:57:40,400<br>So I don’t know exactly what you know what happens. Maybe you can reconnect. I don’t know.</p>
<p>521<br>00:57:40,400 –&gt; 00:57:52,400<br>I had a question about the Y dependency optimization. You mentioned that did you like the hash partitioning. How does that work?</p>
<p>522<br>00:57:52,400 –&gt; 00:57:56,400<br>Okay, I can say a little bit more. So this is not a, you know, has petition is not something that they invent.</p>
<p>523<br>00:57:56,400 –&gt; 00:57:59,400<br>It’s actually something that is.</p>
<p>524<br>00:57:59,400 –&gt; 00:58:04,400<br>It’s a standard database partitioning scheme.</p>
<p>525<br>00:58:04,400 –&gt; 00:58:10,400<br>And it is cool because if you need to computer join, you don’t have to do a lot of communication.</p>
<p>526<br>00:58:10,400 –&gt; 00:58:20,400<br>So let me actually, I can maybe start a new slide because it’s a little bit hard to read. So have a kitchen.</p>
<p>527<br>00:58:20,400 –&gt; 00:58:29,400<br>If you have two datasets, use dataset one, use dataset two, they have keys regularly, you know, he won P2.</p>
<p>528<br>00:58:29,400 –&gt; 00:58:32,400<br>But they have the same set of keys.</p>
<p>529<br>00:58:32,400 –&gt; 00:58:40,400<br>Then what you do by hash partitioning the partition to dataset and number of petitions. So boom, boom, boom.</p>
<p>530<br>00:58:40,400 –&gt; 00:58:45,400<br>And you have to keep.</p>
<p>531<br>00:58:45,400 –&gt; 00:58:51,400<br>And you have to keep the same hash, K1, you have K2 and that actually determines the partition ends up it.</p>
<p>532<br>00:58:51,400 –&gt; 00:59:00,400<br>And so all the keys that actually have the same hash went up in the same place. So like this is machine one is machine two is machine three.</p>
<p>533<br>00:59:00,400 –&gt; 00:59:09,400<br>So you take whatever you hash K1 that goes in here. You know, you hash, you know, whatever K2, maybe somewhere else in the file, who knows where it is.</p>
<p>534<br>00:59:09,400 –&gt; 00:59:17,400<br>And you have to keep the same hash partition. You do the same thing here for the other datasets. So there’s dataset one.</p>
<p>535<br>00:59:17,400 –&gt; 00:59:21,400<br>There’s dataset two.</p>
<p>536<br>00:59:21,400 –&gt; 00:59:32,400<br>Like links and ranks. And you know, what will happen is that all the records in this dataset that have the same keys records in the other dataset.</p>
<p>537<br>00:59:32,400 –&gt; 00:59:41,400<br>And these keys or those records will end up in the same machine. So here you’re going to petition this guy and basically K1 will end up here too.</p>
<p>538<br>00:59:41,400 –&gt; 00:59:44,400<br>On the same machine.</p>
<p>539<br>00:59:44,400 –&gt; 00:59:50,400<br>Correct. And same for the other keys because you basically used the same hash function and you have the same set of keys.</p>
<p>540<br>00:59:50,400 –&gt; 00:59:56,400<br>And so this allows you to take a dataset, you know, partition them both in the same way using this hatching trick.</p>
<p>541<br>00:59:56,400 –&gt; 01:00:07,400<br>And this is cool because now if you need to do a join over these two datasets, quite if you need to do a join over these two datasets, then basically you can just join the petitions.</p>
<p>542<br>01:00:07,400 –&gt; 01:00:09,400<br>And you don’t have to communicate.</p>
<p>543<br>01:00:09,400 –&gt; 01:00:21,400<br>You know each of these machines doesn’t have to communicate with any other machine because it knows it has all the keys that you know that the other dataset has and they’re all on the same machine.</p>
<p>544<br>01:00:21,400 –&gt; 01:00:26,400<br>And it’s just trying to sort not sort but like bucket the different.</p>
<p>545<br>01:00:26,400 –&gt; 01:00:27,400<br>Yeah, exactly.</p>
<p>546<br>01:00:27,400 –&gt; 01:00:29,400<br>In the same machine so that it does not communicate.</p>
<p>547<br>01:00:29,400 –&gt; 01:00:30,400<br>Yeah, exactly.</p>
<p>548<br>01:00:30,400 –&gt; 01:00:32,400<br>Just a bucket in trick.</p>
<p>549<br>01:00:32,400 –&gt; 01:00:34,400<br>Okay, great. Thank you so much.</p>
<p>550<br>01:00:34,400 –&gt; 01:00:35,400<br>You’re welcome.</p>
<p>551<br>01:00:35,400 –&gt; 01:00:42,400<br>So this means that the hash function has to make sure that there are no links that would have to be like.</p>
<p>552<br>01:00:42,400 –&gt; 01:00:50,400<br>Use the computation on another machine, right? Like, for instance, yeah, well, there’s since they used the same hash function and they have the same keys, you know, that will happen.</p>
<p>553<br>01:00:50,400 –&gt; 01:00:56,400<br>Yeah.</p>
<p>554<br>01:00:56,400 –&gt; 01:01:03,400<br>I had a question. I actually want to come back to the question asked before.</p>
<p>555<br>01:01:03,400 –&gt; 01:01:05,400<br>Yeah, yeah, yeah.</p>
<p>556<br>01:01:05,400 –&gt; 01:01:08,400<br>So let me open up.</p>
<p>557<br>01:01:08,400 –&gt; 01:01:11,400<br>Yeah, let me also open the paper again.</p>
<p>558<br>01:01:11,400 –&gt; 01:01:16,400<br>Any other people that have questions, if there’s maybe this will take a little bit of time.</p>
<p>559<br>01:01:16,400 –&gt; 01:01:21,400<br>So I have a question on the fault tolerance of farm.</p>
<p>560<br>01:01:21,400 –&gt; 01:01:25,400<br>So, so just to clarify what happens.</p>
<p>561<br>01:01:25,400 –&gt; 01:01:31,400<br>So if a failure occurs before the decision point.</p>
<p>562<br>01:01:31,400 –&gt; 01:01:33,400<br>Then the entire thing is aborted.</p>
<p>563<br>01:01:33,400 –&gt; 01:01:43,400<br>But if it occurs after the decision point, then after the failed computers come back up, they have to re-ass the coordinator for whether not they should commit.</p>
<p>564<br>01:01:43,400 –&gt; 01:01:49,400<br>And then they re-ask, right? Like the what what happens is they’re after failure, there’s a recovery process runs.</p>
<p>565<br>01:01:49,400 –&gt; 01:01:58,400<br>And the repartured process looks basically and all the logs are drained and and then the recovery process looks at the state of the system.</p>
<p>566<br>01:01:58,400 –&gt; 01:02:04,400<br>And based on the state of the system, it decides what to do with the transaction either the board should or commits it.</p>
<p>567<br>01:02:04,400 –&gt; 01:02:18,400<br>The key aspect here in this protocol is to ensure that at the point when the transaction coordinator actually have reported to the application that the transaction succeeded committed.</p>
<p>568<br>01:02:18,400 –&gt; 01:02:27,400<br>And it has to be the case that there’s sort of enough pieces of evidence left around in the system so that during the recovery process that transaction is definitely committed.</p>
<p>569<br>01:02:27,400 –&gt; 01:02:31,400<br>And that’s not sort of the plan.</p>
<p>570<br>01:02:31,400 –&gt; 01:02:36,400<br>And the reason that there’s enough evidence is because there’s this log records lying around.</p>
<p>571<br>01:02:36,400 –&gt; 01:02:43,400<br>There’s this command backup record right lying around and there’s this one commit record.</p>
<p>572<br>01:02:43,400 –&gt; 01:02:52,400<br>I see. So if something, for example, if a failure occurs on a primary before it gets the commit primary.</p>
<p>573<br>01:02:52,400 –&gt; 01:02:54,400<br>What happens there?</p>
<p>574<br>01:02:54,400 –&gt; 01:03:02,400<br>So there’s enough backup records correct to basically decide that every backup that every chart actually has committed.</p>
<p>575<br>01:03:02,400 –&gt; 01:03:07,400<br>And so that’s enough information for the recovery process to say, yeah, yeah, I’m going to run for it.</p>
<p>576<br>01:03:07,400 –&gt; 01:03:11,400<br>That transaction because it could have committed.</p>
<p>577<br>01:03:11,400 –&gt; 01:03:13,400<br>Got it. So it doesn’t need the primary in that case.</p>
<p>578<br>01:03:13,400 –&gt; 01:03:17,400<br>It can use the backups because the backups have the commit.</p>
<p>579<br>01:03:17,400 –&gt; 01:03:20,400<br>Exactly.</p>
<p>580<br>01:03:20,400 –&gt; 01:03:22,400<br>So I’m going to try to thank you.</p>
<p>581<br>01:03:22,400 –&gt; 01:03:25,400<br>And what happens if the backup fails?</p>
<p>582<br>01:03:25,400 –&gt; 01:03:30,400<br>Well, the one of the backup fails, presumably that means that we commit records are still there.</p>
<p>583<br>01:03:30,400 –&gt; 01:03:35,400<br>And then again, that there’s enough information to decide that actually the transaction needs to commit.</p>
<p>584<br>01:03:35,400 –&gt; 01:03:41,400<br>And there’s enough backups around to actually know what the new values, but there’s also the log entries, which actually contain.</p>
<p>585<br>01:03:41,400 –&gt; 01:03:49,400<br>So if a primary is up and we’ll have a log entry, it will be commit entry plus there’s enough backup to actually finish the transaction.</p>
<p>586<br>01:03:50,400 –&gt; 01:03:52,400<br>Thank you.</p>
<p>587<br>01:03:52,400 –&gt; 01:03:54,400<br>Sorry to follow up on that.</p>
<p>588<br>01:03:54,400 –&gt; 01:04:01,400<br>If you said that the primary failed, then you could use the backups to complete the transaction.</p>
<p>589<br>01:04:01,400 –&gt; 01:04:02,400<br>There’s enough of them.</p>
<p>590<br>01:04:02,400 –&gt; 01:04:06,400<br>Or would you need to elect a new primary?</p>
<p>591<br>01:04:06,400 –&gt; 01:04:10,400<br>I think this is all happened during the.</p>
<p>592<br>01:04:10,400 –&gt; 01:04:13,400<br>Basically, you can think of the recovery process as a primary.</p>
<p>593<br>01:04:13,400 –&gt; 01:04:16,400<br>And I just finished is everything off.</p>
<p>594<br>01:04:16,400 –&gt; 01:04:19,400<br>Oh, so a cover does the recovery is the primary?</p>
<p>595<br>01:04:19,400 –&gt; 01:04:22,400<br>Yeah.</p>
<p>596<br>01:04:22,400 –&gt; 01:04:23,400<br>Okay.</p>
<p>597<br>01:04:23,400 –&gt; 01:04:24,400<br>Makes sense.</p>
<p>598<br>01:04:24,400 –&gt; 01:04:25,400<br>Thank you.</p>
<p>599<br>01:04:25,400 –&gt; 01:04:27,400<br>I don’t think explicitly they promote a primary.</p>
<p>600<br>01:04:27,400 –&gt; 01:04:30,400<br>You just just like go ahead and do it.</p>
<p>601<br>01:04:30,400 –&gt; 01:04:32,400<br>And what is like enough backups?</p>
<p>602<br>01:04:32,400 –&gt; 01:04:35,400<br>Well, we have f plus one.</p>
<p>603<br>01:04:35,400 –&gt; 01:04:36,400<br>Right.</p>
<p>604<br>01:04:36,400 –&gt; 01:04:40,400<br>And so it means that, you know, so long as one is left.</p>
<p>605<br>01:04:40,400 –&gt; 01:04:42,400<br>And we were good.</p>
<p>606<br>01:04:42,400 –&gt; 01:04:45,400<br>So we can have more than f plus one failures.</p>
<p>607<br>01:04:45,400 –&gt; 01:04:48,400<br>We can only have f failures in this particular drawing.</p>
<p>608<br>01:04:48,400 –&gt; 01:04:51,400<br>F is one.</p>
<p>609<br>01:04:51,400 –&gt; 01:04:56,400<br>So there has to be per shard, you know, one machine left.</p>
<p>610<br>01:04:56,400 –&gt; 01:04:57,400<br>Okay. That makes sense.</p>
<p>611<br>01:04:57,400 –&gt; 01:04:58,400<br>Thank you.</p>
<p>612<br>01:04:58,400 –&gt; 01:05:02,400<br>You’re welcome.</p>
<p>613<br>01:05:02,400 –&gt; 01:05:06,400<br>I can’t believe it’s just me and you.</p>
<p>614<br>01:05:06,400 –&gt; 01:05:09,400<br>Well, I’m not as anyone else has questions.</p>
<p>615<br>01:05:09,400 –&gt; 01:05:11,400<br>I’m not a person.</p>
<p>616<br>01:05:11,400 –&gt; 01:05:13,400<br>I’m not a person.</p>
<p>617<br>01:05:13,400 –&gt; 01:05:15,400<br>I’m not a person.</p>
<p>618<br>01:05:15,400 –&gt; 01:05:16,400<br>Yeah.</p>
<p>619<br>01:05:16,400 –&gt; 01:05:19,400<br>So it’s it’s page.</p>
<p>620<br>01:05:19,400 –&gt; 01:05:22,400<br>The explainer page six.</p>
<p>621<br>01:05:22,400 –&gt; 01:05:24,400<br>Yeah, yeah.</p>
<p>622<br>01:05:24,400 –&gt; 01:05:26,400<br>Right below table three.</p>
<p>623<br>01:05:26,400 –&gt; 01:05:28,400<br>Yep.</p>
<p>624<br>01:05:28,400 –&gt; 01:05:29,400<br>So bear.</p>
<p>625<br>01:05:29,400 –&gt; 01:05:32,400<br>It starts the most interesting question, right?</p>
<p>626<br>01:05:32,400 –&gt; 01:05:35,400<br>It goes on to define.</p>
<p>627<br>01:05:35,400 –&gt; 01:05:37,400<br>ρηabчикin double either.</p>
<p>628<br>01:05:37,400 –&gt; 01:05:40,400<br>Right i’m not a person.</p>
<p>629<br>01:05:40,400 –&gt; 01:05:42,400<br>What’s that used?</p>
<p>630<br>01:05:42,400 –&gt; 01:05:43,400<br>It goes on to quite famously.</p>
<p>631<br>01:05:43,400 –&gt; 01:05:45,400<br>That’s not usually the question, but it says.</p>
<p>632<br>01:05:45,400 –&gt; 01:05:46,400<br>That’s.</p>
<p>633<br>01:05:46,400 –&gt; 01:05:48,400<br>The answer for the reason.</p>
<p>634<br>01:05:48,400 –&gt; 01:05:49,400<br>Now.</p>
<p>635<br>01:05:49,400 –&gt; 01:05:53,400<br>If we were to replace the two candidate Philanithotto.</p>
<p>636<br>01:05:53,400 –&gt; 01:05:54,400<br>Okay.</p>
<p>637<br>01:05:54,400 –&gt; 01:05:57,400<br>If we were to replace him for four or five years.</p>
<p>638<br>01:05:57,400 –&gt; 01:05:59,400<br>You know, one of the questions is.</p>
<p>639<br>01:05:59,400 –&gt; 01:06:03,400<br>The answer is incorrect.</p>
<p>640<br>01:06:03,400 –&gt; 01:06:05,400<br>Right.</p>
<p>641<br>01:06:05,400 –&gt; 01:06:07,400<br>Good.</p>
<p>642<br>01:06:07,400 –&gt; 01:06:12,400<br>That’s the narrow. This is the narrow case.</p>
<p>643<br>01:06:12,400 –&gt; 01:06:16,400<br>Right. But,</p>
<p>644<br>01:06:16,400 –&gt; 01:06:19,400<br>I think like the,</p>
<p>645<br>01:06:19,400 –&gt; 01:06:23,400<br>the example I was taking off as, you know,</p>
<p>646<br>01:06:23,400 –&gt; 01:06:29,400<br>each parent is used by at most.</p>
<p>647<br>01:06:29,400 –&gt; 01:06:32,400<br>One partition of the child.</p>
<p>648<br>01:06:32,400 –&gt; 01:06:35,400<br>Right.</p>
<p>649<br>01:06:35,400 –&gt; 01:06:38,400<br>That doesn’t say anything about like it says,</p>
<p>650<br>01:06:38,400 –&gt; 01:06:41,400<br>right, like it doesn’t necessarily mean it’s a one to one relationship.</p>
<p>651<br>01:06:41,400 –&gt; 01:06:42,400<br>Right.</p>
<p>652<br>01:06:42,400 –&gt; 01:06:44,400<br>Well, because more or less have to,</p>
<p>653<br>01:06:44,400 –&gt; 01:06:45,400<br>like just let me,</p>
<p>654<br>01:06:45,400 –&gt; 01:06:47,400<br>let’s say we have a white one correct.</p>
<p>655<br>01:06:47,400 –&gt; 01:06:49,400<br>So then.</p>
<p>656<br>01:06:49,400 –&gt; 01:06:52,400<br>Here we have parent.</p>
<p>657<br>01:06:52,400 –&gt; 01:06:55,400<br>Partition one.</p>
<p>658<br>01:06:55,400 –&gt; 01:06:59,400<br>Here we have, you know, maybe it has end of them correct.</p>
<p>659<br>01:06:59,400 –&gt; 01:07:02,400<br>Right.</p>
<p>660<br>01:07:02,400 –&gt; 01:07:03,400<br>Right.</p>
<p>661<br>01:07:03,400 –&gt; 01:07:07,400<br>In the white one.</p>
<p>662<br>01:07:07,400 –&gt; 01:07:10,400<br>The child is.</p>
<p>663<br>01:07:10,400 –&gt; 01:07:12,400<br>Right.</p>
<p>664<br>01:07:12,400 –&gt; 01:07:14,400<br>What, what I, what I was saying is,</p>
<p>665<br>01:07:14,400 –&gt; 01:07:18,400<br>I think, you know, based on the definition given the paper,</p>
<p>666<br>01:07:18,400 –&gt; 01:07:24,400<br>this could be a narrow partition.</p>
<p>667<br>01:07:24,400 –&gt; 01:07:25,400<br>And in fact, I mean, like,</p>
<p>668<br>01:07:25,400 –&gt; 01:07:28,400<br>if you look at,</p>
<p>669<br>01:07:28,400 –&gt; 01:07:30,400<br>like a join with inputs,</p>
<p>670<br>01:07:30,400 –&gt; 01:07:31,400<br>a quote partition,</p>
<p>671<br>01:07:31,400 –&gt; 01:07:33,400<br>like you have.</p>
<p>672<br>01:07:33,400 –&gt; 01:07:36,400<br>Yeah, the expectation that actually the white one turns into an arrow one.</p>
<p>673<br>01:07:36,400 –&gt; 01:07:37,400<br>Right.</p>
<p>674<br>01:07:37,400 –&gt; 01:07:39,400<br>But, but you still have,</p>
<p>675<br>01:07:39,400 –&gt; 01:07:41,400<br>like a partition,</p>
<p>676<br>01:07:41,400 –&gt; 01:07:43,400<br>a child partition, getting like,</p>
<p>677<br>01:07:43,400 –&gt; 01:07:46,400<br>and like being computed from several.</p>
<p>678<br>01:07:46,400 –&gt; 01:07:47,400<br>Parent partition.</p>
<p>679<br>01:07:47,400 –&gt; 01:07:48,400<br>Yeah.</p>
<p>680<br>01:07:48,400 –&gt; 01:07:49,400<br>Parent partitions.</p>
<p>681<br>01:07:49,400 –&gt; 01:07:50,400<br>Yeah.</p>
<p>682<br>01:07:50,400 –&gt; 01:07:51,400<br>I think they,</p>
<p>683<br>01:07:51,400 –&gt; 01:07:53,400<br>they explicitly mentioned that, right?</p>
<p>684<br>01:07:53,400 –&gt; 01:07:56,400<br>I think they have a very similar example.</p>
<p>685<br>01:07:56,400 –&gt; 01:07:58,400<br>Yeah, but like.</p>
<p>686<br>01:07:58,400 –&gt; 01:08:00,400<br>I think the type of sentence, right?</p>
<p>687<br>01:08:00,400 –&gt; 01:08:07,400<br>I, I mean, I’m not, I’m not sure if it’s, yeah, I’m not sure like if it’s,</p>
<p>688<br>01:08:07,400 –&gt; 01:08:11,400<br>what they meant to like write or.</p>
<p>689<br>01:08:11,400 –&gt; 01:08:14,400<br>Well, we know we conclude, this is the two cases.</p>
<p>690<br>01:08:14,400 –&gt; 01:08:15,400<br>Like I got.</p>
<p>691<br>01:08:15,400 –&gt; 01:08:16,399<br>Yeah.</p>
<p>692<br>01:08:16,399 –&gt; 01:08:18,399<br>I guess there are no other cases.</p>
<p>693<br>01:08:18,399 –&gt; 01:08:19,399<br>Well,</p>
<p>694<br>01:08:19,399 –&gt; 01:08:21,399<br>we can go for every operation, correct?</p>
<p>695<br>01:08:21,399 –&gt; 01:08:24,399<br>And then we can see whether it’s a narrower white one.</p>
<p>696<br>01:08:24,399 –&gt; 01:08:25,399<br>Right.</p>
<p>697<br>01:08:25,399 –&gt; 01:08:26,399<br>Right.</p>
<p>698<br>01:08:26,399 –&gt; 01:08:27,399<br>So,</p>
<p>699<br>01:08:27,399 –&gt; 01:08:28,399<br>and the ones that are,</p>
<p>700<br>01:08:28,399 –&gt; 01:08:31,399<br>and it’s the job of the programmer that defines these operations to actually indicate whether</p>
<p>701<br>01:08:31,399 –&gt; 01:08:33,399<br>there’s a white partition or an arrow,</p>
<p>702<br>01:08:33,399 –&gt; 01:08:34,399<br>which white depends,</p>
<p>703<br>01:08:34,399 –&gt; 01:08:36,399<br>you are a narrow dependency, correct?</p>
<p>704<br>01:08:36,399 –&gt; 01:08:39,399<br>That’s what the figure of a table for you to about.</p>
<p>705<br>01:08:39,399 –&gt; 01:08:40,399<br>Uh-huh.</p>
<p>706<br>01:08:40,399 –&gt; 01:08:41,399<br>Yeah.</p>
<p>707<br>01:08:41,399 –&gt; 01:08:44,399<br>Like what I’m saying is like usually like,</p>
<p>708<br>01:08:44,399 –&gt; 01:08:46,399<br>like the way like I saw it through the paper,</p>
<p>709<br>01:08:46,399 –&gt; 01:08:47,399<br>like a, like,</p>
<p>710<br>01:08:47,399 –&gt; 01:08:49,399<br>your example on the right would be,</p>
<p>711<br>01:08:49,399 –&gt; 01:08:52,399<br>uh, could, would be a narrow dependency on less,</p>
<p>712<br>01:08:52,399 –&gt; 01:08:53,399<br>right?</p>
<p>713<br>01:08:53,399 –&gt; 01:08:57,399<br>Like you have several child and the parent partitions are like,</p>
<p>714<br>01:08:57,399 –&gt; 01:08:58,399<br>uh, okay.</p>
<p>715<br>01:08:58,399 –&gt; 01:09:00,399<br>So in general, okay, it is the case of court.</p>
<p>716<br>01:09:00,399 –&gt; 01:09:02,399<br>Like if there’s another child partition here,</p>
<p>717<br>01:09:02,399 –&gt; 01:09:04,399<br>okay, so I’ll maybe just wipe them, try to get it.</p>
<p>718<br>01:09:04,399 –&gt; 01:09:08,399<br>So let’s just separate the real picture that actually draws this.</p>
<p>719<br>01:09:08,399 –&gt; 01:09:12,399<br>There is another child partition and basically operations.</p>
<p>720<br>01:09:12,399 –&gt; 01:09:17,399<br>Uh, the transformations are.</p>
<p>721<br>01:09:17,399 –&gt; 01:09:19,399<br>Exactly. Yeah.</p>
<p>722<br>01:09:19,399 –&gt; 01:09:21,399<br>And that’s narrow for sure.</p>
<p>723<br>01:09:21,399 –&gt; 01:09:22,399<br>Really.</p>
<p>724<br>01:09:22,399 –&gt; 01:09:25,399<br>This on the right side, this is why that’s white.</p>
<p>725<br>01:09:25,399 –&gt; 01:09:26,399<br>Yeah, yeah, that’s what I meant.</p>
<p>726<br>01:09:26,399 –&gt; 01:09:28,399<br>That that for sure is white.</p>
<p>727<br>01:09:28,399 –&gt; 01:09:32,399<br>Yeah, the one you know what I drew is also white, I believe.</p>
<p>728<br>01:09:32,399 –&gt; 01:09:33,399<br>Okay.</p>
<p>729<br>01:09:33,399 –&gt; 01:09:36,399<br>If you do a, if you do an action like,</p>
<p>730<br>01:09:36,399 –&gt; 01:09:38,399<br>collect the variant,</p>
<p>731<br>01:09:38,399 –&gt; 01:09:40,399<br>yeah, it’s a white dependency.</p>
<p>732<br>01:09:40,399 –&gt; 01:09:41,399<br>Okay.</p>
<p>733<br>01:09:41,399 –&gt; 01:09:42,399<br>It does it saying that it does the same.</p>
<p>734<br>01:09:42,399 –&gt; 01:09:43,399<br>It has to come from different RUDs.</p>
<p>735<br>01:09:43,399 –&gt; 01:09:45,399<br>It just says like it has to come from different partitions.</p>
<p>736<br>01:09:45,399 –&gt; 01:09:47,399<br>So this is narrow.</p>
<p>737<br>01:09:47,399 –&gt; 01:09:50,399<br>So I think narrow is only the case where there’s one to one.</p>
<p>738<br>01:09:50,399 –&gt; 01:09:51,399<br>Okay.</p>
<p>739<br>01:09:51,399 –&gt; 01:09:55,399<br>There is like narrow means no communication.</p>
<p>740<br>01:09:55,399 –&gt; 01:09:56,399<br>Right.</p>
<p>741<br>01:09:56,399 –&gt; 01:10:03,399<br>Okay.</p>
<p>742<br>01:10:03,399 –&gt; 01:10:07,399<br>Yeah.</p>
<p>743<br>01:10:07,399 –&gt; 01:10:13,399<br>Yeah, I think I think the case where I was confused was like the like.</p>
<p>744<br>01:10:13,399 –&gt; 01:10:19,399<br>Yeah.</p>
<p>745<br>01:10:19,399 –&gt; 01:10:21,399<br>And many to one like.</p>
<p>746<br>01:10:21,399 –&gt; 01:10:23,399<br>I think based on the definition of them like of the paper,</p>
<p>747<br>01:10:23,399 –&gt; 01:10:25,399<br>like the many to one relation is still.</p>
<p>748<br>01:10:25,399 –&gt; 01:10:26,399<br>It’s still narrow.</p>
<p>749<br>01:10:26,399 –&gt; 01:10:28,399<br>No, I think they mean it to be.</p>
<p>750<br>01:10:28,399 –&gt; 01:10:29,399<br>I mean, yeah, yeah, yeah.</p>
<p>751<br>01:10:29,399 –&gt; 01:10:31,399<br>But like strictly like if you read like.</p>
<p>752<br>01:10:31,399 –&gt; 01:10:34,399<br>Yeah, I think maybe like an implementation.</p>
<p>753<br>01:10:34,399 –&gt; 01:10:35,399<br>You’ll see like.</p>
<p>754<br>01:10:35,399 –&gt; 01:10:37,399<br>Yeah, what are you saying? Right?</p>
<p>755<br>01:10:37,399 –&gt; 01:10:38,399<br>Like it’s white.</p>
<p>756<br>01:10:38,399 –&gt; 01:10:40,399<br>I was just like, I think if you read like the.</p>
<p>757<br>01:10:40,399 –&gt; 01:10:41,399<br>Yeah, you could be confusing.</p>
<p>758<br>01:10:41,399 –&gt; 01:10:43,399<br>Yeah, you can get confused.</p>
<p>759<br>01:10:43,399 –&gt; 01:10:45,399<br>Like the many you want to relate to that.</p>
<p>760<br>01:10:45,399 –&gt; 01:10:47,399<br>The one to many is clearly why.</p>
<p>761<br>01:10:47,399 –&gt; 01:10:48,399<br>Yeah.</p>
<p>762<br>01:10:48,399 –&gt; 01:10:50,399<br>But yeah, okay.</p>
<p>763<br>01:10:50,399 –&gt; 01:10:51,399<br>Sounds good.</p>
<p>764<br>01:10:51,399 –&gt; 01:10:52,399<br>Okay.</p>
<p>765<br>01:10:52,399 –&gt; 01:10:54,399<br>Yeah, that was.</p>
<p>766<br>01:10:54,399 –&gt; 01:10:58,399<br>I think the paper is easier to understand in general.</p>
<p>767<br>01:10:58,399 –&gt; 01:10:59,399<br>Okay.</p>
<p>768<br>01:10:59,399 –&gt; 01:11:00,399<br>Good.</p>
<p>769<br>01:11:00,399 –&gt; 01:11:01,399<br>Okay.</p>
<p>770<br>01:11:01,399 –&gt; 01:11:04,399<br>That is what we need to understand paper does the farm.</p>
<p>771<br>01:11:04,399 –&gt; 01:11:05,399<br>Oh, yeah.</p>
<p>772<br>01:11:05,399 –&gt; 01:11:06,399<br>Yeah.</p>
<p>773<br>01:11:06,399 –&gt; 01:11:07,399<br>Yeah, I think.</p>
<p>774<br>01:11:07,399 –&gt; 01:11:15,399<br>I think those were probably the most too heavy duty papers that we’ll all seem to start.</p>
<p>775<br>01:11:15,399 –&gt; 01:11:16,399<br>Okay.</p>
<p>776<br>01:11:16,399 –&gt; 01:11:17,399<br>Nice.</p>
<p>777<br>01:11:17,399 –&gt; 01:11:18,399<br>Farm is spanner.</p>
<p>778<br>01:11:18,399 –&gt; 01:11:21,399<br>I think the remaining ones are a little bit more.</p>
<p>779<br>01:11:21,399 –&gt; 01:11:22,399<br>More.</p>
<p>780<br>01:11:22,399 –&gt; 01:11:24,399<br>I’m going to say straight forward, but.</p>
<p>781<br>01:11:24,399 –&gt; 01:11:27,399<br>Perhaps a few are moving pieces.</p>
<p>782<br>01:11:27,399 –&gt; 01:11:29,399<br>Nice.</p>
<p>783<br>01:11:29,399 –&gt; 01:11:31,399<br>Okay. Awesome.</p>
<p>784<br>01:11:31,399 –&gt; 01:11:32,399<br>Thanks for testing.</p>
<p>785<br>01:11:32,399 –&gt; 01:11:34,399<br>Can I ask one last question?</p>
<p>786<br>01:11:34,399 –&gt; 01:11:37,399<br>I just realized that I have.</p>
<p>787<br>01:11:37,399 –&gt; 01:11:39,399<br>It was about the conversation.</p>
<p>788<br>01:11:39,399 –&gt; 01:11:40,399<br>You can paralyze it.</p>
<p>789<br>01:11:40,399 –&gt; 01:11:41,399<br>If.</p>
<p>790<br>01:11:41,399 –&gt; 01:11:44,399<br>If it’s on different predictions, but if it’s also.</p>
<p>791<br>01:11:44,399 –&gt; 01:11:46,399<br>You said if it’s.</p>
<p>792<br>01:11:46,399 –&gt; 01:11:48,399<br>Yeah, it’s a stage, the stages, right?</p>
<p>793<br>01:11:48,399 –&gt; 01:11:50,399<br>You know, there’s sort of.</p>
<p>794<br>01:11:50,399 –&gt; 01:11:54,399<br>What is like streaming parallelism, if you will, or pipeline parallelism.</p>
<p>795<br>01:11:54,399 –&gt; 01:11:57,399<br>Let me see if I can find a picture.</p>
<p>796<br>01:11:57,399 –&gt; 01:11:59,399<br>As one of them.</p>
<p>797<br>01:11:59,399 –&gt; 01:12:02,399<br>Boom.</p>
<p>798<br>01:12:02,399 –&gt; 01:12:05,399<br>I got to find a lineage graph.</p>
<p>799<br>01:12:05,399 –&gt; 01:12:06,399<br>No.</p>
<p>800<br>01:12:06,399 –&gt; 01:12:07,399<br>No.</p>
<p>801<br>01:12:07,399 –&gt; 01:12:09,399<br>Here’s what a lineage graph.</p>
<p>802<br>01:12:09,399 –&gt; 01:12:10,399<br>Greg.</p>
<p>803<br>01:12:10,399 –&gt; 01:12:13,399<br>Maybe here’s a picture that we can modify.</p>
<p>804<br>01:12:13,399 –&gt; 01:12:15,399<br>Do you see it?</p>
<p>805<br>01:12:15,399 –&gt; 01:12:16,399<br>Yes.</p>
<p>806<br>01:12:16,399 –&gt; 01:12:17,399<br>Okay.</p>
<p>807<br>01:12:17,399 –&gt; 01:12:20,399<br>So basically, this is the lineage graph.</p>
<p>808<br>01:12:20,399 –&gt; 01:12:21,399<br>He was to collect.</p>
<p>809<br>01:12:21,399 –&gt; 01:12:25,399<br>And so this is like one stage work.</p>
<p>810<br>01:12:25,399 –&gt; 01:12:27,399<br>And just guys are runs one of these stages.</p>
<p>811<br>01:12:27,399 –&gt; 01:12:29,399<br>And each worker.</p>
<p>812<br>01:12:29,399 –&gt; 01:12:30,399<br>For each petition.</p>
<p>813<br>01:12:30,399 –&gt; 01:12:31,399<br>Right.</p>
<p>814<br>01:12:31,399 –&gt; 01:12:33,399<br>So each.</p>
<p>815<br>01:12:33,399 –&gt; 01:12:35,399<br>Worker.</p>
<p>816<br>01:12:35,399 –&gt; 01:12:39,399<br>Runs a stage on the petition.</p>
<p>817<br>01:12:39,399 –&gt; 01:12:48,399<br>So basically all these petitions are all these stages running parallel on different workers.</p>
<p>818<br>01:12:48,399 –&gt; 01:12:51,399<br>Then within a stage that we’re going to do.</p>
<p>819<br>01:12:51,399 –&gt; 01:12:53,399<br>And then there’s only.</p>
<p>820<br>01:12:53,399 –&gt; 01:12:57,399<br>Because the stage is running parallel on different workers.</p>
<p>821<br>01:12:57,399 –&gt; 01:13:05,399<br>Then within a stage, there’s also parallelism because, you know, every filter.</p>
<p>822<br>01:13:05,399 –&gt; 01:13:06,399<br>Is pipeline.</p>
<p>823<br>01:13:06,399 –&gt; 01:13:12,399<br>Without, you know, they mean, like you really need to like the first end records.</p>
<p>824<br>01:13:12,399 –&gt; 01:13:15,399<br>And then you apply the filter operation.</p>
<p>825<br>01:13:15,399 –&gt; 01:13:16,399<br>And then.</p>
<p>826<br>01:13:16,399 –&gt; 01:13:18,399<br>I’ll produce this.</p>
<p>827<br>01:13:18,399 –&gt; 01:13:24,960<br>and records. And then the next filter processed those and records,</p>
<p>828<br>01:13:24,960 –&gt; 01:13:28,960<br>while it’s processing those and records, the first filter reads the next end.</p>
<p>829<br>01:13:30,559 –&gt; 01:13:36,000<br>And then produces them and passes them on and then makes them results in some number of records.</p>
<p>830<br>01:13:36,000 –&gt; 01:13:45,279<br>And again, it goes on and on. And so basically all these transformations are pipeline.</p>
<p>831<br>01:13:45,279 –&gt; 01:13:51,920<br>And so they’re almost running concurrently. You’re running not truly concurrently,</p>
<p>832<br>01:13:51,920 –&gt; 01:13:57,920<br>but they’re running in a pipeline fashion. Oh, I just said this is the batch thing that they were talking about.</p>
<p>833<br>01:13:59,599 –&gt; 01:14:04,079<br>So things are passing badges and basically every stage of the pipeline processes batch.</p>
<p>834<br>01:14:05,519 –&gt; 01:14:10,159<br>Okay, okay, yeah, that makes it super clear. Yeah, thank you so much. That was that was an interesting</p>
<p>835<br>01:14:10,159 –&gt; 01:14:15,359<br>lecture. Thank you. Okay, you’re welcome. Glad you enjoyed it. It’s a cool system.</p>
<p>836<br>01:14:23,760 –&gt; 01:14:29,599<br>Sorry, sorry, can you hear me now? Yeah, yeah, I didn’t hear. I’m wondering what did you just check that?</p>
<p>837<br>01:14:29,599 –&gt; 01:14:35,199<br>No problem. No, I’m sorry about that. I, yeah, sorry, I was listening. I um,</p>
<p>838<br>01:14:35,199 –&gt; 01:14:39,599<br>I don’t know. I realized we’d be going late. I’m going to try to make this question very quick.</p>
<p>839<br>01:14:39,599 –&gt; 01:14:46,399<br>I’ve gotten to use Spark before. I’ve adjusted my schedule. Thank you. Thank you so much.</p>
<p>840<br>01:14:46,399 –&gt; 01:14:54,000<br>So yeah, yeah, I really, really appreciate this lecture. Spark is actually something that I’m going to use in my future job.</p>
<p>841<br>01:14:54,000 –&gt; 01:15:01,439<br>So I appreciate you teaching this to me. Just one thing probably, I’m not sure this lecture will really help you writing Spark programs.</p>
<p>842<br>01:15:01,439 –&gt; 01:15:09,439<br>No, I, I mean, I did it as an intern really not knowing what I was doing, but like this has helped me give, like,</p>
<p>843<br>01:15:09,439 –&gt; 01:15:17,759<br>get more context with it. So I guess the, the quick question with Spark programs that I, um, the way that I’ve</p>
<p>844<br>01:15:17,759 –&gt; 01:15:26,239<br>understood Spark jobs is how Spark constructs a directed, a directed acyclic graph of all the tasks and.</p>
<p>845<br>01:15:26,239 –&gt; 01:15:31,840<br>This is what you talked about with the wide partitions and narrow partitions.</p>
<p>846<br>01:15:31,840 –&gt; 01:15:36,239<br>So, yeah, I guess they’re called dependencies, not partitions, but.</p>
<p>847<br>01:15:36,239 –&gt; 01:15:48,239<br>Oh, yep, sorry. Okay. Okay. Okay. Sorry. Um, that with, with RDDs, then I guess like, okay, this is different terminology between.</p>
<p>848<br>01:15:48,639 –&gt; 01:16:00,239<br>The dependencies are like these tasks and the directed acyclic graph of all the tasks, but the RDDs that like each task in this graph is not represented by this RDD.</p>
<p>849<br>01:16:00,239 –&gt; 01:16:04,239<br>Let me actually go back. So maybe dispatchers that are right.</p>
<p>850<br>01:16:04,239 –&gt; 01:16:09,239<br>Yeah. And oh gosh, I appreciate you sitting here. Please let me know if you have to go.</p>
<p>851<br>01:16:09,239 –&gt; 01:16:12,239<br>No, no, no, no, no, this point. Yeah, you have some more time. Yeah, yeah.</p>
<p>852<br>01:16:12,239 –&gt; 01:16:16,239<br>Thank you. Um, so.</p>
<p>853<br>01:16:16,239 –&gt; 01:16:19,239<br>Okay, so this is sort of an RDD, correct?</p>
<p>854<br>01:16:19,239 –&gt; 01:16:23,239<br>Let me draw another color so we can agree.</p>
<p>855<br>01:16:23,239 –&gt; 01:16:25,239<br>This is an RDD.</p>
<p>856<br>01:16:25,239 –&gt; 01:16:28,239<br>Okay, an RDD has a bunch of partitions.</p>
<p>857<br>01:16:28,239 –&gt; 01:16:33,239<br>And here’s another RDD.</p>
<p>858<br>01:16:33,239 –&gt; 01:16:35,239<br>Okay. Yep.</p>
<p>859<br>01:16:35,239 –&gt; 01:16:40,239<br>And then the arrows are basically as sort of like a near same story.</p>
<p>860<br>01:16:40,239 –&gt; 01:16:45,239<br>Let me actually finish this picture too on the site here’s more partitions.</p>
<p>861<br>01:16:45,239 –&gt; 01:16:51,239<br>Here’s the RDDs from room, RDD, room, RDD.</p>
<p>862<br>01:16:51,239 –&gt; 01:16:57,239<br>And then the transformations basically between RDDs, correct? And so in the arrows.</p>
<p>863<br>01:16:57,239 –&gt; 01:17:01,239<br>Let me pick another color. These errors.</p>
<p>864<br>01:17:01,239 –&gt; 01:17:04,239<br>Those are transformations.</p>
<p>865<br>01:17:04,239 –&gt; 01:17:09,239<br>And then the only thing is like some of these arrows are white and some of them are narrow.</p>
<p>866<br>01:17:09,239 –&gt; 01:17:13,239<br>And the graph from the graph, he can’t really tell.</p>
<p>867<br>01:17:13,239 –&gt; 01:17:19,239<br>Which ones are narrow or which ones are, which transformations are certain narrow transformations.</p>
<p>868<br>01:17:19,239 –&gt; 01:17:21,239<br>And white transformations.</p>
<p>869<br>01:17:21,239 –&gt; 01:17:23,239<br>You’re talking about the transformation.</p>
<p>870<br>01:17:23,239 –&gt; 01:17:24,239<br>Yeah, I’m not sure.</p>
<p>871<br>01:17:24,239 –&gt; 01:17:25,239<br>I’m not sure.</p>
<p>872<br>01:17:25,239 –&gt; 01:17:26,239<br>I’m not sure.</p>
<p>873<br>01:17:26,239 –&gt; 01:17:27,239<br>I’m not sure.</p>
<p>874<br>01:17:27,239 –&gt; 01:17:28,239<br>I’m not sure.</p>
<p>875<br>01:17:28,239 –&gt; 01:17:29,239<br>I’m not sure.</p>
<p>876<br>01:17:29,239 –&gt; 01:17:30,239<br>I’m not sure.</p>
<p>877<br>01:17:30,239 –&gt; 01:17:31,239<br>I’m not sure.</p>
<p>878<br>01:17:31,239 –&gt; 01:17:34,239<br>And white transformations.</p>
<p>879<br>01:17:34,239 –&gt; 01:17:38,239<br>You’re talking about the graph as the spark program actually shows you in there.</p>
<p>880<br>01:17:38,239 –&gt; 01:17:40,239<br>Yeah, there’s lineage graph.</p>
<p>881<br>01:17:40,239 –&gt; 01:17:41,239<br>You can’t tell.</p>
<p>882<br>01:17:41,239 –&gt; 01:17:42,239<br>Gotcha.</p>
<p>883<br>01:17:42,239 –&gt; 01:17:43,239<br>Okay.</p>
<p>884<br>01:17:43,239 –&gt; 01:17:44,239<br>And that’s the here.</p>
<p>885<br>01:17:44,239 –&gt; 01:17:45,239<br>So here.</p>
<p>886<br>01:17:45,239 –&gt; 01:17:47,239<br>Look at this lineage graph.</p>
<p>887<br>01:17:47,239 –&gt; 01:17:50,239<br>This transformation, that transformation, that transformation, all that transformation.</p>
<p>888<br>01:17:50,239 –&gt; 01:17:52,239<br>No, I would like narrow, because it’s a single arrow.</p>
<p>889<br>01:17:52,239 –&gt; 01:17:54,239<br>But it’s not really true. Right?</p>
<p>890<br>01:17:54,239 –&gt; 01:17:58,239<br>Like the last one, for example, must be a white one.</p>
<p>891<br>01:17:58,239 –&gt; 01:18:00,239<br>Because there’s more collective information from all of that.</p>
<p>892<br>01:18:00,239 –&gt; 01:18:01,239<br>That’s it.</p>
<p>893<br>01:18:03,239 –&gt; 01:18:04,239<br>Okay.</p>
<p>894<br>01:18:04,239 –&gt; 01:18:19,239<br>Then I guess I was wondering, like, do you have recommendations on resources for things that can show me how spark figures out how to construct the directed acyclic graph to do all these tasks.</p>
<p>895<br>01:18:19,239 –&gt; 01:18:20,239<br>Yeah.</p>
<p>896<br>01:18:20,239 –&gt; 01:18:22,239<br>Look at this.</p>
<p>897<br>01:18:22,239 –&gt; 01:18:25,239<br>It’s really a disclaimer that does all the right.</p>
<p>898<br>01:18:25,239 –&gt; 01:18:33,239<br>And yeah, reading the paper, like, I, you know, I was trying to comprehend the paper as best as I could, but it’s, you know, it’s difficult to put.</p>
<p>899<br>01:18:33,239 –&gt; 01:18:34,239<br>Yeah.</p>
<p>900<br>01:18:34,239 –&gt; 01:18:35,239<br>I know all these papers are difficult to read.</p>
<p>901<br>01:18:35,239 –&gt; 01:18:43,239<br>So the scheduler, I think I would go back first to my case thesis, my case doctoral thesis. I’m sure you have to cap around the scheduler.</p>
<p>902<br>01:18:43,239 –&gt; 01:18:44,239<br>Gotcha.</p>
<p>903<br>01:18:44,239 –&gt; 01:18:48,239<br>All right. All right. All good. And that, that’ll show me just how spark figures out how to make this graph.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>MIT6824 P17Lecture16 BigData Spark</div>
      <div>http://example.com/2025/10/25/MIT6824 P17Lecture16-BigData-Spark/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月25日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/25/MIT6824%20P16Lecture15continued-OptimisticConcurrencyControlFaRMpt2/" title="MIT6824 P16Lecture15continued OptimisticConcurrencyControlFaRMpt2">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">MIT6824 P16Lecture15continued OptimisticConcurrencyControlFaRMpt2</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/25/MIT6824%20P18Lecture17-CacheConsistency-MemcachedatFacebook/" title="MIT6824 P18Lecture17 CacheConsistency MemcachedatFacebook">
                        <span class="hidden-mobile">MIT6824 P18Lecture17 CacheConsistency MemcachedatFacebook</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
