

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="100:00:00,000 –&gt; 00:00:09,500As you probably have noticed, I put up the year on the my share screen, the web page, most 200:00:09,500 –&gt; 00:00:11,500of the classes driven from the schedule. 300:">
<meta property="og:type" content="article">
<meta property="og:title" content="MIT6824 P1Lecture1 Introduction">
<meta property="og:url" content="http://example.com/2025/10/25/MIT6824%20P1Lecture1-Introduction/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="100:00:00,000 –&gt; 00:00:09,500As you probably have noticed, I put up the year on the my share screen, the web page, most 200:00:09,500 –&gt; 00:00:11,500of the classes driven from the schedule. 300:">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-25T05:03:39.803Z">
<meta property="article:modified_time" content="2025-10-25T05:03:39.804Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>MIT6824 P1Lecture1 Introduction - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="MIT6824 P1Lecture1 Introduction"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-25 13:03" pubdate>
          2025年10月25日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          8.8k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          74 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">MIT6824 P1Lecture1 Introduction</h1>
            
            
              <div class="markdown-body">
                
                <p>1<br>00:00:00,000 –&gt; 00:00:09,500<br>As you probably have noticed, I put up the year on the my share screen, the web page, most</p>
<p>2<br>00:00:09,500 –&gt; 00:00:11,500<br>of the classes driven from the schedule.</p>
<p>3<br>00:00:11,500 –&gt; 00:00:16,500<br>I’ll talk a little bit later about it, but hopefully you’ll find the URL and you’ll find</p>
<p>4<br>00:00:16,500 –&gt; 00:00:17,500<br>the schedule.</p>
<p>5<br>00:00:17,500 –&gt; 00:00:23,500<br>I’ll return it out a little bit later in more detail.</p>
<p>6<br>00:00:23,500 –&gt; 00:00:29,000<br>Okay, so what’s the point for today?</p>
<p>7<br>00:00:30,000 –&gt; 00:00:34,000<br>I’m going to talk a little bit about what is a distributed system.</p>
<p>8<br>00:00:34,000 –&gt; 00:00:36,000<br>So what is it?</p>
<p>9<br>00:00:36,000 –&gt; 00:00:42,000<br>And maybe get a little bit of historical context, you know, how distributed systems have</p>
<p>10<br>00:00:42,000 –&gt; 00:00:48,000<br>developed over the last couple of decades.</p>
<p>11<br>00:00:48,000 –&gt; 00:00:57,000<br>Then hit a little bit on the course structure, like what you should expect.</p>
<p>12<br>00:00:57,000 –&gt; 00:01:02,679<br>Then talk what are the main topics, what are the main recurring topics that we’ll see</p>
<p>13<br>00:01:02,679 –&gt; 00:01:05,960<br>throughout the term.</p>
<p>14<br>00:01:05,960 –&gt; 00:01:11,920<br>And then we’ll see actually the first illustration of those main topics by the case study that</p>
<p>15<br>00:01:11,920 –&gt; 00:01:17,319<br>was assigned for today to paper and that produced, which is also the topic of the first lap.</p>
<p>16<br>00:01:17,319 –&gt; 00:01:25,719<br>And you watch the piyata, you know, we just posted that lap on piyata, the URL so that you</p>
<p>17<br>00:01:25,719 –&gt; 00:01:29,719<br>can get going and it’s you next Friday.</p>
<p>18<br>00:01:29,719 –&gt; 00:01:32,719<br>All right, so let’s start with the basics.</p>
<p>19<br>00:01:32,719 –&gt; 00:01:37,719<br>I’ll talk a little bit about what is a distributed system.</p>
<p>20<br>00:01:44,719 –&gt; 00:01:49,719<br>And sort of a, you know, maybe you’d use you to start with a little picture.</p>
<p>21<br>00:01:49,719 –&gt; 00:01:55,719<br>The Internet Cloud.</p>
<p>22<br>00:01:55,719 –&gt; 00:02:09,719<br>People’s connected to clients and maybe servers, maybe have servers that actually are complete data centers.</p>
<p>23<br>00:02:09,719 –&gt; 00:02:12,719<br>Clients.</p>
<p>24<br>00:02:12,719 –&gt; 00:02:16,719<br>And the data sends themselves, you know, maybe internally distributed systems that are connected</p>
<p>25<br>00:02:16,719 –&gt; 00:02:19,719<br>by internal networks.</p>
<p>26<br>00:02:19,719 –&gt; 00:02:25,719<br>The data centers themselves might be internal connections, you know, outside of the Internet.</p>
<p>27<br>00:02:25,719 –&gt; 00:02:28,719<br>It’s a large collection of computers connected by networks.</p>
<p>28<br>00:02:28,719 –&gt; 00:02:37,719<br>And you know, sort of informally, you know, the way I think about it, what the distributed system is, there’s a multiple, you know, more than one computer.</p>
<p>29<br>00:02:37,719 –&gt; 00:02:43,719<br>Networked, you know, so they can interact only through, you know, sending or receiving packets.</p>
<p>30<br>00:02:43,719 –&gt; 00:02:54,719<br>And you’re supposed to say a multi processor where you can interact with shared memory and they’re cooperating to deliver some service.</p>
<p>31<br>00:02:54,719 –&gt; 00:03:01,719<br>For the four keywords, you know, that define, you know, for me, you distribute the systems.</p>
<p>32<br>00:03:01,719 –&gt; 00:03:09,719<br>Often, you know, you might not be aware of the interacting of the distributed system, you know, you might be using some clients, for example, the zoom client.</p>
<p>33<br>00:03:09,719 –&gt; 00:03:18,719<br>And then back in the zoom client, you know, there are huge data centers or multiple data centers supporting actually, you know, this particular distributed application.</p>
<p>34<br>00:03:18,719 –&gt; 00:03:26,719<br>And in some ways, you know, we wouldn’t be having these zoom lectures if there are more than the, you know, even there were into the distributed systems.</p>
<p>35<br>00:03:26,719 –&gt; 00:03:34,719<br>And so they often form, you know, the backbone of the infrastructure that supports applications.</p>
<p>36<br>00:03:34,719 –&gt; 00:03:50,719<br>So why are distributed systems interesting or, you know, what, what, what, what are the main sort of use cases, you know, for distributed systems.</p>
<p>37<br>00:03:50,719 –&gt; 00:03:56,719<br>And then you know, there’s a broadly speaking, they’re basically four main reasons.</p>
<p>38<br>00:03:56,719 –&gt; 00:04:12,719<br>One is to just connect physically separated machines.</p>
<p>39<br>00:04:12,719 –&gt; 00:04:18,720<br>You know, you might have, you know, we’re in both all of us over here with many of us.</p>
<p>40<br>00:04:18,720 –&gt; 00:04:35,720<br>We just saw an introduction in our, in different locations. And you get in the inner we’re connecting with our laptop or our phone or our iPad, you know, to some server that actually sits in a completely different part of the world.</p>
<p>41<br>00:04:35,720 –&gt; 00:04:43,720<br>example, usually we can get around more than thousand children of the lanes the site is.</p>
<p>42<br>00:04:43,720 –&gt; 00:04:55,720<br>Because of the Hangover Try maj for most of our reads, just to make sure we have opened all of them after, you know, a giveaway called read, a giveaway.</p>
<p>43<br>00:04:55,720 –&gt; 00:04:58,680<br>to the same computer, then actually we can start sharing data.</p>
<p>44<br>00:04:58,680 –&gt; 00:05:03,720<br>And that enables all kinds of collaborative possibilities.</p>
<p>45<br>00:05:03,720 –&gt; 00:05:08,360<br>And whether it is file sharing, whether it is sharing of screens,</p>
<p>46<br>00:05:08,360 –&gt; 00:05:12,280<br>whether it’s sharing of computing infrastructure,</p>
<p>47<br>00:05:12,280 –&gt; 00:05:15,160<br>it’s all enabled because we can connect to physically</p>
<p>48<br>00:05:15,160 –&gt; 00:05:17,080<br>separated machines.</p>
<p>49<br>00:05:17,080 –&gt; 00:05:19,720<br>So that’d probably a very important reason.</p>
<p>50<br>00:05:19,720 –&gt; 00:05:21,880<br>And a couple other really important reasons</p>
<p>51<br>00:05:21,879 –&gt; 00:05:26,519<br>one is to, another one is to increase capacity,</p>
<p>52<br>00:05:29,399 –&gt; 00:05:31,480<br>you know, through parallelism.</p>
<p>53<br>00:05:33,639 –&gt; 00:05:35,879<br>And you know, the paper that we signed for today</p>
<p>54<br>00:05:35,879 –&gt; 00:05:37,639<br>and with us the topic of the first lab,</p>
<p>55<br>00:05:37,639 –&gt; 00:05:40,360<br>the map to do this paper, there was a good example of that.</p>
<p>56<br>00:05:41,399 –&gt; 00:05:43,399<br>But the other example is, you know, for example,</p>
<p>57<br>00:05:43,399 –&gt; 00:05:45,879<br>there are many many Zoom sessions going on at the same time.</p>
<p>58<br>00:05:45,879 –&gt; 00:05:48,680<br>And you know, zoom.com has to support it all.</p>
<p>59<br>00:05:48,680 –&gt; 00:05:50,040<br>And that requires a lot of computers,</p>
<p>60<br>00:05:50,040 –&gt; 00:05:51,960<br>basically, you know, increased capacity,</p>
<p>61<br>00:05:51,960 –&gt; 00:05:55,879<br>so that you can support all those in parallel Zoom sessions.</p>
<p>62<br>00:05:57,240 –&gt; 00:05:59,640<br>Another important reason is, you know, to tolerate false.</p>
<p>63<br>00:06:05,960 –&gt; 00:06:10,520<br>So for example, you know, because computers might be physically separated,</p>
<p>64<br>00:06:10,520 –&gt; 00:06:12,759<br>you know, one part can go down and hopefully</p>
<p>65<br>00:06:12,759 –&gt; 00:06:15,800<br>bone affect another part of another part of the surface.</p>
<p>66<br>00:06:15,800 –&gt; 00:06:17,720<br>So that, you know, the service can always do delivered,</p>
<p>67<br>00:06:17,720 –&gt; 00:06:19,320<br>you know, so you can get high availability.</p>
<p>68<br>00:06:20,120 –&gt; 00:06:23,080<br>And we’ll see that as a major theme, you know, for this class.</p>
<p>69<br>00:06:24,200 –&gt; 00:06:27,560<br>And then the final one is, you know, sort of be, you know,</p>
<p>70<br>00:06:27,560 –&gt; 00:06:30,439<br>also sort of takes advantage of this physical separation,</p>
<p>71<br>00:06:31,240 –&gt; 00:06:32,760<br>which is going to achieve security.</p>
<p>72<br>00:06:37,080 –&gt; 00:06:43,400<br>And for example, if you have a very sensitive surface,</p>
<p>73<br>00:06:43,400 –&gt; 00:06:46,439<br>you know, the surface that manages your password for your,</p>
<p>74<br>00:06:46,439 –&gt; 00:06:48,439<br>you know, customers, you know, for login,</p>
<p>75<br>00:06:49,079 –&gt; 00:06:52,360<br>through your service, you know, you would like to really</p>
<p>76<br>00:06:53,719 –&gt; 00:06:57,160<br>guard that one, you know, machine, then not share it with anybody else,</p>
<p>77<br>00:06:57,160 –&gt; 00:07:00,040<br>or not share any other application or run any applications on it.</p>
<p>78<br>00:07:00,040 –&gt; 00:07:02,279<br>So you have a very narrow interface, you know,</p>
<p>79<br>00:07:02,279 –&gt; 00:07:03,160<br>to that machine.</p>
<p>80<br>00:07:03,160 –&gt; 00:07:05,000<br>And then how about you hopefully, you know,</p>
<p>81<br>00:07:05,000 –&gt; 00:07:08,600<br>to get better security because you just have to protect that one small interface.</p>
<p>82<br>00:07:08,600 –&gt; 00:07:13,000<br>And so by putting things in separate, you know, computers and isolate them,</p>
<p>83<br>00:07:13,879 –&gt; 00:07:17,159<br>you know, you might actually be able to, it’s a good stepping stone to get security.</p>
<p>84<br>00:07:18,439 –&gt; 00:07:20,199<br>These are major regions,</p>
<p>85<br>00:07:22,360 –&gt; 00:07:25,560<br>main four regions, I think why one wants to,</p>
<p>86<br>00:07:26,360 –&gt; 00:07:27,639<br>why interest system are popular.</p>
<p>87<br>00:07:29,399 –&gt; 00:07:30,920<br>I want to talk a little bit about, you know,</p>
<p>88<br>00:07:30,920 –&gt; 00:07:33,079<br>keeping a little bit of historical context, you know,</p>
<p>89<br>00:07:33,079 –&gt; 00:07:34,839<br>for distributed systems.</p>
<p>90<br>00:07:35,399 –&gt; 00:07:36,920<br>And so where did it came from?</p>
<p>91<br>00:07:36,920 –&gt; 00:07:41,879<br>And what sort of has happened over the decades, actually.</p>
<p>92<br>00:07:48,839 –&gt; 00:07:55,560<br>And it’s sort of basically sort of the distributed systems as we sort of now look at them,</p>
<p>93<br>00:07:55,560 –&gt; 00:08:00,759<br>or the way we recognize them, probably started around the same time the local area networks happened.</p>
<p>94<br>00:08:05,480 –&gt; 00:08:07,720<br>Here, you know, I think early 80s.</p>
<p>95<br>00:08:11,959 –&gt; 00:08:14,600<br>And so, for example, you would have a campus network like at MIT,</p>
<p>96<br>00:08:15,480 –&gt; 00:08:19,560<br>and connecting, for example, the workstations like in Athena Cluster,</p>
<p>97<br>00:08:19,560 –&gt; 00:08:21,800<br>you know, to the Athena servers like AFS.</p>
<p>98<br>00:08:22,840 –&gt; 00:08:25,879<br>And so that was sort of the typical distributed system at that point.</p>
<p>99<br>00:08:25,879 –&gt; 00:08:28,680<br>And AFS also dates, you know, from that period of time.</p>
<p>100<br>00:08:30,439 –&gt; 00:08:32,040<br>Of course, the Internet was there too,</p>
<p>101<br>00:08:32,600 –&gt; 00:08:36,279<br>but there was really not sort of large scale Internet applications the way we,</p>
<p>102<br>00:08:36,279 –&gt; 00:08:38,120<br>you know, are using them now.</p>
<p>103<br>00:08:38,120 –&gt; 00:08:41,960<br>And so the main sort of Internet scale side type distributed systems,</p>
<p>104<br>00:08:41,960 –&gt; 00:08:44,120<br>you know, was DNS, the domain name system.</p>
<p>105<br>00:08:44,519 –&gt; 00:08:46,440<br>We still use and basically email.</p>
<p>106<br>00:08:48,759 –&gt; 00:08:52,039<br>And so when I, early in the early days, when I’d like to do distributed systems,</p>
<p>107<br>00:08:52,039 –&gt; 00:08:57,240<br>you know, those are basically the main examples that we have to discuss.</p>
<p>108<br>00:08:57,720 –&gt; 00:09:00,759<br>Now, things have changed quite dramatically since the 1980s,</p>
<p>109<br>00:09:00,759 –&gt; 00:09:04,039<br>and the importance of distributed systems has just tremendously increased.</p>
<p>110<br>00:09:04,919 –&gt; 00:09:10,919<br>And one, you know, significant point was data centers, the rise of data centers.</p>
<p>111<br>00:09:11,639 –&gt; 00:09:15,559<br>And that went along with basically the big websites.</p>
<p>112<br>00:09:19,399 –&gt; 00:09:25,719<br>And here, you know, we’re talking sort of the roughly speaking in the 1990s or early 1990s.</p>
<p>113<br>00:09:26,439 –&gt; 00:09:29,159<br>And so what happened basically is that, you know,</p>
<p>114<br>00:09:29,159 –&gt; 00:09:33,639<br>somewhere in the late 80s, or in the early 80s, the government or Congress,</p>
<p>115<br>00:09:33,639 –&gt; 00:09:37,719<br>they allowed commercial traffic on the Internet.</p>
<p>116<br>00:09:38,200 –&gt; 00:09:43,240<br>And that basically resulted in in boom, where, you know, you started getting big websites that</p>
<p>117<br>00:09:43,240 –&gt; 00:09:48,120<br>were supporting large, large number of users. And, you know, the applications from those times,</p>
<p>118<br>00:09:48,120 –&gt; 00:09:54,120<br>like, for example, you know, web search, you know, being able to search all the different,</p>
<p>119<br>00:09:54,120 –&gt; 00:09:59,000<br>you know, web pages that actually were on the, on the worldwide web, you know, shopping.</p>
<p>120<br>00:10:00,840 –&gt; 00:10:05,240<br>And so these, you know, applications, you know, gave rise to, you know, two sort of things.</p>
<p>121<br>00:10:05,240 –&gt; 00:10:09,399<br>One, huge datasets, you know, sort of indexing, you know, to support web search, you have to</p>
<p>122<br>00:10:09,399 –&gt; 00:10:14,039<br>index all the web pages on the Internet. So I don’t mean like to have to gather a crawl all the</p>
<p>123<br>00:10:14,039 –&gt; 00:10:18,279<br>web pages, then compute a reverse index. And then, you know, you could use that for your search engine.</p>
<p>124<br>00:10:19,159 –&gt; 00:10:22,919<br>That was just a tremendous amount of data that didn’t fit on one computer. And the amount of</p>
<p>125<br>00:10:22,919 –&gt; 00:10:27,480<br>computation to actually do the reverse indexing, you know, there was also two, much for a single computer.</p>
<p>126<br>00:10:27,480 –&gt; 00:10:31,879<br>As a result, you know, you know, sort of the data centers came about, where, you know, companies started,</p>
<p>127<br>00:10:31,879 –&gt; 00:10:35,080<br>you know, put lots and lots of computers in data centers so that it can support those kinds of</p>
<p>128<br>00:10:35,080 –&gt; 00:10:40,679<br>applications. So that’s one lot of data. And the second one is there’s a lot, a lot of users,</p>
<p>129<br>00:10:41,879 –&gt; 00:10:46,039<br>not uncommon for you know, pick up websites that hundreds of millions of users. And that just</p>
<p>130<br>00:10:46,039 –&gt; 00:10:50,679<br>requires a lot of machines to actually support all those users. And so we see,</p>
<p>131<br>00:10:51,799 –&gt; 00:10:58,600<br>tremendous amount of innovation in this spirit of time, or still continuing. And some of the papers</p>
<p>132<br>00:10:58,600 –&gt; 00:11:03,720<br>that we read, like the MapReduce paper actually sort of started from that period of time.</p>
<p>133<br>00:11:05,879 –&gt; 00:11:10,759<br>That whole thing sort of, sort of, sort of, excelent that held development accelerated with the</p>
<p>134<br>00:11:10,759 –&gt; 00:11:21,480<br>emergency of cloud computing. And other early, you know, whatever, mid to late, you know, two</p>
<p>135<br>00:11:21,480 –&gt; 00:11:27,480<br>thousands. And so here, where we see you move, where users, where customers,</p>
<p>136<br>00:11:28,279 –&gt; 00:11:33,480<br>basically move their computation and the data to data centers, you know, and by other people,</p>
<p>137<br>00:11:33,480 –&gt; 00:11:42,200<br>like Amazon, you know, Google, Microsoft, you know, you name it. And so a lot of the computation,</p>
<p>138<br>00:11:42,200 –&gt; 00:11:46,840<br>data computation that people used to run on their, you know, their desktop or on the laptop,</p>
<p>139<br>00:11:46,840 –&gt; 00:11:51,160<br>just moves inside of the cloud computing and like application change, you know, all that,</p>
<p>140<br>00:11:51,160 –&gt; 00:11:55,320<br>instead of like running an application on your local computer, you run actually the application</p>
<p>141<br>00:11:55,320 –&gt; 00:11:59,399<br>inside of the cloud. And that means, you know, that these data centers, you know, have to grow</p>
<p>142<br>00:11:59,399 –&gt; 00:12:05,879<br>further and support, you know, this new set of applications. And not only that, you know, the,</p>
<p>143<br>00:12:05,879 –&gt; 00:12:11,000<br>so for customers that were outsourcing their computing to cloud computing,</p>
<p>144<br>00:12:11,080 –&gt; 00:12:17,320<br>also started to run large websites themselves. And you know, do gigantic competitions on</p>
<p>145<br>00:12:17,320 –&gt; 00:12:22,759<br>themselves, you know, whether it’s machine learning or large data sets or any other kind of type</p>
<p>146<br>00:12:22,759 –&gt; 00:12:28,919<br>of computation. And so you see is that, you know, the users themselves, one of the builds large</p>
<p>147<br>00:12:28,919 –&gt; 00:12:33,240<br>scale distributed systems. And that meant like the cloud providers, you know, starting building a</p>
<p>148<br>00:12:33,240 –&gt; 00:12:38,120<br>lot of infrastructure to allow other people to scale up, you know, their, you know, their,</p>
<p>149<br>00:12:38,120 –&gt; 00:12:43,639<br>their systems through, you know, large number of machines and achieve, you know, high parallelism,</p>
<p>150<br>00:12:43,639 –&gt; 00:12:49,960<br>high performance and store lots of data. And so, you know, as a result, you know, with the</p>
<p>151<br>00:12:49,960 –&gt; 00:12:57,159<br>current state is basically that, you know, it’s a very active area of research as well as development.</p>
<p>152<br>00:13:00,600 –&gt; 00:13:05,240<br>In fact, you know, so hard, or so active, that is difficult, you know, to sort of keep,</p>
<p>153<br>00:13:05,240 –&gt; 00:13:11,720<br>keep up to date. There’s a lot of developments. And, you know, even in this class, you know,</p>
<p>154<br>00:13:11,720 –&gt; 00:13:16,039<br>we’re going to spend a full semester in distributed systems, you know, we’re going to only, you know,</p>
<p>155<br>00:13:16,039 –&gt; 00:13:22,600<br>be able to sort of look at, you know, a number of small fraction of like older stuff,</p>
<p>156<br>00:13:22,600 –&gt; 00:13:25,560<br>all the kind of distributed systems actually that people are building in practice now.</p>
<p>157<br>00:13:27,560 –&gt; 00:13:33,240<br>One thing that is cool for us, you know, as teachers and students who have distributed systems</p>
<p>158<br>00:13:33,240 –&gt; 00:13:39,080<br>is that the people that build these data centers early on, even though they were building</p>
<p>159<br>00:13:39,080 –&gt; 00:13:44,360<br>the system for their own internal infrastructure, they published papers about it. And we can read</p>
<p>160<br>00:13:44,360 –&gt; 00:13:48,519<br>those papers. And so, in fact, you know, during the semester, we’ll read a number of those,</p>
<p>161<br>00:13:48,519 –&gt; 00:13:53,480<br>you know, papers that were built by people that, you know, really have large scale-descited system</p>
<p>162<br>00:13:53,480 –&gt; 00:13:59,720<br>challenges. And, you know, we can see how they were solved and learn from them. This accelerated,</p>
<p>163<br>00:13:59,720 –&gt; 00:14:04,680<br>even war with cloud computing where, you know, in the early days of data centers, many of these</p>
<p>164<br>00:14:04,680 –&gt; 00:14:10,759<br>services were internal for, you know, the, you know, for Microsoft, Google or Amazon or Yahoo,</p>
<p>165<br>00:14:10,759 –&gt; 00:14:15,480<br>or for themselves. With the rise of cloud computing, you know, these services became public</p>
<p>166<br>00:14:15,480 –&gt; 00:14:20,519<br>services that were used by other people. And so, suddenly, there’s even more sort of systems</p>
<p>167<br>00:14:20,519 –&gt; 00:14:27,639<br>infrastructure that is well documented and usable. And so, we can even, you know, we will study some</p>
<p>168<br>00:14:27,720 –&gt; 00:14:32,679<br>of those cases too. And so, you just sort of look over these sort of four decades, you know,</p>
<p>169<br>00:14:32,679 –&gt; 00:14:39,080<br>to tremendous rise of the importance of distributed computing. As I said, we earlier, I did my,</p>
<p>170<br>00:14:39,720 –&gt; 00:14:43,559<br>doctoral thesis in distributed systems, actually somewhere in the 1980s. And even so, like,</p>
<p>171<br>00:14:43,559 –&gt; 00:14:49,080<br>it was an important field, but not, you know, didn’t blow your way in terms of significance. And,</p>
<p>172<br>00:14:50,360 –&gt; 00:14:53,960<br>and the practicality, you know, was sort of limited to more of these local area plastics.</p>
<p>173<br>00:14:54,759 –&gt; 00:15:01,080<br>And now, you know, it’s just like completely booming research field and development field.</p>
<p>174<br>00:15:03,960 –&gt; 00:15:07,639<br>Any questions a little about the historical context for distributed systems?</p>
<p>175<br>00:15:15,720 –&gt; 00:15:21,400<br>Okay, let me talk a little bit about the challenges. And many of them, you’re going to</p>
<p>176<br>00:15:21,559 –&gt; 00:15:34,439<br>face head-on in the labs. So, so why is it, you know, hard and worth, you know, basically spending a</p>
<p>177<br>00:15:34,439 –&gt; 00:15:41,639<br>semester learning about distributed systems? And there’s sort of two things that drive, you know,</p>
<p>178<br>00:15:41,639 –&gt; 00:15:48,120<br>the complexity and why distributed systems at heart. One is there are many concurrent parts.</p>
<p>179<br>00:15:51,879 –&gt; 00:15:57,639<br>I mean, these data warehouses, you know, today computers are going to run, you know, 10,000,</p>
<p>180<br>00:15:57,639 –&gt; 00:16:02,439<br>100,000 computers in parallel and sometimes know all in the same job. Like, we’ve seen the</p>
<p>181<br>00:16:02,439 –&gt; 00:16:06,679<br>map-produced paper today, which is like from the early 90s, you know, 2,000 machines, you know,</p>
<p>182<br>00:16:06,679 –&gt; 00:16:12,519<br>trying to work on one single problem. So, there’s a lot of concurrent, you know, there’s a lot of</p>
<p>183<br>00:16:12,519 –&gt; 00:16:16,360<br>concurrent software, a lot of things happening concurrently, and so it’s very hard to reason that through,</p>
<p>184<br>00:16:16,360 –&gt; 00:16:25,240<br>like, understand why, you know, things are correct. And this is compounded by the fact that, you</p>
<p>185<br>00:16:25,240 –&gt; 00:16:41,240<br>know, these systems must deal with partial failure. So, you know, one of these machines actually</p>
<p>186<br>00:16:41,240 –&gt; 00:16:45,480<br>might go down, but that doesn’t mean that the whole competition stops. In fact, you know,</p>
<p>187<br>00:16:45,560 –&gt; 00:16:49,159<br>the rest of the machines probably, you know, hopefully, can continue running and maybe, you know,</p>
<p>188<br>00:16:49,159 –&gt; 00:16:54,920<br>take over from the responsibility of the machine that failed. But this drives, you know, these two</p>
<p>189<br>00:16:54,920 –&gt; 00:17:01,159<br>things together, basically drive complexity because it comes harder and harder to reason about why,</p>
<p>190<br>00:17:01,159 –&gt; 00:17:05,720<br>you know, the system actually is working. And particularly partial failure makes things very</p>
<p>191<br>00:17:05,720 –&gt; 00:17:10,120<br>complicated because one system, one part of the system might think that another part of the system</p>
<p>192<br>00:17:10,120 –&gt; 00:17:14,120<br>is down, but it’s not really the case. You know, the only thing that might actually happen is that</p>
<p>193<br>00:17:14,119 –&gt; 00:17:19,079<br>there’s a network partition. And so, both sides of the distributed system, you know, basically</p>
<p>194<br>00:17:19,079 –&gt; 00:17:25,159<br>are keep on computing and maybe interact with, you know, clients, maybe even interact with the same</p>
<p>195<br>00:17:25,159 –&gt; 00:17:28,759<br>set of clients because the clients can talk to do both parts, but, you know, the two-in-a-halfs</p>
<p>196<br>00:17:28,759 –&gt; 00:17:35,399<br>cannot talk to each other. And so, this is a problem known as the split brain syndrome. And that,</p>
<p>197<br>00:17:35,399 –&gt; 00:17:39,879<br>you know, makes, you know, designing the distributed system as a verticals with the systems that</p>
<p>198<br>00:17:39,960 –&gt; 00:17:45,000<br>are complicated as we’ll see. And so, there’s really sort of deep intellectual problems here.</p>
<p>199<br>00:17:45,800 –&gt; 00:17:52,440<br>And finally, sort of really aspect in terms of challenges is actually tricky to realize</p>
<p>200<br>00:17:53,480 –&gt; 00:18:00,040<br>the performance benefits that in principle are possible with distributed systems.</p>
<p>201<br>00:18:05,480 –&gt; 00:18:08,840<br>So, so far, it may actually be talking is like, you know, you want to increase the capacity or you</p>
<p>202<br>00:18:08,839 –&gt; 00:18:12,679<br>want to run things forward in parallel, you buy more machines, you know, or you buy another data center.</p>
<p>203<br>00:18:13,799 –&gt; 00:18:19,319<br>And, you know, of course, you know, only when the task is completely embarrassing parallel, does that</p>
<p>204<br>00:18:19,319 –&gt; 00:18:23,879<br>work. And often in practice, others just not the case. And so, actually achieving that sort of</p>
<p>205<br>00:18:24,759 –&gt; 00:18:31,480<br>high throughput and throughput scaling within the room machines turns out to be not straightforward at all.</p>
<p>206<br>00:18:31,799 –&gt; 00:18:41,480<br>So, that brings me to sort of the next topic, like why you know, it takes 68 to 4, at least, you know.</p>
<p>207<br>00:18:50,440 –&gt; 00:18:53,880<br>You know, so I think there’s sort of four reasons. One, it’s interesting.</p>
<p>208<br>00:18:53,880 –&gt; 00:19:04,840<br>And it’s a set like heart technical problems and with their powerful solutions. So, heart problems,</p>
<p>209<br>00:19:09,560 –&gt; 00:19:14,200<br>but powerful solutions. We’ll see, you know, those solutions through the term.</p>
<p>210<br>00:19:20,200 –&gt; 00:19:22,280<br>Second reason is, no, they’re used in the real world.</p>
<p>211<br>00:19:24,040 –&gt; 00:19:33,000<br>And the norm of amount of appetite, you know, who people that actually understand and can build distributed systems.</p>
<p>212<br>00:19:33,800 –&gt; 00:19:37,720<br>If you were a grad student or were an undergrad in thinking about research, you know, that’s a great area.</p>
<p>213<br>00:19:37,720 –&gt; 00:19:39,560<br>Because it’s a very active area of research.</p>
<p>214<br>00:19:43,480 –&gt; 00:19:50,280<br>There’s still many open problems. And as we go through the semester, we know all, all, well, encounter them.</p>
<p>215<br>00:19:50,839 –&gt; 00:19:54,839<br>So, it’s a good area for research. And finally, you know, if you like building things,</p>
<p>216<br>00:19:55,480 –&gt; 00:20:01,559<br>it’s sort of a unique style of programming. And so, in case of 8 to 4, you’re going to get hands-on</p>
<p>217<br>00:20:01,559 –&gt; 00:20:09,720<br>experience with that by building, you know, distributed systems in the labs. And you’ll discover that,</p>
<p>218<br>00:20:11,879 –&gt; 00:20:17,399<br>one, it’s hard to get them right. And, you know, it sort of builds up another skill,</p>
<p>219<br>00:20:17,400 –&gt; 00:20:20,200<br>type of skill of programming that you might have not have done in the past.</p>
<p>220<br>00:20:23,560 –&gt; 00:20:25,640<br>Let me pass for a second here and see if there are any questions.</p>
<p>221<br>00:20:27,640 –&gt; 00:20:31,880<br>Also, feel free to post in the chat. I’ll try to monitor chat if there are questions there,</p>
<p>222<br>00:20:31,880 –&gt; 00:20:38,200<br>or, you know, raise your hand if you have any questions. And I’m sure the TAs will also be paying</p>
<p>223<br>00:20:38,200 –&gt; 00:20:43,960<br>attention to the raising hands and the chat. So, in case I miss something, you know, they’ll remind me.</p>
<p>224<br>00:20:44,440 –&gt; 00:20:47,960<br>Any questions so far? I think it’s crystal clear.</p>
<p>225<br>00:20:55,319 –&gt; 00:20:58,600<br>I’ll interpret the silences. Things are crystal clear.</p>
<p>226<br>00:21:01,079 –&gt; 00:21:03,240<br>So, let me talk a little bit about the course structure.</p>
<p>227<br>00:21:04,840 –&gt; 00:21:08,360<br>Now, after this sort of quick introduction to distributed systems.</p>
<p>228<br>00:21:14,440 –&gt; 00:21:17,079<br>So, the course structure is as follows. We have lectures,</p>
<p>229<br>00:21:18,440 –&gt; 00:21:21,640<br>like the one today, and basically focuses on big ideas.</p>
<p>230<br>00:21:25,000 –&gt; 00:21:32,360<br>The lectures are typically driven by a paper that we all sign. And these papers are often a</p>
<p>231<br>00:21:32,360 –&gt; 00:21:36,200<br>case study, you know, the particular big idea that we’re covering in lecture.</p>
<p>232<br>00:21:36,200 –&gt; 00:21:44,920<br>I can read the papers are all published or posted on the schedule page.</p>
<p>233<br>00:21:44,920 –&gt; 00:21:50,920<br>And more for most papers, we ask you to answer a question as well as ask a question.</p>
<p>234<br>00:21:50,920 –&gt; 00:21:56,120<br>And we’ll try to cover those questions or answer them during the lecture.</p>
<p>235<br>00:21:56,120 –&gt; 00:22:00,120<br>And so, it’s important, you know, part of the reason we do that is because we’d like you to</p>
<p>236<br>00:22:00,120 –&gt; 00:22:04,440<br>read the paper in advance of the lecture so that we can go a little bit deeper</p>
<p>237<br>00:22:05,320 –&gt; 00:22:11,640<br>into these papers. So, I’m strongly encouraging you to read them in before class.</p>
<p>238<br>00:22:14,440 –&gt; 00:22:19,480<br>So, another component of the class is the labs, the programming labs.</p>
<p>239<br>00:22:20,680 –&gt; 00:22:27,559<br>There are four of them. They’re split in parts, but the four major ones, one is the map</p>
<p>240<br>00:22:27,720 –&gt; 00:22:33,720<br>reduced lab that we just posted today. And that’s due next Friday. And where you build the</p>
<p>241<br>00:22:33,720 –&gt; 00:22:40,200<br>basically your own map reduced library, as similar to the one that actually described in the paper.</p>
<p>242<br>00:22:41,480 –&gt; 00:22:48,919<br>The second lab is a lab that focuses on replication in the presence of failures and</p>
<p>243<br>00:22:48,919 –&gt; 00:22:56,279<br>in the partition networks. We’re going to implement replication using a protocol that’s called</p>
<p>244<br>00:22:56,359 –&gt; 00:23:06,279<br>raft. And this is a lab that consists of multiple components, but at the end of it, you’ll have a</p>
<p>245<br>00:23:06,279 –&gt; 00:23:12,200<br>library that you can use to what’s called, which is you can use to build replicated state machines,</p>
<p>246<br>00:23:12,200 –&gt; 00:23:19,879<br>namely replicating a state machine or multiple machines so that if one of them goes down,</p>
<p>247<br>00:23:19,879 –&gt; 00:23:25,559<br>one of those machines goes down that the service actually keeps running. And you’re going to use that</p>
<p>248<br>00:23:25,559 –&gt; 00:23:30,200<br>library to actually build a replicated service. And the fact you’re going to build a replicated</p>
<p>249<br>00:23:31,720 –&gt; 00:23:32,679<br>key value service.</p>
<p>250<br>00:23:40,679 –&gt; 00:23:44,119<br>In lab three, so lab three is going to basically use multiple machines.</p>
<p>251<br>00:23:45,399 –&gt; 00:23:48,119<br>For fault tolerance or for applications to build one service.</p>
<p>252<br>00:23:49,000 –&gt; 00:23:53,319<br>Unfortunately, you know, as well, so you’ll have more is that just replication decision,</p>
<p>253<br>00:23:53,319 –&gt; 00:23:57,639<br>those should give you more performance, you know, because you can just machine actually have to</p>
<p>254<br>00:23:57,639 –&gt; 00:24:04,679<br>perform the operating in a particular order. And so to actually get performance, what we’re in lab four,</p>
<p>255<br>00:24:04,679 –&gt; 00:24:08,039<br>you can build, you’re going to be able to chart it key value service.</p>
<p>256<br>00:24:13,319 –&gt; 00:24:19,159<br>And that basically consists of many instances of lab three running currently,</p>
<p>257<br>00:24:19,960 –&gt; 00:24:25,480<br>and basically taking care of a part or a chart of the key value service. And so that you get</p>
<p>258<br>00:24:25,480 –&gt; 00:24:30,120<br>parallelism. And so that you can actually use this to actually drive throughput.</p>
<p>259<br>00:24:31,880 –&gt; 00:24:36,840<br>And furthermore, we’re going to actually move keys or key value pair, one machine to another</p>
<p>260<br>00:24:36,840 –&gt; 00:24:44,360<br>machine in response to one load changes. So the last piece, we last two, three and four</p>
<p>261<br>00:24:44,439 –&gt; 00:24:49,639<br>build on top of each other. So if you have a bug in lab two, that might affect you actually in lab four.</p>
<p>262<br>00:24:51,079 –&gt; 00:24:55,159<br>We provide test cases for all of them. So all the test cases are public.</p>
<p>263<br>00:25:03,079 –&gt; 00:25:07,319<br>And we grade you on those test cases. So you submit your solution. We run the same tests on</p>
<p>264<br>00:25:07,319 –&gt; 00:25:13,159<br>our computers and double check, you know, that you’re passing the test. And if you pass all the tests,</p>
<p>265<br>00:25:13,160 –&gt; 00:25:22,759<br>you get full score. Turns out, you know, these test cases are tricky. And we’ll try to</p>
<p>266<br>00:25:22,759 –&gt; 00:25:29,720<br>tickle all kinds of corners in your systems. And so it turns out they are actually reasonable</p>
<p>267<br>00:25:29,720 –&gt; 00:25:35,880<br>hard to pass. And so, and they’re tricky to debug. You might actually have in a particular corner</p>
<p>268<br>00:25:35,880 –&gt; 00:25:40,759<br>case an error. And it may be very difficult to track down when this that happened, why does it</p>
<p>269<br>00:25:40,759 –&gt; 00:25:45,559<br>happen? So you know how to fix it. And so my advice to you is to start to lab you early.</p>
<p>270<br>00:25:46,839 –&gt; 00:25:50,039<br>It’s often the case that, you know, if you just start the night or the two nights before,</p>
<p>271<br>00:25:51,160 –&gt; 00:25:55,400<br>you’re going to have difficulty passing all the tests because you’re going to get stuck,</p>
<p>272<br>00:25:55,400 –&gt; 00:26:00,519<br>you know, trying to debug one particular aspect and run out of time to basically get the other</p>
<p>273<br>00:26:00,519 –&gt; 00:26:12,359<br>test cases to work. There’s an optional project. So instead of doing lab four,</p>
<p>274<br>00:26:13,559 –&gt; 00:26:18,359<br>you can do a project. And the idea of the project is that you can work together or collaborate</p>
<p>275<br>00:26:18,359 –&gt; 00:26:25,240<br>with a group of two or three students and do a project year in year-old. And the projects are</p>
<p>276<br>00:26:25,240 –&gt; 00:26:30,359<br>form or similar type systems that we read about in the papers. You propose one that you would</p>
<p>277<br>00:26:30,359 –&gt; 00:26:37,639<br>like to build. We’ll give you some feedback and we’ll tell you, well, maybe you should just do lab four.</p>
<p>278<br>00:26:38,439 –&gt; 00:26:42,839<br>But if you’re excited about doing project, you know, we sort of like to stimulate that and you</p>
<p>279<br>00:26:42,839 –&gt; 00:26:47,959<br>should start thinking now. And then hopefully we can have some discussion and settle on something that</p>
<p>280<br>00:26:47,959 –&gt; 00:26:56,039<br>may not be cool to do. Okay, and finally, the one other component of the course is actually two</p>
<p>281<br>00:26:56,039 –&gt; 00:27:04,680<br>exams. One roughly halfway the semester on one in the final week. And, you know, we expect</p>
<p>282<br>00:27:04,680 –&gt; 00:27:10,920<br>your course to do all the labs, submit a read right homework questions for the papers and do the two</p>
<p>283<br>00:27:10,920 –&gt; 00:27:19,000<br>exams. If you look at the web pages for 682A or 6824, you’ll see exactly the balance in terms of</p>
<p>284<br>00:27:19,000 –&gt; 00:27:25,399<br>grading for the different components. You know, the labs come for most. The two exams, I think are</p>
<p>285<br>00:27:25,400 –&gt; 00:27:33,800<br>20 or 30% and then some class participation. But the details are on the web page. To get you through</p>
<p>286<br>00:27:33,800 –&gt; 00:27:41,000<br>the semester and help you along, we have excellent course staff. We have four TAs.</p>
<p>287<br>00:27:41,800 –&gt; 00:27:46,680<br>We’re all in, we’re running office hours and to help you basically, you know, get your labs. And</p>
<p>288<br>00:27:47,480 –&gt; 00:27:52,920<br>let me do a quick round. Maybe the TAs can introduce themselves so they can at least know who they are.</p>
<p>289<br>00:27:53,880 –&gt; 00:28:01,640<br>Lily, you want to go first? Sure. So, I’m Lily. I am a third year grad student in PIDAS and</p>
<p>290<br>00:28:01,640 –&gt; 00:28:07,240<br>Franz is actually my advisor. So, I know just tell good to use a teaching so you’re in for a treat.</p>
<p>291<br>00:28:08,519 –&gt; 00:28:13,240<br>Yeah, I’m looking forward to working with you this semester. I’ll pass it off to David.</p>
<p>292<br>00:28:15,640 –&gt; 00:28:21,080<br>Hi, everyone. I’m David. I am a second semester student. I took 6824 last spring when it was like</p>
<p>293<br>00:28:21,079 –&gt; 00:28:26,359<br>having person half remote. So, hopefully we can get the best of both worlds for the semester. I’m excited.</p>
<p>294<br>00:28:27,720 –&gt; 00:28:34,759<br>Yeah, but Jose. Hi, Jose. I’m a four year grad student working on machine learning</p>
<p>295<br>00:28:34,759 –&gt; 00:28:40,839<br>problems. I took this class my first year of the grad student and I really, really enjoyed it. So,</p>
<p>296<br>00:28:40,839 –&gt; 00:28:48,519<br>yeah, looking forward to teaching it. So, yeah, I’m cell. I use data and pronouns. I’m first year</p>
<p>297<br>00:28:48,519 –&gt; 00:28:54,119<br>master’s student in PIDAS, like some of the others. And I took this class few years back.</p>
<p>298<br>00:28:54,119 –&gt; 00:28:57,639<br>I had a great time taking it. So, I’m excited to help everyone learn it.</p>
<p>299<br>00:29:01,559 –&gt; 00:29:06,839<br>Okay, thank you. So, there was a question in the chat. How is the system, how does the</p>
<p>300<br>00:29:06,839 –&gt; 00:29:14,359<br>system where the lab run? Is the machine systems simulated? Yes, we’re basically simulating</p>
<p>301<br>00:29:14,359 –&gt; 00:29:18,439<br>many, many machines by running many, many different processes. In fact, the labs have</p>
<p>302<br>00:29:18,439 –&gt; 00:29:25,959<br>an own RPC library that like pretend you’re running on a separated physical machines,</p>
<p>303<br>00:29:25,959 –&gt; 00:29:28,839<br>but in fact, you’re running many, many processes on the same machine.</p>
<p>304<br>00:29:33,719 –&gt; 00:29:38,199<br>Okay, any questions so far before I sort of continue into the direction of</p>
<p>305<br>00:29:38,200 –&gt; 00:29:48,200<br>actualization technical content? Is the result of lab four? Is it similar to any existing</p>
<p>306<br>00:29:49,720 –&gt; 00:29:55,559<br>programs that exist? Yeah, in fact, what do we be building? It has a lot of similarity to</p>
<p>307<br>00:29:55,559 –&gt; 00:30:00,519<br>sort of popular key value services, you know, thing reddit or, you know, some of the other ones.</p>
<p>308<br>00:30:01,480 –&gt; 00:30:05,559<br>You know, there will be differences after we’ll discover when we grow through this semester,</p>
<p>309<br>00:30:06,200 –&gt; 00:30:13,000<br>but the key value service is a pretty well-known and a common service inside of a data center</p>
<p>310<br>00:30:13,000 –&gt; 00:30:17,799<br>area run by many companies and a couple very popular ones that use by lots of people.</p>
<p>311<br>00:30:18,359 –&gt; 00:30:21,480<br>And they basically struggle with exactly the same issues as you were going to be struggling</p>
<p>312<br>00:30:21,480 –&gt; 00:30:26,200<br>within the labs. We’re going to build a one that actually has pretty strong semantics,</p>
<p>313<br>00:30:26,919 –&gt; 00:30:29,960<br>sometimes a little bit stronger semantics than some people who are actually doing practice,</p>
<p>314<br>00:30:29,960 –&gt; 00:30:33,480<br>and you know, we’ll discuss why that why that happens too, but you guys are very close to</p>
<p>315<br>00:30:33,559 –&gt; 00:30:37,799<br>when people are doing practice. Rapt is white, we’ll use some practice for example.</p>
<p>316<br>00:30:42,279 –&gt; 00:30:43,240<br>Any other questions?</p>
<p>317<br>00:30:49,319 –&gt; 00:30:57,559<br>Yeah, it’s a good question about the labs. Again, if we have a bug on lab two that maybe they’ll</p>
<p>318<br>00:30:57,559 –&gt; 00:31:06,279<br>even get caught by the testers somehow, do we get an answer for the following labs or do we</p>
<p>319<br>00:31:06,279 –&gt; 00:31:10,759<br>just continue to use our code? You’re going to continue using your code.</p>
<p>320<br>00:31:12,599 –&gt; 00:31:16,359<br>We did our best, you know, through the labs and the testers, it’s just as good as possible,</p>
<p>321<br>00:31:17,079 –&gt; 00:31:20,440<br>but I’m sure there are cases that we, you know, it’s hard to do a complete good job.</p>
<p>322<br>00:31:23,079 –&gt; 00:31:27,000<br>But, you know, every time we discover something that we missed, we basically improve the tests.</p>
<p>323<br>00:31:27,559 –&gt; 00:31:31,720<br>So, you’re building, you know, once you pass the test, you know, we’re optimistic that you actually</p>
<p>324<br>00:31:31,720 –&gt; 00:31:36,119<br>have an implementation that actually can support the other use cases that we’re doing the rest of the semester.</p>
<p>325<br>00:31:39,079 –&gt; 00:31:42,919<br>It’s not uncommon for people to rewrite their implementation once and twice.</p>
<p>326<br>00:31:43,799 –&gt; 00:31:48,759<br>As you will see in lab two and lab three, you know, the structure, you know, you have to spend quite a</p>
<p>327<br>00:31:48,759 –&gt; 00:31:54,519<br>bit of time thinking about the structure of your application or your library, and you know, as you</p>
<p>328<br>00:31:54,519 –&gt; 00:32:00,440<br>sort of learn, you may want to go back and redo it. To help you along a little bit,</p>
<p>329<br>00:32:00,440 –&gt; 00:32:03,400<br>this year we’re doing something different that we’ve done in the past years.</p>
<p>330<br>00:32:03,400 –&gt; 00:32:08,440<br>We’re going to do around a couple of Q&amp;A lectures where I’ll share, we’ll share our solutions</p>
<p>331<br>00:32:09,000 –&gt; 00:32:14,440<br>with you or we’ll walk through our solutions and hopefully that will, you know, tell you a little</p>
<p>332<br>00:32:14,440 –&gt; 00:32:18,440<br>bit about, you know, you can learn from that and see how that contrasts with your own solution and</p>
<p>333<br>00:32:18,440 –&gt; 00:32:21,319<br>maybe, you know, pick up some ideas for future labs.</p>
<p>334<br>00:32:25,160 –&gt; 00:32:27,879<br>Any other questions?</p>
<p>335<br>00:32:34,759 –&gt; 00:32:40,519<br>Okay. Again, interrupt me at any time. I’d like to make this more and more</p>
<p>336<br>00:32:40,519 –&gt; 00:32:44,039<br>interact. We’ll take a couple lectures, but hopefully we’ll get there.</p>
<p>337<br>00:32:46,279 –&gt; 00:32:53,799<br>Okay. I want to talk a little bit, you know, sort of set ourselves up for the case study from today.</p>
<p>338<br>00:32:54,599 –&gt; 00:32:58,920<br>But before doing that, I want to talk a little bit about the perspective for the class.</p>
<p>339<br>00:32:58,920 –&gt; 00:33:02,839<br>Our focus in the class is going to be on infrastructure. You can more or less can tell that from</p>
<p>340<br>00:33:02,839 –&gt; 00:33:09,079<br>the labs that, you know, we’re, we just discussed. So, you know, there’s going to be somebody who’s</p>
<p>341<br>00:33:09,079 –&gt; 00:33:13,720<br>writing applications on these distributed systems and we’re not really concerned too much</p>
<p>342<br>00:33:13,720 –&gt; 00:33:18,359<br>with the applications at all. We’re going to be mostly concerned with the infrastructure that</p>
<p>343<br>00:33:18,359 –&gt; 00:33:22,759<br>supports these applications. And the infrastructure falls out in three different categories,</p>
<p>344<br>00:33:22,759 –&gt; 00:33:28,519<br>where very broadly speaking storage, infrastructure, so like devaluing servers,</p>
<p>345<br>00:33:28,519 –&gt; 00:33:31,720<br>or just file systems, not kind of thing, computation,</p>
<p>346<br>00:33:36,519 –&gt; 00:33:41,079<br>you know, from frameworks to actually orchestrate or build a distributed application.</p>
<p>347<br>00:33:41,799 –&gt; 00:33:45,960<br>And you know, the example is the classic example is map produce, and we’ll talk about it in a</p>
<p>348<br>00:33:45,960 –&gt; 00:33:49,319<br>second. And then that gets the first category is communication.</p>
<p>349<br>00:33:53,559 –&gt; 00:33:58,519<br>And we’ll spend less time on communication, and it’s almost more topic of, you know, six, eight to</p>
<p>350<br>00:33:58,519 –&gt; 00:34:03,559<br>nine network systems. But it will show up, you know, in the sense that there’s going to be some</p>
<p>351<br>00:34:03,559 –&gt; 00:34:08,679<br>contract, you know, between the network system and the distributed system. And I will,</p>
<p>352<br>00:34:09,800 –&gt; 00:34:13,720<br>the serious topic, you know, for example, first day we’re going to be talking about, we,</p>
<p>353<br>00:34:14,039 –&gt; 00:34:21,480<br>we’re more a procedure call, RPC, and that’s like the building block in which all labs are built,</p>
<p>354<br>00:34:21,480 –&gt; 00:34:26,839<br>and that’s our communication model. And the questions there are, you know, what kind of semantics</p>
<p>355<br>00:34:26,839 –&gt; 00:34:31,799<br>does actually the RPC system provide? You know, is it at most ones, exactly ones, at least ones,</p>
<p>356<br>00:34:32,359 –&gt; 00:34:37,879<br>and we’ll talk about that in first day’s lecture. But that’s where we’re sort of communication</p>
<p>357<br>00:34:37,879 –&gt; 00:34:43,480<br>and distributed systems, you know, intersect. So if you look at these three, so basically storage,</p>
<p>358<br>00:34:43,480 –&gt; 00:34:49,480<br>you know, the store data for durably, you know, computation to run competitions and communication</p>
<p>359<br>00:34:49,480 –&gt; 00:34:53,719<br>to actually have these different pieces communicate with each other. And so those are the three basic,</p>
<p>360<br>00:34:53,719 –&gt; 00:34:58,039<br>you know, things that sort of from which we will build distributed systems. And what are we</p>
<p>361<br>00:34:58,039 –&gt; 00:35:03,079<br>looking for are sort of abstractions that have been proven to be very helpful in building</p>
<p>362<br>00:35:03,079 –&gt; 00:35:10,039<br>distributed systems? And abstractions are like the remote procedure call, or like a map-produced</p>
<p>363<br>00:35:10,039 –&gt; 00:35:16,639<br>library, or in a storage system like a key value service. And often, you know, our</p>
<p>364<br>00:35:16,639 –&gt; 00:35:21,960<br>funer goal will be to make the abstractions distributed abstraction look very much like, you know,</p>
<p>365<br>00:35:21,960 –&gt; 00:35:26,679<br>the sort of normal standard sequential abstractions that you may familiar with. So, for example,</p>
<p>366<br>00:35:26,679 –&gt; 00:35:31,239<br>when we build a storage system, we want our basically distributed storage system more or less</p>
<p>367<br>00:35:31,239 –&gt; 00:35:37,880<br>behave like, you know, a single machine sequential storage server, like your regular file system on</p>
<p>368<br>00:35:37,880 –&gt; 00:35:42,599<br>your laptop. Except, you know, that, you know, we hope that the storage system is more fall</p>
<p>369<br>00:35:42,599 –&gt; 00:35:47,079<br>tolerance, you know, because may use replication, maybe much more high performance, because we use</p>
<p>370<br>00:35:47,079 –&gt; 00:35:51,640<br>many, many machines, but like the behavior of the system that we’re looking for is sort of similar</p>
<p>371<br>00:35:51,640 –&gt; 00:35:56,680<br>with the abstractions we’re looking for is similar to the single one. Turns out, in practice,</p>
<p>372<br>00:35:56,680 –&gt; 00:36:00,760<br>this actually is very hard to achieve. And, you know, we’ll see that, you know, it looks like it,</p>
<p>373<br>00:36:00,760 –&gt; 00:36:07,720<br>but it’s not exactly. And this is a topic that will show up multiple times. In fact, you know,</p>
<p>374<br>00:36:07,719 –&gt; 00:36:16,519<br>that brings me to sort of like the main recurring themes in this class. We’ll see over and over.</p>
<p>375<br>00:36:23,239 –&gt; 00:36:32,919<br>And the main topics are fall dollars. Not surprising. And that has sort of two aspects,</p>
<p>376<br>00:36:33,880 –&gt; 00:36:38,680<br>I have to say to define a little bit what fall tolerance means. One is availability.</p>
<p>377<br>00:36:39,800 –&gt; 00:36:46,039<br>So we’re going to be looking at techniques. We’re going to be looking at techniques to</p>
<p>378<br>00:36:48,280 –&gt; 00:36:54,760<br>make systems highly available. And so what we mean that is that they continue to deliver their</p>
<p>379<br>00:36:54,760 –&gt; 00:36:59,880<br>service despite, you know, there are being failures. And so this is often expressed as like a number of</p>
<p>380<br>00:36:59,880 –&gt; 00:37:07,559<br>nice, you know, 0.9999 reliability. And so that’s going to be one aspect of fall tones. The second</p>
<p>381<br>00:37:07,559 –&gt; 00:37:11,480<br>aspect of the fall tones that we care a lot about is what I’m going to call recovery ability.</p>
<p>382<br>00:37:17,800 –&gt; 00:37:25,640<br>And when a machine crashes or fails, we like to bring it back into the system once it reboots,</p>
<p>383<br>00:37:25,639 –&gt; 00:37:29,559<br>you know, so that we can keep up the availability because we didn’t like repair the system.</p>
<p>384<br>00:37:29,559 –&gt; 00:37:33,879<br>And basically all the machines would die one by one until we have zero machines. And then we</p>
<p>385<br>00:37:33,879 –&gt; 00:37:38,759<br>have no service anymore. So it’s important that we repair the distributed system. The way we repair</p>
<p>386<br>00:37:38,759 –&gt; 00:37:42,359<br>the distributed system is basically when the machine comes back up, you know, we want to</p>
<p>387<br>00:37:43,079 –&gt; 00:37:47,000<br>it needs to recover its state and then you know, start participating back into the distributed</p>
<p>388<br>00:37:47,000 –&gt; 00:37:54,440<br>systems. And it turns out that is actually heart, that’s a hard aspect. And a key techniques,</p>
<p>389<br>00:37:54,440 –&gt; 00:38:01,480<br>you know, for availability is going to be a replication. And the key technique we’re</p>
<p>390<br>00:38:02,039 –&gt; 00:38:07,320<br>that we’re going to use for recoverability is basically something called logging or transactions.</p>
<p>391<br>00:38:09,320 –&gt; 00:38:15,639<br>Writing things through durable storage. So that may or one, the power goes out, but the machine</p>
<p>392<br>00:38:15,639 –&gt; 00:38:21,559<br>comes back up afterwards, you know, we’re half the data still there on disk.</p>
<p>393<br>00:38:24,679 –&gt; 00:38:33,480<br>So that’s the fault-tauld site. The second part is, you know, something we’re going to call consistency.</p>
<p>394<br>00:38:38,840 –&gt; 00:38:46,679<br>And this is basically the contract, you know, that the server is going to provide or for operations</p>
<p>395<br>00:38:46,679 –&gt; 00:38:52,279<br>with respect to concurrency and failure. And so, loosely speaking, you know, what we</p>
<p>396<br>00:38:54,119 –&gt; 00:39:01,960<br>when we think about consistency, basically the ideal is the same behavior as that a single machine</p>
<p>397<br>00:39:01,960 –&gt; 00:39:06,119<br>would deliver. So we have a replicated fault tolerance high-performance file system,</p>
<p>398<br>00:39:06,119 –&gt; 00:39:10,199<br>considering many machines. We like to behave it to be, I’m almost identical to the sequential</p>
<p>399<br>00:39:10,199 –&gt; 00:39:16,359<br>machine. And so the key question always here is sort of on the forum, let’s say we have a key</p>
<p>400<br>00:39:16,360 –&gt; 00:39:26,280<br>value server, you know, does to get operation, return value of the last put.</p>
<p>401<br>00:39:34,280 –&gt; 00:39:38,599<br>And if you run a single machine, you have nothing, you know, concurrent operations. So you run</p>
<p>402<br>00:39:38,599 –&gt; 00:39:44,120<br>every operation one by one, like you do, put, put, put, then get, then again, then again. Then of course,</p>
<p>403<br>00:39:44,599 –&gt; 00:39:49,239<br>this is a discussion of terminal to answer, you would assume that the return value is stored</p>
<p>404<br>00:39:49,239 –&gt; 00:39:55,639<br>by the last put. But once we have concurrency and we failures and we have many machines, this is</p>
<p>405<br>00:39:55,639 –&gt; 00:40:02,920<br>actually not so obvious. You know, what the right way, what the what a good contract is. And we’ll see</p>
<p>406<br>00:40:02,920 –&gt; 00:40:07,719<br>actually many different contracts. We see ones that have strong consistency, you know, the</p>
<p>407<br>00:40:07,719 –&gt; 00:40:13,799<br>almost behave like a sequential machine or ones that have a very loose guarantees</p>
<p>408<br>00:40:17,239 –&gt; 00:40:22,199<br>and provide very different semantics, for example, they provide eventual consistency. Eventually,</p>
<p>409<br>00:40:22,199 –&gt; 00:40:29,879<br>you will see a get will return the result of a put, but not immediately. And the reason</p>
<p>410<br>00:40:30,599 –&gt; 00:40:34,679<br>there are sort of different types of consistency that’s directly related with performance.</p>
<p>411<br>00:40:37,719 –&gt; 00:40:41,879<br>You know, often one of the goals of the civil system is to deliver high performance, you know,</p>
<p>412<br>00:40:41,879 –&gt; 00:40:48,679<br>scale example within number of machines. And you know, to achieve that performance, that’s sort of</p>
<p>413<br>00:40:48,679 –&gt; 00:40:54,599<br>almost in conflict with, you know, consistency and fault tolerance. You know, to actually achieve</p>
<p>414<br>00:40:54,599 –&gt; 00:40:59,159<br>strong consistency requires communication between the different machines, which might actually</p>
<p>415<br>00:40:59,159 –&gt; 00:41:04,039<br>reduce performance. Similarly, you know, to achieve fault tones, you know, we need to replicate</p>
<p>416<br>00:41:04,039 –&gt; 00:41:08,759<br>data. That means we have to communicate data from one machine to another machine. And if we</p>
<p>417<br>00:41:08,759 –&gt; 00:41:13,320<br>were, I have to write that machine data also to durable storage, you know, that X-bit operation is</p>
<p>418<br>00:41:13,320 –&gt; 00:41:20,920<br>expensive. And so the replication can cost the performance. And so, uh, achieving these sort of free</p>
<p>419<br>00:41:20,920 –&gt; 00:41:25,639<br>things at the same time, uh, it turns out to be extremely difficult. And in fact, what people do</p>
<p>420<br>00:41:25,639 –&gt; 00:41:29,800<br>in practice is they make different trade-offs, you know, they will sacrifice some consistency to get</p>
<p>421<br>00:41:29,800 –&gt; 00:41:33,639<br>better performance, or maybe some fault tolerance to get better performance. And so we’ll see,</p>
<p>422<br>00:41:34,039 –&gt; 00:41:39,880<br>throughout the semester, a wide spectrum of different types of designs that, you know, make that</p>
<p>423<br>00:41:39,880 –&gt; 00:41:47,480<br>trade-off differently. Just a small note of performance, there’s two aspects to it, like one</p>
<p>424<br>00:41:48,039 –&gt; 00:41:56,039<br>is throughput. So you buy more machines, hopefully the throughput scales with the number of machines.</p>
<p>425<br>00:41:56,599 –&gt; 00:42:01,400<br>But there’s another sort of part of aspect performance is basically much harder to achieve,</p>
<p>426<br>00:42:01,480 –&gt; 00:42:08,280<br>which is like low latency. And this is particularly important, like in these websites, where you have</p>
<p>427<br>00:42:08,280 –&gt; 00:42:12,760<br>thousands of thousands of machines, and, you know, maybe one user request, you know, when you click on</p>
<p>428<br>00:42:12,760 –&gt; 00:42:17,880<br>a URL, actually costs a lot of these machines to participate. And if one of those machines is very</p>
<p>429<br>00:42:17,880 –&gt; 00:42:22,680<br>slow, you know, maybe it has, you know, some mechanical issues, or maybe the disk is not working</p>
<p>430<br>00:42:22,680 –&gt; 00:42:31,160<br>100% or some other aspect where it doesn’t really work well. That one slow machine can cost the</p>
<p>431<br>00:42:31,159 –&gt; 00:42:39,000<br>whole user experience to be slow. And this is often referred to as 10 latency. And there’s a concern</p>
<p>432<br>00:42:39,000 –&gt; 00:42:44,519<br>that we’ll show up over and over, you know, throughout the semester, as we were discussing different</p>
<p>433<br>00:42:44,519 –&gt; 00:42:51,399<br>machines, and even shows up in the today’s paper, in the MapReduce beta. So one other final topic that</p>
<p>434<br>00:42:51,400 –&gt; 00:42:57,800<br>will show up a lot, at least in the class, particularly in the lab, is implementation</p>
<p>435<br>00:43:02,039 –&gt; 00:43:07,639<br>aspects. And here is really like how to manage, you know, concurrency, how to do remote procedure</p>
<p>436<br>00:43:07,639 –&gt; 00:43:13,880<br>calling, implementation, and just building the systems by themselves, going to have actually</p>
<p>437<br>00:43:13,880 –&gt; 00:43:17,880<br>serious implementation challenges, and that will come over and over and over and over, and have</p>
<p>438<br>00:43:17,960 –&gt; 00:43:22,280<br>through that to semester. And that partly is because you know, we want to achieve performance</p>
<p>439<br>00:43:22,280 –&gt; 00:43:27,480<br>consistency in fault holes in the crashes, crashes, and concurrency, which just makes, you know, just</p>
<p>440<br>00:43:27,480 –&gt; 00:43:37,320<br>drives complexity. So those are the main topics. Any questions about the spark?</p>
<p>441<br>00:43:37,800 –&gt; 00:43:53,000<br>Okay, then let’s sort of dive in and look at the first case study, and through the MapReduce paper.</p>
<p>442<br>00:43:53,000 –&gt; 00:44:08,760<br>And there’s an illustration of many of the topics in 6.8 to 4, you know, we’re going to be talking</p>
<p>443<br>00:44:08,760 –&gt; 00:44:14,199<br>about fault tolerance, we’re going to talk about performance, tail latency, all kinds of issues</p>
<p>444<br>00:44:14,199 –&gt; 00:44:19,159<br>that actually we see throughout the semester, and we’ll see one cut or one system that deals with that.</p>
<p>445<br>00:44:20,039 –&gt; 00:44:23,480<br>So good illustration of many of the topics.</p>
<p>446<br>00:44:28,519 –&gt; 00:44:30,119<br>The paper is also very influential.</p>
<p>447<br>00:44:35,639 –&gt; 00:44:40,199<br>Although Google internally doesn’t use MapReduce, you know, just write this paper exactly,</p>
<p>448<br>00:44:40,199 –&gt; 00:44:44,759<br>you know, they have systems directly derived, you know, from this MapReduce system,</p>
<p>449<br>00:44:44,840 –&gt; 00:44:51,640<br>that they are still using day to day. There are other libraries that look a lot like MapReduce,</p>
<p>450<br>00:44:52,200 –&gt; 00:44:57,000<br>that they are widely used. It also inspired different types of computation models</p>
<p>451<br>00:44:57,720 –&gt; 00:45:02,760<br>than MapReduce itself, and we’ll see you want to do more later in this semester. So</p>
<p>452<br>00:45:02,760 –&gt; 00:45:09,320<br>hugely influential paper. And then finally, you know, there’s actually the topic of Lab1,</p>
<p>453<br>00:45:09,320 –&gt; 00:45:14,360<br>which is another good reason to talk about it. Now many probably have, you have seen</p>
<p>454<br>00:45:14,840 –&gt; 00:45:19,720<br>the MapReduce paper show up in 633, if you’re an undergrad, you’re an MIT,</p>
<p>455<br>00:45:20,680 –&gt; 00:45:26,440<br>otherwise you might have seen it in other places. But we’re going to go a little bit deeper</p>
<p>456<br>00:45:27,480 –&gt; 00:45:31,560<br>than, for example, 633, because you actually have to implement your own MapReduce library.</p>
<p>457<br>00:45:33,240 –&gt; 00:45:39,160<br>As always, when you implement something, you know, problems that you might not have really</p>
<p>458<br>00:45:39,159 –&gt; 00:45:44,440<br>fought hard about before, you know, certainly start popping up. And so by the end of it,</p>
<p>459<br>00:45:44,440 –&gt; 00:45:50,920<br>you really understand MapReduce. Any questions?</p>
<p>460<br>00:45:50,920 –&gt; 00:46:10,119<br>Let me give you a little bit of context for this paper. This paper is written by, you know,</p>
<p>461<br>00:46:10,119 –&gt; 00:46:20,280<br>two engineers from Google, very well known. And the context is sort of these early data centers.</p>
<p>462<br>00:46:20,280 –&gt; 00:46:27,720<br>So Google has a search engine needed to build a reverse index of the word white web,</p>
<p>463<br>00:46:27,720 –&gt; 00:46:34,120<br>you know, to basically allow users to query the internet. And these, these kind of computations,</p>
<p>464<br>00:46:34,120 –&gt; 00:46:45,160<br>you know, take multi hours to run. And they, you know, process terabyte of data.</p>
<p>465<br>00:46:45,239 –&gt; 00:46:50,759<br>Okay, our computations,</p>
<p>466<br>00:46:55,480 –&gt; 00:47:02,440<br>terabyte of data, terabytes of data. And so thank you, thank you,</p>
<p>467<br>00:47:02,440 –&gt; 00:47:09,879<br>Web Indexing, Web Crawling, others, particularly Web Indexing. This is one of the driving application.</p>
<p>468<br>00:47:10,840 –&gt; 00:47:17,320<br>And you know, as Google built these sort of applications internally, you know, like SunJay and</p>
<p>469<br>00:47:17,320 –&gt; 00:47:21,800<br>JetDing, you know, the two offers, you know, they were very good at that kind of stuff. But they’ve</p>
<p>470<br>00:47:21,800 –&gt; 00:47:27,079<br>discovered that basically where many other Google engineers, you know, one of the right those kind</p>
<p>471<br>00:47:27,079 –&gt; 00:47:31,400<br>of certain types of applications too, they wanted to be able to write their own data analysis</p>
<p>472<br>00:47:31,400 –&gt; 00:47:37,400<br>over all the web pages that have been crawled. And so, and they realized, you know,</p>
<p>473<br>00:47:37,400 –&gt; 00:47:41,160<br>they’re writing these kinds of applications. It was difficult because if you’re running</p>
<p>474<br>00:47:41,160 –&gt; 00:47:45,800<br>multi-hour computation in many, many machines, it is very likely that one of those machines will</p>
<p>475<br>00:47:45,800 –&gt; 00:47:50,599<br>crash during that computation. And therefore, you know, you have to build in some plant</p>
<p>476<br>00:47:50,599 –&gt; 00:47:55,480<br>fault tolerance. And, you know, once you start doing that, then basically requires that you’re</p>
<p>477<br>00:47:55,480 –&gt; 00:48:00,440<br>basically, you know, have taken something like 6824 and able to build, you know, these kinds of</p>
<p>478<br>00:48:00,440 –&gt; 00:48:05,240<br>complicated systems. And their goal was to basically get out of that sort of</p>
<p>479<br>00:48:05,239 –&gt; 00:48:11,719<br>dilemma and make it basically easy for non-experts</p>
<p>480<br>00:48:17,879 –&gt; 00:48:27,719<br>to write the simple applications. And so, that’s the motivation for this paper and why you’re</p>
<p>481<br>00:48:28,359 –&gt; 00:48:36,199<br>very excited about it. And so, the approach they take that produce takes is, it is not a general</p>
<p>482<br>00:48:36,199 –&gt; 00:48:42,199<br>purpose library. You know, you can’t like write, take any application and use map reduced through</p>
<p>483<br>00:48:42,199 –&gt; 00:48:47,959<br>actually make it basically fault-collar. And so, it has to be written in a particular style name</p>
<p>484<br>00:48:47,959 –&gt; 00:48:52,759<br>and using these map functions and reduce functions. And those functions are basically functional</p>
<p>485<br>00:48:53,400 –&gt; 00:49:01,320<br>or stateless. And the program will write these scripts, sequential code.</p>
<p>486<br>00:49:06,280 –&gt; 00:49:09,320<br>And enhance, you know, these two functions, you know, to map into reduced function,</p>
<p>487<br>00:49:09,320 –&gt; 00:49:14,520<br>two sort of the framework and then the framework to map reduced framework deals with all the</p>
<p>488<br>00:49:14,840 –&gt; 00:49:27,960<br>distributedness. So, it will arrange that, you know, the application, the</p>
<p>489<br>00:49:27,960 –&gt; 00:49:31,719<br>binary for the programs, they run on many machines or install the many machines,</p>
<p>490<br>00:49:31,719 –&gt; 00:49:36,360<br>runs on many machines, it deals with load balancing, it deals with certain machines that are slow,</p>
<p>491<br>00:49:37,159 –&gt; 00:49:40,679<br>it will deal with the machines that crash. And so, the application writer itself,</p>
<p>492<br>00:49:40,839 –&gt; 00:49:44,679<br>who wrote the map reduced function, don’t really have to be concerned about this at all.</p>
<p>493<br>00:49:45,639 –&gt; 00:49:51,719<br>And they basically get all that stuff, if you will, transparently. And again, to make that happen,</p>
<p>494<br>00:49:51,719 –&gt; 00:49:55,639<br>you know, the library is actually not in general purpose. So, for example, if you wanted to write a</p>
<p>495<br>00:49:55,639 –&gt; 00:49:59,480<br>key value service, you couldn’t use the map reduced library because it assumes a particular</p>
<p>496<br>00:49:59,480 –&gt; 00:50:04,199<br>computational model and, you know, your application has to fit in that. In the computational model,</p>
<p>497<br>00:50:04,199 –&gt; 00:50:08,919<br>you know, it fits, it’s something that they saw a lot in Google, which is like people wanted to do</p>
<p>498<br>00:50:09,720 –&gt; 00:50:13,240<br>big data analysis on basically, you know, all the web pages in the world.</p>
<p>499<br>00:50:13,960 –&gt; 00:50:17,880<br>And there are many types of computations that just have to process lots and lots of data</p>
<p>500<br>00:50:17,880 –&gt; 00:50:23,079<br>and compute values based on that data. So, that’s sort of the type of applications that</p>
<p>501<br>00:50:23,880 –&gt; 00:50:37,239<br>that we’ve introduced targets. Any questions about the sort of context and the motivation for this paper?</p>
<p>502<br>00:50:41,799 –&gt; 00:50:49,639<br>Okay, let me proceed. So, let me first draw sort of an abstract view of what’s going on.</p>
<p>503<br>00:50:53,639 –&gt; 00:51:03,239<br>And then we’ll dive into more detail. So, sort of view that you sort of need to have in the background</p>
<p>504<br>00:51:05,880 –&gt; 00:51:09,400<br>to understand actually how the app reduced works, which is going to be very important for you when</p>
<p>505<br>00:51:09,400 –&gt; 00:51:16,599<br>you’re doing the lab one, is there’s a bunch of input files, you know, whatever. F1, F2, F3, let’s say.</p>
<p>506<br>00:51:17,159 –&gt; 00:51:21,480<br>Of course, they’re going to be many, many more in Google’s case, but just for pedagogical reasons,</p>
<p>507<br>00:51:21,480 –&gt; 00:51:28,440<br>can are going to decide the size of my display. I’m going to have three files.</p>
<p>508<br>00:51:30,519 –&gt; 00:51:37,960<br>Basically, for every file, this process by map function. So, one written by the programmer,</p>
<p>509<br>00:51:38,679 –&gt; 00:51:43,960<br>and you know, produces some output, some intermediate output. So, for example, the classic example,</p>
<p>510<br>00:51:43,960 –&gt; 00:51:50,760<br>to discuss map-producers’ work out. So, basically counting how many times award occurs in</p>
<p>511<br>00:51:51,639 –&gt; 00:51:56,760<br>the data sets, where the data sets consist of many, many, many files. So, for example, like, you know,</p>
<p>512<br>00:51:56,760 –&gt; 00:52:02,920<br>we’re running the word count function on file one, and it will produce for every word</p>
<p>513<br>00:52:03,960 –&gt; 00:52:09,240<br>and a key value pair. And the key value pair consists of the key, which is the word,</p>
<p>514<br>00:52:09,240 –&gt; 00:52:15,719<br>in account with one. And if you can add multiple times in this file, F1, then you know,</p>
<p>515<br>00:52:15,719 –&gt; 00:52:19,639<br>it would be multiple and record from that multiple key value pairs, A1.</p>
<p>516<br>00:52:21,880 –&gt; 00:52:27,079<br>And so, maybe, you know, this file contains none of many words, you know, maybe has A1 and B1.</p>
<p>517<br>00:52:27,079 –&gt; 00:52:32,360<br>So, the file contains two words. You know, similarly, you know, the function, the map function</p>
<p>518<br>00:52:32,360 –&gt; 00:52:37,240<br>for does the same thing for the file F2, and will produce some key values. And let’s say, maybe</p>
<p>519<br>00:52:37,240 –&gt; 00:52:46,039<br>there’s only the word B appears in the file once. And maybe, you know, F3, the map function</p>
<p>520<br>00:52:46,119 –&gt; 00:52:52,759<br>also runs in the file F3. And let’s assume, let’s just, we’re, the, where is this, assume that A shows</p>
<p>521<br>00:52:52,759 –&gt; 00:52:59,559<br>up once, and you’re going to the word C shows up once. So, basically, you know, these map functions,</p>
<p>522<br>00:52:59,559 –&gt; 00:53:04,039<br>all run in parallel, completely independent of each other. There’s like no communication between</p>
<p>523<br>00:53:04,039 –&gt; 00:53:08,440<br>them on their input files. And so, this is going to give us, you know, hopefully high throughput,</p>
<p>524<br>00:53:08,440 –&gt; 00:53:11,800<br>or, you know, all of us are scaled to much, much, much, much, much from bigger data sets.</p>
<p>525<br>00:53:12,519 –&gt; 00:53:16,760<br>And then, produce on these intermediate values, these key value pairs, we’re not going to</p>
<p>526<br>00:53:16,760 –&gt; 00:53:23,560<br>like A1, B1, you know, B1 alone, or A1, SC2. And then, sort of the second step, you know, this</p>
<p>527<br>00:53:23,560 –&gt; 00:53:27,960<br>often referred to as the shovel, is that basically, you know, you’re going to run the reduce</p>
<p>528<br>00:53:28,760 –&gt; 00:53:35,560<br>functions on basically each row. So, here we got the row of all the A’s, and we’re going to run</p>
<p>529<br>00:53:36,519 –&gt; 00:53:44,519<br>a reduce function. And then, the reduce function basically takes, you know, the one key aggregates all</p>
<p>530<br>00:53:44,519 –&gt; 00:53:50,039<br>the, or the reduce function gets its input, the key plus the aggregated values, or not the aggregated</p>
<p>531<br>00:53:50,039 –&gt; 00:53:55,880<br>value, but the, the, the combined values, you know, from the different outputs of maps. So, in this</p>
<p>532<br>00:53:55,880 –&gt; 00:54:01,639<br>case, the reduce function would get, you know, two intermediate results, you know, both A,</p>
<p>533<br>00:54:01,719 –&gt; 00:54:06,759<br>with the key A and two values, one and one. And in this case, in the case of a work count,</p>
<p>534<br>00:54:06,759 –&gt; 00:54:12,279<br>you know, we just add them up. And so, you know, if we produce the value, you know, key value pair A2.</p>
<p>535<br>00:54:13,319 –&gt; 00:54:17,400<br>And we’re doing it basically, we’re doing, and basically what we’re doing is we’re doing,</p>
<p>536<br>00:54:17,400 –&gt; 00:54:21,000<br>we’re doing, we’re, we’re, we’re going to run the reduce for every, you know, row.</p>
<p>537<br>00:54:22,519 –&gt; 00:54:27,719<br>And so, this will produce, you know, whatever, B2, and then, simply, you know, and you know,</p>
<p>538<br>00:54:27,719 –&gt; 00:54:34,039<br>C1 for the last one. And again, you know, the, once we’ve done sort of the shovel, you know,</p>
<p>539<br>00:54:34,039 –&gt; 00:54:38,039<br>these reduce functions can totally run independently of each other. Now, they can just, you know,</p>
<p>540<br>00:54:38,039 –&gt; 00:54:43,959<br>process, you know, whatever row they, day to day had, and be done with it. And so, the only sort of</p>
<p>541<br>00:54:43,959 –&gt; 00:54:51,079<br>really expensive, you know, piece in this is, is this shovel in the middle, where the reduce functions</p>
<p>542<br>00:54:51,079 –&gt; 00:54:59,799<br>need to obtain, you know, their inputs for basically every mapper. So, when all the mappers are done,</p>
<p>543<br>00:54:59,799 –&gt; 00:55:07,799<br>you know, the reduce function basically gets, you know, needs to contact every mapper, extract,</p>
<p>544<br>00:55:07,799 –&gt; 00:55:14,599<br>you know, the output for, output for the mapper, without particular reduce function, and, you know,</p>
<p>545<br>00:55:14,599 –&gt; 00:55:19,480<br>sort, you know, by T, and then, you know, basically run the reduce function. And so, basically,</p>
<p>546<br>00:55:19,480 –&gt; 00:55:25,240<br>we’re sort of assuming, but the paper sort of points out, expensive operation is really that</p>
<p>547<br>00:55:25,240 –&gt; 00:55:33,320<br>shuffling of data between the mappers and the reduces. Any questions about this abstract picture?</p>
<p>548<br>00:55:38,840 –&gt; 00:55:48,199<br>Okay. Sorry. I had a question. So, is there, I know that not all problems can be expressed</p>
<p>549<br>00:55:48,199 –&gt; 00:55:56,919<br>with a, in MapReduce stage, but is, for example, like sorting an array, is it possible to do?</p>
<p>550<br>00:55:56,919 –&gt; 00:56:01,399<br>Yeah. So, yeah. So, sorting is one of the applications that they, a town to lot actually, the paper,</p>
<p>551<br>00:56:01,960 –&gt; 00:56:07,319<br>and it would be something that’s totally done with MapReduce. So, basically, you split the input files,</p>
<p>552<br>00:56:07,319 –&gt; 00:56:15,960<br>correct, and many things, the mappers sort their piece, and then they split the output, say, like,</p>
<p>553<br>00:56:16,039 –&gt; 00:56:20,519<br>R buckets, and then, it’s reduced functions, you know, basically sorts that particular R bucket,</p>
<p>554<br>00:56:21,320 –&gt; 00:56:22,840<br>and that gives a total sorted file.</p>
<p>555<br>00:56:25,800 –&gt; 00:56:31,159<br>Easy of that. And in this case, you know, in sort this interesting, because, basically, the input,</p>
<p>556<br>00:56:31,880 –&gt; 00:56:38,039<br>the intermediate values, and the output are the same size. I can some other functions, like,</p>
<p>557<br>00:56:38,039 –&gt; 00:56:43,800<br>maybe the map function will reduce the intermediate state to something much smaller than the input size.</p>
<p>558<br>00:56:44,440 –&gt; 00:56:47,000<br>In the case of short, that is not the case.</p>
<p>559<br>00:56:49,720 –&gt; 00:56:53,720<br>Okay, now let’s look at the paper, actually, and get a little bit of sense, actually, how you write them.</p>
<p>560<br>00:56:58,200 –&gt; 00:57:00,039<br>Now, let’s see if I can actually,</p>
<p>561<br>00:57:02,920 –&gt; 00:57:03,800<br>let’s just ignore.</p>
<p>562<br>00:57:06,840 –&gt; 00:57:09,960<br>Last menu, let’s hold it one second.</p>
<p>563<br>00:57:14,039 –&gt; 00:57:15,000<br>Okay.</p>
<p>564<br>00:57:21,000 –&gt; 00:57:26,840<br>There’s not so cool. Give me a second too. Ah, here we go.</p>
<p>565<br>00:57:26,840 –&gt; 00:57:29,000<br>And there’s a save.</p>
<p>566<br>00:57:29,000 –&gt; 00:57:31,480<br>Okay, here we go.</p>
<p>567<br>00:57:31,480 –&gt; 00:57:32,680<br>Let’s go right here.</p>
<p>568<br>00:57:35,320 –&gt; 00:57:37,160<br>Okay, can everybody see this?</p>
<p>569<br>00:57:37,319 –&gt; 00:57:42,759<br>Okay, there’s a couple questions.</p>
<p>570<br>00:57:45,879 –&gt; 00:57:49,960<br>Let me you postpone some of these questions, because I will see them in,</p>
<p>571<br>00:57:49,960 –&gt; 00:57:51,799<br>and we’ll discuss them in a second in more detail.</p>
<p>572<br>00:57:53,000 –&gt; 00:57:56,359<br>If I don’t answer your question, please ask it again.</p>
<p>573<br>00:57:56,920 –&gt; 00:58:00,039<br>So the first thing I want to do is actually look at one of the examples in the paper</p>
<p>574<br>00:58:00,039 –&gt; 00:58:05,319<br>of a map and a reduced function corresponding to the word count example that we just sort of abstractly discussed.</p>
<p>575<br>00:58:06,039 –&gt; 00:58:09,800<br>So here’s the code for the map and a reduced function.</p>
<p>576<br>00:58:10,680 –&gt; 00:58:14,200<br>You see that the map function takes a key value.</p>
<p>577<br>00:58:14,760 –&gt; 00:58:16,600<br>The key is really not that important here.</p>
<p>578<br>00:58:16,600 –&gt; 00:58:22,440<br>It’s the document name, so f1 or f2, and string, the value is basically the content of the file.</p>
<p>579<br>00:58:23,240 –&gt; 00:58:26,840<br>So all the words that actually appear in the file f1.</p>
<p>580<br>00:58:27,480 –&gt; 00:58:31,800<br>And then basically it goes through, you know, the speed piece code goes through the</p>
<p>581<br>00:58:31,800 –&gt; 00:58:38,840<br>words in the file, and as an intermediate value admits, you know, these A1, B1, C1, etc.</p>
<p>582<br>00:58:39,560 –&gt; 00:58:42,360<br>But like for the programmer point of view, you’re correct, you don’t really see these</p>
<p>583<br>00:58:42,360 –&gt; 00:58:44,519<br>intermediate key value pairs at all.</p>
<p>584<br>00:58:45,320 –&gt; 00:58:47,480<br>You just write this one simple map function.</p>
<p>585<br>00:58:48,920 –&gt; 00:58:52,680<br>And then the reduced function is also more or less as expected.</p>
<p>586<br>00:58:53,960 –&gt; 00:58:59,320<br>It takes two arguments, you know, the key, you’re like A, and values, in this case,</p>
<p>587<br>00:58:59,320 –&gt; 00:59:03,480<br>we’re a word count that would be 1111, so the number of times that the word A actually</p>
<p>588<br>00:59:04,280 –&gt; 00:59:06,280<br>showed up in the intermediate output.</p>
<p>589<br>00:59:06,840 –&gt; 00:59:11,640<br>And basically what the function does, it just, you know, goes over the iterates over the list</p>
<p>590<br>00:59:11,640 –&gt; 00:59:17,160<br>of values, and then basically adds 1 plus 1 plus 1 plus 1, and then the midst, you know, the final result.</p>
<p>591<br>00:59:18,920 –&gt; 00:59:23,320<br>And so that’s basically, you know, as you can see from this code, right, like the programmer,</p>
<p>592<br>00:59:23,320 –&gt; 00:59:27,559<br>basically always writes, you know, complete, straightforward sequential code.</p>
<p>593<br>00:59:27,559 –&gt; 00:59:32,360<br>Now this application is, you know, very simple admittedly, but you know, the code for even more</p>
<p>594<br>00:59:32,360 –&gt; 00:59:36,519<br>complex application would also be straight, you know, sequential might be more code, but it would be</p>
<p>595<br>00:59:36,519 –&gt; 00:59:40,759<br>straightforward sequential code. And in this code, the programmer doesn’t really worry about the</p>
<p>596<br>00:59:40,759 –&gt; 00:59:45,159<br>fact that at all that machines might crash, you know, they might unloading balance. That’s just</p>
<p>597<br>00:59:45,159 –&gt; 00:59:50,599<br>basically all taken care of the map reduced library. So this is, and so, you know, the hope,</p>
<p>598<br>00:59:50,599 –&gt; 00:59:54,519<br>and I think this has been proven out to be true, is this actually made both lots and lots of</p>
<p>599<br>00:59:54,519 –&gt; 00:59:59,960<br>people to write, you know, distributed applications and process gigantic data sets that are like,</p>
<p>600<br>00:59:59,960 –&gt; 01:00:04,599<br>could no way fit on a single machine. Like, damn it, the whole world, like, well,</p>
<p>601<br>01:00:07,159 –&gt; 01:00:12,360<br>that does not make sense in terms of, you know, what the programmer actually sees.</p>
<p>602<br>01:00:15,719 –&gt; 01:00:18,280<br>Okay, let’s talk a little bit about the implementation.</p>
<p>603<br>01:00:18,440 –&gt; 01:00:24,920<br>So I’m using the diagram here from the paper.</p>
<p>604<br>01:00:28,360 –&gt; 01:00:34,200<br>So we’ve got the user program. So the user program is like the map in the reduced function that we</p>
<p>605<br>01:00:34,200 –&gt; 01:00:42,760<br>just saw. You submit the map in the reduced function to the, you link it with the map reduced</p>
<p>606<br>01:00:42,760 –&gt; 01:00:50,200<br>library and that forms a binary. And then you give this to the Google drop scheduler and it will</p>
<p>607<br>01:00:50,200 –&gt; 01:00:57,160<br>basically find a whole bunch of machines and run what they call workers there. So like, you know,</p>
<p>608<br>01:00:57,160 –&gt; 01:01:02,520<br>you’re the scheduler will, for example, in the evaluation, as we’ll see in a second, you know,</p>
<p>609<br>01:01:02,520 –&gt; 01:01:07,400<br>there are about like 1800 machines on these 1800 machines, you know, the scheduler will run a</p>
<p>610<br>01:01:07,400 –&gt; 01:01:13,720<br>worker process that actually does the actual work and invokes, you know, map and the reduced functions</p>
<p>611<br>01:01:14,280 –&gt; 01:01:21,160<br>when appropriate. There’s one other process that is important in the paper to call the master</p>
<p>612<br>01:01:21,160 –&gt; 01:01:27,400<br>process in the lab, we’ll call it the coordinator. And the coordinator begs orchestrates the workers</p>
<p>613<br>01:01:27,400 –&gt; 01:01:36,680<br>and hands jobs or maps, yaks to them. So like, the terminology here is that a complete application is</p>
<p>614<br>01:01:36,679 –&gt; 01:01:45,000<br>one job, a map reduced job, and then a reduced ineffication of reduce or ineffication of map is what</p>
<p>615<br>01:01:45,000 –&gt; 01:01:54,039<br>is called the task. And so, you know, basically, you get the coordinator will assign files to</p>
<p>616<br>01:01:54,839 –&gt; 01:01:58,599<br>particular workers and the worker will then invoke the map function on that particular</p>
<p>617<br>01:02:00,199 –&gt; 01:02:04,759<br>file and that will produce some intermediate results. You know, you’re the intermediate results and</p>
<p>618<br>01:02:04,760 –&gt; 01:02:11,240<br>those intermediate results are stored on the local disk of the machine that actually runs that</p>
<p>619<br>01:02:11,240 –&gt; 01:02:17,720<br>particular map function. And when, you know, a worker has run the complete to the particular map</p>
<p>620<br>01:02:17,720 –&gt; 01:02:23,400<br>function, it basically tells the master, I’m done with that map function and, you know,</p>
<p>621<br>01:02:24,440 –&gt; 01:02:31,480<br>tells the master where the intermediate results are. Then at some point, when all the sort of maps</p>
<p>622<br>01:02:31,960 –&gt; 01:02:37,559<br>basically done, you know, the coordinator will start running reduced functions and the reduced</p>
<p>623<br>01:02:37,559 –&gt; 01:02:42,280<br>functions will collect, you know, the intermediate results, you know, from the different map push,</p>
<p>624<br>01:02:42,280 –&gt; 01:02:46,599<br>from the locations that are specified in the sort of the result record,</p>
<p>625<br>01:02:48,280 –&gt; 01:02:52,039<br>retrieved that data sorted by key, and then basically reduce,</p>
<p>626<br>01:02:52,760 –&gt; 01:02:57,480<br>invoke the reduced function on every key and list of values.</p>
<p>627<br>01:02:57,639 –&gt; 01:03:02,760<br>And that, you know, produces an output file and that is the, you know, there’s going to be one</p>
<p>628<br>01:03:02,760 –&gt; 01:03:07,159<br>output file for reduced function. And, you know, you can aggregate, you know, these output files</p>
<p>629<br>01:03:07,159 –&gt; 01:03:12,599<br>are going to calculate the output files to get the final output. That’s sort of the structure.</p>
<p>630<br>01:03:13,240 –&gt; 01:03:19,079<br>The input files live in a global file system, that’s called GFS, although it will use</p>
<p>631<br>01:03:19,079 –&gt; 01:03:24,760<br>a different global file system now, but, you know, the paper uses GFS and we’ll actually read about GFS</p>
<p>632<br>01:03:24,760 –&gt; 01:03:30,920<br>next week. And the output files also go into GFS. The intermediate files don’t are not stored in</p>
<p>633<br>01:03:30,920 –&gt; 01:03:41,080<br>GFS, they’re stored on the local machines, where the work is run. Any questions about the sort of</p>
<p>634<br>01:03:41,080 –&gt; 01:03:49,240<br>rough scheduled implementation? I have a question about the process file for the remote read.</p>
<p>635<br>01:03:49,239 –&gt; 01:03:55,239<br>So in the remote read process is the file actually transferred to the reducer?</p>
<p>636<br>01:03:55,239 –&gt; 01:04:02,599<br>Yes. So the, exactly. So the intermediate results are produced or stored on the disk of a</p>
<p>637<br>01:04:03,719 –&gt; 01:04:10,519<br>machine that run the map, or that map function, and then the reduce goes out and basically fetches</p>
<p>638<br>01:04:10,519 –&gt; 01:04:17,000<br>its, you know, set of keys from every map. And so that point, you know, the data is transferred</p>
<p>639<br>01:04:17,000 –&gt; 01:04:20,519<br>across the network. So the network communication that happens is here.</p>
<p>640<br>01:04:25,000 –&gt; 01:04:29,000<br>The reason that there’s little network communication, no network communication here at all,</p>
<p>641<br>01:04:29,000 –&gt; 01:04:37,480<br>is because the workers, the way the coordinator assigns files to workers is that basically</p>
<p>642<br>01:04:38,679 –&gt; 01:04:44,519<br>the worker is run on the same machine. So every machine runs both of them, a worker process,</p>
<p>643<br>01:04:44,519 –&gt; 01:04:50,679<br>and a GFS process. And the workers are basically assigned to, or the map functions run on the</p>
<p>644<br>01:04:51,880 –&gt; 01:04:56,599<br>machine that actually has that file locally stored in GFS. And so basically this actually</p>
<p>645<br>01:04:56,599 –&gt; 01:05:02,920<br>corresponds to basically local reach, you know, through GFS to a local disk. And then the files are</p>
<p>646<br>01:05:02,920 –&gt; 01:05:08,280<br>produced or mapped, you know, produced into the intermediate files are stored on local disk too.</p>
<p>647<br>01:05:08,280 –&gt; 01:05:11,159<br>So there’s no communication happening in this sort of this part of the picture.</p>
<p>648<br>01:05:11,480 –&gt; 01:05:17,000<br>And then when the reduce functions run, they actually retrieve the files across the network and then</p>
<p>649<br>01:05:17,000 –&gt; 01:05:23,960<br>write it out in GFS. And there’s going to maybe some network communication here when the workers</p>
<p>650<br>01:05:23,960 –&gt; 01:05:33,319<br>actually produce the files in the global file system. I have another question. Is the, is the</p>
<p>651<br>01:05:33,320 –&gt; 01:05:43,640<br>coordinator responsible for partitioning the data and putting it on each worker or</p>
<p>652<br>01:05:44,360 –&gt; 01:05:49,960<br>another machine? No, not really. The, basically, the, the map produced one, you run the user</p>
<p>653<br>01:05:49,960 –&gt; 01:05:55,640<br>program, you’re basically saying like, and I want to run it on F1, F2, F3, F4, whatever, all the input</p>
<p>654<br>01:05:55,640 –&gt; 01:06:03,080<br>files. And those input files live in GFS. And so the, part of the job specification,</p>
<p>655<br>01:06:03,079 –&gt; 01:06:08,039<br>users say like which part input files need to be processed? Okay.</p>
<p>656<br>01:06:13,719 –&gt; 01:06:23,960<br>Sorry, how does the sorting work does like, who does this sorting in? How does the map</p>
<p>657<br>01:06:23,960 –&gt; 01:06:28,759<br>reduce library does a little bit of sorting before it hens it off to the map reduce, to the</p>
<p>658<br>01:06:28,760 –&gt; 01:06:32,920<br>reduce function? So for example, the intermediate results might have like, you know, basically,</p>
<p>659<br>01:06:32,920 –&gt; 01:06:39,160<br>maybe all the intermediate results for a key, A, B, and C go to one worker. And you know, there,</p>
<p>660<br>01:06:40,200 –&gt; 01:06:47,480<br>there’s just a whole bunch of key value pairs like A1, you know, B1, you know, whatever,</p>
<p>661<br>01:06:48,120 –&gt; 01:06:54,760<br>A1 again, you know, C1, whatever. And basically what the map reduced library does,</p>
<p>662<br>01:06:54,760 –&gt; 01:06:59,080<br>it sorts it first by key. So it first all the A’s together, and then all the B’s together,</p>
<p>663<br>01:06:59,080 –&gt; 01:07:03,400<br>and then all the C’s together. And then basically concatenates all the values from</p>
<p>664<br>01:07:03,400 –&gt; 01:07:09,160<br>watching will keep and hens that off to the reduced function. Thank you.</p>
<p>665<br>01:07:17,880 –&gt; 01:07:23,560<br>Okay, so I want to talk a little bit about fault downloads now. And sort of go back to</p>
<p>666<br>01:07:24,840 –&gt; 01:07:26,760<br>the</p>
<p>667<br>01:07:32,840 –&gt; 01:07:39,160<br>Can I ask a question about the map reduced paper real quick? Yeah, fun. So is the larger idea that</p>
<p>668<br>01:07:39,800 –&gt; 01:07:46,840<br>a lot of functional programming could be reduced to the map reduced problem? Yes. Okay.</p>
<p>669<br>01:07:47,400 –&gt; 01:07:52,440<br>So the name hinted at that, right?</p>
<p>670<br>01:07:52,440 –&gt; 01:07:56,200<br>So basically there are two, you know, the notion of the map introduced function is something very</p>
<p>671<br>01:07:56,200 –&gt; 01:08:01,559<br>common in functional programming languages. And use widely functional programming languages,</p>
<p>672<br>01:08:01,559 –&gt; 01:08:06,440<br>where any sort of functional programming style. And so they basically, you know, that’s where the</p>
<p>673<br>01:08:06,440 –&gt; 01:08:14,200<br>inspiration came from. Okay. So actually, there’s a good segment to fault tolerance.</p>
<p>674<br>01:08:14,599 –&gt; 01:08:24,519<br>Because the idea is that, you know, if a worker fails, then the coordinators are in charge of</p>
<p>675<br>01:08:24,519 –&gt; 01:08:30,920<br>noticing that the worker fails and basically restarts that task. And so the coordinator</p>
<p>676<br>01:08:31,000 –&gt; 01:08:43,880<br>is a worker. Rearons map and reduce functions. Of course, the coordinator itself doesn’t</p>
<p>677<br>01:08:43,880 –&gt; 01:08:47,640<br>rerun them, but basically it coordinated the sites, you know, that particular map function</p>
<p>678<br>01:08:47,640 –&gt; 01:08:53,159<br>needs to be run again, because it appears to the coordinator that machine that it handed,</p>
<p>679<br>01:08:54,520 –&gt; 01:08:59,560<br>the task to actually is not responding. And so the typical thing is like, you know, if a machine</p>
<p>680<br>01:08:59,560 –&gt; 01:09:03,080<br>doesn’t respond to some certain amount of time, the coordinators are going to assume that machine</p>
<p>681<br>01:09:03,080 –&gt; 01:09:13,800<br>crashed. And so, and that, that means that when another worker becomes free and, you know,</p>
<p>682<br>01:09:13,800 –&gt; 01:09:18,920<br>is looking for a new, a new task, and it will hand out the same task that it actually handed out earlier,</p>
<p>683<br>01:09:18,920 –&gt; 01:09:25,800<br>and it ended out again. And so that’s sort of the basic plan for fault tolerance is that if the</p>
<p>684<br>01:09:25,800 –&gt; 01:09:31,960<br>coordinators are here about a particular work of reporting back that the other task is done,</p>
<p>685<br>01:09:31,960 –&gt; 01:09:36,680<br>it will rerun the task again. And so an instant question is, like, can a map function</p>
<p>686<br>01:09:37,640 –&gt; 01:09:42,440<br>get a map run twice? Even complete twice.</p>
<p>687<br>01:09:50,039 –&gt; 01:09:54,600<br>Is it possible in this framework that, you know, a particular map will run twice?</p>
<p>688<br>01:09:55,880 –&gt; 01:10:00,360<br>I guess it is because if the machine is down, you can’t really tell</p>
<p>689<br>01:10:01,480 –&gt; 01:10:10,360<br>at which point. So how many of the map tasks that it executed during the specific map</p>
<p>690<br>01:10:10,360 –&gt; 01:10:16,360<br>reducing instance were actually completed. So you would just have to rerun all of them, I guess.</p>
<p>691<br>01:10:17,400 –&gt; 01:10:21,640<br>Yeah, yeah, yeah. So, mostly we just think about this one task at a time. But,</p>
<p>692<br>01:10:22,119 –&gt; 01:10:26,920<br>so the machine, like, does one task, then goes back to the coordinator, asks for the next task,</p>
<p>693<br>01:10:26,920 –&gt; 01:10:31,960<br>and that might be another map test. And so when the coordinator doesn’t hear back,</p>
<p>694<br>01:10:31,960 –&gt; 01:10:36,680<br>it will say, like, okay, we’ll ask another worker to run that map test too. But it could be the case</p>
<p>695<br>01:10:36,680 –&gt; 01:10:41,960<br>that is at your point exactly out that the first worker, the first machine, didn’t actually crash.</p>
<p>696<br>01:10:42,680 –&gt; 01:10:46,520<br>It just happened to be a network petition, or like the word coordinators not able to communicate</p>
<p>697<br>01:10:46,520 –&gt; 01:10:51,079<br>with the machine, but it actually is just running happily and actually doing the map test.</p>
<p>698<br>01:10:51,079 –&gt; 01:10:56,279<br>And the producer, you know, and the intermediate set of results. So the same map function,</p>
<p>699<br>01:10:56,279 –&gt; 01:11:02,840<br>connect exactly your run twice. And so it’s actually one of the reasons that map producer</p>
<p>700<br>01:11:02,840 –&gt; 01:11:08,119<br>functional is because that’s okay if it’s a functional program, right? If you run the same</p>
<p>701<br>01:11:08,119 –&gt; 01:11:13,640<br>program on the same input, if you run a functional program on the same input, it will produce exactly</p>
<p>702<br>01:11:13,640 –&gt; 01:11:18,600<br>the same output. So it doesn’t really matter that it runs twice. You know, while in both cases,</p>
<p>703<br>01:11:19,160 –&gt; 01:11:25,160<br>produce the exact same output. And so these are where this functional aspect is actually really</p>
<p>704<br>01:11:25,160 –&gt; 01:11:28,600<br>important. It basically has to be functional or deterministic.</p>
<p>705<br>01:11:33,000 –&gt; 01:11:36,520<br>Because you know, every run of this map function must produce the same output because we’re</p>
<p>706<br>01:11:36,520 –&gt; 01:11:44,360<br>going to use one of them in the total computation. So similar, it can reduce function run twice.</p>
<p>707<br>01:11:48,600 –&gt; 01:12:05,400<br>Yeah, so we so. Yep, exactly for the same reason, right? I mean, if the machine runs</p>
<p>708<br>01:12:05,400 –&gt; 01:12:08,600<br>the reduced function, there’s no different than a map test. There’s really no from the</p>
<p>709<br>01:12:08,600 –&gt; 01:12:11,880<br>fault-pollens perspective. There’s no really big difference between a map test and a reduced</p>
<p>710<br>01:12:11,880 –&gt; 01:12:17,880<br>test. If you’re going to the machine running, the reduced test doesn’t report back, but happens</p>
<p>711<br>01:12:17,960 –&gt; 01:12:22,680<br>to also finish the job. Another machine might run be running exactly the same reduced function.</p>
<p>712<br>01:12:24,039 –&gt; 01:12:28,680<br>And they will produce output. Now, the only sort of interesting aspect in this is that,</p>
<p>713<br>01:12:28,680 –&gt; 01:12:32,680<br>you know, both reduced function will write, you know, to an intermediate, we will write the</p>
<p>714<br>01:12:32,680 –&gt; 01:12:38,520<br>final output file into GFS. And if you’re, you know, paid attention to it, you will notice that what</p>
<p>715<br>01:12:38,520 –&gt; 01:12:43,239<br>they do is actually they first produce the file in an intermediate file in the global file system,</p>
<p>716<br>01:12:43,239 –&gt; 01:12:54,439<br>and then do an atomic rename to name, move the file or rename the file into which actually final name.</p>
<p>717<br>01:12:56,039 –&gt; 01:12:59,479<br>And because it’s going to set up, make, you know, one of the two reduced functions will win,</p>
<p>718<br>01:13:00,199 –&gt; 01:13:03,880<br>but it doesn’t really matter which one wins because they’re going to produce exactly the same outcome</p>
<p>719<br>01:13:03,880 –&gt; 01:13:04,679<br>because they’re functional.</p>
<p>720<br>01:13:04,760 –&gt; 01:13:15,079<br>So just to double check, so if we have a machine that’s doing a map task, so a single machine can do</p>
<p>721<br>01:13:15,079 –&gt; 01:13:20,680<br>like multiple map tasks. So let’s say that it’s doing like 10 map tasks and it’s in the seventh</p>
<p>722<br>01:13:20,680 –&gt; 01:13:25,400<br>task. And then for some reason, it failed. And then the master knows that this machine failed.</p>
<p>723<br>01:13:25,400 –&gt; 01:13:31,159<br>So then the master will order for all of the seven map tasks that were completed to be re executed</p>
<p>724<br>01:13:32,119 –&gt; 01:13:35,000<br>distributively, maybe on different map machines.</p>
<p>725<br>01:13:36,359 –&gt; 01:13:41,399<br>Except, you know, that’s right. Although I think in general, it just goes one map at the time.</p>
<p>726<br>01:13:41,399 –&gt; 01:13:46,119<br>So basically one machine runs one map function or one reduced function, not multiple.</p>
<p>727<br>01:13:47,639 –&gt; 01:13:48,599<br>Okay, awesome. Thank you.</p>
<p>728<br>01:13:49,319 –&gt; 01:13:55,639<br>But after a worker’s done running the map task, does it immediately rate its files somewhere</p>
<p>729<br>01:13:55,639 –&gt; 01:14:01,079<br>that’s visible to other machines or does it just keep that file and its file system for the time being?</p>
<p>730<br>01:14:01,239 –&gt; 01:14:05,079<br>It keeps a map function always produced the results on the local disk.</p>
<p>731<br>01:14:05,559 –&gt; 01:14:07,559<br>So it sits in this local file system.</p>
<p>732<br>01:14:08,599 –&gt; 01:14:14,039<br>Right. So then even if you were doing map tasks one at a time, in the scenario where you did</p>
<p>733<br>01:14:14,039 –&gt; 01:14:18,439<br>multiple and then the machine crashed, you would lose the intermediate work, right?</p>
<p>734<br>01:14:18,439 –&gt; 01:14:23,239<br>No, it sits in the file system. So when the machine comes back up, you know, maybe the stuff is there.</p>
<p>735<br>01:14:24,199 –&gt; 01:14:28,119<br>Oh, I see. So the data is actually stored durability.</p>
<p>736<br>01:14:29,079 –&gt; 01:14:29,880<br>Oh, I see. Okay.</p>
<p>737<br>01:14:32,119 –&gt; 01:14:36,439<br>And the map or the reduced function directly talked to the map functions, the machines that</p>
<p>738<br>01:14:36,439 –&gt; 01:14:41,479<br>actually have intermediate results. Okay, so let me talk quickly about a couple of other failures.</p>
<p>739<br>01:14:47,479 –&gt; 01:14:50,119<br>And all the questions you’re asking are great questions, where in fact,</p>
<p>740<br>01:14:50,119 –&gt; 01:14:53,479<br>there’s all of us will show up when you’re actually implementing what map producer you’ll have to</p>
<p>741<br>01:14:53,479 –&gt; 01:14:58,839<br>decide exactly how you’re going to do things. So a couple other things. Can the coordinate a fail?</p>
<p>742<br>01:15:09,319 –&gt; 01:15:20,439<br>I don’t think so. That’s great. Like your cat. Excellent. Yeah. The coordinate can not fail.</p>
<p>743<br>01:15:20,679 –&gt; 01:15:23,879<br>So basically when the coordinate fails, the whole job has to be rerun.</p>
<p>744<br>01:15:25,239 –&gt; 01:15:29,559<br>You know, in this particular implementation, I have no plan for failures of the coordinator.</p>
<p>745<br>01:15:31,319 –&gt; 01:15:35,479<br>And that’s sort of making the follow coordinate in more fault tolerance is actually a little bit more tricky,</p>
<p>746<br>01:15:35,479 –&gt; 01:15:40,599<br>right? Because it actually has state that gets modified every time a map function completes or</p>
<p>747<br>01:15:40,599 –&gt; 01:15:45,319<br>reduced function completes. And so it actually turns out to be more complicated than so, basically,</p>
<p>748<br>01:15:45,319 –&gt; 01:15:51,159<br>in this particular library, the coordinator cannot fail. And we’ll see you later in some</p>
<p>749<br>01:15:51,159 –&gt; 01:15:54,840<br>steps of techniques that we can use to make the coordinate of fault tolerance if we wanted to,</p>
<p>750<br>01:15:54,840 –&gt; 01:16:00,119<br>but they decide not to do so. One reason they decide not to do so is because like a single machine,</p>
<p>751<br>01:16:01,079 –&gt; 01:16:05,399<br>they’re hoping basically that the single machine that just runs the coordinators unlikely to crash,</p>
<p>752<br>01:16:05,399 –&gt; 01:16:09,880<br>while it’s very likely that one of the thousands of machines that runs some map are going to crash.</p>
<p>753<br>01:16:10,840 –&gt; 01:16:14,119<br>Okay. How about slow workers?</p>
<p>754<br>01:16:21,079 –&gt; 01:16:25,480<br>So we have another type of failure. I’m going to discuss this issue of like where machine might be slow</p>
<p>755<br>01:16:25,480 –&gt; 01:16:29,720<br>because like some other computation is running on it, like GFS is also running on the same machine.</p>
<p>756<br>01:16:29,720 –&gt; 01:16:34,680<br>Maybe it actually is using a lot of the cycles or bandwidth, or maybe there are like problems with</p>
<p>757<br>01:16:34,680 –&gt; 01:16:38,199<br>the hardware itself. Is there anything special they do?</p>
<p>758<br>01:16:38,199 –&gt; 01:16:45,399<br>I think I have a call reading something about when the job is getting somewhat close to finishing,</p>
<p>759<br>01:16:45,399 –&gt; 01:16:51,639<br>the coordinator will assign the remaining tasks to additional machines, just in case there are</p>
<p>760<br>01:16:51,639 –&gt; 01:16:57,159<br>like machines that are lagging, and then they will take the results that finish first.</p>
<p>761<br>01:16:57,159 –&gt; 01:17:00,119<br>Yeah, exactly. So the slow workers are called the stragglers.</p>
<p>762<br>01:17:01,720 –&gt; 01:17:07,960<br>And what they do is they sort of do backup tasks. So for example, when they close to do</p>
<p>763<br>01:17:08,359 –&gt; 01:17:11,720<br>indeed, as you say, when we get into the computation, almost done to say like there’s a handful of</p>
<p>764<br>01:17:11,720 –&gt; 01:17:17,639<br>reduced task left or a handful of map task left, the coordinator actually just basically runs a</p>
<p>765<br>01:17:17,639 –&gt; 01:17:22,840<br>second instance, or maybe for instance of that task on a separate machine. And it’s totally okay,</p>
<p>766<br>01:17:22,840 –&gt; 01:17:27,800<br>that’s totally okay to do so, correct, because it’s functional. So it’s not no problem. We run the same</p>
<p>767<br>01:17:27,800 –&gt; 01:17:33,639<br>computation several times because it will reduce exactly the same output because it’s given the same input.</p>
<p>768<br>01:17:34,600 –&gt; 01:17:41,480<br>And the hope is that like one of these other guys will finish quickly. And so therefore then we</p>
<p>769<br>01:17:41,480 –&gt; 01:17:46,760<br>were not the performance not limited by the slow worker, but basically the fastest of the ones that</p>
<p>770<br>01:17:46,760 –&gt; 01:17:53,000<br>got replicated. And so this is like one of the issues where like, you know, basically this is a</p>
<p>771<br>01:17:53,000 –&gt; 01:18:00,119<br>common idea to deal with stragglers or to deal with tail latency is to try to busy replicate tasks</p>
<p>772<br>01:18:00,920 –&gt; 01:18:03,640<br>and go for the first that finishes.</p>
<p>773<br>01:18:09,399 –&gt; 01:18:14,439<br>Okay, I think this is time to wrap up. So I think you can go to other classes.</p>
<p>774<br>01:18:15,479 –&gt; 01:18:19,960<br>But these are sort of the major issues that show up in the map reduced library. And you know,</p>
<p>775<br>01:18:19,960 –&gt; 01:18:23,559<br>you will definitely be struggling mostly, you know, the hard part of actually implementing the</p>
<p>776<br>01:18:23,559 –&gt; 01:18:28,840<br>map reduced library is actually doing the fault hauls aspects. And but you should keep in mind,</p>
<p>777<br>01:18:28,840 –&gt; 01:18:33,000<br>as you’re doing that, all the programmers that are using your library or would use your library</p>
<p>778<br>01:18:33,000 –&gt; 01:18:38,440<br>don’t have to worry about all the distributedness that they would have that you have to deal with.</p>
<p>779<br>01:18:38,440 –&gt; 01:18:43,319<br>So you’re in the unfortunate situation. You’re not the target of the map reduced paper, you know,</p>
<p>780<br>01:18:43,319 –&gt; 01:18:48,600<br>making your life of grinding map reduced application DG. You’re in the, so the bad side of the</p>
<p>781<br>01:18:48,600 –&gt; 01:18:52,440<br>equation here, you actually have to deal with the distributedness and you know, become an expert.</p>
<p>782<br>01:18:53,399 –&gt; 01:18:59,960<br>Okay, I’m going to hang around for a little while so people want to go feel free to go. If you want</p>
<p>783<br>01:18:59,960 –&gt; 01:19:07,960<br>to ask a couple more questions, you know, feel free to do so. And I’ll see you first then.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>MIT6824 P1Lecture1 Introduction</div>
      <div>http://example.com/2025/10/25/MIT6824 P1Lecture1-Introduction/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月25日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/25/MIT6824%20P20Lecture19-Peer-to-Peeer-Bitcoin/" title="MIT6824 P20Lecture19 Peer To Peeer Bitcoin">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">MIT6824 P20Lecture19 Peer To Peeer Bitcoin</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/25/MIT6824%20P16Lecture15continued-OptimisticConcurrencyControlFaRMpt2/" title="MIT6824 P16Lecture15continued OptimisticConcurrencyControlFaRMpt2">
                        <span class="hidden-mobile">MIT6824 P16Lecture15continued OptimisticConcurrencyControlFaRMpt2</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
