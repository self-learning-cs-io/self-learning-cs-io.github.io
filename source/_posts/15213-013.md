---
title: 深入理解计算机系统 013-Thread-Level Parallelism
date: 2025-10-12 10:00:12
---

2025年10月02日 08:54
发言人   00:00
Well, hello, everyone. Interesting how far fewer seats are filled than at the beginning of the course. So of course, we're in the final stretch of this course. You're working on the last web and the material that we're covering, both this lecture and next lecture, and not on the exam. And you don't need them for your web. So at some level, you could just tune out and skip it all. And if your only purpose in taking this course is to pass it or to get some grade in it, and that's it, well, go ahead and tune out. 
大家好。有趣的是，座位比课程开始时少了多少。当然，我们正处于这门课程的最后阶段。你正在处理最后一个网站和我们要覆盖的材料，包括这堂课和下一堂课，而不是考试。你的网站不需要它们。因此，在某种程度上，你可以直接退出并跳过所有内容。如果你修这门课的唯一目的是通过它或在其中取得一些成绩，那就去吧，退出来。


发言人   00:38
But on the other hand, the material we're talking about is very relevant to where computers are today and where they're going in the future. And so if you think about the longer term and whatever your investment is in the computer industry and computer technology is, then I think you'll find these very worthwhile. But so think of this more as the stuff icing on the cake you've learned, the hard stuff you've done, the grinding part, and now you get to think beyond the sort of narrow confines of the coarse material and think bigger. But that's really the way you should be viewing this lecture. In the last lecture, which we'll be on Thursday. So today, what we're going to talk about is parallelism. And the issue is that that, wow. 
但另一方面，我们正在谈论的内容与计算机今天的状况以及未来的发展方向非常相关。因此，如果你从更长远的角度考虑，无论你在计算机行业和计算机技术方面的投资是什么，我认为你会发现这些投资非常值得。但是，请将其更多地看作是你所学到的锦上添花、你所做的困难的事情、研磨的部分，现在你可以超越粗糙材料的狭窄范围，思考更大的东西。但这确实是你应该看待这个讲座的方式。在上一次演讲中，我们将在星期四进行。所以今天，我们要讨论的是并行性。问题是，哇。


发言人   01:33
PowerPoint is a product made by a certain company in Seattle that's not always reliable. The issue is, as you know, nowadays when you buy a computer, you don't get just 1, a CPU on the processor chip. You have at least two on a typical laptop. Even my phone has 2 cores in it, as well as 4 graphic processing units and a typical the next generation of ipad will be a 6 core processor. These have become not just the sort of specialized domain of. High end machines, but actually there all the time. And actually, we'll talk some next time. 
PowerPoint是西雅图一家公司生产的产品，但并不总是可靠的。问题是，正如你所知，如今当你购买电脑时，你不会只得到一个处理器芯片上的CPU。你在一台典型的笔记本电脑上至少有两个。甚至我的手机也有2个内核，以及4个图形处理单元，典型的下一代ipad将是6核处理器。这些已经不仅仅是专门的领域。高端机器，但实际上一直都在那里。实际上，我们下次再谈。


发言人   02:21
Why is it that instead of having one fast computer, you get two medium size, medium performance processors on a chip or more? And that's actually a really interesting technology issue that I'll talk about next time, but it's the way it is. 
为什么不用一台快速的计算机，而是在一块芯片上得到两个中等大小、中等性能的处理器或更多？这实际上是一个非常有趣的技术问题，我下次会谈论，但就是这样。

发言人   02:37
So you can think of it when you write a program and it runs as a single thread, then you're basically not making use of the computing resources that you have available to you. So the natural thing is, well, could we make our programs run faster by doing multiple threads? So you've already learned, or you're in the process of applying multithreaded programming as a way to. 
因此，当你编写一个程序并且它作为单个线程运行时，你可以想到它，那么你基本上没有利用你现有的计算资源。所以自然的事情是，我们可以通过多线程来让我们的程序运行得更快吗？所以你已经学过了，或者你正在应用多线程编程作为一种方法。



发言人   03:10
Deal with concurrency of external events. There's multiple clients who want to make use of a server, and instead of serving one and then another and then another, if you can handle them all, it's sort of an external use of concurrency. But what we'll talk about today is more an internal use. 
处理外部事件的并发性。有多个客户端想要使用一个服务器，而不是服务一个，然后另一个，如果你能处理所有这些客户端，这是一种并发的外部使用。但今天我们将讨论的更多是内部使用。

发言人   03:29
Can I make use of multiple threads running on multiple cores to make a program run, a single program run faster? And the message behind that is yes. But and what I mean is it is truly possible. And people spend a lot of time making programs run faster by using multiple threads, but it's harder than you'd think it should be, and it's fraught with, as you've already experienced, programming bugs. But also, it's just really darn hard to get the kind of performance out of a multi-core processor that you would think it would be available. So we'll talk about some of that, and then we'll finish it up a little bit understanding of how how when you're writing concurrent programs, you want to think about the state of memory and how that's a challenge for multi-core processors, or in fact, any concurrent concurrent system. 
我可以利用在多个内核上运行的多个线程来使程序运行，单个程序运行得更快吗？这背后的信息是肯定的。但是，我的意思是，这确实是可能的。人们花了很多时间通过使用多个线程使程序运行得更快，但这比你想象的要困难，而且充满了编程错误，正如你已经经历过的那样。但是，要获得你认为可用的多核处理器的性能确实非常困难。所以我们将讨论其中的一些，然后我们将完成一点，了解当您编写并发程序时，您如何考虑内存状态，以及这对多核处理器或任何并发系统的挑战。




发言人   04:32
There's actually two sources of concurrency on a modern processor, multiple cores, which is you have actually multiple Cpu's on a single chip. But there's also something called hyperthreading, which is, in my experience, less useful. But let me go through this. So this is what a typical modern processor looks like. 
在现代处理器上实际上有两个并发源，多核，也就是在单个芯片上实际上有多个Cpu。但是还有一种叫做超线程的东西，根据我的经验，它不太有用。但让我经历一下。这就是典型的现代处理器的样子。

发言人   04:55
Processor chip is that there's actually on a single chip, there's multiple independent Cpu's and each of them has some part of the CA hierarchy, which is private to that particular core. And then there is another part of the CA hierarchy that's shared across cores. And then they all have a common interface to main memory. So if these cores are running and this is what happens a lot is they're running programs that are completely independent, have nothing to do with each other, then they more or less just exist and run, and they're happy as can be. They state are caching parts of their own state, and sometimes this cash will get polluted by the junk from other programs in terms of performance, but it won't matter with functionality. 
处理器芯片实际上是在一个芯片上，有多个独立的Cpu，每个Cpu都有一部分CA层次结构，这是特定核心的私有部分。然后是CA层次结构的另一个部分，它在核心之间共享。然后它们都有一个公共的主存储器接口。因此，如果这些核心正在运行，并且经常发生的情况是它们运行完全独立的程序，彼此之间没有任何关系，那么它们或多或少只是存在和运行，并且它们很快乐。他们的状态正在缓存自己的状态的一部分，有时这个现金会在性能方面被其他程序的垃圾污染，但这与功能无关。


发言人   05:52
The trick when you're trying to do multicore programming as a parallel computing thing, somehow getting all these cores working on. Different parts of a single problem in a way that makes it so that you get the performance out of it. They don't spend all their time basically arguing with each other about who has access to what. And also they're not stepping over each other and messing up each other's state. 
当你试图将多核编程作为并行计算时，以某种方式让所有这些核心工作时的技巧。一个问题的不同部分，以某种方式使其发挥出最佳性能。他们不会把所有的时间都花在基本上互相争论谁有权访问什么。而且他们不会踩到对方身上，也不会搞乱对方的状态。

发言人   06:22
So hyperthreading is a little bit more into the deep works of how a processor operates. You'll recall from the lecture on performance, or what's chapter 5 of the book, that a modern microprocessor looks absolutely nothing like the model that you get by looking at assembly code instructions. The model of assembly code is you execute one instruction, then you execute the next one, then you execute the next one. Modern processes don't do that at all. They haven't done it for well, they haven't done it that way for 30 years and since 1995. So since 20 years, they do it in a totally different way, which is sometimes referred to as out of order processing. 
因此，超线程技术更深入地研究了处理器的运行方式。你会从关于性能的讲座中回忆起来，或者说是书的第五章，现代的微处理器看起来绝对不像你通过查看汇编代码指令得到的模型。汇编代码的模型是你执行一条指令，然后执行下一个指令，然后执行下一个指令。现代工艺根本不这样做。他们做得不是很好，他们已经有30年没有这样做过了，从1995年开始。因此，20年来，他们以完全不同的方式进行处理，有时被称为 “订单处理”。


发言人   07:10
And so just real quickly, the basic idea is on the processor chip, there's multiple functional units that are capable of doing different types of operation. There's ones for integer arithmetic, there's ones for floating point arithmetic, and so forth. And then there's separate blocks interface to the memory, actually to the cache memories. And they're both loading, meaning reading from the memory and storing, writing out to memory. But these units sort of operate independently. And what happens is there's a block of logic, which is actually an enormous, huge block of logic in an x 86 processor that reads the instructions out of the instruction stream, rips them apart into little pieces, keeps track of data dependencies and control dependencies, and then schedules all the various operations in your program on these different functional units. 
因此，很快地，基本思想是在处理器芯片上，有多个功能单元能够执行不同类型的操作。有整数运算的，有浮点运算的，等等。然后有单独的块接口到内存，实际上是到缓存内存。它们都在加载，即从内存读取和存储，并向内存写入。但这些单位有点独立运作。发生的事情是有一个逻辑块，它实际上是x86处理器中的一个巨大的逻辑块，它从指令流中读取指令，将它们分解成小块，跟踪数据依赖关系和控制依赖关系。然后在这些不同的功能单元上安排程序中的所有各种操作。

发言人   08:08
So we talked some about that in the context of how can you write a program that will sort of maximize how much is going on down here by writing your code in particular ways? 
因此，我们在讨论如何编写一个程序的背景下，通过以特定的方式编写代码来最大化这里发生的事情？

发言人   08:23
So all this is an introduction to say this is how you have to understand what hyperthreading is so in a single execution mode, there's basically one instruction decoder, and it has its own set of state here, its own program counter, its own queue of operations that it's already decoded and haven't completed yet. It has its own set of registers. They're actually not registers like you'd expect, they're highly virtualized registers, but all this state is there to help to service the execution of one thread of execution. 
所以所有这些都是一个介绍，说明这是你必须理解超线程的方式，因此在单个执行模式中，基本上只有一个指令解码器，并且在这里有自己的状态集，自己的程序计数器，它自己的操作队列，它已经解码并且还没有完成。它有自己的寄存器组。它们实际上并不是您所期望的寄存器，它们是高度虚拟化的寄存器，但所有这些状态都是为了帮助服务一个执行线程的执行而存在的。

发言人   09:03
With hyperthreading. Basically what you do is the idea of it is to say 90% of all programs don't really make use of all these functional units, especially if you're blocking on a load because there's a miss in a cache, then all these arithmetic units are sitting there without any useful work to do, and so why don't we just double up or quadruple up or K times up the state associated with the decoding and control parts of the program so that you can have multiple threads running and sharing these functional units among each other they operating really independently? Their states are not intertwined, but they're sort of making more use of the available hardware for performing functions. And so that's called hyperthreading. That's an Intel term you also sometimes hear called SMT, simultaneous multithreading. And in my experience, and we'll see here the numbers, it doesn't really make that big a difference. It turns out to be, in the large picture of things, a relatively inexpensive feature for them to throw onto processors, and so they do it, and so nowadays, at least with an x 86 processor, usually have two way hyperthreading in them. 
超线程。基本上你所做的想法是说90% 的程序并没有真正利用所有这些功能单元，特别是如果你因为缓存中存在错过而阻塞负载，那么所有这些算术单元都坐在那里，没有任何有用的工作要做。那么，为什么我们不将与程序的解码和控制部件相关的状态增加一倍、四倍或K倍，以便您可以运行多个线程并在彼此之间共享这些功能单元，它们真正独立运行？它们的状态并不是交织在一起的，但它们在某种程度上更多地利用可用的硬件来执行功能。这被称为超线程。这是一个英特尔术语，你有时也会听到，叫做SMT，同时多线程。根据我的经验，我们将在这里看到数字，这并没有太大的区别。事实证明，从大局上看，这是一个相对廉价的功能，可以投入到处理器上，所以他们就这样做了，所以现在至少在x86处理器上，通常会有双向超线程。



发言人   10:34
So given that, if you look at our shark machines, which are a little bit old, they're sort of 2010 era machine, but they were high end machines in their day, and so they still actually are more powerful than what you'd buy as, say, a desktop and way more powerful than as a laptop that you'd get today. So they're actually pretty decent machines, actually. 
考虑到这一点，如果你看看我们的鲨鱼机器，它们有点老了，它们有点像2010时代的机器，但它们在当时是高端机器，所以它们实际上仍然比你买的更强大，比如，一款台式机，比你今天得到的笔记本电脑更强大。所以它们实际上是相当不错的机器。


发言人   10:56
We'll talk next time about why computers aren't a lot faster than they were five years ago. That's actually an interesting technology thing. They're server class machines, so they have multiple cores and they have eight of them, which is a lot. You can buy 10 core machines, x 86 machines on a single chip, but I don't think you can get more yet. So these were fairly advanced machine in their day, and they also have two way hyperthreading, so in theory. 
我们下次会讨论为什么计算机没有比五年前快很多。这实际上是一件有趣的技术事情。它们是服务器级机器，所以它们有多个核心，它们有八个核心，这是很多的。你可以在一个芯片上购买10个核心机器，x86机器，但我认为你还不能再买到更多了。所以这些在他们当时相当先进的机器，并且它们也有双向超线程，所以在理论上。



发言人   11:37
You should be able to get 16 independent threads running, sort of 16 way parallelism potentially out of a program. 
你应该能够运行16个独立的线程，潜在地从程序中并行16种方式。

发言人   11:45
If you can keep everything working and keep bad things from happening. So let's give a really trivial application that should be very simple to make, run in parallel. And that says, imagine we want to sum up the numbers between 0 and n -1, which is, by the way, a really stupid thing to do because there's a very simple closed form formula for it, which is good in the sense that will let us check our work. But it's a completely stupid application. But it just shows you this idea. And so what we're just going to do is block off if we have nway parallelism, we're just going to split our range of numbers, n wayss and just have a single threads, sum up one 10th of the numbers. And then they'll collectively sum together the results in some way or another. So this is about as easy a parallel program as you could imagine. 
如果你能保持一切正常，防止不好的事情发生。所以让我们给出一个非常简单的应用程序，应该非常容易制作，并行运行。也就是说，想象一下我们想把0和n -1之间的数字相加，顺便说一下，这是一件非常愚蠢的事情，因为它有一个非常简单的封闭形式公式，这在感知上是很好的，可以让我们检查我们的工作。但这是一个完全愚蠢的应用程序。但它只是向你展示了这个想法。所以我们要做的就是阻止，如果我们有nway并行，我们只是将我们的数字范围分开，n种方式，只有一个线程，将数字的十分之一相加。然后他们会以某种方式集体将结果汇总在一起。所以这是一个你能想象到的简单的并行程序。



发言人   12:48
So let's do 1 version, which is said, well, gee, I understand how to use threads pthreads, and I know about these things called semaphores or mutual exclusion. So what I'll do is just I'll have one place in memory where I'm collecting the sum over all n values. And for a thread to be able to add to that, it will lock it. It will get a mutual exclusive access to it, increment it, and then unlock it. And we'll just let all the threads go. He your skelter locking and unlocking this, these within a single thread. 
所以让我们做一个版本，也就是说，好吧，我明白如何使用threads，而且我知道这些叫做信号量或互斥的东西。所以我要做的就是在内存中有一个地方收集所有n个值的总和。对于一个线程来说，如果要添加它，它将会锁定它。它将获得对它的相互独占访问权限，增加它，然后解锁它。我们就让所有的线索都结束了。你的乱七八糟的锁定和解锁，这些都在一个线程内。




发言人   13:28
Things have to occur in the sequential order of that thread, but across threads, whether write A or write B occurs first is completely arbitrary, and whether writing of B occurs. These two actions are before is also arbitrary. So what it means is you can take two different threads and you can interweave their events in any way, but you should be able to pull out of that interweaving the sequential order of either of both of the two threads. So when you do that, you end up, you can enumerate an example like this, all the possibilities. You can say, well, look, it first is either going to be write A or write a, let's pick, write A, so now the next event will be either a read of B or write of B, and then if I do write a write read B, then I've completed this thread. And so now the only possibility is to write, to be, and read A and so forth. 
事情必须按照线程的顺序发生，但在线程之间，写A还是写B首先发生是完全任意的，写B是否发生是完全任意的。这两个行动之前也是任意的。所以这意味着你可以采用两个不同的线程，并且可以以任何方式交织它们的事件，但你应该能够从交织中拉出两个线程中任何一个的顺序。所以当你这样做时，你最终可以列举像这样的例子，所有的可能性。你可以说，看，首先要么写A，要么写a，让我们选择写A，所以现在下一个事件将是读B或写B，然后如果我写了读B，那么我已经完成了这个线程。因此，现在唯一的可能性是写作、成为和阅读等等。


发言人   14:43
You work out all the possible things. You get six different event orderings. And then what will be printed is, well, first of all, whether you print B before A will depend on the relative ordering of those two threads. So that's showing I'm showing the B value in blue and the value in red. Sorry, a value in red. And you'll get these different possibilities. 
你计算出所有可能的事情。你会得到六个不同的事件订单。然后要打印的是，首先，您是否在A之前打印B将取决于这两个线程的相对顺序。这表明我用蓝色显示B值，用红色显示值。抱歉，红色的值。你会得到这些不同的可能性。


发言人   15:11
These are all the six possible outputs of this program, but you'll see that there are two other outputs 1 could imagine that won't arise. One is to print 101, in other words, to have them both print the original values of these two variables. And that's impossible because I have to have done at least one right before I can reach either of these two print statements, right? So it's not possible for these to still be in their original values when I hit these print statements. And whichever order I hit these two. So those two are possible. So that's the idea of sequential consistency, that there's some very large number, but of possible outputs of a program. But in any case, they can't violate the ordering implied by the individual threads. 
这些是这个程序的所有六个可能的输出，但是你会看到还有另外两个不会出现的输出。一种是打印101，换句话说，让它们都打印这两个变量的原始值。这是不可能的，因为我必须至少做一个正确的事情才能得到这两个打印语句中的任何一个，对吗？所以当我点击这些打印语句时，这些语句不可能仍然处于原始值。无论我击中这两个顺序是什么。所以这两个是可能的。这就是顺序一致性的想法，即有一些非常大的数字，但是程序的可能输出。但在任何情况下，它们都不能违反单个线程所隐含的顺序。



发言人   16:18
So you'd say OK du, that seems like a pretty obvious thing. But actually, if you think from a hardware perspective, it's not that trivial to make that happen. So let me just. Show you a scenario of multi-core hardware that would violate sequential consistency. 
所以你会说 “OK du”，这似乎是一件很明显的事情。但实际上，如果你从硬件的角度考虑，实现这一点并不是那么简单。那就让我来吧。向您展示一个会违反顺序一致性的多核硬件场景。

发言人   16:39
Assume that each of our threads has its own private cache. And so if I execute this statement, what I'll do is I will grab a copy of A from the main memory and bring it into my cache. And I will assign this new value to it. And similarly, thread two will grab a copy of it of B and update that. And now if I do my 2 print statements, if thread 2 picks up the value from the memory, not knowing that thread one has a modified copy of that value, then it would naturally print 1. And similarly, if thread one picked up a copy of B from main memory, it would print 100. So we'd see exactly this unallowable execution. 
假设我们的每个线程都有自己的私有缓存。因此，如果我执行此语句，我将从主内存中获取一份副本，并将其带入我的缓存中。我将为它分配这个新值。同样，线程二将获取B的副本并更新它。现在，如果我执行2个打印语句，如果线程2从内存中获取值，而不知道线程1具有该值的修改副本，那么它将自然地打印1。类似地，如果线程1从主内存中获取B的副本，它将打印100。所以我们会确切地看到这种不允许的执行。


发言人   17:35
And the reason is because each of these threads have their own private copies of these shared variables, and they're not properly synchronized. But you could see in a hardware scenario, it would be easy to build this hardware and make that mistake. 
原因是因为每个线程都有这些共享变量的私有副本，并且它们没有正确同步。但是你可以看到在硬件场景中，构建这个硬件并犯那个错误是很容易的。

发言人   17:51
So how does it work in a multi-core processor? Well, they have a trick. They call it Snoopy caches. And it's a little bit like the reader's writers synchronization that you're working on for your proxies that you want to make it so that if everyone's just reading some shared value, they should be able to get copies into their own caches to optimize the performance of it. But if one of them wants to write to it, it needs to get an exclusive copy of it and lock out any other thread from accessing that either to read it or to write it from long enough to make the update. 
那么它在多核处理器中是如何工作的呢？他们有一个诡计。他们称之为史努比缓存。这有点像读者的写入器同步，你正在为代理服务器工作，你想让它变得这样，如果每个人都只是阅读一些共享价值，他们应该能够将副本存储到自己的缓存中，以优化其性能。但是如果其中一个想要写入它，它需要获取它的独占副本，并锁定任何其他线程访问它，以便读取或写入足够长的时间以进行更新。


发言人   18:39
They have a protocol where they tag, actually. And these tags are at the level of cache lines typically. So the tag cache line in main memory with its state and the typical state would be invalid. It's shared or it's exclusive. So shared means there can be copies of it, but they can only be read only copies, and meaning that it's exclusively available to a single thread. So this is built into, then, the hardware of a multi-core processor. 
他们有一个协议，实际上可以标记。这些标记通常处于缓存行级别。因此，主内存中的标记缓存行及其状态和典型状态将无效。它是共享的还是独家的。所以共享意味着可以有它的副本，但它们只能是只读副本，这意味着它仅可用于单个线程。因此，这被内置到多核处理器的硬件中。


发言人   19:19
So what will happen then is in order to do a write to a thread, one will acquire an exclusive copy of this element, and that actually tagging happens down here at the main memory and in the cache both. And similarly, if thread two wants A to write to be, it must get an exclusive copy of that. And then when the read occurs, what happens is actually this cache miss will send out a signal on a bus, a shared communication medium, saying, I want to read A, and instead of the main memory responding to it, actually, well, that result will be supplied by the other cache. And it will convert the state of this element to being a shared element. 
那么接下来会发生的是，为了对一个线程进行写入，一个人将获得这个元素的独占副本，并且实际上在主内存和缓存中都进行标记。类似地，如果线程二想要一个写出来，它必须获得一个独占副本。当读取发生时，实际上这个缓存未命中会在共享通信介质总线上发送一个信号，说，我想读取一个，而不是主存储器响应它，实际上，这个结果将由另一个缓存提供。它会将此元素的状态转换为共享元素。


发言人   20:18
Locally, but you'll see that the main memory element isn't updated yet. It goes through the whole write back protocol you've already seen. And sometimes it will update that those different implementations. 
本地，但您会看到主内存元素尚未更新。它经历了你已经看到的整个回写协议。有时它会更新那些不同的实现。


发言人   20:29
But this is why it's called a Snoopy cache is that basically Thread two is peeking into or getting access to information that's available in thread one's cache. And so now thread two will correctly get a copy of A that's in this shared state. And the same goes with B, it will snoop over and thread 2, 1 will get a readable copy. These are now all marked as shared state, and so if. Either of them wanted to write. They'd have to now basically get exclusive access to it. And that would have to then disable the copy. The other in the other locations. 
但这就是为什么它被称为史努比缓存的原因，基本上是线程二正在窥视或获取线程一缓存中可用的信息。现在，线程二将正确获取处于共享状态的的副本。对于B也是如此，它将窥探并线程2，1将获得一个可读的副本。这些现在都被标记为共享状态，所以如果。他们两个都想写作。他们现在基本上必须获得它的独家访问权。然后必须禁用副本。另一个在其他地方。

发言人   21:15
You can imagine this protocol being non-trivial, actually, to get right and to implement. 
你可以想象这个协议并非易事，实际上，要正确实施。

发言人   21:20
And it gets way more complicated than this with all the variations on it. But it's become the norm in multi-core hardware design. But it's actually part of the factor that limits the core count on a processor, because just the hardware involved in keeping the consistency across the caches is non-trivial. It has to work very fast. We're talking at the cash rate access speeds, so there's not a lot of time involved in there. So actually implementing this stuff, making it run, making it scale across, say, 8 cores, 10 cores, 16 cores is not a trivial thing. But that goes on in the background. And so for most systems nowadays, you can assume that there's some memory consistency model that you can program too, that's supported by the hardware of the system and this serial Liz ability that's referred to as sort of the easiest to understand, there's others that are a little bit more nuanced. 
而且由于它的各种变化，它会变得比这复杂得多。但它已经成为多核硬件设计的常态。但它实际上是限制处理器内核数的因素之一，因为仅保持缓存之间的一致性所涉及的硬件是不平凡的。它必须工作得非常快。我们正在以现金速率访问速度进行讨论，因此不会花费太多时间。因此，实际实施这些东西，使其运行，使其在例如8核、10核、16核之间扩展并不是一件小事。但这是在背景中发生的。因此，对于当今的大多数系统，您可以假设有一些内存一致性模型可以编程，这得到了系统硬件的支持，这种串行能力被称为最容易理解的，还有其他一些更加微妙的模型。


发言人   22:29
I guess that fell off the bottom here. That doesn't seem right. 
我想这是从底部掉下来的。这似乎不太对。

发言人   22:41
Nope, that's it. Just to wrap that up, then it gives you a flavor of. And you can see that getting programs to run fast through multithreading is not not easy. You often have to rewrite your application. You have to think about the algorithm. You have to worry about debugging it. As you've already discovered, both the shell lab and the proxy lab, the concurrency where you can't predict the order of events, makes it much more difficult to debug code. 
不，就是这样。只是为了包装它，然后它会给你一个味道。你可以看到，通过多线程让程序快速运行并不容易。你经常需要重写你的申请。你必须考虑算法。你必须担心调试它。正如您已经发现的那样，无论是shell实验室还是代理实验室，由于并发无法预测事件的顺序，因此调试代码变得更加困难。

发言人   23:16
So all these factors come in, and you have to have some understanding of the underlying mechanisms that are used and what their performance implications are. 
因此，所有这些因素都要考虑在内，你必须对所使用的基础机制以及它们对性能的影响有一些了解。

发言人   23:27
So in particular, let me just observe here that if I'm like doing synchronization across threads, like you saw that original one where they were fighting over this global variable p sum or whatever it was called, you can imagine the caches in this battle with each other to try and get exclusive access to this single memory value. Because each one is running as fast as it possibly can, but each one requires getting exclusive copy writing to it and releasing it. So the locking mechanism is flying back and forth between these caches. And it's really not very fast. That the kind of thing is, why? And also, as an application programmer, you're making calls. Semaphore call bounces you up into the OS kernel, which is a cost involved. So this thing has all the bad, all the things that make programs not run the way you really like them to. 
所以特别地，让我在这里观察一下，如果我喜欢跨线程进行同步，就像你看到的那样，他们在争夺这个全局变量p sum或任何它被称为的东西，你可以想象在这场战斗中，缓存彼此试图获得对这个单一内存价值的独占访问。因为每个都尽可能快地运行，但每个都需要获取独家副本并发布。因此锁定机制在这些缓存之间来回飞行。而且速度真的不是很快。这种事情是为什么呢？而且，作为一名应用程序员，你也在打电话。信号量调用将您反弹到操作系统内核中，这涉及到成本。所以这个东西有很多不好的东西，所有让程序不能按你真正喜欢的方式运行的东西。


发言人   24:43
So that's one of the challenges in parallel programming is how do you actually make use of the parallelism that's there without getting bogged down by the cost of the various mechanisms of control? So anyways, this is part of what you have to appreciate and understand as a programmer is how these things work at a level deep enough that you'll have some sense of what makes programs run faster or slower, where the mistakes go on. 
因此，并行编程面临的挑战之一是如何真正利用并行机制，而不被各种控制机制的成本所困扰？因此，无论如何，作为一个程序员，你必须欣赏和理解的一部分是这些事情如何在足够深入的层面上工作，你将对什么使程序运行得更快或更慢有一些感知，哪里出错了。



发言人   25:13
So that's just a little flavor of a much bigger topic. So that's it for today. 
所以这只是一个更大话题的一小部分。今天就到这里。

