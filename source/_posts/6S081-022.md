---
title: 操作系统工程 022-RCU
date: 2025-10-18 10:00:22
---

发言人   00:05
All right? The sort of underlying topic for today is really getting multicore, getting good multicore performance, getting good performance on multi-core hardware. And that's actually like a very interesting and deep fascinating topic with many, many different interesting aspects. Today, we're just going to bite off a fairly small piece, and that's how to get good performance for shared data in the kernel that's read much more often than it's written. And, you know, it turns out there's many kind of specific cases in which different ideas for getting good multicore performance are useful. What we're going to look at today is Linux is RCU, which has been very successful for sort of read heavy data, read heavy kernel data. 
一切都好吗？今天的基本主题是真正获得多核，获得良好的多核性能，在多核硬件上获得良好的性能。这实际上是一个非常有趣和深刻迷人的话题，有许多不同的有趣方面。今天，我们只是要咬咬一口相当小的部分，那就是如何在内核中获得良好的共享数据性能，这一数据的读取频率要比写入频率高得多。而且，你知道，事实证明，在许多具体情况下，获得良好的多核性能的不同想法是有用的。我们今天要看的是Linux是RCU，它在读取大量数据，读取大量内核数据方面非常成功。

发言人   01:11
The general sort, that background here is that if you have modern machines with four or 8 or 16 or 64 or however many cores running in parallel and sharing memory, the kernel is really a parallel process. It's a parallel program. And if you're going to get good performance, you need to make sure that the kernel can run a lot of its work as much as possible in parallel on different cores. In order to get that much more. You know, if you can run the kernel in parallel on 8 cores, all of them doing useful work, you can get eight times the performance. Then if the kernel could only run on a single core and at a high level, this should clearly be possible. 
一般的背景是，如果你有4个、8个、16个或64个内核并行运行并共享内存的现代机器，内核实际上是一个并行进程。这是一个并行程序。如果你想获得良好的性能，你需要确保内核可以在不同的内核上尽可能地并行运行大量的工作。为了得到更多。你知道，如果你能在8个内核上并行运行内核，所有这些内核都做有用的工作，你可以获得8倍的性能。那么如果内核只能在单核和较高的级别上运行，这显然是可能的。

发言人   02:01
If you have lots and lots of processes running on your computer. Well, first of all, the processes aren't running and executing in the kernel, then we have very little to worry about. They're likely to run in parallel without any kernel having to do anything if the process is if you have many applications running and they're all making system calls a lot of the time, different system calls made by different processes just seem like they ought to be independent and should be able to proceed. 
如果您的计算机上有很多进程正在运行。嗯，首先，进程没有在内核中运行和执行，那么我们就没有什么可担心的了。它们很可能并行运行，而无需任何内核执行任何操作，如果进程是如果您有许多应用程序正在运行，并且它们都在很多时间进行系统调用，则不同进程发出的不同系统调用似乎应该是独立的，并且应该能够继续进行。

发言人   02:29
In many cases, though certainly not all, but should be able to proceed completely without interference, like if 2 processes are forking or two processes that are reading different pipes or, you know, reading or writing different files, there's no obvious reason why they should interfere with each other, why they shouldn't be able to execute in parallel at n times the total throughput. The problem is the kernel has a lot of shared resources in order to you. For other good reasons. The kernel shares a lot of resources like memory and Cpu's and a disk cache and an Ino cache and all this other stuff that's actually under the hood shared between different processes. That means that even if two processes are doing system calls, two processes that have totally never heard of each other and aren't trying to interact make system calls. If those system files happen to allocate memory or use the disk cache or involve scheduling decisions, they may well end up both using data structures in the kernel, and therefore we need some story or how they're both supposed to use the same data without getting underfoot, without interfering with each other. 
在许多情况下，虽然肯定不是全部，但应该能够完全不受干扰地进行，例如，如果2个进程正在分叉或两个进程正在读取不同的管道，或者读取或写入不同的文件，则没有明显的理由说明它们应该相互干扰。为什么它们不应该能够以总吞吐量的n倍并行执行。问题是内核有很多共享资源为了你。有其他好的理由。内核共享许多资源，如内存、Cpu、磁盘缓存、Ino缓存以及所有其他在不同进程之间共享的底层资源。这意味着即使两个进程正在进行系统调用，两个完全从未听说过并且没有尝试交互的进程也会进行系统调用。如果这些系统文件碰巧分配内存或使用磁盘缓存或涉及调度决策，它们可能最终都使用内核中的数据结构，因此我们需要一些故事，或者它们应该如何使用相同的数据而不被踩在脚下，而不会相互干扰。

发言人   03:42
There's been enormous effort over the years in making kernels, in making all these cases and kernels run fast. We've seen one, of course, that's oriented towards correctness, namely spin locks, spinlocks are straightforward as such things go, and easy to reason about. But what a spinlock does is prevent execution. It prevents its job is to prevent parallelism in cases where there might be a problem between 2 processes. So spin locks are just directly a way to decrease performance. That's all they do. Of course, they make it easy to reason about correctness, but they absolutely prevent parallel execution. And that's not always that desirable. 
多年来，在制造内核方面付出了巨大的努力，使所有这些案例和内核快速运行。当然，我们已经看到了一种指向正确性的方法，即自旋锁，自旋锁就像这样简单明了，易于推理。但是自旋锁的作用是防止执行。它防止它的工作是在两个进程之间可能存在问题的情况下防止并行。所以旋转锁只是直接降低性能的一种方式。这就是他们所做的一切。当然，它们使得对正确性的推理变得容易，但它们绝对阻止了并行执行。这并不总是令人满意的。

发言人   04:29
Okay, so again, we're going to focus on read heavy on the case in which you have data that's mostly red and relatively rarely lit. And the main example I'm going to use is a linkless, a singly linked list. And so you can think of just the standard link, this diagram, some sort of maybe global variable. That's a pointer, this head pointer. Is it just a pointer? And there's a bunch of list elements, and each list element has some data in it. And we'll just say it's a string, like, you know, hello is the sort of data in this element, and each element also has a next pointer that points the next list element. And then finally, there's a pointer that points to 0 to mark the answer. 
好的，再次强调，我们将重点关注在数据大多是红色且相对很少亮起的情况下。我要使用的主要示例是无链接的单链表。因此，你可以想到标准链接，这个图，某种可能是全局变量。那是一个指针，这个头指针。这只是一个指针吗？还有一堆列表元素，每个列表元素都有一些数据。我们只会说它是一个字符串，就像你知道的，hello是这个元素中的数据类型，每个元素还有一个指向下一个列表元素的下一个指针。最后，有一个指向0的指针来标记答案。

发言人   05:26
Very straightforward. And again, we're going to assume that most users of this list that we're interested in are just reads, you know, the kernel thread or whatever it is that's using this list is just scanning the list, looking for something that's not trying to modify the list. And occasional writers, though, know if there were zero writers ever, we wouldn't need to have to worry about this at all because it be a completely static list never changes. We can read it freely, but we're going to imagine that every once in a while somebody comes along and wants to write the list, So that may mean that some others red wants to change the data stored in the list element, or maybe delete an element, or maybe insert a new element somewhere. So even though it's aiming at mostly reads, we do have to worry about writes. We need to make the reads safe in the face of writes. 
非常简单。而且，我们将假设我们感兴趣的这个列表的大多数用户都只是读取，你知道，使用这个列表的内核线程或任何东西只是扫描列表，寻找一些不是试图修改列表的东西。而偶尔的作家，虽然知道如果没有作家，我们根本不需要担心这个问题，因为它是一个完全静态的列表，永远不会改变。我们可以自由阅读它，但是我们会想象偶尔有人来写列表，这可能意味着其他一些红色想要更改存储在列表元素中的数据，或者可能删除一个元素。或者在某个地方插入一个新元素。因此，即使它的主要目标是阅读，我们也必须担心写作。面对写作，我们需要确保阅读的安全。


发言人   06:19
And of course, the next V 6, we just have a lock protecting this list. And a reader. You know, not only would writers and XV 6, not only would writers have to acquire the lock, but readers would have to acquire the lock too, because we've got to rule out the situation in which while we're reading, somebody's actually modifying the list, because that could cause sort of the reader to see a half updated value or follow an invalid pointer or something. So in XV 6, we have locks. But that has the defect that if the common cases there's no writers, it means that every time somebody comes along and reads in XV 6, they grab an exclusive like XV 6. 
当然，在下一个v6中，我们只有一个锁来保护这个列表。一个读者。你知道，不仅作家和XV 6，不仅作家必须获取锁，而且读者也必须获取锁，因为我们必须排除在我们阅读时，有人实际上正在修改列表的情况。因为这可能会导致读者看到一半更新的值，或者跟随无效的指针或其他东西。所以在XV 6中，我们有锁。但这有一个缺陷，如果普通情况下没有作者，这意味着每当有人进来阅读XV 6时，他们都会抓住一个排他性的东西，比如XV 6。

发言人   07:00
Spin locks are exclusive. Even if you have just two readers, only one of them can proceed at a time. So what we'd like or sort of one way to improve this situation would be to have a new kind of lock that allows multiple readers, but only one writer. So I can explore those next. 
自旋锁是独占的。即使你只有两个读者，一次也只能有一个读者可以继续阅读。因此，我们想要或改善这种情况的一种方法是拥有一种新的锁，允许多个读取器，但只能有一个写入器。所以我可以探索下一个。

发言人   07:22
Actually, both because they're interesting and because they help motivate the need for RCU, which we'll talk about in a little while. So there's this notion called read write locks. And the interface is a little more complicated than the spin locks we're used to. 
实际上，这既是因为它们很有趣，也是因为它们有助于激发对RCU的需求，我们稍后会讨论这个话题。所以有一个叫做读写锁的概念。而且界面比我们习惯的旋转锁稍微复杂一些。

发言人   07:42
We're going to imagine that there's one set of call that you call if you just want to read something. So we're going to imagine an R lock call. You pass a lock, and then also an R unlock call. And readers call these. And then there's a write lock call and a write unlock call. 
我们将想象有一组电话，如果你只是想阅读一些东西，你可以打电话。所以我们将想象一个R锁调用。您传递了一个锁，然后还传递了一个R解锁呼叫。读者称之为。然后有一个写入锁定调用和一个写入解锁调用。

发言人   08:04
So, and the semantics are that you can either have multiple readers acquire the lock for reading. So we do then would get parallelism, or you can have exactly one writer have acquired the lock, but you can never have a mix. You can never be in the locks out the read, read, write locks, rule out the possibility. Of somebody having lock the lock for writing and also reading at the same time, you either get one writer or lots of readers, but nothing else. 
因此，语义是您可以让多个读取器获取读取锁。那么我们就会获得并行性，或者你可以只有一个编写器获取了锁，但你永远不能混合使用。你永远不能处于读取、写入锁定的锁定中，排除这种可能性。如果有人锁了锁，同时写作和阅读，你要么得到一个作者，要么得到很多读者，但没有别的。


发言人   08:41
So that's the question. 
这就是问题。

发言人   08:43
Yes, this may be an implementation detail, but what kind of mechanisms does this locking scheme put in place to prevent someone writing while they hold a read lock? Nothing, nothing. It's just like XV 6 locks. We're talking about kernel code written by trusted, responsible developers. And so just like spinlocks and XV 6, if the code that's using locks incorrect, it's incorrect. There's no, okay. And this is the way, you know, typical kernels are bitten. You just have to assume that people developing the kernel are following their own rules, okay? 
是的，这可能是一个实现细节，但是这种锁定方案采用什么样的机制来防止有人在持有读锁的情况下写入？没什么，没什么。它就像XV 6锁一样。我们谈论的是由值得信赖、负责任的开发人员编写的内核代码。就像自旋锁和XV 6一样，如果使用锁的代码不正确，那就是不正确的。没有，好吧。这就是典型的玉米粒被咬的方式。你只需要假设开发内核的人遵循自己的规则，好吗？

发言人   09:26
Okay, and again, the reason why we care is that if we have a mostly a read, mostly data structure, we'd love to have multiple readers be able to use it at the same time to get genuine speed up from having multiple cores. All right, so if there were no problem here, this would just be the answer and we wouldn't have need to read today's paper. But it turns out that if you dig into the details of what actually happens when you use read, write locks, especially for data that's actually read a lot, there's some problems. And in order to see what's going on, we actually have to look at the implementation. 
好的，再次强调，我们关心的原因是，如果我们有一个主要是读取和数据结构的系统，我们希望多个读者能够同时使用它，从拥有多个内核中获得真正的速度提升。好的，所以如果这里没有问题，这就是答案，我们就不需要阅读今天的报纸了。但事实证明，如果您深入了解使用读写锁时实际发生的情况，尤其是对于实际读取大量数据，会出现一些问题。为了了解发生了什么，我们实际上必须看实现。

发言人   10:09
Linux indeed has read write lock implementation in it. And this is a kind of simplified version of the Linux code. 
Linux确实有读写锁实现。这是Linux代码的一种简化版本。


发言人   10:24
The idea is that we have a struct RW lock, which is like struct lock in XV 6, and it has a count in it. If the count is 0, that means that the lock is not held by anybody in any form. If the count is negative one, that means that a writer has it locked. And if the count is greater than 0, that means that n readers have it locked, And we need to keep track of them because we can only let a writer in if the number of readers descends to 0. 
这个想法是我们有一个结构RW锁，就像XV 6中的结构锁一样，它有一个计数。如果计数为0，则意味着锁不被任何人以任何形式持有。如果计数为负一，则意味着作者已将其锁定。如果计数大于0，则意味着n个读者已将其锁定，我们需要跟踪它们，因为只有在读者数量降至0时，我们才能让写作者进入。

发言人   10:58
Okay, so somebody asked about adding so. No I'm not sure if there's a question in the chat. Interrupt me through this. 
好的，有人问过添加so的问题。不，我不确定聊天中是否有问题。通过这个打断我。

发言人   11:12
The reed lock function. It has to sit in a loop because if there's a writer, we have to wait for the writer. It looks. It grabs a copy of the current n value. If it's less than 0, that means there's a writer. And we just need to continue our loop. And we're going to spin, waiting for the writer to go away. 
芦苇锁功能。它必须坐在一个循环中，因为如果有一个作家，我们必须等待作家。看起来很不错。它获取当前n值的副本。如果它小于0，那意味着有一个作家。我们只需要继续我们的循环。我们要旋转，等待作者离开。

发言人   11:41
Otherwise, we want to increment that value, but we only want to increment it if it's still greater than or equal to 0. So we can't, There's many things we can't do. We can't, for example, just add one with standard n equals n plus 1, because if a writer sneaks in between, when we check the value of n and when we actually try to increment it, then we may actually go ahead and increment it at the same time that some writer is setting it to -1, which is wrong. So we need to increment it only if it hasn't changed value since we checked it and verify that it's greater than or equal to 0. 
否则，我们想增加该值，但只有当它仍然大于或等于0时，我们才想增加它。所以我们做不到，有很多事情我们做不到。例如，我们不能只用标准的n等于n加1，因为如果一个作者偷偷溜进来，当我们检查n的值和实际尝试增加它时，那么我们实际上可以在某个写入器将其设置为-1的同时进行递增，这是错误的。因此，仅当我们检查它并验证它大于或等于0后，它才没有改变值时，我们才需要递增它。

发言人   12:18
And the way people do that is they take advantage of special atomic or interlocked instructions which you saw before for the for implementation of spinlocks in XV 6 and the interlocked instruction that or one that's particularly convenient to use is something called compare and swap. The idea is that comparison and swap takes three arguments, the address of some location of memory that we want to act on, the value that we think it holds, and the value that we'd like it to hold, and the semantics of compare and swap are that the hardware checks the hardware first, sort of basically sets an internal lock that makes only one compare and swap execute at a time on a given memory location. Then the hardware checks that the current value of that location is indeed still x, and if it's still X sets it to this third argument, which is going to be x plus one. And then the instruction yields 1 it its value. If compare and swap observes that the current value isn't x, then it doesn't change the value, the memory location, and it returns zero. So this is basically an atomic. If the location is x, set it to x plus one, and it has to be atomic because there's really two things going on. The hardware is checking the current value and setting it to a new value. 
人们这样做的方式是利用您之前在XV 6中看到的特殊原子指令或互锁指令来实现自旋锁，而互锁指令或特别方便使用的指令称为比较和交换。这个想法是比较和交换需要三个参数，我们想要操作的某个内存位置的地址，我们认为它所持有的值，以及我们希望它所持有的值，而比较和交换的语义是硬件首先检查硬件。基本上设置了一个内部锁定，使得在给定的内存位置上一次只能执行一次比较和交换。然后硬件检查该位置的当前值确实仍然是x，如果仍然是X，则将其设置为第三个参数，这将是x加一。然后该指令产生1它的值。如果比较和交换观察到当前值不是x，则它不会更改值，即内存位置，并返回零。所以这基本上是一个原子。如果位置是x，则将其设置为x加一，并且它必须是原子的，因为实际上有两件事情发生。硬件正在检查当前值并将其设置为新值。



发言人   13:45
Any questions about compare and swap? I have a question if there would be a reader and our log needs to continue, would W unlock reset the value back to x or am well w unlock? If there's a writer w unlock, which I'm afraid I didn't show sets n to 0 because there can only be one writer if there's what read unlock does is use another compare and swap to decrement in, okay? 
有任何关于比较和交换的问题吗？我有一个问题，是否会有一个阅读器，我们的日志需要继续，W解锁会将值重置为x还是会解锁？如果有一个作家w解锁，恐怕我没有显示设置n到0，因为如果读取解锁所做的是使用另一个比较和交换来递减，那么只能有一个作家，好吗？

发言人   14:28
Because what happens if? A rider locks, the lock between when x is being computed and see it here? No, between the if and X somehow. Okay? I'm not sure I understand exactly what time you're asking, but it's absolutely good question. What happens if W lock is called somewhere during this sequence? And for me, the most dangerous time for W lock to be called is after this check, but before the compare and swap. So let's imagine that Reed lock has gotten as far as seeing that x, that x, or l.l.n.p.s.. 
因为如果发生了什么？一个骑手锁，当x被计算和在这里看到它之间的锁？不，不知何故在if和X之间。好吗？我不确定我确切理解你问的时间，但这是一个非常好的问题。如果在这个序列的某个地方调用W lock会发生什么？对我来说，调用W锁最危险的时间是在此检查之后，但在比较和交换之前。所以让我们想象一下，芦苇锁已经看到那个x，那个x，或者l.n.p.s ..

发言人   15:21
Okay, so maybe we're right here, and x is equal to 0. 
好的，也许我们就在这里，x等于0。

发言人   15:31
We've already checked. So the check is finished. And then right at this time, on our another core, some other thread calls W lock, and it actually gets its compare and swap in first. So on the other core that's trying to grab the right lock, compare and swap is going to see if LRO n is 0. And let's assume that n is 0, that this test is true, and the comparing swap is on that other core is going to set n to negative one. So now the locks locked, but we still think that n is 0 in this code, even though lock's locked. 
我们已经检查过了。所以检查结束了。然后就在这个时候，在我们的另一个核心上，一些其他线程调用W锁，它实际上首先得到它的比较和交换。因此，在试图获取正确锁的另一个核心上，比较和交换将查看LRO n是否为0。让我们假设n为0，这个测试是正确的，比较交换在另一个核心上，将n设置为负1。所以现在锁被锁定了，但我们仍然认为这段代码中的n是0，即使锁被锁定了。

发言人   16:06
And now, now back on the reading core, we're going to execute compare and swap, but we're going to pass 0 here, right? This is the value we we're going to pass in the value we actually looked at, not the current value of n, and when we looked at it, it was 0. So we're going to pass 0 here, and we're telling compare and swap look only add one to only set it to one if its current value is 0, but it's not zero at this point, it's -1. And so this comparison and swap fails, does not modify n return 0. And so that means we'll go back to the top of this loop and try again, of course, now, and is -1. 
现在，回到读取核心，我们将执行比较和交换，但我们将在这里传递0，对吗？这是我们要传递的值，我们实际查看的值，而不是当前的n值，当我们查看它时，它是0。所以我们将在这里传递0，并且我们将告诉比较和交换仅在当前值为0时将其设置为1，但此时它不是零，它是-1。因此，此比较和交换失败，不会修改n返回0。这意味着我们将回到这个循环的顶部并再试一次，当然，现在是-1。


发言人   16:45
This might be related to the previous question a bit, but is it possible for an interrupt to occur when that x plus one is being computed in the Cas parameter or Cas parameter actually execute Cas? But while we're computing its arguments, right? So like you compute or you pass in the x argument, and that's okay. But you, before you compute the x plus one, or while you're competing x plus one and interrupt occurs. And so x plus one is wrong. If an interrupter occurs while we're computing x plus one, that means we haven't. Cas is actually an instruction, it's a single machine instruction. So if we're computing s plus one, that means we haven't called Cas yet if the interrupt happens and all kinds of things may happen, we're going to get the same. 
这可能与前面的问题有点相关，但是当在Cas参数中计算x加一或Cas参数实际执行Cas时，是否有可能发生中断？但是当我们计算它的参数时，对吧？就像你计算或者传入x参数一样，没关系。但是，在你计算x加一之前，或者当你在竞争x加一时，会发生中断。所以x加1是错误的。如果在我们计算x加1的时候出现了中断，那就意味着我们还没有。Cas实际上是一条指令，它是一条单独的机器指令。因此，如果我们在计算s加一，这意味着我们还没有调用Cas，如果发生中断并且可能发生各种事情，我们将得到相同的结果。

发言人   17:34
If we originally read 0 here, then interrupt or no interrupt, we're going to pass one as this third argument because the interrupts not going to reach out and change. This is a local, this x is a local variable for this code, so interrupt or context, which or anything ISS not going to change x? So that means we're going to pass 0 and 1 here. 
如果我们最初在这里读0，然后是中断或没有中断，我们将传递一个作为第三个参数，因为中断不会到达并改变。这是局部的，x是这段代码的局部变量，所以中断或上下文，哪个或任何东西不会改变x？这意味着我们将在这里传递0和1。

发言人   17:57
You know, if n is still 0, then we'll set it to one, and that's what we want. If it's not still 0, then compare and or won't change it, right? I guess you would have problems if you didn't set that local variable. Then if you used LRO n here and LRO n plus one, you would almost certainly be in big trouble because then and could change underfoot at any time. That's why we actually grab a copy and grab a copy here in order to sort of fix a specific value, yes. 
你知道，如果n仍然是0，那么我们会将其设置为1，这就是我们想要的。如果它不是仍然为0，那么进行比较或者不会更改它，对吗？我猜如果你没有设置那个局部变量，你会遇到问题。那么如果你在这里使用LRO n和LRO n + 1，你几乎肯定会遇到大麻烦，因为那时可能随时变化。这就是为什么我们实际上会抓取一个副本并在这里抓取一个副本以修复特定的值，是的。

发言人   18:32
Okay? If two readers, okay, so I cover the case of whatever writer calls the same W lock is called the same time as R lock. It's also interesting to wonder, what if our lock is called at the same time? So supposing n starts out as zero, we know of two R locks are called at the same time. What we want is for n to end up with value 2 and for both R locks to return. That's what we want because we want two readers to be able to execute in parallel, to use the data in parallel. 
好吗？如果有两个读者，好的，所以我涵盖了任何作者调用相同的W锁和R锁同时调用的情况。也很有趣的是，如果我们的锁同时被调用呢？因此，假设n开始为零，我们知道同时调用两个R个锁。我们想要的是让n的值为2，并且两个R锁都返回。这就是我们想要的，因为我们希望两个读取器能够并行执行，并行使用数据。


发言人   19:09
Okay, so they're both going to see 0 at this point. So at this point, both of them are going to have x equal to 0. They're both going to compare and swap with 0 and 1. Only one of those two compare and swaps, hopefully exactly one of those two compare and swaps will succeed. 
好的，那么他们现在都会看到0。所以在这一点上，他们两个都将有x等于0。它们都会比较并交换0和1。这两个比较和交换中只有一个会成功，希望这两个比较和交换中只有一个会成功。


发言人   19:31
Whichever one parent swap is an atomic instruction, only one of them happens at a time on a given memory location. So whichever compare hop is first, we'll see that at n is equal to 0, and we'll set it to one the other cores simultaneous call to R, lock it, comparison and swap will then execute and it'll still pass 0 and 1 here, but n will now be equal to one. And so the comps off will fail for the second core and return zero the second ch. We'll go back to the top of this loop. At this point, it'll read 1, that's not less than zero, we'll go on to the compare and swap, and now it'll pass one and 2, and now the second read lock will succeed. Both of them will have to lock. So the first one succeeded in the first try, the second one actually go back to the loop and try again. 
无论哪个父交换是原子指令，在给定的内存位置上一次只发生其中一个。所以无论比较跃点是什么，我们会看到在n等于0，并且我们将其设置为另一个内核同时调用R，锁定它，然后执行比较和交换，它仍然会在这里通过0和1，但是现在n将等于1。因此，第二个核心的补偿将失败，并将第二个核心的补偿返回零。我们将回到这个循环的顶部。此时，它将读取为1，不小于零，我们将继续进行比较和交换，现在它将传递1和2，现在第二个读取锁定将成功。他们两个都必须锁上。所以第一个尝试成功了，第二个实际上回到了循环并再次尝试。

发言人   20:31
Any questions? Oh sorry, so it is somehow possible that so a bunch of reads come and they, they're eating their stuff and then a right also comes and it also wants to write, but then some mother reads also come after the right, but then somehow they read like Iru, the right and the right still has to wait somehow. So so with the sequences that a reader managed to acquire the lock one or more readers have the lock so now and you know, each of them is this called a compare and swap, you know, adds one to n for each reader, so now n is greater than 0 because there's all readers if a writer tries to acquire the lock at this point, the writers compare and swap the compare value is 0. So compare and swap will only change n to -1 if its current value is 0, but we know the current because there's multiple meters, the current value of n is not 0, and so the comparisons off will fail and return 0 in the writer will sit here in this loop, basically waiting until n is equal to 0 before its comp and swap will succeed and return, give the lock to the writer. So this certainly means the rate can be starved if it's a lot of readers and may never be 0, and so the rate may never succeed, so that's a defect in this locking scheme. 
有问题吗？哦，对不起，所以在某种程度上可能会有一大堆阅读，他们在吃他们的东西，然后一个权利也来了，它也想要写作，但是有些母亲也读到了权利，但不知何故，他们读起来像Iru一样。右翼和右翼仍然需要以某种方式等待。因此，对于读取器设法获取锁的序列，一个或多个读取器有锁，所以现在你知道，每个读取器都被称为比较和交换，你知道，为每个读取器添加一到n，所以现在n大于0，因为如果此时一个写入器试图获取锁，那么所有的读取器都存在，写入器比较并交换比较值为0。所以比较和交换只有在当前值为0时才会将n更改为-1，但我们知道当前值，因为有多个仪表，n的当前值不是0，因此比较关闭将失败并返回0，写入程序将在此循环中，基本上一直等到n等于0，然后它的交换和交换将成功并返回，将锁交给写入器。因此，这当然意味着如果读者很多，速率可能会不足，并且可能永远不会为0，因此速率可能永远不会成功，因此这是此锁定方案的缺陷。



发言人   22:06
Thank you I also have a question about the two readers scenario that I just mentioned. It appears that in the worst case, the reader that arrived second has to go through another iteration of the loop. It sounds somewhat wasteful and I wonder if this generalizes to other writers. They all have to get lost and start again. You put your finger on why people don't like this scheme. If there's a lot of simultaneous reader. And so for the reason you just mentioned, our lock, even if there's no writers at all, if there's lots of readers or there's readers on many chords, our lock can be very, very expensive. 
谢谢，我也有一个关于我刚刚提到的两个读者的情况的问题。似乎在最坏的情况下，第二个到达的读者必须经历另一个循环迭代。这听起来有点浪费，我想知道这是否也适用于其他作家。他们都必须迷路并重新开始。你指出为什么人们不喜欢这个方案。如果有很多同步读者。因此，由于您刚才提到的原因，即使根本没有作者，如果有很多读者或许多和弦上有读者，我们的锁也可能非常昂贵。

发言人   22:58
And one thing you need to know about the rlo scheme, which I think we've already mentioned in class, is that. On a multicore system, every core has an associated cache. We will say it's the L 1 cache. So each core has a bit of cache memory and whenever it reads or writes something. It sits in the cache. And so there may be lots and lots of core. And there's some kind of interconnect network that allows the course to talk to each other. 
关于rlo计划，你需要知道的一件事是，我想我们已经在课堂上提到过了。在多核系统上，每个核心都有一个关联的缓存。我们会说它是L 1缓存。所以每个核心都有一个缓存，每当它读取或写入某些内容时。它坐落在缓存中。所以可能有很多很多的核心。并且有某种互连网络，允许课程相互通信。

发言人   23:31
Because, of course, if lots, of course, have some data cached, and one of the cores writes that data, the writing core has to tell the other cores that they're not allowed to cache the data anymore is called invalidation. So what actually happens if you have n readers and people calling R lock at about the same time on n cores? 
因为当然，如果很多内核都缓存了一些数据，而其中一个内核写入了该数据，则写入内核必须告诉其他内核不再允许缓存该数据，这称为失效。那么，如果您在n个内核上有n个读取器和大约同时调用R锁的人，实际上会发生什么？

发言人   23:58
They're all going to read n, sorry, this LRO n value, and load this memory location into their caches. They're all going to call compare and swap. The first one actually call compare and swap is going to modify the data. But in order for it to modify the data, it has to invalidate all these other copies. And so the compare swap instruction that one has to send out an invalidate message over this little network, each of the other n cores, right? And then it returns all the other cores, the n -1 cores, they have their comp swaps now actually have to reread again, requiring traffic over the network. 
他们都将读取n，抱歉，这个LRO n值，并将这个内存位置加载到他们的缓存中。他们都会调用比较和交换。第一个实际调用比较和交换的操作将修改数据。但是为了让它修改数据，它必须使所有这些其他副本无效。因此，比较交换指令必须在这个小网络上发送无效消息，每个其他n个核心，对吧？然后它返回所有其他核心，即n -1核心，它们的compswaps现在实际上必须重新读取，需要通过网络进行流量。

发言人   24:41
Reread this data, this memory location, compare it with x, and they'll have fail because they all call x with 0. Then the remaining n -1 readers go back to the top of the loop, and all n -1 of them again read the data, and again, one of them writes it, right? So on each. So there's going to be n times through the loop, once for each core, trying to acquire the lock. 
重新读取此数据，此内存位置，将其与x进行比较，它们将失败，因为它们都以0调用x。然后剩下的n-1个读者回到循环的顶部，所有的n-1个读者再次读取数据，再次，其中一个写出来，对吗？每个都如此。因此，将有n次循环，每个核心一次，试图获取锁。

发言人   25:07
Each of those trips through the loop involves order n messages on the network, because at least every copy of the cached LRN has to be invalidated. And that means that the total cost for n cores to acquire a particular lock, even for reading, is order n, and that means as you increase the number of cores for a popular piece of data, the cost for everybody to lock it just once goes up. 
每个通过循环的行程都涉及网络上的顺序n条消息，因为至少缓存的LRN的每个副本都必须失效。这意味着n个内核获取一个特定锁的总成本，甚至是读取的总成本，都是n的顺序，这意味着随着你增加一个流行数据的内核数量，每个人锁定一次的成本就会增加。


发言人   25:43
Sorry, it's order n squared, the total cost and time or messages sent over this interconnect is n squared. And this is a very bad deal, right? 
抱歉，顺序是n平方，通过此互连发送的总成本和时间或消息是n平方。这是一个非常糟糕的交易，对吧？

发言人   25:56
You would hope that if you needed to do something 10 times, 10 different chords needed to do something, especially, you know, given that they're just reading the lists, they're not modifying it, you'd hope that they could really run in parallel. That is the total wall clock time for 16 cores to read something should be the same as the total wall clock time for one quarter read something. Because that's what getting parallelism means is that you can do things at the same time. But here, the more chords that try to read this, the more expensive the lock acquisition is. 
你会希望，如果你需要做某件事10次，需要10个不同的和弦来做某件事，特别是，你知道，考虑到他们只是在阅读列表，他们不是在修改它，你希望他们真的可以并行运行。也就是说，16个核心读取某些内容的总挂钟时间应该与四分之一读取某些内容的总挂钟时间相同。因为这就是获得并行意味着你可以同时做事情。但是在这里，尝试阅读的和弦越多，锁的获取就越昂贵。


发言人   26:28
And so what's going on is that this style of locks has converted read only access to data. You know, the list is probably sitting in the cache already because nobody's modifying the list, right? So the actual access to the list might only take a few dozen cycles. But if the data is popular, getting the lock can take hundreds or thousands of cycles because of this n squared effect. And the fact that instead of it being cached accesses, it's these accesses that have to go over the bus, this interconnect, in order to invalidate and do these CA coherence operations. So these locks have turned very cheap, read only access to data into an extremely expensive read, write access to this data. And we probably completely destroy any possible parallel performance. 
所以发生的情况是，这种类型的锁已经将只读访问转换为数据。你知道，这个列表可能已经被缓存了，因为没有人在修改这个列表，对吧？因此，对列表的实际访问可能只需要几十个周期。但如果数据很流行，由于这种n平方效应，获取锁可能需要数百或数千个周期。事实上，不是被缓存的访问，而是这些访问必须经过总线，这个互连，以便使这些CA一致性操作无效和进行。所以这些锁已经变得非常便宜，对数据的只读访问变成了对这些数据的极其昂贵的读写访问。我们可能会完全破坏任何可能的并行性能。


发言人   27:23
If what you were doing, you know, the actual data was fairly simple to read, the lock will dominate and destroy parallel performance. So any questions about this performance story? 
如果你正在做的事情，你知道，实际数据非常容易读取，锁将主导并破坏并行性能。对于这个表现故事，有什么问题吗？

发言人   27:43
In a sense, you know, the bad performance of read write locks is the reason for the existence of RCU. Because if this was efficient, then there'll be no need to do better than that, right? But it's terribly inefficient. 
在感知中，读写锁的性能不佳是导致RCU存在的原因。因为如果这是有效的，那么就没有必要做得比这更好，对吧？但效率非常低。

发言人   28:02
And there's two things going on. One is the details of this. Oh, there needs to be a total of n squared trips through this loop. If we have n cores is sort of one way of looking at it. The other way of looking at it is that we're writing, you know, regardless of the details of what's going on here, these locks have turned a read-only access, which could be cache and extremely fast into an access that one way or another involves a right one or more rights. And rights are just much more expensive than reads if we're writing data that might be shared with other cores because a read for data that's not modified can be satisfied in a couple cycles out of your own cache a rate any rate to data that may be cached by other cores has to involve communication between cores to invalidate other copies. So no matter how you slice it, anything that involves a right to share data is a disaster for performance if you otherwise could have been read only. So the details of this loop are sort of less important than the fact that it did write to share data. 
有两件事情正在发生。一个是这个的细节。哦，这个循环总共需要n个平方行程。如果我们有n个核心，这是一种看待它的方式。另一种看待它的方式是，我们在写，你知道，不管这里发生了什么细节，这些锁已经变成了只读访问，这可能是缓存并且非常快地变成了一种访问方式，这种方式涉及正确的一个或多个权限。如果我们要写入可能与其他核心共享的数据，那么权限比读取昂贵得多，因为对未修改的数据的读取可以得到满足在您自己缓存的几个周期内，任何可能被其他核心缓存的数据速率都必须涉及核心之间的通信以使其他副本。因此，无论你如何切片，任何涉及共享数据权利的事情都是性能的灾难，如果你本来可以被只读。因此，此循环的细节不如它确实写入以共享数据这一事实更重要。

发言人   29:10
So what we're looking for is a way to be able to read data without, right? We want to be able to scan that list without doing any rights whatsoever, including any rights that might be required to do some kind of locking thing. They were looking for really, really read only access the data. Okay? So one possibility, it's not a possibility, but it's sort of a thought experiment. 
所以我们正在寻找的是一种无需读取数据的方式，对吧？我们希望能够在不执行任何权利的情况下扫描该列表，包括执行某种锁定操作可能需要的任何权利。他们正在寻找真正只读访问的数据。好吗？所以有一种可能性，它不是一种可能性，但它有点像一个思想实验。


发言人   29:42
We could just tell the readers not bother locking, right? You know, occasionally you get lucky and it turns out that readers can read stuff and that only writers need to lock. So we'll just do a quick experiment to see whether we could have lock, just have readers just read the list without locking it. 
我们可以告诉读者不要费心锁定，对吗？你知道的，偶尔你会幸运，结果发现读者可以阅读东西，只有作家需要锁定。所以我们只是做一个快速的实验，看看我们是否可以有锁，只是让读者读取列表而不锁定它。

发言人   30:02
So suppose we have this list and it has some, you know, strings, And. We're going read it, okay? So nothing goes wrong if there's no writer, right? Just read the list, it's not a problem. So we got to imagine there's a writer, and there's probably three cases if you read a list while some other courses modifying it. So one case is that the writer is just changing the content, that is not adding or deleting elements necessarily. 
所以假设我们有这个列表，它有一些字符串。我们去读它，好吗？所以如果没有作家，就不会有什么问题，对吧？只需阅读列表，这不是问题。所以我们必须想象有一个作家，如果你读了一个清单，而其他一些课程在修改它，可能有三种情况。因此，一种情况是作者只是更改内容，而不一定是添加或删除元素。

发言人   30:41
A writer is just changing the string to be some other string. Writers changing the content two is the writer is inserting a new list element. And the third case is if the writer is deleting a list element. 
编写器只是将字符串更改为其他字符串。作者更改内容二是作者正在插入一个新的列表元素。第三种情况是如果作者正在删除一个列表元素。

发言人   30:58
And I want to examine these because we need a story for each. And RCU actually kind of has a story for each, the danger. So I'm just talking about what goes wrong if somebody's reading a list while another the course writing it, if the writer wants to just changes this string, then the danger is that the reader will be actually reading the bytes of this string or whatever else is in the list element while the writer is modifying the same bytes. And so if we don't do anything special, the reader will see some mixture of the old bytes and the new bytes. And that's probably a disaster. That's one case we have to worry about. 
我想检查这些，因为我们需要为每个人准备一个故事。而RCU实际上对于危险都有一个故事。所以我只是在谈论如果有人在阅读一个列表，而另一个课程在写它，如果作者只想更改此字符串，会出现什么问题，那么危险的是，当写入器修改相同的字节时，读取器将实际读取此字符串或列表元素中的任何其他内容的字节。因此，如果我们不做任何特别的事情，读者将会看到旧字节和新字节的一些混合。这可能是一场灾难。这是我们必须担心的一个案例。

发言人   31:38
Another possibility is that the writer is inserting a new element. And of course, what that means is that supposing the writer wants to insert the new element at the head, the writer is going to cook up some new element, going to change the head pointer to point to it. And I'm going to change the new element to point at the old first element, right? So the danger here, if a reader reads, is reading a list while the writer is inserting, is that maybe, you know, if we really blow it the. The writer may set the head pointer to point to the new element before the new elements initialized. That is, while it maybe contains garbage for the string or some illegal pointer as the next element. So that's the thing that could go wrong if a writer is inserting. 
另一种可能性是作者正在插入一个新元素。当然，这意味着假设作者想在头部插入新元素，作者将制作一些新元素，将改变头部指针指向它。我将更改新元素以指向旧的第一个元素，对吧？所以这里的危险在于，如果一个读者在阅读，而作者在插入一个列表，那就是，你知道，如果我们真的把它搞砸了。编写器可以在新元素初始化之前将头指针设置为指向新元素。也就是说，尽管它可能包含字符串的垃圾或某个非法指针作为下一个元素。所以这就是如果一个作家正在插入可能出错的事情。

发言人   32:37
And if the writer is deleting, then what it means to delete an element is first to change. Let's say we're deleting the first element. We change the head pointer to point to the second element, and then call free on the first element to return this to the free list. And the danger here, if the reader sees the new head pointer, that's fine. They're just going to go on to the second elements, to the first. If the reader actually was looking at the first element and then the writer freed it, then the problem we have is now the reader is looking at element that's on the free list and could be allocated for some other use and overwritten for some completely other use, while the reader's still looking at this element. So from the reader point of view, now all of a sudden the elements filled with garbage and said it was expecting. 
如果写入器正在删除，那么删除元素的含义是首先要更改。假设我们要删除第一个元素。我们更改头指针以指向第二个元素，然后在第一个元素上调用free以将其返回到可用列表。这里的危险是，如果读者看到新的头部指针，那没关系。他们只会继续到第二个元素，到第一个。如果读者实际上正在查看第一个元素，然后作者释放了它，那么我们现在的问题是读者正在查看空闲列表上的元素，并且可能被分配用于其他用途并被覆盖用于完全其他用途。读者还在关注这个元素。因此，从读者的角度来看，现在所有的元素突然都充满了垃圾，并说它期待着。

发言人   33:24
So that's the third case. We have to, if we want to have lock, we want to have absolutely no locks for beaters, we have to worry about these three situations. 
这是第三种情况。我们必须这样做，如果我们想要有锁，我们想要完全没有打手的锁，我们必须担心这三种情况。


发言人   33:34
Now I'm not talking about writer versus writer problems here because I'm just assuming for this entire lecture that writers still use locks, that there's still some ordinary like XV six style spin lock here, and writers acquire this lock before doing anything, but readers don't acquire any lock whatsoever questions about these dangers. Okay? The point is, we can't just simply have readers read with locks, but it turns out we can't fix these specific problems. 
现在我在这里不是在谈论作家与作家的问题，因为我只是假设在整个讲座中作家仍然使用锁，这里仍然有一些普通的，如XV六风格的旋转锁，作家在做任何事情之前获得这个锁，但是读者对这些危险没有任何疑问。好吗？重点是，我们不能简单地让读者用锁读取，但事实证明我们无法解决这些特定的问题。

发言人   34:13
And that takes us to RCU. 
这就把我们带到了RCU。

发言人   34:19
And RCU has a couple of ideas in it. That RCU is, by the way, it's as much a kind of approach to con concurrency control as it is particular algorithm, right? It's a way of structuring approach to structuring readers and writers so that they can get along with the readers not having to take locks. The general game with we copy update is we're going to fix those three situations in which readers might get into trouble if there's concurrent writers. And we're going to do it by making the writers a little bit more complicated. So the writer is going to end up somewhat slower. They still need to lock plus follow some extra rules, but the reward will be the readers will be dramatically faster because they can operate without locks and without ever writing memory. 
而RCU有几个想法。顺便说一下，RCU是一种既可以实现并发控制又可以实现特定算法的方法，对吧？这是一种结构化读者和作者的方法，这样他们就可以与读者相处而不必采取锁定。复制更新的一般游戏是，我们将解决这三种情况，如果有并发的作者，读者可能会遇到麻烦。我们将通过使作者更加复杂一点来做到这一点。所以作者最终会慢一些。他们仍然需要锁定并遵循一些额外的规则，但奖励将是读者将显著更快，因为他们可以在没有锁定和不写入内存的情况下操作。


发言人   35:11
Okay, so the first big idea in RCU. Is that in that first trouble situation we talked about before, where the writer is updating a list element, the content of a list element, we're going to actually outlaw that. We're going to say writers aren't not allowed to modify the contents of list elements. Instead, if we have a linked list like this with a couple of elements. 
好的，所以在RCU中的第一个大想法。就是在我们之前提到的第一个麻烦的情况下，作者正在更新列表元素，列表元素的内容，我们将实际取缔。我们要说的是，作者不允许修改列表元素的内容。相反，如果我们有一个像这样具有几个元素的链表。

发言人   35:52
If a writer wanted to update the content of element 2 instead of changing it in place, which it wouldn't do, it would actually cook up. It would call the allocator to allocate a new element. It would initialize the element completely. So whatever new content, you know, we wanted to put here instead of the old content, the writer would set the next pointer on this new element so that this new element is now completely correct looking. And then in a single right to E one's next pointer, the writer would switch E1 from pointing to the from pointing to the old version of E 2 to pointing to the new version of E 2. So the game is, instead of updating things in place, we're going to replace them with new versions. Of the same data. And so. 
如果一个作者想更新元素2的内容而不是在原地改变它，它不会这样做，它实际上会做饭。它将调用分配器分配一个新元素。它将完全初始化元素。所以无论是什么新内容，你知道的，我们想放在这里而不是旧内容，作者会将下一个指针设置在这个新元素上，这样这个新元素现在就完全正确了。然后在指向E1的下一个指针的单个右侧，作者将把E1从指向e2的旧版本切换到指向e2的新版本。所以游戏是，我们将用新版本替换它们，而不是更新现有的东西。相同的数据。所以。

发言人   36:50
So now a reader know a reader's gotten as far as E1, it's just looking at E one's next pointer. The readers can either see the old next pointer, which points to E 2. And that's fine because nobody was changing E 2, or the reader is going to see the new next pointer and look at the new. 
所以现在读者知道读者已经到达了E1，它只是看着E1的下一个指针。读者可以看到旧的下一个指针，它指向E 2。这没问题，因为没有人更改e2，或者读者将看到新的下一个指针并查看新的。


发言人   37:09
List element. And either way, since the writer initial fully initialize this list element before setting in one's next pointer, either way, the reader is going to see like a correct next pointer that points to E3. So the point is, the reader will never see a string that's in the process of being a content, that's in the process of being modified. 
列表元素。无论哪种方式，由于作者在设置下一个指针之前初始完全初始化此列表元素，无论哪种方式，读者都会看到一个指向e3的正确下一个指针。所以重点是，读者永远不会看到一个字符串正在成为内容，正在被修改。

发言人   37:37
Any questions about this particular idea? What about sorry? Okay, go ahead. 
对这个特别的想法有什么问题吗？抱歉呢？好的，继续吧。

发言人   37:51
Will the link between a two and a three be deleted or will it be left there in case that are either somehow reach you? Now we're just going to leave it, we're so I'll come to this. This is an excellent question and it's actually the main piece of complexity in RCU. But for now, we're just going to imagine that E 2 is left alone for the moment. 
两个和三个之间的链接会被删除，还是会留在那里以防万一，或者以某种方式联系到你？现在我们要离开它了，所以我会来谈这个。这是一个很好的问题，它实际上是RCU复杂性的主要部分。但现在，我们要想象一下，E 2暂时被单独留下。

发言人   38:19
The link from E2E to E3, we don't need to worry about it anyway, right? Because that's a part of E 2. And like in normal implementations, we just free that anyway. Like with no RC involved, we don't ever need to worry about that link, right? The danger is that that just before we changed this next pointer, that some reader had followed the next pointer to E 2. So what we're worried about here is that I read on some course is actually right now reading E 2. We'd better not free it right away, right? Right, because that's what I think that's all we're saying is you better not free to right away, just leave it alone. As a piece of jargon. 
从E2E到E3的链接，我们不需要担心，对吧？因为这是E 2的一部分。就像在正常的实现中一样，我们只是释放它。就像没有RC一样，我们不需要担心那个链接，对吧？危险在于，就在我们更改下一个指针之前，一些读者已经按照下一个指针指向了E 2。所以我们在这里担心的是，我现在读的某些课程实际上是读E 2。我们最好不要马上释放它，对吧？对的，因为我认为这就是我们所说的，你最好不要马上自由，就别管它了。作为一种行话。

发言人   39:03
The right, the swap of E ones. Next pointer from the old E 2 to the new E 2. I, in my head, I call this a committing right? There's. And part of the reason why this works is that with a single committing rate, which is atomic like rights to pointers, and the machines we use are atomic in the sense that either the right to the point or happened or didn't happen from the perspective of readers because of atomic, basically with that one instruction, with that one atomic store, we can, it's an ordinary store, it's indivisible. We switch E1 from pointing to the old, the next pointer from pointing to the old one to the new one. And that rate is what sort of commits us to now using the second version. 
右边是E的交换。从旧的E 2到新的E 2的下一个指针。在我的脑海中，我把这称为承诺，对吗？有。这之所以有效，部分原因在于单一提交速率是原子的，就像指针的权限一样，我们使用的机器在感知上是原子的，因为原子性，从读者的角度来看，要么是正确的，要么是发生了，要么不是发生了，基本上，通过那一条指令，通过那一个原子存储，我们可以，它是一个普通的存储，它是不可分割的。我们将E1从指向旧的，下一个指针从指向旧的切换到新的。这个速率就是我们现在使用第二个版本提交的类型。

发言人   39:54
This is a very basic technique, a very important technique R, and what it means is that RCU is really mostly applicable to data structures for which you can have single committing rights. So that means there's some data structures which are quite awkward in the scheme, like a doubly linked list where every element is pointed to from two different pointers. Now, we can't get rid of this element with a single committing, right? Because there's two pointers to it, we can't. Most machines, you can't atomically change two different memory locations at the same time. So doubly linked lists are not so good for RCU, a data structure that is good as a tree, and if you have a tree of. 
这是一项非常基本的技术，一项非常重要的技术，它意味着RCU实际上最适用于您可以拥有单个提交权限的数据结构。这意味着在方案中有一些非常尴尬的数据结构，比如双向链接的列表，其中每个元素都是从两个不同的指针指向的。现在，我们不能只用一次提交就摆脱这个元素，对吗？因为有两个指针指向它，我们不能。大多数机器不能同时自动更改两个不同的内存位置。因此，双向链表对于像树一样好的数据结构RCU来说并不是很好，如果你有一棵树。

发言人   40:49
A tree of nodes like this. Then we can do you. Supposedly we want to change. We want to modify this value down here. What we can do? There's some head to the tree. What we can do is cook up a new, a new version of this part of the tree here and with a single committing right to the head pointer, switch to the new version of the tree. And so the new version of the tree, which will, you know, the writer will allocate, sort of create. Can actually share for convenience, can share structure, the unmodified part with the old tree and then with a single committing, right? 
像这样的节点树。然后我们可以做你。据说我们想要改变。我们想在这里修改这个值。我们能做什么？有一些头在树上。我们能做的是在这里制作一个新版本的树的这一部分，并用一个对头指针的单一承诺，切换到树的新版本。所以，新版本的树，你知道，作者将分配，有点像创建。实际上可以为了方便而共享，可以共享结构，与旧树共享未修改的部分，然后进行单个提交，对吗？

发言人   41:34
We're going to change the head pointer to the tree head pointer to point to the new version. But for other data structures that don't look like Lister trees use, it's not so easy to use RCU. Okay, so that's the first idea. Any last questions? 
我们将把head指针更改为tree head指针，以指向新版本。但是对于看起来不像Lister树使用的其他数据结构，使用RCU并不是那么容易。好的，这是第一个想法。有没有最后的问题？


发言人   42:04
The second idea? 
第二个想法？


发言人   42:14
One of the problems with one of the potential problems with. 
其中一个问题是潜在问题之一。

发言人   42:24
The scheme I just described, we're going to cook up a new E 2 prime. And what I said was, oh, well, we'll initialize the content for E 2 prime, and we'll set its next pointer correctly. And after that, we'll set E one's next pointer to point to E 2. As you may recall from discussions of XV 6, by default, there's no after that on these machines. 
我刚才描述的方案，我们将制作一个新的e2质数。我说的是，哦，我们将初始化E 2素数的内容，并正确设置其下一个指针。之后，我们将设置e1的下一个指针指向e2。你可能还记得在XV 6的讨论中，默认情况下，这些机器上没有后面的东西。

发言人   42:54
The compiler and the hardware, basically all compilers and many microprocessors reorder memory operations. So if you simply say, we allocate a new element, and we just wrote this C code, you know, eero next to equals, you know, E3, and then E1 arrow next equals E, this is not going to work well. This is not going to work reliably. It's going to work fine when you test it, but it won't work in real life all the time. Occasionally it'll go wrong. And the reason is that the compiler may end up reordering these rights or the machine may end up reordering these rights, or the reading code, which reads these various things. The compiler or the machine, the microprocessor, may end up reordering the reader's reads. Of course, if we set E1 arrow next to point to E 2 before we initialize the content V2 so that it's string it holds or its next pointer point off into space, then some readers going to see this pointer, follow it, read garbage, and crash. 
编译器和硬件，基本上是所有编译器和许多微处理器重新排序内存操作。所以如果你只是说，我们分配了一个新元素，然后我们就写了这段C代码，你知道，eero紧挨着equals，你知道，E3，然后E1箭头紧挨着equals E，这不会很好地工作。这不会可靠地工作。当你测试它时，它会很好地工作，但它不会一直在现实生活中工作。偶尔会出错。原因是编译器可能最终重新排序这些权限，或者机器可能最终重新排序这些权限，或者读取这些不同事物的读取代码。编译器或机器，微处理器，可能最终会重新排序读者的读取。当然，如果我们在初始化内容V2之前将E1箭头设置为指向e2，使其成为字符串或其下一个指针指向空间，那么一些读者会看到这个指针，跟随它，读取垃圾并崩溃。


发言人   44:12
So the second idea is that both readers and writers have to use memory barriers, even though we're not locking or really because we're not locking the writers and the readers have to use a barrier. And for writers, the place the barrier has to go is before the committing, right? So we need a barrier here. That tells the hardware and the compiler, look all the rights before this barrier. Please finish them before doing any writes after the barrier so that E 2 is fully initialized before we set E1 to point to it. 
所以第二个想法是读者和作者都必须使用记忆屏障，即使我们没有锁定作者，或者因为我们没有锁定作者，读者必须使用屏障。对于作家来说，障碍必须去的地方是在承诺之前，对吧？所以我们需要一个屏障。告诉硬件和编译器在此障碍之前查看所有权利。请在屏障后执行任何写入之前完成它们，以便在我们设置E1指向它之前完全初始化e2。

发言人   44:51
And on the read side. The reader needs to load E1 arrow next into some, you know, temporary location or register. So we'll just say, you know, register 1 equals E1 arrow next. Then the reader needs a barrier. And then the reader is going to look at R 1 arrow is content, and R 1 arrow next. And what this barrier on the reader says is, don't issue any of these loads until after we've completed this load. So the reader is going to look at E1 or next and either get the old E 2 or the new E 2. And then the barrier says that only then are we going to start looking at only after we've grabbed this. 
在阅读方面。读者需要将E1箭头加载到某个临时位置或寄存器中。所以我们就说，你知道，寄存器1等于E1箭头。那么读者就需要一道屏障。然后读者会看到r1箭头是内容，接下来是r1箭头。读者的这个障碍所说的是，在我们完成这个加载之前，不要发布任何这些加载。所以读者将会看到E1或下一个，要么得到旧的e2，要么得到新的e2。然后屏障说只有在我们抓住这个之后我们才会开始寻找。

发言人   45:51
All these reads have to execute after this read, and since the writer are guaranteed to initialize the content before committing the pointer to E, the new E 2, that means these reads. If this pointer points to the new E 2, that means these reads are guaranteed to see the initialized content. 
所有这些读取都必须在读取之后执行，并且由于编写器在提交指向E的指针之前保证初始化内容，这意味着这些读取。如果此指针指向新的e2，则意味着这些读取可以保证看到初始化的内容。


发言人   46:15
Okay, so we also have a little bit of this. Oh yeah, I was just, I was confused about the reader. So how can you read R 1 Iran? Like anything before you read R 1, I guess, how would the? Yeah, I guess like if even if it reordered it, how, how did you be able to read R 1 and X the fourth red you want next? 
好的，我们也有一点。哦，是的，我只是，我对读者感到困惑。那么你怎么能读懂R 1伊朗呢？在你读R 1之前，我想，你会怎么做呢？是的，我想即使它重新排序，你怎么能够读取R 1和X你想要的第四个红色？

发言人   46:55
I think you've stumped me. Yeah, I mean, what you're pointing out is that before you even know what the pointer is, you can't possibly actually issue the reads. 
我想你把我难住了。是的，我的意思是，你指出的是，在你甚至不知道指针是什么之前，你不可能真正发出读取。

发言人   47:09
A possibility is that whatever this pointer points to maybe is already cached on this core due to some, maybe, you know, this memory was had been, you know, a minute ago used for something else, something totally else. We have an old version of this cached on our core at the address at this address, But for some previous use of the memory, if this read was to use the old cached value I'm not sure this can happen. I'm just making this up for you. But if this read could use the old cache value, then we'd be in big trouble. And I don't know if the machine would actually do that or whether. 
一种可能性是，无论这个指针指向什么，也许已经缓存在这个核心上，因为也许，你知道，这个内存在一分钟前被用于别的东西，完全是别的东西。我们在这个地址的核心上缓存了这个的旧版本，但是对于以前使用的内存，如果这次读取是使用旧的缓存值，我不确定会发生这种情况。我只是给你讲这些。但是如果这个读取可以使用旧的缓存值，那么我们就有大麻烦了。我不知道这台机器是否真的能做到这一点，或者是否能做到。

发言人   47:53
Another possibility is that the compiler, you know. The real answer is, I don't know. I should go off and think about what a specific example would be, Okay? Okay, I see the CA version makes some sense, Yes, yeah. I'm not actually completely sure it could could happen in real life. That's a good question? Okay, so that's the second idea. 
另一种可能性是编译器，你知道的。真正的答案是，我不知道。我应该去思考一个具体的例子，好吗？好的，我看到CA版本有一些感知，是的，是的。我并不是完全确定这是否会在现实生活中发生。这是个好问题？好的，这是第二个想法。

发言人   48:26
The third problem we have, which is something somebody raised before, is that the writer is going to swap the E1 pointer to point to the new E 2. But there could be readers, you who started looking at who followed this pointer just before we, the writer, or change it, who are still looking at E 2. We need to free this list element someday, but we better not free it while some reader still using it. So we need to somehow wait until the last reader has finished using E 2 before we can free it. 
我们遇到的第三个问题，也就是之前有人提出的问题，就是编写者将要交换E1指针以指向新的e2。但是可能有读者，你开始关注谁在我们作者之前跟随了这个指针，或者改变了它，他们仍然在关注E 2。我们需要有一天释放这个列表元素，但最好不要在一些读者仍在使用它的时候释放它。所以我们需要以某种方式等待，直到最后一个读者使用完e2，然后才能释放它。

发言人   48:58
Now, that's the sort of third and final main problem that RCU solves, is how long should the writer wait before it frees E 2? There's, you could imagine, a number of ways of doing this. For example, we could put a little reference count in every list element and have readers incremented and have readers readers increment it when they start using a list element decal, when they're done using the list element and have the writer wait for the reference count on this element go to 0. We would regret that instantly because the whole point of RCU is to allow reading without writing, because we know that if lots of readers are changing this reference count, it's going to be terribly expensive to do the rights involved in maintaining a reference count. So we absolutely don't want reference counts. 
现在，这就是RCU解决的第三个也是最后一个主要问题，作者应该等待多长时间才能释放E 2？你可以想象，有很多方法可以做到这一点。例如，我们可以在每个列表元素中放入一点引用计数，并让读者递增，当读者开始使用列表元素贴花时，当他们完成使用列表元素时，让作者等待此元素上的引用计数达到0。我们会立刻后悔，因为RCU的全部意义在于允许读取而不写入，因为我们知道如果很多读者更改了这个引用计数，那么维护引用计数所涉及的权利将会非常昂贵。所以我们绝对不想要引用计数。

发言人   49:47
Another possibility would be use a garbage collected language. And a garbage collected language, you don't ever free anything explicitly. Instead, the garbage collector does the bookkeeping required decide if any thread, for example, or any data structure has a still has a reference to this element and the garbage collector, once it proves this element can't possibly be ever used again, only then will the garbage collector free this. So that's another quite possibly reasonable scheme for deciding when to free this list element. You know Linux, which uses RCU, it's not written in a garbage collected language, so, and we're not even sure that garbage collection be would improve performance. So we can't use a standard garbage collector here. And it's dead. 
另一种可能性是使用垃圾收集语言。和一种垃圾收集语言，你永远不会明确地释放任何东西。相反，垃圾回收器会按照要求进行记账，例如决定是否有任何线程或任何数据结构仍然引用此元素和垃圾回收器，一旦证明此元素无法再次使用，只有这样垃圾回收器才会释放此元素。所以这可能是决定何时释放此列表元素的另一个非常合理的方案。你知道Linux，它使用RCU，它不是用垃圾收集语言编写的，所以，我们甚至不确定垃圾收集是否会提高性能。所以我们在这里不能使用标准的垃圾收集器。它已经死了。

发言人   50:38
RCU uses another. Sort of a trick that works well in the kernel for delaying freeze. 
RCU使用另一个。一种在内核中运作良好的技巧，用于延迟冻结。

发言人   50:53
So that? Idea is that readers and writers have to each follow a rule that will allow writers to delay the freeze. Readers are not allowed to hold a pointer to RCU protected data across a context switch, so a reader is not allowed to hold a pointer to one of those list elements across a context switch. So the readers. They can not yield the CPU. In a RCU critical section. And then what the writers do is they delay the free. Until every core. As context switches at least once. 
所以呢？这个想法是读者和作者必须各自遵循一条规则，允许作者延迟冻结。阅读器不允许在上下文切换中保存指向受RCU保护的数据的指针，因此不允许阅读器在上下文切换中保存指向这些列表元素之一的指针。因此，读者。他们无法产生CPU。在RCU临界部分中。然后作家们的做法是延迟免费的。直到每一个核心。至少切换一次上下文。

发言人   52:00
So this is easy enough. Like this is actually also a rule for spin locks and a spinlock critical section. 
所以这很容易。像这样实际上也是旋转锁和自旋锁临界区的规则。

发言人   52:05
You can't yield the CPU, but nevertheless, you do have to be a bit careful. This is a little more involved, but it's relatively clear when each core knows its context switching. And so this is a pretty well defined point for the writer to have to wait for and just requires some implementation. This also requires, this may be a significant delay. It may be a millisecond or a significant fraction of a millisecond that the writer has to wait before it's allowed to free that list element to be sure that no reader could possibly still be using it. 
你不能放弃CPU，但尽管如此，你必须小心一点。这有点复杂，但是当每个核心都知道它的上下文切换时，就相对清楚了。因此，这是作者必须等待的一个非常明确的点，只需要一些实现。这也需要，这可能是一个重大的延迟。它可能需要一毫秒或毫秒的很大一部分，编写者必须等待才能释放该列表元素，以确保没有读者可能仍在使用它。

发言人   52:41
People have come up with a bunch of techniques for actually implementing this weight. Most the straightforward one the paper talks about is that the writing thread simply arranges with the scheduler to have the rating thread be executed briefly on every one of the cores in the system. And what that means is that that every one of the cores must have done a context switch during this process. And since readers can't hold self across context switches, that means that the writer is not waited long enough. 
人们已经想出了一堆技巧来实际实施这个重量。这篇论文讨论的最直接的一个方法是，写入线程只是与调度程序进行安排，以便在系统中的每个核心上短暂执行评级线程。这意味着每个核心在此过程中必须进行上下文切换。由于读者无法跨越上下文切换保持自我，这意味着作者没有等待足够长的时间。

发言人   53:20
And so the way the actual writer code looks like is the rating code does whatever modifications it's going to do to the data. And then it calls this synchronize RCU call. Which actually implements 2? And then the writer frees whatever the old element was. And so that means that now the writer is doing whatever it's doing. You know, at this point, let's say it's doing the, you know, E1 arrow next. Is equal to the new list element? 
因此，实际的作者代码看起来是评级代码对数据所做的任何修改。然后它调用这个同步RCU调用。哪一个真正实现了2？然后作者释放了任何旧元素。这意味着现在作者正在做它正在做的事情。你知道，在这一点上，假设它正在做，你知道，E1箭头。是否等于新列表元素？

发言人   54:11
This synchronized RCU causes forces a context switch on every core, So any core that could have read, you know, any court that could have read the old value must have read it at this point. Must have read it at this point in time. If after that point in time, we've done a context switch on every core, that means that no core that read the old value could still have a pointer to that value at this point in time due to rule 1. And that means that we're allowed to free the old value. Any questions? 
这个同步的RCU会在每个核心上强制进行上下文切换，因此任何可能读取旧值的核心，你知道，任何可能读取旧值的法院此时必须已经读取了它。一定是在这个时间点读过它。如果在那个时间点之后，我们在每个核心上进行了上下文切换，这意味着由于规则1，读取旧值的核心此时仍然无法拥有指向该值的指针。这意味着我们可以释放旧值。有问题吗？


发言人   54:55
You may object that this synchronized RCU will take a significant, perhaps fraction of a millisecond. That's quite true. So that's too bad. One of the justifications is that writing, you know, for RCU protected data, writing is going to be relatively really rare. So the fact that the rights take longer may not will probably not affect overall performance very much for the situations in which the writer really doesn't want to wait. 
您可能会反对这个同步的RCU将花费显著的时间，也许是毫秒的小数部分。这是完全正确的。那太糟糕了。其中一个理由是写作，你知道，对于受RCU保护的数据，写作相对来说非常罕见。因此，在作者真的不想等待的情况下，权利需要更长时间的事实可能不会对整体性能产生太大影响。

发言人   55:24
There's another call that defers even the weight called call RCU. And the idea is you pass it the in the usual use case, you pass it a pointer to the object you want to free, and then a callback function that just calls free on this pointer. And the RCU system basically stashes away the call. RCU stashes away these two values on a list, and then immediately returns and then does some bookkeeping, typically involving basically looking at the counts of how many contexts, which is have occurred on each core, The system sort of in the background after caller C returns, does some bookkeeping to wait until all cores a context switch, and then calls this callback function with this argument. And so this is a way of avoiding the weight because this call returns instantly. 
还有另一个呼叫甚至会延长权重，称为呼叫RCU。这个想法是在通常的用例中传递给它，传递给它一个指向你想要释放的对象的指针，然后传递一个回调函数，只在此指针上调用free。而RCU系统基本上会将呼叫存放起来。RCU将这两个值存储在一个列表中，然后立即返回，然后进行一些记账，通常涉及基本上查看每个核心上发生了多少个上下文的计数，系统在调用方C返回后在后台运行。执行一些簿记以等待所有核心进行上下文切换，然后使用此参数调用此回调函数。因此，这是一种避免重量的方法，因为此调用会立即返回。


发言人   56:30
On the other hand, you're discouraged from using it because now this list that people, if the kernel calls, call RCU a lot, then the list that holds these values can get very long. And it means that there may be a lot of memory that's not being freed. All the all the this list goes very long. Each list element is has a pointer in it that should be freed, a pointer to an object that should be freed. And so under extreme circumstances, you can run a system at, if you're not careful, a lot of calls to RCU call RCU can run a system out of memory because all the memory ends up on this list of deferred freeze. So people don't like to use this if they don't have to. 
另一方面，你不鼓励使用它，因为现在这个人们经常调用RCU的列表，那么包含这些值的列表可能会变得非常长。这意味着可能有很多内存没有被释放。所有的这个列表都非常长。每个列表元素都有一个应该释放的指针和一个应该释放的对象的指针。因此在极端情况下，如果您不小心，可以在运行系统时，对RCU调用RCU的大量调用可能会导致系统内存不足，因为所有内存最终都位于此延迟冻结列表中。所以如果他们没有必要的话，人们不喜欢使用这个。

发言人   57:21
Okay to. Please ask questions so far. If you have questions, so this doesn't, this prevents us free. That's the us from freeing something that somebody is still using, but it doesn't prevent us from mod like having the read or see a half baked version of something because it's being modified, right? Idea one prevented that, Yeah, okay, so the idea behind idea one is that instead of updating a list element in place, which would absolutely cause the problem you mentioned, when writers are not allowed to update RCU protected data in place, instead they cook up a new data element and sort of swap it into the data structure with a single committing rate. Oh, and the sloping will be atomic, so there's no problem of like, because that's a single pointer, right, which is atomic, whereas overwriting a string is completely not atomic, that makes sense? 
好的。到目前为止，请提出问题。如果您有问题，那么这不会，这将阻止我们免费。这是美国从释放某些人仍在使用的东西中解放出来的原因，但这并不妨碍我们像阅读或看到某些东西的半熟版本一样，因为它正在被修改，对吧？想法一阻止了这一点，是的，好的，所以想法一背后的想法是，当作者不允许更新受RCU保护的数据时，与其更新一个列表元素，这绝对会导致你提到的问题，相反，他们会制作一个新的数据元素，并将其交换到具有单一提交速率的数据结构中。哦，倾斜将是原子性的，所以没有问题，因为那是单个指针，对吧，这是原子性的，而覆盖字符串完全不是原子性的，这就感知了？


发言人   58:31
Other questions? Does condition 1? Idea 3 mean we need to be careful about how much work we put inside those protected sections since it kind of hogs the core for that entire section. Yes, yes, so this is, that's right. So readers in the sort RCU critical section, while they're looking at the protected data, they can't context switch. And so you. You know, you want to keep those critical sections short now, and that's a consideration. 
其他问题？条件1？想法3意味着我们需要小心我们在这些受保护部分中投入了多少工作，因为它有点占用了整个部分的核心。是的，是的，这就是，没错。因此，处于排序RCU关键部分的读者在查看受保护数据时，无法进行上下文切换。所以你。你知道，你现在想保持那些关键部分简短，这是一个考虑因素。

发言人   59:09
The way it plays out, though, is that the way RCU has been deployed, it's typically that there'll be some piece of code in Linux that was protected with ordinary locks or read write locks. And somebody, you know, for some workloads will see, oh, that lock is a terrible performance problem. And they're going to replace the locking critical section with an RCU critical section. Although sometimes it's more involved than that. And since locking critical sections were already, it was extremely important to make them short because while you hold a lock, there may be lots of other cores waiting for that lock. So there was a lot of pressure to keep ordinary lock critical sections short because RCU critical sections are often sort of revised lock critical things that used to be lock critical sections, they tend to be short also. And you know, that means that, you know, not always, but usually there's not. 
然而，它的作用方式是RCU部署的方式，通常在Linux中会有一些代码受到普通锁或读写锁的保护。有人，你知道，对于某些工作负载，会看到，哦，那个锁是一个可怕的性能问题。他们将用一个RCU临界部分替换锁定临界部分。虽然有时比这更复杂。由于锁定的关键部分已经存在，因此使它们简短非常重要，因为当您持有锁时，可能会有许多其他内核等待该锁。因此，有很大的压力要求保持普通锁定关键部分简短，因为RCU关键部分通常是经过修订的锁定关键部分，曾经是锁定关键部分，它们往往也很短。你知道，这意味着，你知道，并不总是如此，但通常不是。


发言人   01:00:09
A direct worry about keeping the RCU critical section short. Although it is a constraint, the real constraint actually is you're not allowed to hold pointers over pointers to RCU data over context switches. And that's actually AI mean, you can't, for example, read the disk and wait for the disk weed to complete while holding on a pointer onto a pointer to RCU protected data. So it's not quite so much the or the thing that usually comes up is not the length of the critical section so much as the prohibition against yielding the CPU all. 
保持RCU关键部分简短的直接担心。尽管这是一个约束，但真正的约束实际上是不允许您在上下文切换中保留指向RCU数据的指针。这实际上是AI的意思，例如，您不能读取磁盘并等待磁盘杂草完成，同时按住指向RCU受保护数据的指针。因此，通常出现的不是临界部分的长度，而是禁止全部产生CPU。


发言人   01:00:48
Okay? Let's see, so just to kind of firm up what I all the stuff I just talked about. Kind of what you would see in a simple use of RCU. So this is code you might see for reading a list. An RCU protected list. And this is the code you might see on the right side for code that just wants to the particular case of replacing the first list element. 
好吗？让我们看看，所以只是为了巩固我刚刚谈到的所有东西。这有点像你在简单使用RCU时看到的东西。所以这是您可能会看到的用于读取列表的代码。一个受RCU保护的列表。这是您可能在右侧看到的代码，用于仅希望替换第一个列表元素的特定情况。

发言人   01:01:20
So in the read side, there is actually this, these read lock and read unlock calls. Those do almost nothing, almost nothing. The only, the only little thing they do is set a flag that says or RCU relock sets a flag that says if a timer interrupt happens, please don't contact switch because I'm in the middle of a RCU critical section. So that's all it really does is set a flag that prohibits timer interrupt context switches. Interrupt may still happen, but it won't context switch and then read unlock UN sets that flag. 
所以在读取端，实际上有这个，这些读取锁定和读取解锁调用。他们几乎什么都不做，几乎什么都不做。他们做的唯一一件小事就是设置一个标志，上面写着或者RCU重新锁定设置一个标志，上面写着如果计时器中断发生，请不要联系switch，因为我正处于RCU关键部分的中间。所以它真正做的就是设置一个标志，禁止计时器中断上下文切换。中断仍然可能发生，但它不会进行上下文切换，然后读取解锁并设置该标志。

发言人   01:01:56
Really, it's a counter of nested RCU criticals, So these two functions are extremely fast and do almost nothing. And then this loop would sort of scan down. 
实际上，它是一个嵌套RCU关键的计数器，因此这两个函数非常快，几乎什么也不做。然后这个循环会向下扫描。

发言人   01:02:10
Our list. This is the call that inserts the memory barrier. So what RC This really boils down to just a couple of instructions. It just reads. It grabs a copy of this pointer from memory, issues, a memory barrier, and then returns that pointer, And then we can look at the content and go on to the next list element. 
我们的名单这是插入内存屏障的呼叫。所以这实际上遥控归结为几个指令。它只是读。它从内存中获取此指针的副本，问题，内存屏障，然后返回该指针，然后我们可以查看内容并转到下一个列表元素。

发言人   01:02:41
So the readers quite simple, the writers a little more involved writers. Still, the RCU doesn't help writers avoid interfering with each other. So writers still have to have some way of making sure only one RA modifies the list at a time. 
所以读者非常简单，作家稍微更投入一些。不过，RCU并没有帮助作家们避免互相干扰。因此，作家仍然必须有一些方法来确保一次只有一个RA修改了列表。

发言人   01:02:57
In this case I'm just imagining we're going to use ordinary spin locks. The writer requires the lock. If we're replacing the first list element, we need to save a copy at the beginning because we're going to need to eventually free it. So we save this copy of the oldest element and now we this code plays that trick. 
在这种情况下，我只是想象我们将使用普通的旋转锁。作者需要锁。如果我们要替换第一个列表元素，我们需要在开始时保存一个副本，因为我们最终需要释放它。所以我们保存这个最老元素的副本，现在我们用这段代码来扮演这个角色。

发言人   01:03:16
I talked again about allocating a complete new list element to hold a sort of updated content. So we're going to allocate a new list element. We're going to set its content. We're going to set the next pointer to the next pointer in the old first list element because we're replacing it. And then this RCU assign pointer is choose a memory barrier so that all these rates happened and then sets the pointer pointed to by this first argument to be equal to that. So basically, this just issues a memory barrier and then sets head equal to the E, and now we can release the lock. 
我再次谈到分配一个完整的新列表元素来保存一种更新的内容。所以我们将分配一个新的列表元素。我们将设置它的内容。我们将把下一个指针设置为旧的第一个列表元素中的下一个指针，因为我们正在替换它。然后这个RCU分配指针选择一个内存屏障，以便所有这些速率发生，然后将第一个参数指向的指针设置为等于那个。所以基本上，这只是发出一个内存屏障，然后将head设置为等于E，现在我们可以释放锁定。

发言人   01:03:56
We still have a pointer to the old first list element called synchronize RCU to make sure every CPU that could have grabbed a pointer to the old list element before we did the committing right has yielded the CPU and therefore given up its pointer RC protected data. And now we can free the old this element. Any questions? 
我们仍然有一个名为synchronize RCU的指向旧的第一个列表元素的指针，以确保每个CPU在我们进行提交之前都抓住了指向旧列表元素的指针，因此已经产生了CPU并放弃了其遥控受保护数据的指针。现在我们可以释放旧的这个元素。有问题吗？


发言人   01:04:35
All right? 
一切都好吗？

发言人   01:04:45
RCU One thing to note about this is that while in the reader, while we're allowed to look at this list element inside the loop here, one thing we're not allowed to do is return the list element. So for example, we using RCU, we couldn't write a lookup, a list lookup function that returned either the list element or a pointer into data held in the list element, like a string that was embedded in the list element. Because then we be in, then we would no longer be in control. 
RCU需要注意的一件事是，在阅读器中，虽然我们允许在这里查看循环中的列表元素，但我们不允许做的一件事是返回列表元素。例如，我们使用RCU，我们无法编写查找，这是一个列表查找函数，它返回列表元素或指向列表元素中保存的数据的指针，就像嵌入列表元素中的字符串。因为那时我们处于其中，那么我们就不再掌控局面。

发言人   01:05:21
You know, it has to be the case that we don't look at RCU protected data outside this RCU critical section, or we don't do a context, which if we just write a generic function that returns a list element, then for all we know, the caller, I mean, maybe we can persuade the caller to follow some rules too. For all we know, the caller may contact switch or or we'd run into a trouble I that we call RCB unlock before returning the list element, which is illegal because now a timer interrupt could force a switch or we don't call our CB unlock so the use of RCU sort of, does put some additional constraints on readers that wouldn't have existed before. 
你知道，必须确保我们不在RCU关键部分之外查看RCU保护的数据，或者我们不进行上下文操作，如果我们只是编写一个返回列表元素的通用函数，那么我们都知道，调用者，我的意思是，也许我们可以说服打电话的人也遵守一些规则。我们都知道，呼叫者可能会联系交换机，或者我们会遇到麻烦，我们在返回列表元素之前调用RCB解锁，这是非法的，因为现在计时器中断可能会强制开关，或者我们不调用我们的CB解锁，因此使用RCU有点，确实对读者施加了一些以前不存在的额外限制。


发言人   01:06:06
A question about that, Yes, so are you saying in particular that if we had some form of like read element at index I method that there's no way to structure this so that it could return the value held by the node at element I could return a copy? So what would work? You know, if EOS is a string, we could return a copy of this string and that's fine. What would be a violation of the RCU rules is if we returned a pointer to this very string sitting inside you point it would be a mistake to return a pointer into E somewhat into E like if the string is stored inside the list element, we better not return a pointer to that string because then it will be. We have to not context switch while we're holding a pointer into RCU protected data. And the convention is you. You just use that data within this critical section. And so it would almost certainly be breaking the convention or this setup would have to be much more complicated if we ended up returning pointers in 2, the protected data. 
关于这个问题，是的，那么你特别想说的是，如果我们在index I方法中有某种形式的像read元素，那么就没有办法构建它，以便它可以返回元素处节点持有的值，我可以返回一个副本？那么什么会起作用？你知道，如果EOS是一个字符串，我们可以返回这个字符串的一个副本，这很好。违反RCU规则的是，如果我们返回一个指向这个字符串的指针，将指针返回到E中的某个字符串将是一个错误，就像如果字符串存储在列表元素中一样，我们最好不要返回指向该字符串的指针，因为那样会出现问题。当我们持有一个指针进入受RCU保护的数据时，我们不能进行上下文切换。公约就是你。您只需在此关键部分中使用该数据。因此，几乎可以肯定，如果我们最终在2 (受保护的数据) 中返回指针，那么这个设置将会更加复杂。


发言人   01:07:29
The I just want to sort of return briefly to the performance story. It's hard to characterize sort of what the performance is in a sense. Let's see, the overall performance story is that if you use RCU, reads are extremely fast. They just proceed that, you know, whatever. They have sort of no overhead above the ordinary overhead of looking at that data. So if your list is a billion elements long, yeah, reading the list will take a long time, But it's not because of synchronization is just because you're doing a lot of work for readers. So you can almost view RCU as having 0 overhead for readers and the. Exceptions are minor RCU, reed lock, you know, just a tiny amount of work to set this flag saying no context switches and RCU dereference issues of memory barrier, which actually might slow you down by dozens, a few dozen cycles, but it much cheaper then a lot. 
我只想简要地回到性能故事。很难描述感知的性能。让我们看看，整体的性能故事是，如果您使用RCU，读取速度会非常快。他们只是继续这样做，你知道的，不管怎样。它们没有比查看这些数据的普通开销更多的开销。因此，如果您的列表有十亿个元素长，是的，阅读列表将需要很长时间，但这不是因为同步只是因为您为读者做了很多工作。因此，您几乎可以将RCU视为对读者和读者的零开销。异常是次要的RCU，里德锁，你知道，只需要很小的工作量来设置这个标志，表明没有上下文切换和RCU取消对内存屏障的引用问题，这实际上可能会减慢你的速度几十个，几十个周期，但它比很多便宜得多。

发言人   01:08:41
The performance story for writers is much sadder. You had to do all the stuff you always had to do using locks. In fact, you have to acquire and release locks in the writer. And you have this potentially extremely expensive called or time consuming called the synchronize RCU. In fact, you can give up internally. Synchronize RCU gives up the CPU so you don't doesn't spin necessarily, but it may require a lot of elapsed time waiting for every other core to context switch. So depending on the mix of reads and writes and how much work was being done inside the read critical section, the performance increase varies tremendously from much, much faster if these critical sections were short and there's few rights to perhaps even slower if rights are very common. And so when people apply RCU to kernel stuff, they actually, you absolutely have to do performance tests against a bunch of workloads in order to figure out whether using RCU is a win for you because it's so dependent on the workload. 
对于作家来说，表演故事要悲伤得多。你必须使用锁来做你一直要做的所有事情。事实上，你必须在写入器中获取和释放锁。并且你有一个名为synchronize RCU的潜在极其昂贵或耗时的调用。事实上，你可以在内部放弃。同步RCU放弃CPU，这样你就不必旋转，但可能需要花费很多时间等待每个其他核心进行上下文切换。因此，根据读取和写入的混合以及在读取临界区中完成的工作量，性能的提高会有很大的差异，如果这些临界区很短，性能的提高会更快，如果权限很常见，性能的提高甚至会更慢。因此，当人们将RCU应用于内核时，实际上，您绝对需要针对大量工作负载进行性能测试，以便确定使用RCU是否对您来说是一个胜利，因为它非常依赖于工作负载。




发言人   01:09:55
I have a maybe a tangential question, but. We've seen that, I guess when there's multiple cores being used, there's some added complexity to our usual implementations. And it's often the these atomic instructions can come to the rescue. And that's assuming there's one shared memory system. But I wonder, like, what happens if a machine is trying to maintain like multiple Ram systems? How does it unify those? The ordinary? Well? 
我可能有一个无关紧要的问题，但是。我们已经看到，我想当有多个内核被使用时，我们通常的实现会增加一些复杂性。而且通常这些原子指令可以拯救。这是假设有一个共享内存系统。但是我想知道，如果一台机器试图维护像多个Ram系统一样会发生什么？它是如何统一这些的？普通人？好吧？

发言人   01:10:42
At the level we're talking about, the machine has one Ram system. Okay, you know? Yeah, it's for all the sort of ordinary computers you would buy that have multiple cores. You can pretty much program them as if they were just one Ram system shared among all the cores. That's the logical model the hardware provides you at a physical level. It's not like that often. There's plenty of machines out there that have this physical arrangement. 
在我们谈论的级别上，机器有一个Ram系统。好吧，你知道吗？是的，它适用于所有你会购买的具有多核的普通计算机。你几乎可以对它们进行编程，就好像它们只是一个在所有核心之间共享的Ram系统一样。这就是硬件在物理层面提供给你的逻辑模型。这不是经常发生的事情。有很多机器都有这种物理排列。

发言人   01:11:13
We have a CPU chip. So here's 1 CPU chip, maybe with lots of cores on it, right? And you know, you can get CPU chips with, I don't know how many cores these days, 32 cores. Say, let's say you want to build a 64 core machine, you can only buy 32 core chips. Well, you can make a board like it has two sockets for chips on it. So now we have two chips. 
我们有一个CPU芯片。所以这是1个CPU芯片，可能有很多内核，对吧？你知道，你可以得到CPU芯片，我不知道现在有多少个核心，32个核心。比如说，假设你想要构建一个64制芯机，你只能购买32个核心芯片。好的，你可以制作一个有两个插槽用于芯片的板子。所以现在我们有了两个芯片。

发言人   01:11:38
The fastest way to get at memory is to have the memory more or less as directly attached to the CPU chip as possible. So what you would do is you'd have like a very fat set of wires here to right next to the chip, a bunch of Ram. So it has direct access. And of course, this chip's going to want its own Ram also. So this I'm just drawing a picture of what you would see if you opened up a PC with two processor chips in it. 
获取内存的最快方法是将内存尽可能直接连接到CPU芯片。所以你会做的是在芯片旁边有一堆很胖的电线。所以它可以直接访问。当然，这个芯片也需要自己的Ram。所以我只是在画一张你会看到的图片，如果你打开一台带有两个处理器芯片的电脑。

发言人   01:12:10
But now we're faced with a problem. What happens if a software over on this chip uses a memory location that's actually stored in this Ram? So in fact, there's also a interconnect between these two chips, generally an extremely fast interconnect, like gigabytes per second. 
但现在我们面临着一个问题。如果该芯片上的软件使用了实际存储在该Ram中的存储位置，会发生什么？事实上，这两个芯片之间也有互连，通常是非常快的互连，比如每秒千兆字节。

发言人   01:12:29
And the chips are smart enough to know that certain physical memory locations are in this bank of Ram and other physical locations, physical memory addresses are in this bank of Ram. And the software here uses a physical address is over In this one, the chip is clever enough to send a message. It basically a little network, send a message over to this chip saying it, look, I need to read some Ram. Please do it and go read this Ram and send the result back. You know, you can buy 4 chip arrangements with the same thing with a complex interconnect like this. So there's a huge amount of engineering going on in order to map the straightforward shared Ram model onto what's sort of feasible to build with high performance in real life and fit in two or three dimensions. 
并且芯片足够智能，知道某些物理内存位置在这个内存库中，而其他物理位置、物理内存地址在这个内存库中。这里的软件使用了物理地址，而这个芯片足够聪明，可以发送消息。它基本上是一个小网络，向这个芯片发送一条消息，说看，我需要读一些内存。请去做并阅读这个内存并将结果发送回来。你知道，你可以用同样的东西购买4个芯片排列，像这样复杂的互连。因此，为了将简单明了的共享内存模型映射到在现实生活中具有高性能并适合二维或三维的可行方式上，需要进行大量的工程。


发言人   01:13:14
That answer your question? Yeah, that provides a lot of context, thank you. Okay? 
这就回答了你的问题？是的，这提供了很多背景，谢谢。好吗？

发言人   01:13:32
Right, any questions on the actual technique? 
对实际技术有什么问题吗？

发言人   01:13:40
All right, so as I'm sure you've gotten the sense, RCU is not universally applicable, there's not, you can't take every situation in which you're using spin locks and getting bad parallel performance and convert it to RCU and get better performance. Because the main reason it's completely doesn't help writes, makes them slower, and really only helps performance you. If the reads outnumber the writes considerably, it has this restriction that you can't hold pointers to protect the data across sleep, which just makes some kind of code quite awkward. If you actually need to sleep, you may then need to relook up whatever it is to do another RCU critical section after the sleep completes in order to look again for the data that you were originally were looking at, assuming it still even exists. So it just makes code a bit more complicated. 
好的，所以我相信你已经得到了感知，RCU并不是普遍适用的，不是，你不能接受你使用旋转锁和糟糕的并行性能的每种情况，并将其转换为RCU以获得更好的性能。因为它的主要原因完全无助于编写，使它们变得更慢，并且真的只有助于提高性能。如果读取数量远远超过写入数量，它就有这种限制，你不能持有指针来保护睡眠中的数据，这只会让某种代码非常尴尬。如果您确实需要睡眠，则可能需要在睡眠结束后重新查找另一个RCU关键部分，以便再次查找您最初查看的数据，假设它仍然存在。所以它只会使代码更加复杂一些。

发言人   01:14:36
The data structures, The most straightforward way to apply it is data structures that have a structure that's a minimal to single committing rights for updates. You can't modify things in place, so you have to replace stuff. And so list some trees, not more complex data structures. The paper mentions some more complicated ways, like sequence locks to be able to update stuff in place. You know, despite readers that aren't using locks, but they're going to a more complicated end. The situations under which they actually improved performance or more restricted. 
在数据结构方面，应用它最直接的方法是具有最新进展最小或单次提交权限的结构的数据结构。你不能就地修改东西，所以你必须替换东西。所以列出一些树，而不是更复杂的数据结构。这篇论文提到了一些更复杂的方法，比如序列锁，以便能够更新原地的东西。你知道，尽管读者不使用锁，但他们将走向更复杂的结局。他们实际上提高了绩效或受到更多限制的情况。

发言人   01:15:16
Another subtle problem is that readers can see stale data. Without any obvious bound on how long they can see stale data for because of some reader gets a pointer to an RCU protected object just before a writer replaces it, the reader may still hold on to that data for quite a long time, at least on the scale of modern computer instructions. And a lot of the time, this turns out not to matter much. But the paper mentions some situations which I actually don't really understand, in which people expect rights to actually take effect after the right completes, and therefore in which readers seeing stale data is a bit of a surprise. 
另一个微妙的问题是读者可能会看到陈旧的数据。如果没有任何明显的限制，他们可以看到过时数据的时间，因为某些阅读器在写入程序替换之前得到了一个指向RCU保护对象的指针，阅读器可能仍然保留该数据相当长的时间，至少在现代计算机指令的规模上。很多时候，这并不重要。但是这篇论文提到了一些我实际上不太理解的情况，人们期望权利在权利完成后真正生效，因此读者看到陈旧的数据有点意外。

发言人   01:16:15
You may also, as a separate topic, wonder what happens if you have write heavy data like RCU is all about read heavy data. But that's just one of many situations you might care about. We're parallel performance. You also care about right heavy data actually in the extremes in some extreme cases of rate heavy data, you can do quite well. There's no technique I know of for rate heavy data that's quite as universally applicable as RCU, but there are still ideas for coping with data that's mostly written. So the most powerful idea is to restructure your data, restructure the data structure so it's not shaped, and sometimes you can do that, sometimes the sharing is just completely gratuitous and you can get rid of it once you realize it's a problem. 
作为一个单独的主题，你也可能想知道如果你有像RCU这样写大量数据会发生什么，就像读大量数据一样。但这只是你可能关心的许多情况之一。我们是并行的表现。你也关心真正的重度数据，实际上在一些极端的情况下，你可以做得很好。就我所知，没有一种技术可以像RCU一样普遍适用，用于处理大量数据，但仍然有一些想法可以处理大多是编写的数据。所以最强大的想法是重组你的数据，重组数据结构，使其不成形，有时你可以这样做，有时共享完全是免费的，一旦你意识到这是一个问题，你可以摆脱它。

发言人   01:17:04
But it's also often the case that that while you do sometimes need to have shared data, that the common case doesn't require different core to write the same data, even though they need to write some of the data a lot, and so you've actually seen that in the labs, in the locking lab, in the klo part of the lab, you restructured the free list so that each core has a dedicated free list, thus converting a right heavy data structure, the free list into one that was sort of semi private per course. So most of the time course just have to don't conflict with other cores because they have their own private free list. And the only time you have to look at other free lists is if your free list runs out. So there's actually many examples of this way of dealing with right heavy data in the kernel, but of the allocator in Linux is like this. 
但是通常的情况是，虽然您有时需要共享数据，但常见情况并不需要不同的核心来写入相同的数据，即使它们需要写入大量的数据，所以您实际上在实验室中已经看到了这一点，在锁定实验室里，在实验室的klo部分，您重新调整了空闲列表的结构，使每个核心都有一个专用的空闲列表，从而将一个右重的数据结构，即空闲列表转换为一个每门课程半私有的数据结构。因此，大部分时间课程只需要不与其他核心冲突，因为它们有自己的私有空闲列表。唯一需要查看其他免费列表的时候就是您的免费列表用尽了。因此，实际上有很多这样的例子来处理内核中的大量数据，但是Linux中的分配器是这样的。

发言人   01:17:58
Linux is scheduling list. There's a sort of separate set of threads for each core that the scheduler looks at most of the time. And cores only have to look at each other's scheduling list if they run out of work to do. 
Linux是调度列表。每个核心都有一组独立的线程，调度器大部分时间都会查看它们。只有在工作耗尽时，和核心才需要查看彼此的调度列表。

发言人   01:18:14
Another example of statistics counters. If you're counting something and the counts go change a lot, but they're rarely read, that is the counter truly dominated by rights and not reads? You can restructure your counter so that each core has a separate counter, and so each chord just modifies its own counter when it needs to change the count. And if you want to read something, then you have to go out and lock and read all the per core counters. So that's a technique to make writes very fast because the writers just modify the local per core counter. But the reads are now very slow. And but if your counters are right heavy, as statistic counters often are, that could be a big win. Shifting the work now to the reads. 
另一个统计计数器的例子。如果你正在数一些东西，计数会发生很大的变化，但它们很少被阅读，那就是真正由权利主导而不被阅读的柜台？您可以重构计数器，使每个核心都有一个单独的计数器，因此每个和弦在需要更改计数时只是修改自己的计数器。如果你想阅读一些东西，那么你必须出去锁定并阅读所有每个内核的计数器。所以这是一种使写入速度非常快的技术，因为写入程序只是修改本地的每个核心计数器。但现在读取速度非常慢。但是如果你的计数器很重，就像统计计数器经常那样，那可能是一个巨大的胜利。现在将工作转移到阅读。

发言人   01:19:04
So the point is, there are techniques, even though we didn't talk about that much, there are also sometimes techniques that help for right intensive workloads. 
所以重点是，有一些技术，即使我们没有谈论太多，有时也有一些技术可以帮助处理右密集型工作负载。

发言人   01:19:16
To wrap up the RCU, the stuff we read about in the paper is actually a giant success story for Linux. It's used all over Linux to get at all kinds of different data because it just turns out and sort of read mostly data, read intensive data is extremely common, like cached file blocks, for example, mostly read. So a technique that speeds up only reads is really very widely applicable, and arsen uses particularly magic. There's lots of other interesting currency techniques, synchronization techniques. 
为了总结RCU，我们在论文中读到的内容实际上是Linux的一个巨大成功故事。它在Linux上用于获取各种不同的数据，因为它只是读取大部分数据，读取密集型数据非常常见，例如缓存文件块，大部分是读取的。因此，一种仅加速阅读的技术确实非常广泛适用，而阿尔森使用了特别的魔法。还有许多其他有趣的货币技术，同步技术。

发言人   01:19:53
RCU is magic because it completely eliminates locking and writing for the readers. So that was just like a big breakthrough compared to things like read write locks, which were the previous state of the art. And the key idea that really makes it work is the sort of garbage collection, like deferring a freeze for the what they call the grace period until all the readers are guaranteed to be finished using the data. So you can, as well as a synchronization technique, it's actually fair to view it very much so as a kind of specialized garbage collection technique. And that is all I have to say. So I'm happy to take questions. 
RCU很神奇，因为它完全消除了对读者的锁定和写入。因此，与之前的技术水平的读写锁相比，这就像是一个重大突破。而真正使它起作用的关键思想是垃圾收集的类型，例如将冻结推迟到他们所谓的宽限期，直到所有读者都能保证完成使用数据。因此，除了同步技术外，将其视为一种专门的垃圾收集技术实际上是非常公平的。这就是我要说的全部。所以我很乐意回答问题。

发言人   01:20:43
Oh sorry, can you explain the stale data for readers? So I don't understand why, how that can happen, because you're reading your critical section and you just get whatever data is there at that point, and then you just. It actually usually is not a problem. The reason why it ever might come up? Well, ordinarily, you know, if you have code that says x equals one, and then you print done. Gosh, it's pretty surprising if after this point, someone reading the data sees the value before you set it to one. That's a maybe a bit of a surprise, right? Well, there's a sense in which RCU allows that to happen. If this is really what we're really talking about is. 
抱歉，您能向读者解释一下这些陈旧的数据吗？所以我不明白为什么，为什么会发生这种情况，因为你正在阅读你的关键部分，那时你只是得到了那里的任何数据，然后你就&hellip;&hellip;这通常不是问题。它可能出现的原因是什么？通常，你知道，如果你有代码说x等于一，然后你打印完成。天哪，如果在这一点之后，有人在读取数据之前看到了值，那就相当令人惊讶了。这可能有点意外，对吧？嗯，有一个感知，RCU允许这种情况发生。如果这真的是我们真正在谈论的是什么。

发言人   01:21:52
Lists replace. Whatever, you know, find the element that has one in it and change it to two with, you know, using RCU right after that finishes. And we print. Oh, yeah, we're done. 
列表替换。无论如何，你知道，找到其中有一个的元素，并在完成后立即使用RCU将其更改为两个。我们打印出来。哦，是的，我们完成了。

发言人   01:22:07
If there's some reader that was looking at the list, right, they may have, you know, just gotten to the list element that held one that we replaced with two and then a good deal longer, you know, And then they do the actual read of the list element. You know, they look at whatever the content is in the list element. After we've done this, you know, they're reading the list element only at this point later in time and they see the old value. So if you're not prepared for this, so this is like a little bit odd now. I mean, they may even, they may even do a memory barrier, right? 
如果有读者正在查看列表，他们可能已经，你知道，刚刚进入保存了一个元素的列表元素，我们将其替换为两个，然后花费更长的时间，你知道，然后他们实际阅读列表元素。你知道，他们会查看列表元素中的任何内容。在我们这样做之后，你知道，他们只在稍后的这个时间点读取列表元素，他们会看到旧值。所以如果你没有为此做好准备，那么现在这就有点奇怪了。我的意思是，他们甚至可能，他们甚至可能做记忆障碍，对吧？

发言人   01:22:49
I mean, it's not a memory barrier issue. It's just like. And indeed, most of the time it doesn't matter, I see. So this is when this replace is very close. So like the read somehow like starts before the replace, but it just takes a while. 
我的意思是，这不是记忆障碍的问题。就像。事实上，大多数时候这并不重要，我明白了。所以这是替换非常接近的时候。所以像阅读在某种程度上就像在替换之前开始，但这只需要一段时间。


发言人   01:23:12
And if the reader is slower than the writer or something, now, you know, I think this mostly doesn't matter because after all, the reader and the writer were acting concurrently. And, you know, if two things happen concurrently, usually you would never have imagined that you could have been guaranteed much about the exact order if the two operations were invoked concurrently. 
如果读者比作者慢，现在，你知道，我认为这通常并不重要，因为毕竟读者和作者是同时行动的。而且，你知道，如果两件事情同时发生，通常你永远不会想到，如果这两个操作同时被调用，你可以得到很多关于确切顺序的保证。

发言人   01:23:42
The paper claims. I mean, the paper has an example in which they said it matters. It turned out to cause a real problem, although I don't really understand. Why that was, I see this makes sense. And my other question was, it's called RCU because of idea one, is that right? Read, copy, update? Yes, I believe it's because of idea one. That is that instead of modifying things in place, you make a copy and you sort of. Copy, not the not the real thing, right? This makes sense, thank you so much. 
报纸声称。我的意思是，这篇论文有一个例子，他们说这很重要。这导致了一个真正的问题，虽然我不是很理解。为什么会这样，我觉得这很感知。我的另一个问题是，由于第一个想法，它被称为RCU，对吗？阅读、复制、更新？是的，我相信这是因为第一个想法。也就是说，你不是在原地修改东西，而是复制一份。复制，不是真实的东西，对吗？这很感知，非常感谢。

发言人   01:24:26
So at the beginning of lecture or towards the beginning, we talked about the O n squared runtime for the cache coherence protocols for updating the read write locks. Isn't this also a problem with spin locks where, yeah, so, so what is the reason why we didn't discuss that aspect, why we didn't? Yeah, or like, is there a reason that that that still exists? Or like, what do spin locks do to address that? 
因此，在课程开始或接近开始时，我们讨论了用于更新读写锁的缓存一致性协议的O n平方运行时。这难道不是旋转锁的问题吗？是的，那么我们没有讨论这个方面的原因是什么，为什么我们没有这样做？是啊，或者说，那还有什么原因让它仍然存在？或者，旋转锁如何解决这个问题？

发言人   01:24:58
Oh, and locks are hideously expensive if they're or standard spin locks like XV 6 has are extremely fast if the lock is not particularly contended, and terribly slow if lots of chords try to get the same lock at the same time. Gotcha, this is one of the things that makes life interesting. And you know, there's. There's locks that are have better scaling, but worse that have better high load performance, but worse load load performance. But I'm not aware of a lock that is right. Anyway, it's hard. It's hard to get this stuff right. It's hard to get good performance in these machines. 
如果锁不是特别争用的话，那么锁的价格会高得离谱，或者像XV 6这样的标准旋转锁，如果锁不是特别争用的话，速度会非常快，如果很多和弦试图同时获得相同的锁，速度会非常慢。抓住你，这是让生活变得有趣的事情之一。你知道，有。有些锁具有更好的可扩展性，但更糟糕的是具有更好的高负载性能，但负载负载性能更差。但我不知道哪把锁是正确的。无论如何，这很难。把这些东西做好是很困难的。在这些机器中很难获得良好的性能。

发言人   01:25:51
Other questions? This might be unrelated, but can there ever be like lock-ins between multiple different systems? Like for like not just contained to one system, maybe multiple servers? Perhaps? 
其他问题？这可能是无关的，但在多个不同的系统之间是否存在锁定呢？例如，不仅仅包含在一个系统中，可能包含多个服务器？也许？

发言人   01:26:10
There are absolutely distributed systems in which there's a sort of locking. In which the sort of universe of locks spans multiple machines. One of places this comes up is in distributed databases, where you split your data over multiple servers. But if you want to have a transaction that uses data, that's different pieces of data on different servers, you're going to need to collect locks. You need to basically collect logs from multiple servers. 
有一些完全分布式的系统，其中有一种锁定。在这种情况下，锁的宇宙跨越多个机器。其中一个出现这种情况的地方是分布式数据库，你可以将数据拆分到多个服务器上。但是，如果您希望有一个使用数据的事务，这些数据在不同的服务器上，那么您将需要收集锁。你基本上需要从多个服务器收集日志。

发言人   01:26:44
Another place that comes up, although, well, there's been a number of systems that are essentially try to mimic shared memory across independent machines with machines. You know, if I use a memory that's in your machine, then if there's some infrastructure stuff that causes my machine to talk to your machine and ask for the memory and. You know, and the game is usually to run existing parallel programs on a cluster of workstations instead of on a big multiscope machine, hoping this is going to be cheaper and something needs to be done about spin locks there or whatever locking you're going to use. And so people have invented various ways if the locking work out well, in that case, too, using techniques that are not, you know, often not quite the same as this, although the pressure to avoid the pressure to avoid costs is even higher in that case. 
另一个出现的地方，尽管有许多系统本质上试图用机器模拟独立机器之间的共享内存。你知道，如果我使用你机器中的内存，那么如果有一些基础设施的东西导致我的机器与你的机器交谈并请求内存。你知道，游戏通常是在一组工作站上运行现有的并行程序，而不是在一个大的多范围机器上运行，希望这会更便宜，并且需要做一些关于旋转锁定或你要使用的任何锁定的事情。因此，如果锁定效果良好，人们发明了各种方法，在这种情况下，也使用与这种方法不完全相同的技术，尽管在这种情况下避免压力和避免成本的压力甚至更高。

发言人   01:27:54
Anything else? 
还有什么吗？

发言人   01:28:01
Thank, you're welcome. 
谢谢，不客气。
