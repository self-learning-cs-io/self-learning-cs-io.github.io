---
title: CMU15445 P10F202309 IndexConcurrencyControl
---

1
00:00:00,000 --> 00:00:14,300
Brad Paulson, DJ, 2PO!

2
00:00:14,300 --> 00:00:21,000
0-0...

3
00:00:21,000 --> 00:00:24,400
10 or 8 L.

4
00:00:24,400 --> 00:00:25,400
David EmilyLittle

5
00:00:25,399 --> 00:00:26,399
Balthus Rats.

6
00:00:26,399 --> 00:00:27,399
Balthus Rats.

7
00:00:27,399 --> 00:00:28,399
Hi, I'm Luke.

8
00:00:28,399 --> 00:00:31,399
I'm sorry, Ram Balthus and DJ TPL.

9
00:00:31,399 --> 00:00:33,399
Recipes 5thog, right?

10
00:00:33,399 --> 00:00:34,399
How's life?

11
00:00:34,399 --> 00:00:35,399
Good?

12
00:00:35,399 --> 00:00:36,399
You going with him?

13
00:00:36,399 --> 00:00:37,399
Yeah.

14
00:00:37,399 --> 00:00:43,399
Did you know that Jizzah, before the age of 27 had gang and his solo album under his belt already?

15
00:00:43,399 --> 00:00:46,399
I mean, yeah, the Wu-Tang clan people were pretty young.

16
00:00:46,399 --> 00:00:47,399
Yeah.

17
00:00:47,399 --> 00:00:49,399
But like, Rizzah's older, I think.

18
00:00:49,399 --> 00:00:50,399
Yeah.

19
00:00:50,399 --> 00:00:52,399
And yeah, his first album was Prince Rakeem.

20
00:00:52,399 --> 00:00:53,399
That was kind of goofy.

21
00:00:53,399 --> 00:00:54,399
But then you got to **** with Wu-Tang.

22
00:00:54,399 --> 00:00:55,399
Yeah.

23
00:00:55,399 --> 00:00:56,399
Yeah.

24
00:00:56,399 --> 00:00:58,399
There's no Wu-Tang questions on the exam this year.

25
00:00:58,399 --> 00:00:59,399
Previous years have been.

26
00:00:59,399 --> 00:01:01,399
So don't feel like you need to know these things.

27
00:01:01,399 --> 00:01:02,399
All right.

28
00:01:02,399 --> 00:01:03,399
Again, awesome.

29
00:01:03,399 --> 00:01:04,400
Thank you, DJ TPL.

30
00:01:04,400 --> 00:01:07,400
You guys, not to go over.

31
00:01:07,400 --> 00:01:08,400
The list is getting longer.

32
00:01:08,400 --> 00:01:11,400
So project one is do this Sunday at midnight.

33
00:01:11,400 --> 00:01:15,400
Again, we're having the special office hours on Saturday.

34
00:01:15,400 --> 00:01:16,400
That's in person.

35
00:01:16,400 --> 00:01:18,400
I think Nounson on Piazza.

36
00:01:18,400 --> 00:01:20,400
I think it's on the 5th floor.

37
00:01:20,400 --> 00:01:21,400
And one of the carols.

38
00:01:21,400 --> 00:01:24,400
Homework 2 is then bumped out to October 4th.

39
00:01:24,400 --> 00:01:26,400
It's a Wednesday.

40
00:01:26,400 --> 00:01:30,400
Make sure it's making out have to be the same due date as project one.

41
00:01:30,400 --> 00:01:32,400
Homework 3 will be out this week.

42
00:01:32,400 --> 00:01:34,400
That's going to do four days later.

43
00:01:34,400 --> 00:01:35,400
And again, all right, that kind of sucks.

44
00:01:35,400 --> 00:01:36,400
That it kind of cram it so quickly.

45
00:01:36,400 --> 00:01:41,400
But because the midterm exam, which will cover things.

46
00:01:41,400 --> 00:01:47,400
The will cover things that in homework 3, we want to get that back to you guys and graded.

47
00:01:47,400 --> 00:01:50,400
Before the midterm exam on Wednesday.

48
00:01:50,400 --> 00:01:54,400
So midterm is going to be in class here on October 11th.

49
00:01:54,400 --> 00:01:55,400
It's a Wednesday.

50
00:01:55,400 --> 00:01:57,400
And we're in the regular class time.

51
00:01:57,400 --> 00:01:59,400
If you need accommodations, please email us.

52
00:01:59,400 --> 00:02:03,400
And so we can start organizing and taking care of the logistics.

53
00:02:03,400 --> 00:02:04,400
Don't post on Piazza.

54
00:02:04,400 --> 00:02:05,400
Yes.

55
00:02:05,400 --> 00:02:08,400
Is it possible to work 3 absolutely a bit earlier?

56
00:02:08,400 --> 00:02:10,400
I'm trying to get homework 3 out now.

57
00:02:10,400 --> 00:02:11,400
It should be out today.

58
00:02:11,400 --> 00:02:12,400
I don't know if it's about today.

59
00:02:12,400 --> 00:02:13,400
Yes.

60
00:02:13,400 --> 00:02:14,400
That's the plan.

61
00:02:14,400 --> 00:02:16,400
But the trouble is like it doesn't cover things.

62
00:02:16,400 --> 00:02:19,400
It's going to cover like sorting and joins, which will cover like next week.

63
00:02:19,400 --> 00:02:22,400
Any questions about any of these things?

64
00:02:22,400 --> 00:02:23,400
Yes.

65
00:02:23,400 --> 00:02:27,400
Is there any idea of what to work to object?

66
00:02:27,400 --> 00:02:29,400
It will be in the future.

67
00:02:29,400 --> 00:02:31,400
It's whatever it says on the website now.

68
00:02:31,400 --> 00:02:33,400
Is the current plan.

69
00:02:33,400 --> 00:02:34,400
Yes.

70
00:02:34,400 --> 00:02:35,400
Yes.

71
00:02:35,400 --> 00:02:41,400
The only thing about it is that it's going to work for the end of fall break.

72
00:02:41,400 --> 00:02:42,400
At the end of fall break?

73
00:02:42,400 --> 00:02:43,400
All right.

74
00:02:43,400 --> 00:02:45,400
We'll go double check that.

75
00:02:45,400 --> 00:02:46,400
I thought we moved it.

76
00:02:46,400 --> 00:02:48,400
So we didn't have to do it over in fall break.

77
00:02:48,400 --> 00:02:49,400
Yes.

78
00:02:49,400 --> 00:02:51,400
We will take care of that.

79
00:02:51,400 --> 00:02:52,400
Other questions?

80
00:02:52,400 --> 00:02:55,400
All right.

81
00:02:55,400 --> 00:02:56,400
Cool.

82
00:02:56,400 --> 00:02:57,400
All right.

83
00:02:57,400 --> 00:03:00,400
So the last two classes we've talked about data structures.

84
00:03:00,400 --> 00:03:03,400
We talked about hash tables and then we talked about B plus trees.

85
00:03:03,400 --> 00:03:08,400
And I prefaced our conversation going into discussing the hash tables B plus trees

86
00:03:08,400 --> 00:03:15,400
stuff to say that to simplify the discussion and the explanation of these data structures

87
00:03:15,400 --> 00:03:20,400
and the algorithms that are used to manipulate them or work with them.

88
00:03:20,400 --> 00:03:25,400
We're going to assume that it's single threaded because that just makes your life easier.

89
00:03:25,400 --> 00:03:34,400
But of course in any modern system in today's hardware, you need to support multiple threads or multiple workers running at the same time.

90
00:03:34,400 --> 00:03:38,400
Again, I'm going to try to use the word workers because that can mean either thread or process.

91
00:03:38,400 --> 00:03:39,400
Postgres is not multi-threaded.

92
00:03:39,400 --> 00:03:40,400
It's multi-process.

93
00:03:40,400 --> 00:03:43,400
Most modern systems are multi-threaded but the idea is the same.

94
00:03:43,400 --> 00:03:48,400
But still again, we want to be able to have multiple workers running at the same time,

95
00:03:48,400 --> 00:03:54,400
be able to access these data structures so that if one of them has to stall because they're going to disk,

96
00:03:54,400 --> 00:03:58,400
we can have other workers run at the same time and do useful things.

97
00:03:58,400 --> 00:03:59,400
Right?

98
00:03:59,400 --> 00:04:01,400
The system will look very unresponsive.

99
00:04:01,400 --> 00:04:04,400
If you only have a single worker, again, assuming it's a thread,

100
00:04:04,400 --> 00:04:07,400
and then I'm going to go access and run some query.

101
00:04:07,400 --> 00:04:11,400
And then as soon as I have to go my page table and the thing I don't need is the page I need isn't there,

102
00:04:11,400 --> 00:04:14,400
I have to stall because I have to go to disk and get it.

103
00:04:14,400 --> 00:04:16,399
Well, why that while we're stalled, the CPU essentially stalled,

104
00:04:16,399 --> 00:04:19,399
we can have other threads, other workers do useful things.

105
00:04:19,399 --> 00:04:25,399
So that's the goal of what we're going to talk about today is how do you actually make these data structures thread safe.

106
00:04:25,399 --> 00:04:29,399
And so while I'll say is that this is how most systems are going to be implemented,

107
00:04:29,399 --> 00:04:33,399
most systems will try to take advantage of multiple threads.

108
00:04:33,399 --> 00:04:38,399
There's a sort of category of systems that actually don't do any of the things we're going to talk about today.

109
00:04:39,399 --> 00:04:43,399
And the most famous one is play a redis that it's a single process, single thread.

110
00:04:43,399 --> 00:04:49,399
So all the latching stuff we're going to talk about today, they don't have to do because they know no other threads running at the same time.

111
00:04:49,399 --> 00:04:54,399
There'll be other systems, although they'll still be multi-threaded,

112
00:04:54,399 --> 00:04:59,399
but they'll maybe have only one writer thread, but multiple reader threads running at the same time.

113
00:04:59,399 --> 00:05:03,399
And that simplifies a bunch of things, but you still need the latching protections that we're going to talk about today.

114
00:05:03,399 --> 00:05:04,399
Okay?

115
00:05:05,399 --> 00:05:14,399
So the thing that we're going to use to enforce the threads or workers to behave a certain way

116
00:05:14,399 --> 00:05:21,399
so that we don't end up with corrupted data and invalid data structures is going to be called a concurrency tool protocol.

117
00:05:21,399 --> 00:05:25,399
And again, for today's class, we're going to see how we do this for workers.

118
00:05:25,399 --> 00:05:31,399
After the midterm we'll discuss how we use concurrency tool to coordinate transactions.

119
00:05:31,399 --> 00:05:43,399
And so you can sort of think of this currency tool protocol as the traffic cop of the system that allows you to tell different workers who's allowed to do what at what given time.

120
00:05:43,399 --> 00:05:50,399
Right? And the idea is that we're going to be operating on some shared object or some critical section, and we don't want to have them interfering with each other and cause problems.

121
00:05:50,399 --> 00:05:55,399
And the two types of problems we could have are logical correctness and physical correctness.

122
00:05:55,399 --> 00:05:58,399
And I think I mentioned this last week as well.

123
00:05:58,399 --> 00:06:10,399
So the logical correctness, the idea is that if I insert a key into my B plus tree, I insert key five, and then if I come back and try to look for key five, I should see it.

124
00:06:10,399 --> 00:06:16,399
What the other thing I said was if I delete key five, and I come back and try to look for key five again, I shouldn't see it.

125
00:06:16,399 --> 00:06:22,399
So at a logical level, we want to make sure that we're seeing the things we should see in our data structures.

126
00:06:22,399 --> 00:06:38,399
The thing that we care about in today's class is the physical correctness, meaning how do we ensure that if we're walking through a hash table or traversing the B plus tree, and at some point we've got to follow a pointer, like a page ID to take us somewhere else, that page ID is correct.

127
00:06:38,399 --> 00:06:43,399
Like it's not going to take us to a table page that has a bunch of garbage in it.

128
00:06:43,399 --> 00:06:55,399
Because what happens if you go follow page and you start looking at data that doesn't look like you expect to look like, you're going to have a seg fall because you're going to try to read back past some buffer or things things are going to break or you get corrupted data.

129
00:06:55,399 --> 00:06:58,399
So again, logical correctness, we'll worry about later at the midterm.

130
00:06:58,399 --> 00:07:01,399
Today's class is really about physical correctness.

131
00:07:01,399 --> 00:07:09,399
So I first want to describe, go every quickly of what, what latches are again, and how do you actually implement them inside of a database system.

132
00:07:09,399 --> 00:07:16,399
And again, the takeaway here is that we don't want to ideally, we don't want to rely on the operating system, gives us in terms of latches.

133
00:07:16,399 --> 00:07:20,399
Then we'll see a simplified example of how do you hash table latching.

134
00:07:20,399 --> 00:07:26,399
As I said, most of our time doing B plus tree latching, and we'll see a sort of a basic version and an optimized version.

135
00:07:26,399 --> 00:07:31,399
And then we'll finish off talking about how to handle leaf node scans.

136
00:07:31,399 --> 00:07:34,399
Okay.

137
00:07:34,399 --> 00:07:38,399
All right, so I think I showed this slide before, and again, I just want to revisit it again.

138
00:07:38,399 --> 00:07:41,399
This destates between locks and latches.

139
00:07:41,399 --> 00:07:50,399
And again, if you're coming from the OS world or distributed world, they might mean when I say latch, they might think lock.

140
00:07:50,399 --> 00:07:55,399
But in database, that's what we care about mostly in my life, and in this course.

141
00:07:55,399 --> 00:07:59,399
So we need to make sure we understand what we're talking about when we say lock versus the latch.

142
00:07:59,399 --> 00:08:10,399
So locks going to be this high level protection primitive that allows us to protect the logical contents of our database, like a tuple, a database, a table.

143
00:08:10,399 --> 00:08:21,399
Right. And the when we acquire one of these locks, the transaction will hold that lock for the duration of that transaction.

144
00:08:21,399 --> 00:08:28,399
It's not always true. We'll see examples where we can release locks maybe early, but for our purposes today, we'll assume that's the case.

145
00:08:28,399 --> 00:08:36,399
And then there'll be some higher level mechanism within our concurrently protocol that's going to ensure that we don't have any deadlocks.

146
00:08:36,399 --> 00:08:47,399
And then if a deadlock does arise, then the database is how a mechanism to be able to roll back the changes that the transaction made to make it look as if it didn't make any changes.

147
00:08:47,399 --> 00:08:51,399
So we don't we don't have any partial updates.

148
00:08:51,399 --> 00:09:02,399
Today, we're focused on latches. And so the latches are going to be the low level parameters that were used to protect critical sections in our data structures from one worker versus, you know, against another.

149
00:09:02,399 --> 00:09:14,399
And so the duration of the lifetime that we're going to hold a latch is going to be very short. Like go like finger critical section. I'm going to go take a latch on a page, make some change and then release that latch immediately.

150
00:09:14,399 --> 00:09:26,399
And because it's going to be very simple, we know when we minimize amount of bookkeeping we're taking for these latches, we don't want to have we don't the database is not going to be automatically rolled back any changes for us.

151
00:09:26,399 --> 00:09:38,399
We don't avoid deadlocks and try to not make any changes unless we quiet a latch for something. So that we don't have the roll things back. We're going to have sort of minimal coordination between the between the different workers running at the same time.

152
00:09:38,399 --> 00:09:47,399
Whereas in the lot case, I mean, I'm jumping ahead of it. There will be a table literally internally called a lock table and you go look in there, you can see who holds locks for different objects.

153
00:09:47,399 --> 00:09:56,399
In latches, we don't want to maintain any of that because that's so expensive relative to the amount of work we want to do within a critical section in our data structures.

154
00:09:56,399 --> 00:10:06,399
So there's this table I also like from the book I recommended last time from this guy, Gritz graphy, the B tree book. When he again, he shows this this changed between the locks versus latches.

155
00:10:06,399 --> 00:10:15,399
And the way to sort of to read this table is within a column, you read down and say, you know, what the thing is protecting how it's protecting it and the different ways is protecting things.

156
00:10:15,399 --> 00:10:24,399
So for example, a lock is going to separate transactions from each other and it's going to protect the logical database contents pages or sorry, tuples, tables, databases.

157
00:10:24,399 --> 00:10:36,399
And we can hold them for the entire length of the transaction. We'll talk about modes in a second. We can take a lock for an object in different modes like exclusive shared and attention updates. We'll get with it later.

158
00:10:36,399 --> 00:10:45,399
And then the data system provide you know, deadlock detection or deadlock prevention mechanisms built in to avoid these problems. And then again, these are the mechanisms to do this.

159
00:10:45,399 --> 00:10:52,399
And then the information that we're going to keep track of what locks are being held is being kept in a lock manager, a centralized data structure.

160
00:10:53,399 --> 00:11:04,399
Today again, we're focused on latches. So latches are going to protect workers from each other. This will be only for in memory data structures. So like this is literally for like the you know B plus trees in memory.

161
00:11:04,399 --> 00:11:14,399
But once is I you know if a page within our B plus tree gets flush out the disk, I wouldn't hold the latch for that thing when it goes out to disk because it's meaningless.

162
00:11:15,399 --> 00:11:28,399
It's protecting the critical sections. There's only two modes. We can hold our latches in reading right. And the way we're going to handle deadlocks is through coding discipline, but us as the systems developers have to write good code to make sure there's no deadlocks.

163
00:11:28,399 --> 00:11:35,399
It's easier set and then done sure, but like there's not going to be something some other part of the system that's going to bail us out.

164
00:11:35,399 --> 00:11:44,399
And we're going to keep the information about these latches are actually embedded in the data structure itself. So there won't be a centralized centralized thing.

165
00:11:44,399 --> 00:11:53,399
Again, this make more sense when we start walking through the different data structure types. And so the lock stuff will color in after the term in lecture 15.

166
00:11:54,399 --> 00:12:09,399
All right, so our latches only have two modes. It can be their read mode or a right mode. So read mode are their commutative operations where you can have multiple workers take a latch in your read mode at the same time because you know whatever they're doing isn't going to.

167
00:12:10,399 --> 00:12:21,399
It isn't going to break the whatever the data structure is or calls any conflicts. If I need to, if two workers need to read the same page, I can take that in read mode.

168
00:12:21,399 --> 00:12:27,399
Well, that doesn't they're not doing right. So it doesn't break anything. So I can go ahead and have them both run at the same time.

169
00:12:28,399 --> 00:12:35,399
Right mode or exclusive mode is when you know one thread is is wants to access the object and actually make changes to it.

170
00:12:35,399 --> 00:12:47,399
And I don't want any other threads to run the same to operate on my object at the same time. So only one only one worker can hold the latch in and right mode and that blocks everyone else out.

171
00:12:48,399 --> 00:12:57,399
And a really simple compatibility matrix will look like this. If I have a read mode, if I have a latch in a read mode, someone wants to get a latch in read mode, I can do that. That's allowed.

172
00:12:57,399 --> 00:13:06,399
But any other combination where at least one of the latches either holds in a right mode or wants to get it in right mode, I have to deny that.

173
00:13:06,399 --> 00:13:20,399
So again, going back to what I said before coding discipline, like the stupidest thing to do is to take every latch in right mode, even though you're only going to read it, it'll protect all your data structures, but like you, you know, it's basically going to get relegated to a single thread system.

174
00:13:20,399 --> 00:13:30,399
And likewise, if I take my latch in read mode, but I start making changes to whatever it's protecting, then that's that's our fault. That's the programmer's fault and it's just crashes. That's on us.

175
00:13:31,399 --> 00:13:44,399
And there isn't any without getting into like verifiable languages, there isn't really any mechanism in see-pump slots or rust that's going to protect us for these things.

176
00:13:44,399 --> 00:13:47,399
Right? Because the compiler can't know.

177
00:13:47,399 --> 00:13:52,399
All right, so let's look at how you want implement latches.

178
00:13:52,399 --> 00:14:04,399
So ideally, we want to latch that has a small memory footprint because we don't want to store a lot of a lot of additional metadata for a latch because we're again, these are being embedded in the data structure itself.

179
00:14:04,399 --> 00:14:16,399
And ideally, we want to have it be when there's no contention in the system, meaning there's no two threads or workers trying to acquire latch at the same time, we want to go as fast as possible, minimum, minimum overhead.

180
00:14:16,399 --> 00:14:27,399
Now, I acquired the latch and do my thing right away. If we can't get the latch we need, then we have to make a decision of how long we should wait and how we want to wait.

181
00:14:27,399 --> 00:14:32,399
And we'll see different scenarios how we want to do this.

182
00:14:32,399 --> 00:14:45,399
And ideally, also, too, we don't want to have a bunch of metadata per latch about like who's waiting for this latch because that's now basically a queue for every single latch you could have in your data structure.

183
00:14:45,399 --> 00:14:54,399
And I think of like a giant D plus tree with a billion entries, how many pages you're going to have in there, each of those could have now their own priority queue.

184
00:14:55,399 --> 00:15:07,399
So again, coming from the database world, we say we don't want to align the OS to do any of these things, but then the OS people say the data don't know what they're doing and they should not be implementing their own latches.

185
00:15:07,399 --> 00:15:21,399
And you can see this in the Linux mailing list. So here's a post from Linus saying like, oh yeah, like, you know, don't you should not be writing your own latching thing and basically says here, like, you should not use spin locks, but I'll clean that into the second in user space.

186
00:15:21,399 --> 00:15:29,399
That's us where the data system running user space says you should not be using spin locks, you roll yourself unless you know what you're doing and the chances are you know what you're doing is low.

187
00:15:29,399 --> 00:15:34,399
He's wrong despite being Linus, right?

188
00:15:34,399 --> 00:15:40,399
So I'll go through three basic implementations of how to implement latches.

189
00:15:40,399 --> 00:15:49,399
There's more advanced ones like the parking lot stuff from Apple. This is probably the best one to use right now. And then there's the MCS locks, which is a queuing thing.

190
00:15:49,399 --> 00:15:57,399
We'll cover this in the advanced class, but for our purpose here, we don't need to know this, but we need to understand the basic implementation of what latches are actually doing.

191
00:15:57,399 --> 00:16:02,399
So that when you start sprinkling them in your code, you understand the ramifications of them. Yes.

192
00:16:02,399 --> 00:16:06,399
What's the question? What's the question?

193
00:16:06,399 --> 00:16:11,399
I mean, P thread mute taxes? We'll get to that in a second. Yes. Her question is, what's wrong with C++ mute taxes?

194
00:16:11,399 --> 00:16:15,399
So if you call mute tax in C++, what do you actually get?

195
00:16:15,399 --> 00:16:17,399
What?

196
00:16:17,399 --> 00:16:21,399
Yeah, but like, but like, who's implementing that?

197
00:16:21,399 --> 00:16:25,399
P thread. But how does the P thread work?

198
00:16:25,399 --> 00:16:28,399
Two more slides. Okay.

199
00:16:28,399 --> 00:16:33,399
Okay.

200
00:16:33,399 --> 00:16:38,399
All right. So, actually, maybe there's just a slide. All right. So the most basic, all right.

201
00:16:38,399 --> 00:16:41,399
I take that. This is this is Davis World, which is probably the OS one next.

202
00:16:41,399 --> 00:16:47,399
So the most basic latch you can implement is called a test and set spin lock.

203
00:16:47,399 --> 00:16:52,399
It's called a spin lock. And I realize I'm calling it spin locks when they're really latches, right?

204
00:16:52,399 --> 00:16:59,399
But this is the most simplest way to implement this is because it literally is a 64-bit memory address

205
00:16:59,399 --> 00:17:04,400
that you're just going to do an atomic compare and swap on to see whether you can set it.

206
00:17:04,400 --> 00:17:07,400
And if you can't set it, then you spin and keep trying to set it over and over again.

207
00:17:07,400 --> 00:17:11,400
Like, the code would literally look like this. I declare a ontonic bullion.

208
00:17:11,400 --> 00:17:15,400
This is just a syntactic sugar for declaring something as atomic.

209
00:17:15,400 --> 00:17:18,400
And then now I have this my latch here, and I call it test and set.

210
00:17:18,400 --> 00:17:23,400
And literally is just trying to set a check to see whether it's set current value zero.

211
00:17:23,400 --> 00:17:29,400
If yes, then I can set it to one. And I can do that in a single instruction automatically.

212
00:17:29,400 --> 00:17:34,400
So it's not like if then this, then that and somebody else can come and swoop in and change it before I can.

213
00:17:34,400 --> 00:17:38,400
Before I can. It literally is one instruction to go and apply this change.

214
00:17:38,400 --> 00:17:42,400
And if I can't get it, then I just spin. Because we're doing this in the database level.

215
00:17:42,400 --> 00:17:50,400
And user space, we get aside whether how many times you want to retried, whether you want to yield the threat to the OS or abort ourselves and restart.

216
00:17:50,400 --> 00:17:56,400
All right. Why is this bad?

217
00:17:56,400 --> 00:18:01,400
What's that? Wait. Sorry, I had, I had, I had, I had, you said, you're busy waiting.

218
00:18:01,400 --> 00:18:06,400
Yeah, so you're basically spinning the cycle, spinning the CPU. Check, check, check, check over and over again.

219
00:18:06,400 --> 00:18:12,400
But I can put maybe like, I could put a, you know, exponential back off to say, okay, I try to get it. I couldn't get it.

220
00:18:12,400 --> 00:18:17,400
Wait, one millisecond, two milliseconds, four milliseconds, right?

221
00:18:17,400 --> 00:18:22,400
Actually, the, the, that's a challenge that you're just spinning over and over again.

222
00:18:22,400 --> 00:18:31,400
Another problem is going to be cash coherence traffic, right? So again, assuming I have a two-stock at CPU, the, the latch I'm trying to acquire is over here on this, this Numa region.

223
00:18:31,400 --> 00:18:44,400
That way, I know what Numa is. Not uniform memory access. Basically, like if I have two sockets in my C, or two, two sockets or more, two or more sockets in my motherboard, each CPU socket is going to have DRAM that's close to it.

224
00:18:44,400 --> 00:18:52,400
And it's really fast to talk to that. But I can also talk to memory that's on, that's over on another socket, they call it a Numa region.

225
00:18:52,400 --> 00:18:57,400
But that, that traffic is much slower because I got to go over this interconnect between one socket to the other, right?

226
00:18:58,400 --> 00:19:06,400
And so, Intel, they do a lot of work to make sure like you, when you write programs, you don't know and, and don't technically have to know where memory actually is physically located.

227
00:19:06,400 --> 00:19:11,400
But of course, now you could have your program access something that's on another socket and it gets really, really slow.

228
00:19:11,400 --> 00:19:16,400
And the whole world tries to play games and move things around for you to try to speed things up. We can ignore that for now.

229
00:19:16,400 --> 00:19:26,400
And for this class, we don't have to worry about Numa. I just want to explain that like this, this, this, there's a worker running on this pot, this CPU over here wants to acquire the latch on this, in this, this other socket,

230
00:19:26,400 --> 00:19:33,400
CPU memory. So it's going to keep spinning it over again. But now like it's, it's all this traffic over this interconnect.

231
00:19:33,400 --> 00:19:44,400
That's going to slow my, my entire database system down, right? So this is inefficient because I'm spinning, but also like the traffic on the actual hardware itself is, is expensive.

232
00:19:44,400 --> 00:19:51,400
So now her question is, you know, why, what's wrong or how does actually the, the C++ view text actually work?

233
00:19:51,400 --> 00:20:03,400
So this is called a blocking mutex. It's the easiest thing to use because it's going to fill in C++. And it basically you acquire and release it's, there's not a lot of mechanisms in it.

234
00:20:03,400 --> 00:20:09,400
And the way you use it is sort of like this, like you lock it and unlock, do whatever you want in the middle.

235
00:20:09,400 --> 00:20:14,400
So I asked her how, how is this thing actually implemented? Does anybody know?

236
00:20:14,400 --> 00:20:18,400
So if you call it SD mutex, what do you get into the C++?

237
00:20:19,400 --> 00:20:25,400
Pthread mutex. What is how is Pthread mutex implemented?

238
00:20:25,400 --> 00:20:31,400
It's got a few text, it's in Linux. Anybody heard of a few texts before?

239
00:20:31,400 --> 00:20:41,400
Fast user space mutex. So the way it works is, it has the spin lock that I just showed in the last slide. In user space, they'll have their own little test and set thing you can do.

240
00:20:41,400 --> 00:20:49,400
But if you can't, if you try to acquire it and you can't, then you fall back to a heavy, heavy weight mutex inside the kernel.

241
00:20:49,400 --> 00:21:01,400
So, so if the, if the, no one holds the latch with a few text, I try to acquire it. If no one holds it, then I just do a compare and swap real, real fast in user space and I'm done it and I, my program or my thread keeps running.

242
00:21:01,400 --> 00:21:12,400
If I can't get it, then the OS takes control of us and we go down now into the kernel and then we get descheduled because it knows that I can't run until the thing I'm waiting for is available.

243
00:21:12,400 --> 00:21:18,400
But what's down in the kernel? How they keep track of threads? What's that?

244
00:21:18,400 --> 00:21:19,400
Sorry to say that?

245
00:21:19,400 --> 00:21:32,400
Blocking queue, but like what is like there's also a, wait no, there's a schedule has its own hash table to keep track of like what threads are running. So like and they have to be using their own latches to protect that data structure.

246
00:21:32,400 --> 00:21:38,400
So if I can't acquire this, I go down to the kernel and I get descheduled and that's very expensive. Cis calls are expensive. We want to avoid them.

247
00:21:38,400 --> 00:21:46,400
Right? Right. So actually this is just the diagram like this. So again, I have two my two two workers running different sockets. They both try to quiet at the same time.

248
00:21:46,400 --> 00:21:52,400
One of them will get the user space latch. The other guy tries to go down to the OS latch and they get desched. Right?

249
00:21:52,400 --> 00:21:59,400
And again, this is this is slow because any time you involve the OS, this is bad.

250
00:21:59,400 --> 00:22:05,400
So the last two, first two latches I showed you, they didn't really have modes. It was just like all or nothing.

251
00:22:05,400 --> 00:22:19,400
And so the way you implement this in a, in, in C++ with the reader writer latch, you can use shared mutex. I think we do the read write lock in bus tub, which is just a p thread rewrite lock.

252
00:22:19,400 --> 00:22:33,400
And the way this basically works is that the latch itself is going to have its own priority queues, its own counters, keep track of like how many threads are waiting. You actually can find the scheduling policy for the latch itself.

253
00:22:33,400 --> 00:22:45,400
So the idea here is that if I have a, a reader thread comes along and wants to acquire the latch, I go check to see whether anybody's waiting for the read latch or the right, the latch reading the read mode or write mode.

254
00:22:45,400 --> 00:22:52,400
If it's available, then I increment my counter to say somebody's holding the read latch now and I go ahead and do whatever I want.

255
00:22:52,400 --> 00:23:01,400
And now if anybody else comes along also wants to acquire the read mode, the system knows I'm, the latch knows I'm in read mode right now, so it can let the other guy run as well.

256
00:23:01,400 --> 00:23:09,400
But now for, for a write, write worker comes along, tries to acquire the latch where we have to read, read workers already holding the latch and read mode.

257
00:23:09,400 --> 00:23:15,400
So it's going to have to stall and they maintain an internal priority queue to keep track of like what threads are waiting for this.

258
00:23:15,400 --> 00:23:27,400
So then depending on the policy you can figure in the latch, if another thread comes along, then once I get in read mode, and in theory, I could, I could acquire it because it's commutative with all the other latches or the workers that hold it in read mode.

259
00:23:27,400 --> 00:23:37,400
I could acquire it right away, but you can set the policy to say I know another thread is waiting for it in write mode, so let me go ahead and put it to sleep.

260
00:23:37,400 --> 00:23:51,400
And in C++, I think they're doing this all in user space, not down on the kernel, but when you have to then block and wait for the choir to latch your looking for, then that's going to be an OS, or OS, new text, which we don't want to do.

261
00:23:51,400 --> 00:24:03,400
So the, just getting to show you a high level overview of the test and set operation, the compare and swap is the basing built and block we used to build more complicated latch primitives.

262
00:24:03,400 --> 00:24:15,400
And depending on not where you want the OS to do it or not, most systems, most, most, most bigger database systems, the enterprise ones will not rely on the OS for anything.

263
00:24:15,400 --> 00:24:22,400
And it's a combination for portability and also it's just faster to avoid the OS and line us as wrong.

264
00:24:22,400 --> 00:24:27,400
Okay. So now let's, sorry, yes.

265
00:24:27,400 --> 00:24:34,400
The question is, is parking mute text, Apple, is that bell on the set all the more? Yes.

266
00:24:34,400 --> 00:24:38,400
Yeah, like that's the, do you ever know what compare and swap is?

267
00:24:38,400 --> 00:24:42,400
No, okay. I have slides.

268
00:24:42,400 --> 00:24:57,400
One slide. All right. So compare and swap is this atomic destruction that, that modern CPUs provide that allow you to check a memory location to see whether it's a current, whether the current value of that memory address is what you expected to be.

269
00:24:57,400 --> 00:25:03,400
And if it is, then I can go ahead and overwrite it with my new value and again, in a single instruction.

270
00:25:03,400 --> 00:25:09,400
And think if you had to do this in C++ code to be like, if the value equals this, then set it to that.

271
00:25:09,400 --> 00:25:18,400
But again, if that was just the actual instructions to do that, that would be multiple instructions. And by the time you go check to see whether it's that value is, you know, is a full, the flag set the true.

272
00:25:18,400 --> 00:25:24,400
By the time you go and then go update it, somebody else might have squeaked in, and sneak in before you did and updated before before.

273
00:25:24,400 --> 00:25:32,400
And so on modern CPUs, you can do this in a single instruction that's atomic to guarantee that, but when you check it and set it, no one else can get in before you do.

274
00:25:32,400 --> 00:25:40,400
And then that's the basic perimeter allows to do more complicated things. So there's a bunch of different trends in C++. You can use for this.

275
00:25:40,400 --> 00:25:50,400
They have different versions of this, like someone, like if the test and set can succeed, or the parents opposite seed, they'll return back the old value or the new value or true and false.

276
00:25:50,400 --> 00:25:57,400
They're all basically doing the same thing. So let's say in this case here, for this in trends, I'm saying, here's the address I want to check and assume it's a 64 bit integer.

277
00:25:57,400 --> 00:26:04,400
Here's the value that I want to see whether it's currently set to. And then if it is, here's the value I want you to set it to right now.

278
00:26:04,400 --> 00:26:11,400
So we jumped to this memory address here. It's 20, just 20 equal 20. Yes, then I can go ahead and overwrite it with 30.

279
00:26:11,400 --> 00:26:19,400
Pretty simple. But that's again, that's the building block we need to have all, you know, to build our all our more complicated latches.

280
00:26:20,400 --> 00:26:26,400
I don't know when this comparison swap stuff was added. I think it was like the late 90s, at least in x86.

281
00:26:26,400 --> 00:26:31,400
So we good? Okay, cool. All right.

282
00:26:31,400 --> 00:26:36,400
So let's see how we can do this for hash tables now or use latch and hash tables.

283
00:26:36,400 --> 00:26:46,400
So hash tables are going to be easy to sport because assuming we're going to linear pro-passion is that there's only certain many ways you can actually access the hash table.

284
00:26:46,400 --> 00:26:59,400
I have, assuming linear linear pro-passion, I hashed some location into my hash array or hash table and then I scanned down from top to down looking for the entries I'm looking, you know, that I need.

285
00:26:59,400 --> 00:27:14,400
And in this case here, because the threads are all moving in the same direction, like going top down, even though they may start at different locations in the hash table, I can't have any deadlocks because there isn't one thread going top down, I thought thread going bottom up.

286
00:27:14,400 --> 00:27:26,400
Right? So the question is going to be to what granularity do we want to have our latch to protect our data structure because that's going to determine the amount of parallelism will be it'll support.

287
00:27:26,400 --> 00:27:39,400
For this lecture, we're going to ignore how to handle resizing the table. The way, the simplest way to handle that is you have a sort of a right latch that protects the access to the data structure itself.

288
00:27:39,400 --> 00:27:51,400
So if I get full when I need to double the size of it, I just switch that latch into right mode and then do my resizing and that prevents everybody else from coming in. That's the easiest way to do this.

289
00:27:51,400 --> 00:27:58,400
So the scopes are our latches can either be within a page or slot. And again, this is going to determine amount of parallelism we have.

290
00:27:58,400 --> 00:28:09,400
So obviously, when a page latch is going to protect the entire page itself with a latch, and no matter whether you want to read one entry or all the entries in the page, you would you would hold a latch on the entire thing.

291
00:28:09,400 --> 00:28:22,400
The alternative would be the you would have a latch for every single slot in a page. And this is going to have you allow more fine grain access, but again, now the the challenges that I can take more space because every single slot needs to have a latch.

292
00:28:22,400 --> 00:28:29,400
And now as I'm scanning through my my hash table, I got to acquire the latch for every single slot as I'm going along.

293
00:28:29,400 --> 00:28:38,400
So again, there's no free lunch in systems or computer science. It's either I have a single latch per page, which I don't have to acquire once for the page.

294
00:28:38,400 --> 00:28:46,400
And it doesn't take a lot of space, but then it blocks a run-else out from a deep entire page or I have it for every single slot.

295
00:28:46,400 --> 00:28:55,400
So say real simple hash table like this. T1 wants to come along, thread 1 wants to come along and find D. If we hash D, we land a little location here.

296
00:28:55,400 --> 00:29:05,400
We get the entire page in in in right mode. Look, try to find the the entries we're looking for by just scanning down, but at the same time another thread wants to come along once the insert E.

297
00:29:05,400 --> 00:29:14,400
Same thing I hashed to this page, but the latch is already the page is already latched in in read mode. And that's not commutative with the right mode latch needs to do the insert.

298
00:29:14,400 --> 00:29:23,400
So we'll have to stall thread 2. I think whether it's spinning in user space or got to schedule by the kernel, that depends on your latch implementation.

299
00:29:23,400 --> 00:29:36,400
So now when the when thread 1 is done scanning this page, it can jump to the next page. It still holds the latch the page it started at because it needs to know how to where to look at next.

300
00:29:36,400 --> 00:29:50,400
Right, to make sure nobody's moving things around. And so we can then release the latch on page number one, I acquired a latch on page number two, and then now thread 2 can start running and try to figure out where was the insert.

301
00:29:50,400 --> 00:30:04,400
And the same thing once it once come down here to do the right, the read latch is not commutable with the right latch. So therefore it has to wait. And then once thread 1 is done thread 2 can then acquired a latch and do the update to the service entry.

302
00:30:04,400 --> 00:30:09,400
Right. Yes, question.

303
00:30:09,400 --> 00:30:21,400
If we have a read latch acquired and write latch away, then we decide to read again. The order we schedule things in, isn't that sort of a race condition?

304
00:30:21,400 --> 00:30:28,400
Your question is if I hold it, if I hold the I have a read latch, say it's an area again, sorry.

305
00:30:28,400 --> 00:30:39,400
I guess more broadly, I'm trying to ask you, don't remember the race condition is like based on whether the writer happens to be before the reader or wherever it's given to the one policy.

306
00:30:39,400 --> 00:30:41,400
So going back to like the very beginning here.

307
00:30:41,400 --> 00:30:51,400
Yeah, I was sorry, like a race condition in terms of like what, what are output is like, if we have something trying to read the same time, right?

308
00:30:51,400 --> 00:30:58,400
Sorry, sorry, say again, say this in our hero, say T2 shows up before T1.

309
00:30:58,400 --> 00:31:02,400
And what's the race condition you're on deal with?

310
00:31:02,400 --> 00:31:12,400
Like, if you read, then write, then read again. But the scheduling policy has the two reads happen before the write.

311
00:31:12,400 --> 00:31:20,400
So when you say read, write, you read, and then write, then read, like that second read is that like another find a key?

312
00:31:20,400 --> 00:31:23,400
So say you find and then insert that to find again.

313
00:31:23,400 --> 00:31:30,400
Yes. And what's the scheduling policy puts the find before the write, then one of that create a data read.

314
00:31:30,400 --> 00:31:32,400
Is it wrong?

315
00:31:32,400 --> 00:31:33,400
Yeah.

316
00:31:33,400 --> 00:31:40,400
We don't know if we're going to get the old value before the write, or the new value after the write necessarily, right?

317
00:31:40,400 --> 00:31:42,400
Yes. So.

318
00:31:42,400 --> 00:31:44,400
So what did you find the uncertainty?

319
00:31:44,400 --> 00:31:46,400
Find the again?

320
00:31:46,400 --> 00:31:48,400
So yes.

321
00:31:48,400 --> 00:31:54,400
So there's a, this is the logical correctist thing, right?

322
00:31:54,400 --> 00:32:04,400
Like, if say I do select, I run a, run a little select query that doesn't look up in this hash table, and I find D, right?

323
00:32:04,400 --> 00:32:06,400
And I get back the answer.

324
00:32:06,400 --> 00:32:11,400
Now the, the, some other transaction, another thread comes along and deletes D, right?

325
00:32:11,400 --> 00:32:17,400
And roots of the hash table, then, then the first thread again, now runs another select query that finds D and it doesn't come back with it, right?

326
00:32:17,400 --> 00:32:18,400
Yeah.

327
00:32:18,400 --> 00:32:22,400
And the data structure perspective, that's fine for what we're talking about today.

328
00:32:22,400 --> 00:32:31,400
When we talk about it from the midterm, that's a, that's an anomaly of inconsistent reads, and that's something that the

329
00:32:31,400 --> 00:32:34,400
cultural mechanism for the system will handle in transaction level.

330
00:32:34,400 --> 00:32:37,400
The low level of the data structure, we don't care.

331
00:32:37,400 --> 00:32:38,400
It's correct.

332
00:32:38,400 --> 00:32:45,400
Who decides what, in the order what, what, what, what rights we should see or not see, that's a higher level thing that comes later.

333
00:32:45,400 --> 00:32:54,400
Yes. And the great thing about databases or these transactions is like, there's multiple answers that are all technically correct potentially.

334
00:32:54,400 --> 00:32:55,400
But we'll get that later.

335
00:32:55,400 --> 00:33:06,400
No, there's, well, in the specification of SQL, there's a, there's a, there's a description about what is considered correct or not correct at different

336
00:33:06,400 --> 00:33:08,400
isolation levels, which we haven't covered yet.

337
00:33:08,400 --> 00:33:15,400
Right? And the, the, the, the, the, the, the, the, the most strictest correctness level would be isolation level, be called strict,

338
00:33:15,400 --> 00:33:17,400
strict, sterilizable or strong, sterilizable.

339
00:33:17,400 --> 00:33:25,400
Basically, it means that like, whoever comes first should, should see the system as if it was running by itself and it changes,

340
00:33:25,400 --> 00:33:28,400
get applied first, right?

341
00:33:28,400 --> 00:33:30,400
And then everything else comes after that.

342
00:33:30,400 --> 00:33:35,400
But getting way ahead of ourselves from the data structure perspective, I don't care that your thread came in and,

343
00:33:35,400 --> 00:33:40,400
and deleted deep fat, look for D, then D got lit it and you go to look for again, it's missing.

344
00:33:40,400 --> 00:33:42,400
That's not the data structure problem.

345
00:33:42,400 --> 00:33:43,400
That's somebody else's problem.

346
00:33:43,400 --> 00:33:46,400
And it's our problem, but like not, not today's lecture.

347
00:33:46,400 --> 00:33:48,400
Future problem.

348
00:33:48,400 --> 00:33:49,400
Future problem.

349
00:33:49,400 --> 00:33:50,400
Future problem. And it's really hard.

350
00:33:50,400 --> 00:33:51,400
Yeah.

351
00:33:51,400 --> 00:33:56,400
Oh, it's why all the no SQL guys did new transactions at the beginning, because it's shit's hard.

352
00:33:56,400 --> 00:33:57,400
Right?

353
00:33:57,400 --> 00:33:59,400
All right, we'll come back later.

354
00:33:59,400 --> 00:34:02,400
There's a lot of things they didn't do because it's hard.

355
00:34:02,400 --> 00:34:04,400
And then they didn't you had to do it later.

356
00:34:04,400 --> 00:34:05,400
All right.

357
00:34:05,400 --> 00:34:09,400
So again, let me just show how to do slot latches.

358
00:34:09,400 --> 00:34:12,400
Again, now I have a latch on every single slot itself at my hash table.

359
00:34:12,400 --> 00:34:17,400
So now when I do a find D, I get the latch on the read latch on the slot itself.

360
00:34:17,400 --> 00:34:21,400
I go ahead and read what I'm looking for and say this guy now wants to jump to this page.

361
00:34:21,400 --> 00:34:26,400
He wants to get the read latch on C.

362
00:34:26,400 --> 00:34:29,400
Ignore how he found that because he hashed there.

363
00:34:29,400 --> 00:34:38,400
The now when the first thread tries to get the read latch on the next slot, he can't because the other thread has the latch.

364
00:34:38,400 --> 00:34:40,400
So he has to stall.

365
00:34:40,400 --> 00:34:49,400
But at this point, even though he's going to stall and wait, it's safe for him to release the latch on the previous slot because he has his.

366
00:34:49,400 --> 00:34:52,400
There's no issue.

367
00:34:52,400 --> 00:34:55,400
There's no reason for him to keep holding this latch, right?

368
00:34:55,400 --> 00:34:59,400
Because he's just going to spin and wait for this thing here and everything still can sit against the rack.

369
00:34:59,400 --> 00:35:04,400
Now like there's the, I mentioned like the reorganization of the hash table itself.

370
00:35:04,400 --> 00:35:09,400
Like if you have to resize it, assume that there's some other latch protecting the entire thing that's in like read mode.

371
00:35:09,400 --> 00:35:11,400
So that's okay.

372
00:35:11,400 --> 00:35:13,400
And we only set that to right mode if we have to resize all things.

373
00:35:13,400 --> 00:35:19,400
So so the global latch for threat T2 would have the global latch in read mode.

374
00:35:19,400 --> 00:35:20,400
Yes.

375
00:35:20,400 --> 00:35:23,400
So these are the slot and page.

376
00:35:23,400 --> 00:35:29,400
No, this is just thinking like it's like some offset and some page for our hash table.

377
00:35:29,400 --> 00:35:31,400
So it means all fixed length.

378
00:35:31,400 --> 00:35:32,400
Yeah.

379
00:35:32,400 --> 00:35:33,400
So like this.

380
00:35:33,400 --> 00:35:35,400
Yeah, correct.

381
00:35:35,400 --> 00:35:36,400
Yes.

382
00:35:36,400 --> 00:35:37,400
Yeah.

383
00:35:37,400 --> 00:35:39,400
And like basically do you want five grain fine grain latches or coarse grain latches?

384
00:35:39,400 --> 00:35:41,400
That's all it is.

385
00:35:41,400 --> 00:35:43,400
Okay.

386
00:35:43,400 --> 00:35:45,400
So this should be pretty straightforward.

387
00:35:45,400 --> 00:35:46,400
Okay.

388
00:35:46,400 --> 00:35:47,400
Let's get to B plus trees.

389
00:35:47,400 --> 00:35:51,400
This is harder and more fun.

390
00:35:51,400 --> 00:35:54,400
So again, just like before in our hash table in the B plus tree,

391
00:35:54,400 --> 00:35:57,400
we want to have multiple threads read right at the same time.

392
00:35:57,400 --> 00:36:02,400
The challenge here is now in the hash table, with at least in linear probing,

393
00:36:02,400 --> 00:36:04,400
the number of pages was fixed.

394
00:36:04,400 --> 00:36:08,400
And the organization of the data structure was fixed.

395
00:36:08,400 --> 00:36:14,400
Meaning like, you know, no matter if I had, if I create my hash table and I have a, say a million slots,

396
00:36:14,400 --> 00:36:18,400
it doesn't matter whether I have a thousand threads going at it or one thread going at it,

397
00:36:18,400 --> 00:36:20,400
I'm always going to have a million slots.

398
00:36:20,400 --> 00:36:22,400
It doesn't matter if I have a, you know, a half a million keys in it or not,

399
00:36:22,400 --> 00:36:24,400
the data structure is always the same.

400
00:36:24,400 --> 00:36:29,400
And at B plus tree, since the data structure is self-organizer, self-balancing,

401
00:36:29,400 --> 00:36:33,400
as I insert things into it, I start deleting things from it,

402
00:36:33,400 --> 00:36:36,400
it's going to start reorganizing itself.

403
00:36:36,400 --> 00:36:38,400
So I need to make sure that as I'm reorganizing things,

404
00:36:38,400 --> 00:36:43,400
you're doing splits or merges, I make sure that, I have to make sure that the data structure is correct.

405
00:36:44,400 --> 00:36:46,400
All right, so let's see how things can go bad.

406
00:36:46,400 --> 00:36:51,400
Say we have our thread here and they want to delete a key 44 at the bottom.

407
00:36:51,400 --> 00:36:52,400
So what do I do?

408
00:36:52,400 --> 00:36:56,400
I traverse down and I look at the guide post markers to figure out whether I'm going to go left and right

409
00:36:56,400 --> 00:37:01,400
and I reach down to my, my leaf node here and I go ahead and delete it.

410
00:37:01,400 --> 00:37:07,400
But now I have to rebalance, I have to do a merge because this, this leaf node is less than half full.

411
00:37:07,400 --> 00:37:11,400
So maybe I'll steal an entry from my sibling, right?

412
00:37:12,400 --> 00:37:14,400
He's going to move 41 over, right?

413
00:37:14,400 --> 00:37:20,400
But before I can do that, my thread gets descheduled for whatever reason.

414
00:37:20,400 --> 00:37:23,400
The OS decided to do something, gamma rays came down, it doesn't matter.

415
00:37:23,400 --> 00:37:25,400
My thread's not running anymore.

416
00:37:25,400 --> 00:37:30,400
So now, while thread 1 is asleep, thread 2 comes along and they want to find 41

417
00:37:30,400 --> 00:37:34,400
and they search a person down just like before and they get here, right?

418
00:37:34,400 --> 00:37:40,400
And again, now they look at the guide post, they follow the, they realize they want to go down to the right to the node H.

419
00:37:40,400 --> 00:37:43,400
But now it gets descheduled for whatever reason.

420
00:37:43,400 --> 00:37:47,400
And then thread 1 wakes up, moves 41,

421
00:37:47,400 --> 00:37:54,400
T1, T, thread 2 wakes up, goes down to the node, thought it would need to go to, and now the key's not there.

422
00:37:54,400 --> 00:37:55,400
Right?

423
00:37:55,400 --> 00:38:00,400
Best case scenario, you get a false negative here, worst case scenario, you crash.

424
00:38:00,400 --> 00:38:04,400
And the system fails and you could, you could, you know, corrupt your data.

425
00:38:04,400 --> 00:38:05,400
Right?

426
00:38:05,400 --> 00:38:08,400
So we need latches to protect this thing.

427
00:38:08,400 --> 00:38:13,400
And so the technique we're going to use is called latch crabbing or latch coupling.

428
00:38:13,400 --> 00:38:15,400
I think the textbook calls it latch coupling.

429
00:38:15,400 --> 00:38:17,400
I think the Wikipedia calls it latch coupling.

430
00:38:17,400 --> 00:38:18,400
Right?

431
00:38:18,400 --> 00:38:25,400
But it's basically the protocol we're going to use to, to decide as we traverse down to our tree,

432
00:38:25,400 --> 00:38:32,400
what latches we want to take, and then when can we release the latches up above us?

433
00:38:32,400 --> 00:38:37,400
Because again, the easiest way to protect this to higher data structure is put a giant latch and top the whole thing.

434
00:38:37,400 --> 00:38:41,400
And then everyone has to get gatekeep through, but then that becomes a bottleneck.

435
00:38:41,400 --> 00:38:48,400
So we want to be more clever and selectively release our latches as we go down when we know it's okay.

436
00:38:48,400 --> 00:38:57,400
So the basic protocol is in order for me to go into B plus 3, I, you always have to last the root, but once I'm in my root,

437
00:38:57,400 --> 00:39:03,400
and I figure out whether I want to go left and right, and then I acquire the latch from my child that I'm going to go down from the current parent of that.

438
00:39:03,400 --> 00:39:12,400
And then once I'm know I'm okay, and I'm able to go there, I can release my parent latch for that current node of that if I know it's safe.

439
00:39:12,400 --> 00:39:24,400
And the definition of safe is going to be that we know that based on the operation we're trying to do, like an insert or a delete, if it's an insert, we know that the child isn't full.

440
00:39:24,400 --> 00:39:35,400
And therefore, if I had to do a split, I'm sorry, when I insert my key, it's not going to call to do a split at that child node, which may get propagated up to the parent.

441
00:39:35,400 --> 00:39:45,400
If I'm going to do a delete, if I know that it's more than half full, then again, if I remove a key, I know I'm not going to do a merge, which again will propagate things up to my parent node here.

442
00:39:46,400 --> 00:39:57,400
So again, the basic protocol is that I started the route, where it's down, acquire read latches and every child as I'm going down, because again, I'm doing a read operation, I'm not doing updates, and then I just unlatch my parent.

443
00:39:57,400 --> 00:40:12,400
For doing insert or delete, I start the route, taking right latches as I go down, and then once I have my child node that I'm going to move to in right mode, then I go check to see whether it's safe, and if it is safe, then I can go release any latches I have up above me.

444
00:40:15,400 --> 00:40:18,400
So let's go back to our example here, so I want to find key 38.

445
00:40:18,400 --> 00:40:39,400
Again, I start at the route, take the route in latch and route node in a right latch mode, then I acquire the read latch on B, then I move down, and at this point here, it's safe for me to release the latch on A, because again, it's doing a read operation, I've already arrived at B where I need to go, so I'm going to go there.

446
00:40:39,400 --> 00:40:50,400
So I can go ahead and release the latch on B, get the latch read latch on D, same thing, it's safe for me to release the latch on B, so I go ahead and do that, and I get down to H, and so forth, and I finally get the key I'm looking for, and I'm done.

447
00:40:50,400 --> 00:40:54,400
All right.

448
00:40:54,400 --> 00:41:23,400
So I get the route in right mode latch, I get a right latch on B, and I get this point here, since I don't know what's going to happen below me in the tree, below B, because if I know if I had to delete a key from the node I'm currently at on B, I'm going to have to do a merge, so therefore I can't release the latch on my parent A,

449
00:41:23,400 --> 00:41:49,400
because I may have to go make changes to A above. So I'm going to hold the right latch on B and A, get down to D, now at D I see that, no matter what happens to me below in the tree, if I had to remove a key from D, D is not going to do a merge, and therefore it's not going to make any changes up above it, so I, it's safe for me to go ahead and release the latch on A and D.

450
00:41:49,400 --> 00:42:11,400
Does the order matter? Sorry, question. Which node is the safe node? Whatever you're at now, whatever, whatever the, like, so if I'm at B, I get the, and I'm, I need to go to D, so I get, I get it in right, I get the latch in right mode, and then now I have in right mode, then I check in my safe, because you can't check whether you're safe, and do you hold the latch on it?

451
00:42:11,400 --> 00:42:17,400
Is the safe and go ahead and release, release everybody up above me, yes.

452
00:42:17,400 --> 00:42:21,400
Is being more than half full the only sort of form?

453
00:42:21,400 --> 00:42:32,400
This question is, is it being half full the only check to see whether it's safe? What else would call, what else would cause a, a, a merge in case of a, a delete?

454
00:42:34,400 --> 00:42:39,400
That's the thing we all care about, like splits emerges, we're trying to make sure that we don't screw ourselves at that.

455
00:42:42,400 --> 00:42:54,400
All right, so, so, so at this one here at D, we had to release the latches on A and B, because again, D is safe. Does the order in which we release those latches matter?

456
00:42:55,400 --> 00:43:06,400
He says it makes more sense to release the latches on A, why?

457
00:43:06,400 --> 00:43:16,400
If someone is waiting on A, I'll be the release and if B is also the release, then it's not half the way for B.

458
00:43:18,400 --> 00:43:25,400
So, if I, so if I, if I release B, and someone's waiting for A, and like it has to go down the same path as well, yes.

459
00:43:25,400 --> 00:43:30,400
So, if the release is B, and then A, then it does not have to wait for B.

460
00:43:31,400 --> 00:43:37,400
So, he says if he release B, then it does, sorry, the other thread's waiting for B or A, sorry.

461
00:43:37,400 --> 00:43:39,400
It has to go down the same path.

462
00:43:43,400 --> 00:43:45,400
Latch, but yes, yeah.

463
00:43:45,400 --> 00:43:50,400
Okay, but if I release it on B, it's still waiting for A.

464
00:43:51,400 --> 00:44:00,400
Right, also too. What if I have a thread that's waiting for A, but once go down this side of the tree, but I release B, it's still blocked.

465
00:44:01,400 --> 00:44:06,400
So, from a correctness standpoint, it doesn't actually matter.

466
00:44:06,400 --> 00:44:11,400
Like the system, the data structure will still be correct, whether you go from the bottom to the top to the bottom.

467
00:44:11,400 --> 00:44:19,400
For performance reasons, we want to go top down, because we want to release, like you wait to think about it as the latch is protecting everything below it.

468
00:44:20,400 --> 00:44:25,400
So, if I hold a right latch on the root, I'm protecting the entire data structure in right mode.

469
00:44:28,400 --> 00:44:44,400
Now, there may be a bunch of like read threads and other threads doing stuff over here, but that's okay, because it would, if their modifications would have caused us to do a split or merge, then up the entire to the root, they would have to still hold the right latch on this.

470
00:44:45,400 --> 00:44:57,400
So, again, the main takeaway is that we want to release latches as soon as possible, and we want to release latches that will have the most, that would free up the most, the most of our workers in our data structure.

471
00:44:57,400 --> 00:45:00,400
So, we always want to release from the top going down.

472
00:45:03,400 --> 00:45:09,400
All right, so then we get down here and we get the right latch on H, go ahead and do our delete and we're done.

473
00:45:10,400 --> 00:45:30,400
All right, so let's look at another example. We're in certain 45, same idea. I get the right latch on the root, on A, go down to B. At this point here, I know that if, whatever bolt is below me in the tree, if it has to do a split, I have room in B to accommodate another key.

474
00:45:30,400 --> 00:45:40,400
So, it's okay for me to go release the latch on A, get down here on D. Now D is completely full. I don't know what's below me in the tree because I haven't gone there yet.

475
00:45:40,400 --> 00:45:56,400
So, at this point here, it's not safe to release the latch on D until I go down, and now I can see that I'm inserting the I would not cause a split on node I, so I can go ahead and release the latches on B and D, and going top down.

476
00:45:57,400 --> 00:45:59,400
And I can start my key.

477
00:46:01,400 --> 00:46:16,400
All right, let's look at one where there is a split. For simplicity, we could ignore sibling pointers. So, I'm doing an insert 25, B is safe. I'm going to release the latch on A. I get down to C, C is safe, release the latch on B.

478
00:46:17,400 --> 00:46:30,400
Then I get down to F. F is not safe, so I can't release the latch on C. Now I need to do a split, and that split's going to cause me to insert a new entry up into node C.

479
00:46:31,400 --> 00:46:40,400
So, I'll go ahead and do that. Add my new node J, and we're now at a space because it's PowerPoint. Again, we're ignoring sibling pointers for now.

480
00:46:40,400 --> 00:46:50,400
Go ahead and now do the update to the C, and now include this pointer. And then once I apply all these changes, then I release the latches going top down.

481
00:46:53,400 --> 00:47:00,400
So, I've already given this answer before. What was the very first thing I did for all these scenarios when I'm going to do updates and insert some leads? What's the very first thing you have to do?

482
00:47:01,400 --> 00:47:05,400
Get a latch on the root node.

483
00:47:06,400 --> 00:47:14,400
And, yeah, get that correct, but this is a bottleneck now because it basically becomes a single threaded data structure.

484
00:47:15,400 --> 00:47:22,400
Everybody has to go into the system, start going into our data structure, the first thing you have to do is acquire the latch and right node in the root.

485
00:47:23,400 --> 00:47:26,400
In view of doing reads, that's not going to be compatible, so it'll block all the readers too.

486
00:47:28,400 --> 00:47:32,400
Again, it's correct, but from a performance reason, it's not ideal.

487
00:47:33,400 --> 00:47:40,400
And so, the common technique everyone uses is this optimistic, a latching scheme I'll talk about now.

488
00:47:41,400 --> 00:47:47,400
I don't think the algorithm has a name. It's from this paper from, I think it's from the 70s. Is there a date on that?

489
00:47:48,400 --> 00:47:51,400
It says 77. Yeah, from these guys that I be on bear and schlock neck.

490
00:47:52,400 --> 00:47:54,400
Sometimes it's called the bear schlock neck algorithm, which is kind of commerce in the say.

491
00:47:55,400 --> 00:48:01,400
But it's based on this observation that you know that most of your threads, most of your workers, their operations are not going to be good.

492
00:48:02,400 --> 00:48:07,400
So, it's not going to cause a split emerge into your B plus three nodes.

493
00:48:07,400 --> 00:48:11,400
Again, my example is here, I'm showing you nodes with two keys in it because that's a fit of the PowerPoint.

494
00:48:12,400 --> 00:48:17,400
But in a real system, you know, the size window, it's going to be the page size of your database, like 8 kilobytes, 16 kilobytes.

495
00:48:18,400 --> 00:48:23,400
And you can store a lot of keys. So, the most of the times you can do a bunch of inserts, and that's not going to cause any splits.

496
00:48:24,400 --> 00:48:25,400
And likewise for deletes.

497
00:48:26,400 --> 00:48:36,400
So, if you assume that splits emerges in me rare, then instead of taking right latches all the way down, even if you're doing the latch coupling scheme,

498
00:48:37,400 --> 00:48:43,400
you're going to take red latches all the way down until you get to a leaf node right above the wreath node.

499
00:48:44,400 --> 00:48:48,400
And then now you check to see whether that assumption that you're not going to do a split emerge is correct.

500
00:48:49,400 --> 00:48:55,400
And if it is, then you go ahead and quiet the leaf node in right mode and then do your change.

501
00:48:55,400 --> 00:49:01,400
But if you're wrong, you just restart and then do the pessimistic approach or just taking right latches all the way down.

502
00:49:02,400 --> 00:49:06,400
So, this will be a common theme you see not just in databases in a bunch of different systems in general.

503
00:49:07,400 --> 00:49:11,400
This is an optimistic scheme where you assume that you're not going to be any issues, not any problems.

504
00:49:12,400 --> 00:49:17,400
And you do the sort of fast way of making some change or doing something in your system.

505
00:49:18,400 --> 00:49:21,400
And then if you're wrong, you just roll it back and take care of it.

506
00:49:23,400 --> 00:49:25,400
Intel actually had this in, it was called TSX.

507
00:49:25,400 --> 00:49:28,400
We actually had this in the CPU itself.

508
00:49:28,400 --> 00:49:31,400
I think there's a bug and I think they turned it off.

509
00:49:31,400 --> 00:49:32,400
They might have got turned back on.

510
00:49:32,400 --> 00:49:42,400
But like, there's optimistic memory stuff where you could have a critical section where you assume you're not going to have any conflicts in some critical section.

511
00:49:42,400 --> 00:49:49,400
And then when you went to go apply the change, then you just don't check to see whether that assumption was correct.

512
00:49:49,400 --> 00:49:51,400
And if not, it would roll you back automatically.

513
00:49:52,400 --> 00:49:55,400
But again, we'll see this when we do talk about commercial for transactions.

514
00:49:55,400 --> 00:49:57,400
This is a very common technique.

515
00:49:57,400 --> 00:50:00,400
You do the fast thing because most of the times there won't be any issues.

516
00:50:00,400 --> 00:50:03,400
And if you're wrong, then you have to roll back and try again.

517
00:50:04,400 --> 00:50:10,400
All right, so with this better latching scheme for doing lookups and finds, that's the same as before.

518
00:50:10,400 --> 00:50:18,400
For inserts and deletes, again, we take, we basically do the search, taking right latches, sorry, read latches all the way down,

519
00:50:18,400 --> 00:50:21,400
until we're one level above the leaf node.

520
00:50:21,400 --> 00:50:29,400
And we know where we're in the data structure because, you know, we can keep track of as, you know, how many levels down we are.

521
00:50:30,400 --> 00:50:32,400
Either in the page or a simple counter would work too.

522
00:50:34,400 --> 00:50:41,400
You go acquire the level right above the leaf node, you acquire the leaf node in right mode, then you check to see where it's safe.

523
00:50:41,400 --> 00:50:45,400
If it is safe, then you release all your read latches that you took from before.

524
00:50:46,400 --> 00:50:48,400
Apply your change and you're done.

525
00:50:48,400 --> 00:50:53,400
If you're wrong, then you just release all your latches and go back and take right latches all the way down.

526
00:50:53,400 --> 00:50:58,400
You could take, you know, do the optimistic seam again because you assume the next time you come back around, things will be safe.

527
00:50:59,400 --> 00:51:00,400
It depends on the implementation.

528
00:51:00,400 --> 00:51:07,400
And this works really well in low-contention environments because you, you obviously assume that it won't be any conflicts and most of the time you're correct.

529
00:51:07,400 --> 00:51:09,400
And so things run faster.

530
00:51:11,400 --> 00:51:13,400
Alright, so let's go back from the example before.

531
00:51:13,400 --> 00:51:15,400
Let's delete key 38.

532
00:51:15,400 --> 00:51:24,400
Again, instead of taking the root node in a right latch mode, taking a read mode, keep going down to I get down to D.

533
00:51:24,400 --> 00:51:27,400
Now, D recognizes that it's one level above the leaf node.

534
00:51:27,400 --> 00:51:32,400
So I want to delete a key 38 from node H.

535
00:51:32,400 --> 00:51:35,400
So I take the H into right mode, check to see it's safe.

536
00:51:35,400 --> 00:51:36,400
It is.

537
00:51:36,400 --> 00:51:37,400
I can go delete it.

538
00:51:37,400 --> 00:51:41,400
And I know I'm not going to do any merges.

539
00:51:41,400 --> 00:51:49,400
So again, best case scenario, I traverse the data structure almost as if I was doing a read.

540
00:51:50,400 --> 00:51:58,400
And therefore, I can have a maximum on a parallelism, but just only at the bottom do I check to see whether that assumption was correct.

541
00:51:58,400 --> 00:52:07,400
See how I do insert insert 25 again, take the root in in read mode, take being read mode, do the latch coupling as I release latches as I go down.

542
00:52:07,400 --> 00:52:13,400
Now I get down here into F in case of F because we're trying to do an insert, F doesn't have any more room.

543
00:52:14,400 --> 00:52:21,400
So it's not safe. So we're going to restart the whole operation and then just take right latches on the way down.

544
00:52:23,400 --> 00:52:26,400
Neat trick, right?

545
00:52:27,400 --> 00:52:29,400
Okay.

546
00:52:30,400 --> 00:52:37,400
So in all these examples, as I've shown so far, we were only going in one direction.

547
00:52:37,400 --> 00:52:39,400
We were only going top to the bottom.

548
00:52:40,400 --> 00:52:44,400
And as I said, there weren't any deadlocks because nobody, you know, everyone's going to the top.

549
00:52:44,400 --> 00:52:46,400
They're already starting at the same point and they're going down.

550
00:52:46,400 --> 00:52:50,400
There's no, you know, as I said last class, there's no pointers to your parent.

551
00:52:50,400 --> 00:52:54,400
You can't go back up and then because that's where you could have conflicts.

552
00:52:54,400 --> 00:52:59,400
But again, because we're at B plus dream, we could have sibling pointers.

553
00:52:59,400 --> 00:53:06,400
And now we have a challenge where we could have one thread going one way, another thread going another way.

554
00:53:06,400 --> 00:53:11,400
And they both hold latches for what the other person wants with the other thread once.

555
00:53:11,400 --> 00:53:12,400
Right?

556
00:53:12,400 --> 00:53:15,400
So now we got to deal with that scenario.

557
00:53:15,400 --> 00:53:18,400
It took again, the original B plus dream paper, this wasn't an issue.

558
00:53:18,400 --> 00:53:24,400
But the B link stuff that came from CMU, that's where they added sibling pointers and that's where you can have deadlocks.

559
00:53:24,400 --> 00:53:26,400
So let's look at simple example here.

560
00:53:26,400 --> 00:53:28,400
So I want to, I have a thread one.

561
00:53:28,400 --> 00:53:31,400
We want to find all keys less than four.

562
00:53:31,400 --> 00:53:36,400
So we're going to get the root in in read mode and then get the C in read mode.

563
00:53:36,400 --> 00:53:43,400
And then let's say, then once the scan scan across, so it's going to follow the sibling pointers.

564
00:53:43,400 --> 00:53:51,400
So just look at four, I hold whatever node I'm at now, I hold that in the current latch mode that I have it.

565
00:53:51,400 --> 00:53:54,400
And then I then try to acquire the latch where I want to go to.

566
00:53:54,400 --> 00:53:58,400
So this case here, again, wants to scan, see when we go from C to B.

567
00:53:58,400 --> 00:54:06,400
I hold the latch on C, get the latch on B, move over here, and then I can release the latch on C and then do whatever it is that I need to do.

568
00:54:06,400 --> 00:54:07,400
Right?

569
00:54:07,400 --> 00:54:13,400
So the protocol is basically the same thing, even though we're now removing horizontally instead of vertically.

570
00:54:13,400 --> 00:54:14,400
Yes.

571
00:54:14,400 --> 00:54:19,400
I think this might be the same thing for earlier, but if we're reading this one, then a guy can start writing the other one.

572
00:54:19,400 --> 00:54:21,400
Then we move across the read.

573
00:54:21,400 --> 00:54:23,400
That's the same scheduling problem.

574
00:54:23,400 --> 00:54:25,400
We'll get to that in a second.

575
00:54:25,400 --> 00:54:26,400
Yes.

576
00:54:26,400 --> 00:54:32,400
So again, the remos are so commutives, so I can have two threads doing the same time.

577
00:54:32,400 --> 00:54:38,400
First thing that goes down, or goes down to C, second thread goes down to B, and they want to go across each other.

578
00:54:38,400 --> 00:54:46,400
And in this case here, the two latches that are holding are commutative, so therefore they can both do whatever they need to do.

579
00:54:46,400 --> 00:54:48,400
Right?

580
00:54:48,400 --> 00:54:51,400
That's fine.

581
00:54:51,400 --> 00:54:57,400
So let's now do when we have a one on one to do a write, one to do a read.

582
00:54:57,400 --> 00:55:02,400
So T1 was a delete, the key for, and T2 wants to find all keys greater than one.

583
00:55:02,400 --> 00:55:06,400
So they both start at the same time, and assume we're doing the optimistic lock coupling.

584
00:55:06,400 --> 00:55:14,400
I just talked about a latch coupling where they start the route, both in read mode.

585
00:55:15,400 --> 00:55:16,400
Right?

586
00:55:16,400 --> 00:55:27,400
Thread 2 goes down, takes B into read mode, thread C goes down, and takes, sorry, one goes down, takes node C in write mode.

587
00:55:27,400 --> 00:55:29,400
And that's the key that it wants to delete.

588
00:55:29,400 --> 00:55:30,400
Right?

589
00:55:30,400 --> 00:55:42,400
But now, thread 2 is scanning across the leaf nodes, and it wants to acquire the latch on C in read mode, but it can't, because T1 holds that in write mode.

590
00:55:43,400 --> 00:55:46,400
So we have to decide what we want to do here.

591
00:55:46,400 --> 00:55:47,400
Right?

592
00:55:47,400 --> 00:55:56,400
And T2 doesn't know anything about T1, because I said there's no centralized data structure, just says here's the threads that are running, here's what they're doing.

593
00:55:56,400 --> 00:56:06,400
All that C's, all it knows is that there's a latch on this other node that I want to go to, and it's currently in write mode, and that's not compatible with the mode I want to put it in, so you have to do something.

594
00:56:06,400 --> 00:56:09,400
So what can T2 do here?

595
00:56:09,400 --> 00:56:10,400
Right?

596
00:56:10,400 --> 00:56:15,400
The weight, that's one option, what else?

597
00:56:15,400 --> 00:56:18,400
What's that?

598
00:56:18,400 --> 00:56:21,400
Okay, let's see, are you reading the slides?

599
00:56:21,400 --> 00:56:22,400
Okay.

600
00:56:22,400 --> 00:56:27,400
What's the third option?

601
00:56:28,400 --> 00:56:33,400
Ingo, street on the other thread, and try to kill it, right?

602
00:56:33,400 --> 00:56:36,400
And take the latch from it.

603
00:56:36,400 --> 00:56:39,400
So what do you think is a good idea here?

604
00:56:39,400 --> 00:56:41,400
What's that?

605
00:56:41,400 --> 00:56:45,400
Just wait for how long?

606
00:56:45,400 --> 00:56:49,400
I heard forever.

607
00:56:50,400 --> 00:56:53,400
How long?

608
00:56:53,400 --> 00:56:55,400
How do you know?

609
00:56:55,400 --> 00:56:59,400
That's just waiting, right?

610
00:56:59,400 --> 00:57:07,400
It's just spinning until the latch is available, but like, how does this?

611
00:57:07,400 --> 00:57:12,400
Do you know what T1 is doing?

612
00:57:12,400 --> 00:57:15,400
No, we don't know anything.

613
00:57:15,400 --> 00:57:18,400
What's that?

614
00:57:18,400 --> 00:57:21,400
Do you know that T1 is like editing a week, right?

615
00:57:21,400 --> 00:57:22,400
Do you?

616
00:57:22,400 --> 00:57:27,400
Well, yeah, you know it's in write mode, but how long is it going to take?

617
00:57:27,400 --> 00:57:32,400
Uh, how?

618
00:57:32,400 --> 00:57:34,400
Which one?

619
00:57:34,400 --> 00:57:38,400
Like, do you kill yourself from your schedule or do you kill the other guy in your schedule?

620
00:57:38,400 --> 00:57:39,400
You kill the other guy.

621
00:57:39,400 --> 00:57:42,400
She says kill the other one, fantastic, right?

622
00:57:42,400 --> 00:57:45,400
How do you do that?

623
00:57:45,400 --> 00:57:47,400
What's that?

624
00:57:47,400 --> 00:57:50,400
Yeah.

625
00:57:50,400 --> 00:57:55,400
You have to keep on modding like everything that you've done in the past.

626
00:57:55,400 --> 00:57:58,400
What do you mean, keep on log everything you've done in the past?

627
00:57:58,400 --> 00:58:04,400
Like, you look for like all the operations you want to make, and if you want to be scheduled with some go back to the PS3,

628
00:58:04,400 --> 00:58:09,400
kill the other guy in the next third, Q, and say that I will come back in the next three or four days.

629
00:58:09,400 --> 00:58:12,400
Yeah, that sounds expensive.

630
00:58:12,400 --> 00:58:15,400
I'm going to get wait for a normal month of time.

631
00:58:15,400 --> 00:58:23,400
I've just come for like, uh, he says wait for the average time for a right to happen.

632
00:58:23,400 --> 00:58:32,400
Um, but like, you don't have since in my, my simple example here because I have to fit on PowerPoint.

633
00:58:32,400 --> 00:58:34,400
It's there's two nodes in the leaf, right?

634
00:58:34,400 --> 00:58:37,400
What if there was a bunch of leaf nodes all over here?

635
00:58:37,400 --> 00:58:41,400
I got to hold all these guys in everything in write mode, right?

636
00:58:41,400 --> 00:58:43,400
Because I want my changes to happen automatically.

637
00:58:43,400 --> 00:58:46,400
So I don't know whether this other thread keep going in the other direction.

638
00:58:46,400 --> 00:58:47,400
I don't like what is a normal time?

639
00:58:47,400 --> 00:58:50,400
You don't know.

640
00:58:50,400 --> 00:58:56,400
He says give up your right lock and let the other guy read.

641
00:58:56,400 --> 00:58:57,400
But how do you know?

642
00:58:57,400 --> 00:58:59,400
How do you know they're waiting for you?

643
00:58:59,400 --> 00:59:03,400
Like if you're T1, how do you know somebody else is trying to get your latch?

644
00:59:03,400 --> 00:59:04,400
Yeah.

645
00:59:04,400 --> 00:59:05,400
You know, yes.

646
00:59:05,400 --> 00:59:07,400
I don't know what you think about the other threads.

647
00:59:07,400 --> 00:59:08,400
Do you like it?

648
00:59:08,400 --> 00:59:11,400
You know, if it's about ourselves and what you did, you should kill ourselves?

649
00:59:11,400 --> 00:59:12,400
I think so.

650
00:59:12,400 --> 00:59:13,400
There you go.

651
00:59:13,400 --> 00:59:14,400
Excellent.

652
00:59:14,400 --> 00:59:15,400
Yes.

653
00:59:15,400 --> 00:59:16,400
So you kill yourself, right?

654
00:59:16,400 --> 00:59:17,400
The answer.

655
00:59:17,400 --> 00:59:18,400
Right.

656
00:59:18,400 --> 00:59:21,400
So killing other thread is hard because how, think of how do you implement that?

657
00:59:21,400 --> 00:59:22,400
Can you send an interrupt?

658
00:59:22,400 --> 00:59:23,400
And then I got an interrupt panel.

659
00:59:23,400 --> 00:59:24,400
That's expensive, right?

660
00:59:24,400 --> 00:59:25,400
That's a syscall.

661
00:59:25,400 --> 00:59:26,400
Right?

662
00:59:26,400 --> 00:59:27,400
Is there like a flag you say?

663
00:59:27,400 --> 00:59:28,400
Like, should I kill myself?

664
00:59:28,400 --> 00:59:30,400
I need to check that every so often.

665
00:59:30,400 --> 00:59:31,400
How would that work?

666
00:59:31,400 --> 00:59:32,400
Right?

667
00:59:32,400 --> 00:59:34,400
Uh, that's not going to check in some other memory location.

668
00:59:34,400 --> 00:59:35,400
Right?

669
00:59:35,400 --> 00:59:36,400
And what do you get?

670
00:59:36,400 --> 00:59:40,400
You get the, you know, you don't know how much work the other thread is done.

671
00:59:40,400 --> 00:59:46,400
And therefore, like, uh, you don't know whether him aborting a woman back is way more expensive

672
00:59:46,400 --> 00:59:48,400
than then you aborting yourself.

673
00:59:48,400 --> 00:59:50,400
You know nothing at this point.

674
00:59:50,400 --> 00:59:51,400
Right?

675
00:59:51,400 --> 00:59:56,400
So the best thing to do is just kill yourself.

676
00:59:56,400 --> 01:00:01,400
And then you maybe also could wait a little bit in the beginning and then give up right away,

677
01:00:01,400 --> 01:00:03,400
depending on what you know you need to do.

678
01:00:03,400 --> 01:00:04,400
Right?

679
01:00:04,400 --> 01:00:06,400
And so this is the simplest thing to do.

680
01:00:06,400 --> 01:00:09,400
And it's, it's, it's, it's, it's not going to be the best thing to do.

681
01:00:09,400 --> 01:00:13,400
The most scenarios, almost all scenarios because again, it's, you don't know anything about the thread.

682
01:00:13,400 --> 01:00:16,400
You can't communicate with the other thread because that's expensive.

683
01:00:16,400 --> 01:00:20,400
And you're just better off just aborting and starting over again.

684
01:00:20,400 --> 01:00:21,400
Yes?

685
01:00:21,400 --> 01:00:25,400
Are these things the same thing when going downwards in the street like when,

686
01:00:25,400 --> 01:00:28,400
does it sound like the right cloud from the north to the lower?

687
01:00:28,400 --> 01:00:31,400
And have the similar option of the other side?

688
01:00:31,400 --> 01:00:32,400
Yes.

689
01:00:32,400 --> 01:00:38,400
So what statement is, you would have to wait also to or care itself to like when you're doing the

690
01:00:38,400 --> 01:00:40,400
traversal of the tree going top down.

691
01:00:40,400 --> 01:00:43,400
Like if I try to acquire a latch on somebody else on the next node,

692
01:00:43,400 --> 01:00:45,400
but that's already being held, what do I do?

693
01:00:45,400 --> 01:00:47,400
It's the same scenario here.

694
01:00:47,400 --> 01:00:48,400
It's that same scenario.

695
01:00:48,400 --> 01:00:53,400
But in that case, it's not, you're not, you don't, you won't deadlock though, right?

696
01:00:53,400 --> 01:00:57,400
The problem is like again, if T2 wants to get the latch on,

697
01:00:57,400 --> 01:01:02,400
if T1 wants to get the latch on B and T2 wants to get the latch on C,

698
01:01:02,400 --> 01:01:03,400
that's a deadlock.

699
01:01:03,400 --> 01:01:09,400
You don't know that it's a deadlock or just contention on trying to some latch, right?

700
01:01:09,400 --> 01:01:13,400
So the best thing to do is just immediately give up.

701
01:01:13,400 --> 01:01:16,400
Is that mean you could have a scenario where like you hold the latch and I hold the latch

702
01:01:16,400 --> 01:01:21,400
and then one of us should only give up, but we end up both giving up and killing ourselves, right?

703
01:01:21,400 --> 01:01:28,400
But again, the cost of maintaining metadata about who's waiting for what and what way,

704
01:01:28,400 --> 01:01:33,400
that's more expensive to do in like the regular case where you assume there isn't contention.

705
01:01:33,400 --> 01:01:38,400
Can I add some cases of both of these points out that the focus is like four hours,

706
01:01:38,400 --> 01:01:42,400
so T2, but T1 will not need to go to three days.

707
01:01:42,400 --> 01:01:44,400
Yeah, so it was this one here, right?

708
01:01:44,400 --> 01:01:47,400
So they're both doing reads, but assume they're both doing rights, right?

709
01:01:47,400 --> 01:01:51,400
I need to do, I need to update all keys greater than one or something.

710
01:01:51,400 --> 01:01:53,400
He needs to update keys less than four, right?

711
01:01:53,400 --> 01:01:55,400
I'm going this way, he's going that way.

712
01:01:55,400 --> 01:01:57,400
That's a deadlock.

713
01:01:57,400 --> 01:02:01,400
Updating my two keys in the same type of thing.

714
01:02:01,400 --> 01:02:03,400
Think in terms of latches.

715
01:02:03,400 --> 01:02:08,400
I'm trying to acquire latch on this direction, trying to question the latch direction, and we're deadlocked.

716
01:02:08,400 --> 01:02:11,400
How do you think that's not the best thing about the deadlock?

717
01:02:11,400 --> 01:02:13,400
The question is how do you prevent both of them killing themselves?

718
01:02:14,400 --> 01:02:20,400
So you can't, because I don't know you exist, I don't know what you're doing, right?

719
01:02:20,400 --> 01:02:27,400
If you just think of computers in general, it's very unlikely that you and I are going to be

720
01:02:27,400 --> 01:02:34,400
exact lockstep in our threads and the exact same number of cycles where both are going to try to acquire a latch together

721
01:02:34,400 --> 01:02:36,400
that would deadlock and we would kill each other.

722
01:02:36,400 --> 01:02:40,400
It's a race condition, but it's rare, but you can't prevent it.

723
01:02:40,400 --> 01:02:43,400
Because the end of the cost of preventing it is so expensive.

724
01:02:43,400 --> 01:02:45,400
Did I see something?

725
01:02:45,400 --> 01:02:48,400
Did I see something you think you killed the other one very quickly?

726
01:02:48,400 --> 01:02:53,400
The statement is, for a philosophical standpoint, it would be more efficient to kill the other one.

727
01:02:53,400 --> 01:02:55,400
Would that be better?

728
01:02:55,400 --> 01:03:00,400
It's not only philosophical questions, it looks straight up, is it better?

729
01:03:00,400 --> 01:03:08,400
You take their wallet, whatever, like, you, like, this is a toy example where it's only one node.

730
01:03:08,400 --> 01:03:14,400
Like, if you think about, I don't know what the, I don't know what work you've done, I don't know what work I've done.

731
01:03:14,400 --> 01:03:18,400
If you can sort of keep checking that, then we may be made out of sight.

732
01:03:18,400 --> 01:03:25,400
Okay, well, you hold five latches, I hold one, it's better to kill me, because you had a way to much time to get those five latches.

733
01:03:25,400 --> 01:03:30,400
So that's sort of a high level what the, what we'll do when we do transactions.

734
01:03:30,400 --> 01:03:34,400
They'll figure out who gets priority or others based on how much work they've done so far.

735
01:03:34,400 --> 01:03:40,400
At this lowest level, the latches are meant to be so, like, fine grain and short, it's better just to carry yourself.

736
01:03:40,400 --> 01:03:42,400
Right?

737
01:03:42,400 --> 01:03:47,400
So let's say T2 isn't waving on C1.

738
01:03:47,400 --> 01:03:48,400
Okay.

739
01:03:48,400 --> 01:03:56,400
Then, T1 kills itself, tries again, still bad, kills itself again, does it a million times, right?

740
01:03:56,400 --> 01:03:57,400
Yes.

741
01:03:57,400 --> 01:03:59,400
Who would just say that isn't possible?

742
01:03:59,400 --> 01:04:04,400
The question is, could you basically have, could you starve a thread, is the term you want to use?

743
01:04:04,400 --> 01:04:08,400
Could you starve a thread because every single time you try to get something, it can't because someone else is in there.

744
01:04:08,400 --> 01:04:09,400
Could you starve that?

745
01:04:09,400 --> 01:04:10,400
Yes.

746
01:04:10,400 --> 01:04:16,400
So, I think I have a slide on this.

747
01:04:16,400 --> 01:04:23,400
Yeah, so there's, I'll answer your question in a second.

748
01:04:23,400 --> 01:04:31,400
The latches aren't going to have anything to handle deluxe for us, and it's not going to have anything that can prevent starvation.

749
01:04:31,400 --> 01:04:39,400
In the read write latches, you can set priorities like writers or readers, or you want to fight foe around ramen scheduling.

750
01:04:39,400 --> 01:04:42,400
But there's a high level construct that's scheduled that can slide.

751
01:04:42,400 --> 01:04:50,400
Oh, this worker is trying to run this query, teach trying to touch this data structure, and it keeps getting aborted.

752
01:04:50,400 --> 01:04:55,400
And I know it's getting aborted because it's coming back with a retry message.

753
01:04:55,400 --> 01:05:01,400
And therefore, maybe I want to deschedule other workers running at the same time to make sure I always get through.

754
01:05:01,400 --> 01:05:04,400
That's how to basically handle that.

755
01:05:04,400 --> 01:05:12,400
Most systems basically let, you know, let Jesus take the wheel or whatever phrase you want to use, and just, you just let it go at it, right?

756
01:05:12,400 --> 01:05:16,400
Because eventually, you should get through.

757
01:05:16,400 --> 01:05:26,400
Now again, if I have like a thousand query trying to run at the same time trying to all update the same key, there's no magic scheduler that's going to be able to handle that.

758
01:05:26,400 --> 01:05:31,400
Everything's going to get contended and it ended up being a single thread system.

759
01:05:31,400 --> 01:05:43,400
So we want to optimize for the case where we assume that the, we assume content is going to be low, and we want to sort of fast, fail fast, no wait policy, we just check.

760
01:05:43,400 --> 01:05:46,400
Can I do it? No, okay, let me retry again.

761
01:05:46,400 --> 01:05:52,400
Because by the time, you know, when I go, go retry, then I'll be able to do what I need to do.

762
01:05:52,400 --> 01:05:55,400
Why is sending a sound series over about any changes?

763
01:05:55,400 --> 01:06:00,400
Is question, if you, while you're killing yourself, do you have to roll back any changes that you do? Yes, in the code, yes.

764
01:06:00,400 --> 01:06:10,400
So again, if I, going back to the writer example here, like if, if this thread that kills us out, they had updated a bunch of things.

765
01:06:10,400 --> 01:06:16,400
That's why you hold the right latches for those things you've updated so you can go back and reverse those changes.

766
01:06:16,400 --> 01:06:37,400
So what's scenario? Is the backwards, because in this scenario, like for a fine key, what's important is start at the beginning and then go across.

767
01:06:37,400 --> 01:06:42,400
So what, in what scenario do you even need backwards?

768
01:06:42,400 --> 01:06:47,400
What scenario do you need the backwards pointer in the sibling? Because it seems like it's causing problems for us.

769
01:06:47,400 --> 01:06:51,400
You queries this, fine keys less than four, fine keys greater than one.

770
01:06:51,400 --> 01:06:57,400
So for the, because we know everything's ordered, in this case, we could start at one and go into a fine port.

771
01:06:57,400 --> 01:07:01,400
You could start at one and go into a fine port.

772
01:07:02,400 --> 01:07:07,400
So in what scenario? I have a billion keys.

773
01:07:07,400 --> 01:07:13,400
Right? And I, yeah, that's just way more sensitive. Nobody does that.

774
01:07:13,400 --> 01:07:14,400
Yeah.

775
01:07:14,400 --> 01:07:19,400
Essentially, we didn't talk about skitless.

776
01:07:19,400 --> 01:07:30,400
There was a, actually, was single store before, before it was single store with MemSQL, they had these skitless and skitless only have, because it's a lock free data structure, which is a bad idea.

777
01:07:30,400 --> 01:07:35,400
It's another topic, but like they, they had their skitless can only go in one direction.

778
01:07:35,400 --> 01:07:45,400
So they had to do a bunch of tricks of like having ways to jump into the data structure to like try to do reverse and then sort it in reverse after, you know, after you get it out.

779
01:07:45,400 --> 01:07:54,400
It just makes life harder. You can do it and avoids this deadlock issue.

780
01:07:54,400 --> 01:08:00,400
But it's still, for example, he's the boy, if you're traversing down, if I can't acquire the latch as I'm going down, it's not a deadlock.

781
01:08:00,400 --> 01:08:06,400
I still want to kill myself, potentially, usually.

782
01:08:06,400 --> 01:08:18,399
It's still, it's still, you still, you won't have deadlocks if you do what you're proposing, but you still could have latch contention where I can't get the latch, because somebody else holds it.

783
01:08:18,399 --> 01:08:23,399
And in that case, again, usually you want to spin for a little bit and then kill yourself.

784
01:08:23,399 --> 01:08:28,399
Yeah, you still want to kill yourself. That sounds weird, but you don't need.

785
01:08:28,399 --> 01:08:29,399
Yes.

786
01:08:29,399 --> 01:08:38,399
The only issue that we saw was like, the waiting, we don't know how long we're using it for, but, for example, if we're, rather than we know, we've done a lot of work, and you see what we've constantly seen ourselves up.

787
01:08:38,399 --> 01:08:44,399
Would there be like a theoristic where we like want to wait for us in a lot of times, and then like, if you still want the latch, then I don't know how to do it.

788
01:08:44,399 --> 01:08:50,399
He's right. I don't know if he already said, so he basically said, it's, if we're a threat, we know how much work we've done.

789
01:08:50,399 --> 01:08:55,399
And we say we did a lot of updates. We know that there were expensive to do.

790
01:08:55,399 --> 01:09:02,399
So, could we have a theoristic that says, when we spin, we can determine how long we want to wait based on how much work we've done.

791
01:09:02,399 --> 01:09:04,399
Yes, you could do that.

792
01:09:04,399 --> 01:09:08,399
I don't think Post-Gust, my sequel, actually do that. It might be wrong.

793
01:09:08,399 --> 01:09:14,399
Would the shorter is like, just the kind of, just like, going back and doing that in the connected universe?

794
01:09:14,399 --> 01:09:26,399
Would it be beneficial? But you can imagine really simplistic, a counter, I have a counter in my, you know, local address, local memory for my worker, how many pages have I updated?

795
01:09:26,399 --> 01:09:33,399
And for each page of updated, wait, maybe an extra, you know, 100 microseconds. Something like that. You get simple heuristics.

796
01:09:33,399 --> 01:09:38,399
I don't know whether it actually makes sense or not to do it. Again, it's a cop-out. For all cases and databases, it depends on the workload.

797
01:09:38,399 --> 01:09:48,399
It depends on like, you know, if everything, everybody's updating a bunch of stuff, then maybe that's a bad idea. But you have one thread, one worker that updates a little bit of things, then maybe, yeah, that might make sense.

798
01:09:48,399 --> 01:09:56,399
But once again, if everybody's trying to update the same key, it gets, everything gets bogged, bogged down to a single threaded system. That's the extreme case stuff.

799
01:09:56,399 --> 01:09:57,399
Yes.

800
01:09:57,399 --> 01:10:10,399
How do we handle that? They're dead. I mean, they restart.

801
01:10:10,399 --> 01:10:12,399
Yeah.

802
01:10:12,399 --> 01:10:22,399
Yeah, so, so very clear. I mean, maybe a set of some slide. The restart mechanism is transparent to the user.

803
01:10:22,399 --> 01:10:34,399
Yeah, I don't have a slide to this. So like, I run a query and I had to traverse a B-plustry. And I, and to go look at the primary key and I can't get a latch as I'm going down, I don't want to abort the query and go back to the user.

804
01:10:34,399 --> 01:10:47,399
Hey, look, I couldn't get a latch because they don't know what a latch is. Right. And then tell it and restart. We do this transparently for you. So like, so like, you submit one query. It may restart the traversal of the B-plustry multiple times.

805
01:10:47,399 --> 01:10:53,399
And, but you don't see that from the end user of the application. We're doing it internally. It's just the query got a little bit slower because of that.

806
01:10:53,399 --> 01:11:01,399
Yeah, that means a lot of questions. Sorry. Yes.

807
01:11:01,399 --> 01:11:14,399
Is there a scenario where someone has a right latch on the root and they're worried if you restart, you're going to come back and delete.

808
01:11:14,399 --> 01:11:20,399
Yeah, absolutely. Yes. It's unavoidable.

809
01:11:20,399 --> 01:11:38,399
I mean, as the more keys you insert, the the tree gets taller. And therefore the, you know, the likelihood that someone's going to hold a right latch on the root goes down.

810
01:11:38,399 --> 01:11:51,399
Going back to the like the stall stuff too, like, it's not just how much work is the other thread doing like, you have to sort of wait for.

811
01:11:51,399 --> 01:12:02,399
Remember, these, these data structures are backed by pages in the buffer pool that are on a disk. So even though I'm updating one key, the key I need to update might be not in memory.

812
01:12:02,399 --> 01:12:11,399
I got to go out to disk and get it. So that's why like, and you don't want to stall, you know, spin forever for a long time because you don't know like, you know, it has to go get the disk.

813
01:12:11,399 --> 01:12:17,399
If it's a really slow disk, and that's going to be a long time and you can be waiting for 100 milliseconds.

814
01:12:17,399 --> 01:12:27,399
You know, 500 milliseconds. So you said like, oh, yeah, do the average time. I mean, depends on so many factors that be impossible to track these things.

815
01:12:27,399 --> 01:12:34,399
Again, this is again, this is why this is different than like taking a regular data structure, algorithm slots is going to be things are backed by disk.

816
01:12:34,399 --> 01:12:43,399
And we're having multiple threads around at the same time. And we, there's a bunch of things we need to do to like, to hide that those distals.

817
01:12:43,399 --> 01:12:54,399
SQL server is a whole another beast SQL server. They actually had their own user space cover teams. So like, if you're traversing the data structure.

818
01:12:54,399 --> 01:13:04,399
And the thing I need, I can't get the latch instead of just spinning, they go back to their own user space schedule and says, I can't run because I'm waiting for this latch.

819
01:13:04,399 --> 01:13:13,399
And then they take your thread away and have it do some other work. And then they may know what latch are waiting for. They're actually doing some tracking about who's waiting for what latches inside of it.

820
01:13:13,399 --> 01:13:18,399
And they can do that because everything is covered teams in user space.

821
01:13:18,399 --> 01:13:23,399
Very few said nobody else does that. SQL server does some really cool things.

822
01:13:23,399 --> 01:13:28,399
All right, cool. Any other questions? Yes.

823
01:13:28,399 --> 01:13:33,399
Oh, is there like some more? Yeah, I did.

824
01:13:33,399 --> 01:13:35,399
Yeah.

825
01:13:35,399 --> 01:13:38,399
So I said a pretty basic question about the whole set. Yes.

826
01:13:38,399 --> 01:13:47,399
If there's two threads going into the direction, why do you need to hold the latch on the whole page before it's been so you require the latch?

827
01:13:47,399 --> 01:13:56,399
Yes. So this question is, then go back here. This question is, if I'm traversing along with sibling nodes.

828
01:13:56,399 --> 01:14:11,399
This one here, right? If they're trying to get across, why does so T2 is that B, T1 is that C? Why does T2 need to hold the latch on B in order to get to C?

829
01:14:11,399 --> 01:14:18,399
Because you need to know that the sibling pointer is still valid and this is the right node. This is the right node you should be looking to.

830
01:14:18,399 --> 01:14:23,399
And you know that if there was an update because you hold this in read mode, nobody can update it.

831
01:14:23,399 --> 01:14:29,399
So you know that no one's going to replace B with something with some other new version other than how it points to something else.

832
01:14:29,399 --> 01:14:34,399
But you're still going to follow the pointer to whatever you thought was there before.

833
01:14:34,399 --> 01:14:39,399
So you have to hold the latch until you know you're safe on the other side, then you can go ahead and release it.

834
01:14:39,399 --> 01:14:47,399
Same thing going from top down, you need to know that the thing I'm jumping to next is what I should be jumping into.

835
01:14:47,399 --> 01:14:48,399
Yes.

836
01:14:48,399 --> 01:15:01,399
So essentially there are two cases that this is from the leads node, both are open with the addition, but you can log from the top and you mentioned that there are seven converges in the main node.

837
01:15:01,399 --> 01:15:02,399
Yes.

838
01:15:02,399 --> 01:15:14,399
So how do you distinguish that this, you want to want to write and your case will log on the whole strut.

839
01:15:14,399 --> 01:15:24,399
Now how do you know that this is not the case where the board of them are going to open the direction for the two cases, how do they distinguish this from that?

840
01:15:24,399 --> 01:15:37,399
Yes. So it's question, save it is a question that like last time I talked about how systems like Postgres have sibling pointers at internodes, even though I'm only showing leaf nodes here.

841
01:15:37,399 --> 01:15:45,399
And if I use those internodes sibling pointers to jump again horizontally, how do I take latches on those and make sure things are still correct.

842
01:15:45,399 --> 01:15:49,399
So the protocol, everything I'm describing here would still work.

843
01:15:49,399 --> 01:15:56,399
If for read that simple, could you just take the read latch across.

844
01:15:56,399 --> 01:16:05,399
Because anybody else coming up, coming above you once you are right, they'll see your read latch and they'll stop anything below that side of the tree that was doing update, you'll get blocked.

845
01:16:05,399 --> 01:16:08,399
You take the read down. So that's fine.

846
01:16:08,399 --> 01:16:18,399
For doing updates, I think it works the same way as you come across if what you're trying to do below is not safe, then you still hold latches for those things.

847
01:16:18,399 --> 01:16:23,399
So the protocol still works even if you have to go across horizontally and go down.

848
01:16:23,399 --> 01:16:30,399
You still have the deadlock if everyone's trying to go across vertically or horizontally on you too, and then you do the same thing on the strap here.

849
01:16:30,399 --> 01:16:34,399
There's another question, sorry.

850
01:16:34,399 --> 01:16:38,399
Okay, cool.

851
01:16:38,399 --> 01:16:40,399
So, all right.

852
01:16:40,399 --> 01:16:41,399
Just to finish up.

853
01:16:41,399 --> 01:16:44,399
So, this is hard.

854
01:16:44,399 --> 01:16:51,399
And I'm showing you the most simple version to do latch crapping and that's detection.

855
01:16:51,399 --> 01:16:56,399
We're not going to cover this in this class, but there's way more complicated schemes.

856
01:16:56,399 --> 01:16:58,399
You can have Virgin latches.

857
01:16:58,399 --> 01:17:01,399
You can have delayed updates.

858
01:17:01,399 --> 01:17:03,399
You can do the BF salon tree stuff for you to lay things.

859
01:17:03,399 --> 01:17:06,399
There's a bunch of other stuff you can do this.

860
01:17:06,399 --> 01:17:09,399
The BW tree is a lock free BF plus tree from Microsoft.

861
01:17:09,399 --> 01:17:11,399
That's a whole other nightmare.

862
01:17:11,399 --> 01:17:23,399
But again, this is hard, but this is good because you take this class and this is why you don't want your random JavaScript program or building your BF plus trees or data structures in your database systems.

863
01:17:23,399 --> 01:17:29,399
You want to see new students like you guys that know what the hell they're doing and make sure that you don't cause problems.

864
01:17:29,399 --> 01:17:32,399
And so, again, we talked about hash tables.

865
01:17:32,399 --> 01:17:39,399
We talked about B plus trees today, but these techniques of this idea of like everything's going the same direction or I kill myself as soon as I can't get something and restart.

866
01:17:39,399 --> 01:17:46,399
Like this is relevant to a bunch of other data structures in systems as well.

867
01:17:46,399 --> 01:17:52,399
I feel like we should just call this course, kill yourself, right, which is not.

868
01:17:52,399 --> 01:17:55,399
I was asking for CMU to get involved. I don't need that trouble.

869
01:17:55,399 --> 01:17:59,399
One year somebody did complain that I did say, kill yourself a lot.

870
01:17:59,399 --> 01:18:00,399
Sorry.

871
01:18:00,399 --> 01:18:05,399
All right. So, next class, we're talking about sorting or sort of aggrautions.

872
01:18:05,399 --> 01:18:09,399
So, like this point, like we're moving up the stack, now we can actually start executing queries.

873
01:18:09,399 --> 01:18:11,399
Fantastic, right?

874
01:18:11,399 --> 01:18:15,399
So, I won't be here on Monday or I'm not teaching Tignesh Patel.

875
01:18:15,399 --> 01:18:17,399
He'll be the other professor.

876
01:18:17,399 --> 01:18:19,399
He's going to start teaching on Monday.

877
01:18:19,399 --> 01:18:22,399
And then Wednesday next week, P and I are both going to be gone.

878
01:18:22,399 --> 01:18:26,399
I'm going to the Post-Ghost Conference in New York. I'm giving a keynote there about databases.

879
01:18:26,399 --> 01:18:30,399
I don't know what Tignesh is. He might have to go talk to his pro officer.

880
01:18:30,399 --> 01:18:32,399
But like, what are my PhD students?

881
01:18:32,399 --> 01:18:36,399
My number one PhD student, Matt Boucherbitch, will be teaching on Wednesday next week about joints.

882
01:18:36,399 --> 01:18:37,399
Okay?

883
01:18:37,399 --> 01:18:44,399
And then, yeah, Tignesh is awesome. Tignesh asked him about growing up in India because he, like,

884
01:18:44,399 --> 01:18:48,399
before he joined the CME, he was telling me crazy stories. He used to get in fights every morning on the bus going to school.

885
01:18:48,399 --> 01:18:50,399
And I think he cared a knife.

886
01:18:50,399 --> 01:18:52,399
Ask him about that.

887
01:18:52,399 --> 01:18:56,399
And then we'll talk about the midterm on next week as well.

888
01:18:56,399 --> 01:18:57,399
Okay?

889
01:18:57,399 --> 01:18:58,399
All right.

890
01:18:58,399 --> 01:18:59,399
Good day.

891
01:19:14,399 --> 01:19:16,399
Yeah.

892
01:19:44,399 --> 01:19:46,399
Yeah.

