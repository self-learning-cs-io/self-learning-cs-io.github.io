---
title: CMU15445 P7F202306 DatabaseMemoryDiskIâ§¸OManagement
---

1
00:00:00,000 --> 00:00:09,220
besonders system commit such a

2
00:00:09,220 --> 00:00:30,660
How was the concert on Saturday?

3
00:00:30,660 --> 00:00:33,620
Even though it might be a red line of bunch of speakers.

4
00:00:33,620 --> 00:00:34,859
What do you mean to red line speakers?

5
00:00:34,859 --> 00:00:40,539
I mean, if the speaker can work at 100%, we were working at 200%.

6
00:00:40,539 --> 00:00:41,979
Did you break them?

7
00:00:41,979 --> 00:00:43,140
Next time maybe.

8
00:00:43,140 --> 00:00:43,859
OK, nice.

9
00:00:43,859 --> 00:00:44,420
All right.

10
00:00:44,420 --> 00:00:45,619
How many of you showed up?

11
00:00:45,619 --> 00:00:46,299
200?

12
00:00:46,299 --> 00:00:46,619
200.

13
00:00:46,619 --> 00:00:47,579
That's impressive.

14
00:00:47,579 --> 00:00:47,859
OK.

15
00:00:47,859 --> 00:00:49,219
And your next show is when?

16
00:00:49,219 --> 00:00:51,259
October, after the fall break.

17
00:00:51,259 --> 00:00:52,299
That's off campus, though.

18
00:00:52,299 --> 00:00:52,820
OK.

19
00:00:52,820 --> 00:00:53,980
We'll post that on PSA.

20
00:00:53,980 --> 00:00:54,460
All right, guys.

21
00:00:54,460 --> 00:00:55,620
Lots of cover.

22
00:00:55,620 --> 00:00:58,659
Some quick administrative things to cover quickly.

23
00:00:58,659 --> 00:01:01,620
So the homework, too, has been out.

24
00:01:01,620 --> 00:01:04,099
We bumped the due date to be October 4th,

25
00:01:04,099 --> 00:01:09,219
because we didn't want to have it lined up on the same date that Project 1 is due.

26
00:01:09,219 --> 00:01:12,500
So the material problem, all it's due, is the sketching with Labor Day,

27
00:01:12,500 --> 00:01:13,099
everything.

28
00:01:13,099 --> 00:01:15,140
So the material of that homework discusses things.

29
00:01:15,140 --> 00:01:15,939
We'll discuss next week.

30
00:01:15,939 --> 00:01:17,659
So we postponed that a day.

31
00:01:17,659 --> 00:01:18,899
And then Project 1 still on track.

32
00:01:18,899 --> 00:01:20,619
We do October 2nd.

33
00:01:20,619 --> 00:01:21,979
And again, that's on a Sunday.

34
00:01:21,979 --> 00:01:27,419
We're having the Q&A session today, Monday, 6.30 PM.

35
00:01:27,419 --> 00:01:29,179
And that'll be on Zoom.

36
00:01:29,179 --> 00:01:31,299
And there's a post on that in Piazza.

37
00:01:31,299 --> 00:01:33,299
And then, as for all the projects again,

38
00:01:33,299 --> 00:01:37,819
there won't be any office hours on Sundays when the project is due.

39
00:01:37,819 --> 00:01:40,259
But there'll be a special office hour session

40
00:01:40,259 --> 00:01:47,219
on the Saturday before it's due on campus with like 4.0 TAs between 3 and 5 PM.

41
00:01:47,219 --> 00:01:49,340
This meant to be a forcing function for you guys actually,

42
00:01:49,340 --> 00:01:52,979
like, start working on the project instead of showing up the dates due to office hours.

43
00:01:52,979 --> 00:01:55,340
And you know, it doesn't compile, right?

44
00:01:55,340 --> 00:02:00,060
We want you to start sooner rather than later, OK?

45
00:02:00,060 --> 00:02:01,620
So many's already gotten, I think there's a couple of people already got

46
00:02:01,620 --> 00:02:03,540
100% on Project 1.

47
00:02:03,540 --> 00:02:07,980
Even though today's lecture discusses Project 1, so impressive.

48
00:02:07,980 --> 00:02:08,980
Yes.

49
00:02:08,980 --> 00:02:10,300
How long do you expect it to be in Project 1?

50
00:02:10,300 --> 00:02:11,539
Sorry, how much you expected what?

51
00:02:11,539 --> 00:02:15,700
How much time do you expect the complete entire project?

52
00:02:15,700 --> 00:02:20,060
It's question is how much time should we expect to complete the entire project?

53
00:02:20,060 --> 00:02:21,580
And it depends on your background.

54
00:02:21,580 --> 00:02:23,500
Like, some people have ripped through C++, no problems.

55
00:02:23,500 --> 00:02:26,580
Other people struggle, right?

56
00:02:26,580 --> 00:02:30,219
Project 1 is, I mean, it's more work than Project 0.

57
00:02:30,219 --> 00:02:31,699
It's less work than Project 1.

58
00:02:31,699 --> 00:02:33,099
It's reasonable.

59
00:02:33,099 --> 00:02:35,099
It's not bad.

60
00:02:35,099 --> 00:02:36,060
Any other questions?

61
00:02:36,060 --> 00:02:38,259
I'll give you a little more.

62
00:02:38,259 --> 00:02:42,259
All right, so a bunch of other things that are extra-crickly are things that you can do

63
00:02:42,259 --> 00:02:43,460
in the class.

64
00:02:43,460 --> 00:02:48,659
Today, we're having another seminar talk from CME alum, Dana Van Aken.

65
00:02:48,659 --> 00:02:50,060
Just talk about auto-tune.

66
00:02:50,060 --> 00:02:51,699
Can I start up that I'm involved in?

67
00:02:51,699 --> 00:02:55,379
The next week we'll have one of the co-founders of Process ML.

68
00:02:55,379 --> 00:03:03,659
This is a hosted version of Postgres where they put UDFs or you can make like PyTorch calls

69
00:03:03,659 --> 00:03:05,659
directly in SQL.

70
00:03:05,659 --> 00:03:09,900
They also been working on a Postgres proxy called PGCAT that's written in Rust, Engage,

71
00:03:09,900 --> 00:03:10,900
and Property.

72
00:03:10,900 --> 00:03:15,620
And then the week after that, it should be not the 18th, October 2nd, that we'll have

73
00:03:15,620 --> 00:03:20,300
the CTO and co-founder of Weve8, one of these vector databases, come and talk with us

74
00:03:20,300 --> 00:03:21,300
on Zoom.

75
00:03:21,300 --> 00:03:22,300
Okay.

76
00:03:22,300 --> 00:03:23,300
Yes?

77
00:03:23,380 --> 00:03:26,580
Are there any internship opportunities for any of these companies?

78
00:03:26,580 --> 00:03:29,380
This question is, are there any opportunities for any of these companies?

79
00:03:29,380 --> 00:03:32,860
Absolutely, yes.

80
00:03:32,860 --> 00:03:37,540
We can, we may figure out what best way to free contact them, but yes.

81
00:03:37,540 --> 00:03:41,740
All the companies are hiring, right?

82
00:03:41,740 --> 00:03:44,700
So there was two big data that's used in the last week since the last half class.

83
00:03:44,700 --> 00:03:51,860
They already know what the note they were.

84
00:03:51,860 --> 00:03:54,940
Postgres released version 16.

85
00:03:54,940 --> 00:03:58,380
This is not a big game changer in terms of like they don't have like, you know, amazing

86
00:03:58,380 --> 00:03:59,380
new features.

87
00:03:59,380 --> 00:04:01,220
It's a lot of refinement and improvements.

88
00:04:01,220 --> 00:04:03,220
They're nice to have.

89
00:04:03,220 --> 00:04:08,620
You know, it's, it's, it says, a game changer improvement be like in my opinion is when

90
00:04:08,620 --> 00:04:14,460
they added just in time compilation for ware clauses, which we'll cover in a few weeks.

91
00:04:14,460 --> 00:04:16,580
De-duplication for Btrees is kind of nice.

92
00:04:16,579 --> 00:04:22,180
It's a bunch of I.O. stuff that's still not in ready for production, which we'll talk

93
00:04:22,180 --> 00:04:25,180
about a little bit about today, but like, it's a nice to have.

94
00:04:25,180 --> 00:04:27,659
Postgres has been putting out releases once a year and it's been kind of nice.

95
00:04:27,659 --> 00:04:34,259
And then Databricks announced they raised a series I for $500 million with a, like, a $43

96
00:04:34,259 --> 00:04:35,259
billion valuation.

97
00:04:35,259 --> 00:04:37,419
That's a lot of money.

98
00:04:37,419 --> 00:04:40,659
Goes, goes that saying.

99
00:04:40,659 --> 00:04:45,299
Yeah, I don't, I don't, snow like didn't raise this amount, this amount before they went

100
00:04:45,299 --> 00:04:46,299
IPO.

101
00:04:47,020 --> 00:04:49,020
Anyway, they're hiring.

102
00:04:49,020 --> 00:04:52,780
All right, so, all right.

103
00:04:52,780 --> 00:04:59,500
So last class was, we finished up the discussion on the storage aspect of database systems and

104
00:04:59,500 --> 00:05:05,819
it was really focusing on how the database system is going to represent data in, in disk.

105
00:05:05,819 --> 00:05:09,580
We talked about the two point in the storage, log structure storage, the index or any storage,

106
00:05:09,580 --> 00:05:10,580
right.

107
00:05:10,580 --> 00:05:13,860
Now we know what these files look like on disk, how they're broken up into pages.

108
00:05:13,860 --> 00:05:19,460
And so today's class and going forward is really about how do we get those pages from

109
00:05:19,460 --> 00:05:24,379
disk, bring those into memory, and then do something with them.

110
00:05:24,379 --> 00:05:27,980
And of course, the whole goal of what we're trying to build in this conceptual system

111
00:05:27,980 --> 00:05:32,300
we're talking about is having a database system that gives the illusion that we have more

112
00:05:32,300 --> 00:05:34,620
memory than we actually have, right.

113
00:05:34,620 --> 00:05:36,699
The database is larger than what fits a memory.

114
00:05:36,699 --> 00:05:39,620
We want to make it look like we could fit everything in memory.

115
00:05:39,620 --> 00:05:43,819
So today's really about how do we go get the things we need from disk, those pages,

116
00:05:43,819 --> 00:05:48,099
bring them into memory and then make decisions on how to remove them in order to save space

117
00:05:48,099 --> 00:05:51,259
when we want to bring new things in.

118
00:05:51,259 --> 00:05:53,300
All right.

119
00:05:53,300 --> 00:05:58,139
So there's two key aspects where we have to consider in any decisions we're making on

120
00:05:58,139 --> 00:06:01,019
how we move data back and forth in disk.

121
00:06:01,019 --> 00:06:05,579
The first is, wherever you want to write our pages on disk, can we sort of lay them out

122
00:06:05,579 --> 00:06:09,019
in such a way that we can maximize the amount of sequential IO we're doing instead of

123
00:06:09,019 --> 00:06:11,980
doing random IO.

124
00:06:11,980 --> 00:06:16,420
And the idea here is that we want to keep pages that are going to be used together, possibly

125
00:06:16,420 --> 00:06:22,460
close to each other physically on disk, so that when a query is running or some task is

126
00:06:22,460 --> 00:06:25,540
running inside our database system, we have to go fetch a bunch of pages.

127
00:06:25,540 --> 00:06:28,540
We want this page to be sequential when we bring those into memory.

128
00:06:28,540 --> 00:06:33,780
So the next thing that we consider is, when we have to go read things into memory, and

129
00:06:33,780 --> 00:06:38,259
this is typically done on demand, meaning we'll see prefetching in a second, but you're

130
00:06:38,259 --> 00:06:41,819
not just the system that isn't going to go randomly reading things because it wants to.

131
00:06:41,819 --> 00:06:42,819
All right.

132
00:06:42,819 --> 00:06:46,060
If you insert about your data and then you never go read it, it's not going to go read

133
00:06:46,060 --> 00:06:49,339
back into memory just for the hell of it, right?

134
00:06:49,339 --> 00:06:50,899
And so it's going to be on demand.

135
00:06:50,899 --> 00:06:55,060
We have to go get things from disk, bringing it into memory.

136
00:06:55,060 --> 00:06:59,740
And then the question is going to be, when do we want to evict that from memory?

137
00:06:59,740 --> 00:07:00,740
Right.

138
00:07:00,740 --> 00:07:01,980
What was the last time it was access?

139
00:07:01,980 --> 00:07:02,980
How was access?

140
00:07:02,980 --> 00:07:04,779
Was it updated since we brought it into memory?

141
00:07:04,779 --> 00:07:06,420
And therefore we need to write it out.

142
00:07:06,420 --> 00:07:12,100
We're not going to talk about how to make sure that we can save our changes in case of

143
00:07:12,100 --> 00:07:13,780
a crash or a failure.

144
00:07:13,780 --> 00:07:19,379
We'll talk a little bit about that at the end, but that will be a major focus on the

145
00:07:19,379 --> 00:07:21,860
major focus of a lecture after the midterm.

146
00:07:21,860 --> 00:07:25,660
This is really about, OK, I got to decide what data to evict.

147
00:07:25,660 --> 00:07:29,180
How do I make that decision?

148
00:07:29,180 --> 00:07:31,980
And so this is that same diagram I showed before.

149
00:07:31,980 --> 00:07:36,379
Well we have the database file on disk and it's broken up to a bunch of pages.

150
00:07:36,379 --> 00:07:40,460
And then now what we're talking about today is this piece here called the buffer pool.

151
00:07:40,460 --> 00:07:45,939
I'll call it the buffer pool manager, some places or some of systems called the buffer pool

152
00:07:45,939 --> 00:07:50,060
cache, cache manager.

153
00:07:50,060 --> 00:07:53,220
The all basically mean the same thing, but it's the memory that the database system is going

154
00:07:53,220 --> 00:07:57,259
to allocate from the operating system and control on its own.

155
00:07:57,259 --> 00:08:02,139
So when you have your execution engine, it starts doing something like it saves running

156
00:08:02,139 --> 00:08:03,139
a query.

157
00:08:03,139 --> 00:08:05,579
It doesn't have to, but it could.

158
00:08:05,579 --> 00:08:08,659
At some point it says I need to get page number two.

159
00:08:08,659 --> 00:08:12,019
So again we go get the page directory from disk, bring that into memory if it's not already

160
00:08:12,019 --> 00:08:13,019
there.

161
00:08:13,019 --> 00:08:15,219
Look on our page directory and that's going to tell us our page number two.

162
00:08:15,219 --> 00:08:18,620
Here's the file and the offset where to find that particular page.

163
00:08:18,620 --> 00:08:24,379
We go fetch that memory, put into one of our free space in our buffer pool, and then give

164
00:08:24,379 --> 00:08:31,459
back the execution engine a pointer to that page sitting in the buffer pool.

165
00:08:31,459 --> 00:08:36,860
So now let's say again we run some kind of a fiction policy, a replacement policy and

166
00:08:36,860 --> 00:08:41,019
we decide to remove page two for whatever reason because we need a more space.

167
00:08:41,019 --> 00:08:43,500
We ran about your other data, it doesn't matter.

168
00:08:43,500 --> 00:08:47,860
But now when the execution engine comes back and says I give me page two again, it's not

169
00:08:47,860 --> 00:08:51,980
memory, you got to go go to disk and get it, but this time it actually might land in a

170
00:08:51,980 --> 00:08:56,220
different location in our buffer pool.

171
00:08:56,220 --> 00:08:59,580
And again our system, the execution engine, all the other parts of the system above the

172
00:08:59,580 --> 00:09:07,060
buffer pool, they should obviously not be not care that it's now in a different location.

173
00:09:07,060 --> 00:09:08,940
This is different than men map, right?

174
00:09:08,940 --> 00:09:12,580
If you end map a file, the memory map file, when you bring that into your address space,

175
00:09:12,580 --> 00:09:17,379
anytime you jump to that sort of map file address space, you're always going to get the same

176
00:09:17,379 --> 00:09:21,379
page or the odd or oddity service guarantee that for you.

177
00:09:21,379 --> 00:09:25,439
It may not be in memory when you're accessing and you get installed while it gets fetches

178
00:09:25,439 --> 00:09:28,340
in, but it's always going to be in the same address space.

179
00:09:28,340 --> 00:09:29,740
In our system we're not going to do that.

180
00:09:29,740 --> 00:09:33,580
The same page can be in different locations every time it's brought in and out of memory.

181
00:09:33,580 --> 00:09:34,580
Right?

182
00:09:34,580 --> 00:09:38,139
Because we need that freedom because who knows what's going to be in memory the next time

183
00:09:38,139 --> 00:09:39,620
we go fetch a page.

184
00:09:39,620 --> 00:09:43,580
And so again, this is much different than when then calling malloc.

185
00:09:43,580 --> 00:09:47,700
When you call malloc, the OS is taking care of us all for you.

186
00:09:47,700 --> 00:09:50,860
The database system is managing all this memory.

187
00:09:50,860 --> 00:09:53,980
Because as we see, as we go along, it's always going to be in a better position to make

188
00:09:53,980 --> 00:09:56,580
the best decision how to optimize this.

189
00:09:56,580 --> 00:09:57,580
Right?

190
00:09:57,580 --> 00:10:00,860
So for today's agenda, we're going to talk about a high level of up-to-people manager is.

191
00:10:00,860 --> 00:10:04,100
They're talking about some optimizations we can add to it.

192
00:10:04,100 --> 00:10:05,860
The order is actually switched.

193
00:10:05,860 --> 00:10:09,220
Then we'll talk about buffer-pull-for-place-and-polices, then disk-Ios-casualing, and then

194
00:10:09,220 --> 00:10:15,420
we'll briefly mention that there's other memory pools in our database system that may not

195
00:10:15,420 --> 00:10:17,420
be always backed by a buffer-for-manager.

196
00:10:17,420 --> 00:10:18,420
Right?

197
00:10:18,419 --> 00:10:21,860
We're going to be looking at an ephemeral cache for certain things.

198
00:10:21,860 --> 00:10:23,659
So again, I'm going to call it a buffer-for-manager.

199
00:10:23,659 --> 00:10:25,500
The textbook calls a buffer manager.

200
00:10:25,500 --> 00:10:28,740
I think Oracle might call it the buffer cache.

201
00:10:28,740 --> 00:10:32,459
We're all talking about the same thing.

202
00:10:32,459 --> 00:10:34,299
So the high level looks like this.

203
00:10:34,299 --> 00:10:39,740
Again, it's just a region of memory that we've allocated from the OS, and we're going to

204
00:10:39,740 --> 00:10:44,980
logically chunk it up into fixed-size pages, again, based on the page size of the database

205
00:10:44,980 --> 00:10:45,980
system.

206
00:10:45,980 --> 00:10:46,980
Right?

207
00:10:46,980 --> 00:10:49,220
We said, Postgres is 8 kilobytes.

208
00:10:49,220 --> 00:10:50,220
My SQL is 16 kilobytes.

209
00:10:50,220 --> 00:10:51,460
It's going to have compression size.

210
00:10:51,460 --> 00:10:52,460
Right?

211
00:10:52,460 --> 00:10:53,460
It doesn't matter.

212
00:10:53,460 --> 00:10:56,420
We're breaking it up based on those page sizes.

213
00:10:56,420 --> 00:11:03,860
And then an entry or a location in our buffer-poled memory that we could use to install a page,

214
00:11:03,860 --> 00:11:06,659
we're going to call that a frame.

215
00:11:06,659 --> 00:11:08,060
So I think the system boots up.

216
00:11:08,060 --> 00:11:09,060
It calls malloc.

217
00:11:09,060 --> 00:11:10,580
It gets a bunch of memory.

218
00:11:10,580 --> 00:11:13,539
And then it says, say, break it up and divide it up into frames.

219
00:11:13,539 --> 00:11:19,059
And then as the database system or other parts of the execution or whatever, start requesting

220
00:11:19,059 --> 00:11:25,339
pages, we're going to make an exact copy of the pages from disk into memory and put it

221
00:11:25,339 --> 00:11:26,339
into one of these frames.

222
00:11:26,339 --> 00:11:29,419
And the reason why we get to call it frames is because we're running out of terms.

223
00:11:29,419 --> 00:11:30,419
Right?

224
00:11:30,419 --> 00:11:31,419
Can't call it a page.

225
00:11:31,419 --> 00:11:32,419
Can't call it a block because we already used that.

226
00:11:32,419 --> 00:11:35,179
Can't call it a slot because we have a slot array.

227
00:11:35,179 --> 00:11:37,819
So for whatever reason, we're going to call it a frame.

228
00:11:37,819 --> 00:11:38,819
All right.

229
00:11:38,819 --> 00:11:41,059
So again, so somebody needs page one.

230
00:11:41,059 --> 00:11:46,579
And we go find a free frame in our buffer pool and we just copy that page on disk into memory.

231
00:11:46,579 --> 00:11:47,579
Same thing here.

232
00:11:47,579 --> 00:11:50,779
I need page three, find a free slot, and I copy it into memory here.

233
00:11:50,779 --> 00:11:57,059
But again, here we can see that page one and page three are not continuous on disk because

234
00:11:57,059 --> 00:11:58,659
there's page two in between them.

235
00:11:58,659 --> 00:12:01,819
But when we put in our buffer pool and bring it into memory, we're again, we're free to

236
00:12:01,819 --> 00:12:04,500
put it in any location that we want.

237
00:12:04,500 --> 00:12:07,939
It doesn't matter.

238
00:12:07,940 --> 00:12:16,580
So now, if we modify one of these pages, we're not going to, we're not required to flush

239
00:12:16,580 --> 00:12:18,180
the data back to disk right away.

240
00:12:18,180 --> 00:12:22,540
And again, we will cover durability and the recovery after the midterm.

241
00:12:22,540 --> 00:12:26,580
But this is a key difference then between a right through cache or a right back cache.

242
00:12:26,580 --> 00:12:27,580
Right?

243
00:12:27,580 --> 00:12:31,820
With a right through cache in the OS, when you write something to the cache, it then gets

244
00:12:31,820 --> 00:12:34,020
immediately written out the disk as well.

245
00:12:34,019 --> 00:12:38,139
And a right back cache will write it in memory, but we're not required to write it back

246
00:12:38,139 --> 00:12:39,139
right away.

247
00:12:39,139 --> 00:12:40,139
We'll do it at some later point.

248
00:12:40,139 --> 00:12:44,059
There'll be a background thread or an eviction policy that'll do this.

249
00:12:44,059 --> 00:12:48,699
And so we won't talk about this today, but there'll be a separate log file right ahead

250
00:12:48,699 --> 00:12:52,460
log that'll keep track of what changes we made.

251
00:12:52,460 --> 00:12:57,299
And we'll make sure that thing gets flushed to disk before our dirty pages do.

252
00:12:57,299 --> 00:12:58,299
We don't have to know that for now.

253
00:12:58,299 --> 00:13:02,579
I'm just be mindful that even though we may update pages, we're not required to write

254
00:13:02,580 --> 00:13:06,660
them back right away in memory.

255
00:13:06,660 --> 00:13:10,540
I said that the internal data structure we're going to use to keep track of what is actually

256
00:13:10,540 --> 00:13:13,180
in our frames is going to be called the page table.

257
00:13:13,180 --> 00:13:15,300
Again, the OS has its own page table.

258
00:13:15,300 --> 00:13:16,780
This is the database system's page table.

259
00:13:16,780 --> 00:13:17,780
It's better.

260
00:13:17,780 --> 00:13:23,940
And so it's typically going to be a fixed size hash table that is just keeping track of,

261
00:13:23,940 --> 00:13:28,259
here's all my frames, right, identified by some frame ID.

262
00:13:28,259 --> 00:13:35,100
And then here's the page information that's currently residing in that page.

263
00:13:35,100 --> 00:13:39,340
I could just be a pointer to where that page is actually located.

264
00:13:39,340 --> 00:13:42,500
And we're going to have to protect this page table with a latch.

265
00:13:42,500 --> 00:13:47,460
But I'll describe in a second, think of it as a mutex, that allows us to have multiple

266
00:13:47,460 --> 00:13:53,259
threads or multiple workers accessing the page table at the same time.

267
00:13:53,259 --> 00:13:54,860
I don't want to use term threads.

268
00:13:54,860 --> 00:13:59,100
I would better use terms workers because I can postgres and older systems.

269
00:13:59,100 --> 00:14:00,940
They're not multi-ferred.

270
00:14:00,940 --> 00:14:02,220
They're multi-process.

271
00:14:02,220 --> 00:14:04,220
And so the idea is still the same.

272
00:14:04,220 --> 00:14:07,539
We want to make sure that if there's multiple workers touching things and updating things,

273
00:14:07,539 --> 00:14:13,500
they don't have, when we're in the critical sections, we don't break things.

274
00:14:13,500 --> 00:14:18,180
So in addition to keeping track of like, here's the pointer to the page in our buffer pool

275
00:14:18,180 --> 00:14:22,120
and in the frame, we're also going to have additional metadata about how the pages are

276
00:14:22,120 --> 00:14:25,519
being used throughout the system.

277
00:14:25,519 --> 00:14:29,480
So the first thing we would have, obviously, is a dirty flag that tells us whether a query

278
00:14:29,480 --> 00:14:32,120
has updated a page since we last brought it into memory.

279
00:14:32,120 --> 00:14:39,740
We'll also have a pin or reference counter that keeps track of the number of workers that

280
00:14:39,740 --> 00:14:44,360
require this page to remain in memory, and therefore it can't be evicted when we run our

281
00:14:44,360 --> 00:14:46,440
addiction policy.

282
00:14:46,440 --> 00:14:51,840
So for each page, say page three here, say there's some query that is accessing it at this

283
00:14:51,840 --> 00:14:53,000
given time.

284
00:14:53,000 --> 00:14:54,840
So in our page table, we have a little counter.

285
00:14:54,840 --> 00:15:00,040
It says there's at least one worker that's accessing it.

286
00:15:00,040 --> 00:15:06,560
And then now say if another query comes along and is looking for another page that's not

287
00:15:06,560 --> 00:15:11,600
in our page table, we'll put a latch on it, protect it, go fetch the data we need.

288
00:15:11,600 --> 00:15:13,360
So in this case, we need page two.

289
00:15:13,360 --> 00:15:18,360
We update a free frame in our buffer pool, update the page table to now point to this buffer

290
00:15:18,360 --> 00:15:21,560
pool, update any metadata we need to know about it.

291
00:15:21,559 --> 00:15:26,439
Like who accessed it when they last accessed it and so forth.

292
00:15:26,439 --> 00:15:31,239
And then once this query is done doing whatever its update needs to the page table, we can

293
00:15:31,239 --> 00:15:35,679
release the latch for turn that worker back to whatever it's doing.

294
00:15:35,679 --> 00:15:39,399
And then now any other worker that comes along looking for page two will find it in this

295
00:15:39,399 --> 00:15:44,319
page table.

296
00:15:44,319 --> 00:15:48,639
Pretty simple, right?

297
00:15:48,639 --> 00:15:54,639
So I use this term latch and purposely did not say lock.

298
00:15:54,639 --> 00:15:57,480
Everything I guess, why?

299
00:15:57,480 --> 00:15:58,480
Yes.

300
00:15:58,480 --> 00:16:06,279
So he says the standard locks that we have, locks and latches we have are different, different

301
00:16:06,279 --> 00:16:07,279
from who or what.

302
00:16:07,279 --> 00:16:27,079
So he says that there's that latches have some database magic that's better for our purposes

303
00:16:27,079 --> 00:16:30,000
versus locks.

304
00:16:30,000 --> 00:16:31,000
Not quite.

305
00:16:31,000 --> 00:16:32,000
Yes.

306
00:16:32,000 --> 00:16:33,000
Yes.

307
00:16:33,000 --> 00:16:34,720
Locks are for user space.

308
00:16:34,720 --> 00:16:35,720
Yes.

309
00:16:35,720 --> 00:16:37,720
He says locks are for user space.

310
00:16:37,720 --> 00:16:41,500
In the, I mean the database system is running in user space, but it's for, let's say logical

311
00:16:41,500 --> 00:16:42,519
things in the database.

312
00:16:42,519 --> 00:16:43,519
Correct.

313
00:16:43,519 --> 00:16:44,519
Yes.

314
00:16:44,519 --> 00:16:47,879
So this trips people up when they come from like a more OS background.

315
00:16:47,879 --> 00:16:51,679
So in the database world, we have this decision between locks and latches.

316
00:16:51,679 --> 00:16:56,559
So a lock is to protect these higher level concepts or objects in our database.

317
00:16:56,559 --> 00:16:59,960
A tuple, a table, a database, right?

318
00:16:59,960 --> 00:17:02,120
I take locks on these things.

319
00:17:02,120 --> 00:17:04,460
And what will happen is we haven't discussed what transactions are just, yeah, but think

320
00:17:04,460 --> 00:17:05,960
of like I want to do multiple updates.

321
00:17:05,960 --> 00:17:08,720
It's like multiple round trips of SQL queries.

322
00:17:08,720 --> 00:17:14,640
And so if I take a lock on something, I want to hold it for the length of that transaction.

323
00:17:14,640 --> 00:17:19,519
And because he's saying user space, but it's like the application is the one that's creating

324
00:17:19,519 --> 00:17:24,000
these locks or the database is creating these locks of the application, we assume that they're

325
00:17:24,000 --> 00:17:28,680
stupid and therefore we need to make sure that they don't have dead locks or other problems.

326
00:17:28,680 --> 00:17:32,680
So we have to have these digital protection mechanisms to make sure that the JavaScript

327
00:17:32,680 --> 00:17:36,560
program or doesn't do something they shouldn't be doing.

328
00:17:36,560 --> 00:17:40,519
Latches are the low level internal primitives we're going to use to protect the critical

329
00:17:40,519 --> 00:17:43,560
sections of our database system.

330
00:17:43,560 --> 00:17:45,680
And these are what the database system developers are using.

331
00:17:45,680 --> 00:17:48,680
You had to use in Project Zero, right?

332
00:17:48,680 --> 00:17:50,440
You had to take a mutex, right?

333
00:17:50,440 --> 00:17:55,640
So a latch is basically like a low level mutex, right?

334
00:17:55,640 --> 00:18:01,240
And because the latches are being used by the database system developers, meaning us,

335
00:18:01,240 --> 00:18:06,200
right, it's not going to have the the the the deadlock detection and other protection mechanisms

336
00:18:06,200 --> 00:18:09,640
we need because if we're the ones building a database system, we need to be smart enough

337
00:18:09,640 --> 00:18:12,400
to make sure we don't have deadlocks.

338
00:18:12,400 --> 00:18:16,080
And so latches are really meant to be like quick in and out critical section, do something

339
00:18:16,080 --> 00:18:17,080
and release it.

340
00:18:17,080 --> 00:18:20,400
And we need to do through through program or discipline, we need where are the ones

341
00:18:20,400 --> 00:18:24,400
that have to make sure that we don't have deadlocks.

342
00:18:24,400 --> 00:18:29,200
Now this is confusing because this is also in C++, the standard library, there's also

343
00:18:29,200 --> 00:18:30,280
they use the term latches.

344
00:18:30,280 --> 00:18:32,240
But that's just a countdown barrier.

345
00:18:32,240 --> 00:18:34,920
We don't want that, we don't need that, we're going to roll around latches.

346
00:18:34,920 --> 00:18:38,480
We'll see this more next week when we talk about index and currency troll and for B plus

347
00:18:38,480 --> 00:18:39,480
trees.

348
00:18:39,480 --> 00:18:44,800
But for now, just assume you can treat it as the mutex.

349
00:18:44,800 --> 00:18:49,680
We don't want to use the OS mutex, as that has other problems, we'll cover that later.

350
00:18:49,680 --> 00:18:53,759
All right, again, another point distinction, I've already said this, between the page directory

351
00:18:53,759 --> 00:19:01,200
and the page table, the page directory is just a disk resident mapping between page IDs

352
00:19:01,200 --> 00:19:05,599
and their locations on the physical disk on the actual files themselves.

353
00:19:05,599 --> 00:19:11,000
But the page table is going to be the sephemeral memory mapping that we use to identify, for

354
00:19:11,000 --> 00:19:14,720
a given page ID, here's the frame where it's actually located.

355
00:19:14,720 --> 00:19:18,079
And if it's not in our page table, we know we have to look in the page directory to go

356
00:19:18,079 --> 00:19:21,839
find where it is on disk and go bring it in.

357
00:19:21,839 --> 00:19:23,720
All right.

358
00:19:23,720 --> 00:19:27,240
So most of the time, the query is going to be hitting up the page table, but it's only

359
00:19:27,240 --> 00:19:31,279
when the page table says something's not there, then there needs to be some mechanism through

360
00:19:31,279 --> 00:19:34,559
like the disk module or something that says, all right, let me look at the page directory,

361
00:19:34,559 --> 00:19:38,839
let me go get the fetch the page from disk and then put it into the page table.

362
00:19:38,839 --> 00:19:39,839
Okay?

363
00:19:39,839 --> 00:19:43,680
So what I've shown you so far is a basic page table, right?

364
00:19:43,680 --> 00:19:46,440
It's a hash table, it's a maximum metadata.

365
00:19:46,440 --> 00:19:50,400
If the page is there, if you could forgive a page ID, you get back a pointer to it.

366
00:19:50,400 --> 00:19:54,720
If it's not there, the sum mechanism to go get it from disk, then put install it into

367
00:19:54,720 --> 00:20:00,120
a free frame, then the page table has the entry, right?

368
00:20:00,120 --> 00:20:05,680
But this is going to be a big bottleneck unless we're clever and smart about exploiting

369
00:20:05,680 --> 00:20:12,240
the information we know about what is going on inside of our database system.

370
00:20:12,240 --> 00:20:17,040
To approximate decisions, how we want to allocate things and decide who gets what page,

371
00:20:17,039 --> 00:20:21,119
what time, what location, and how we decided to make things.

372
00:20:21,119 --> 00:20:25,079
Because we don't exploit the information we know about what our queries want to do, what

373
00:20:25,079 --> 00:20:29,839
our data looks like, what our access patterns are, then we're no better than the OS, right?

374
00:20:29,839 --> 00:20:32,720
The OS doesn't see anything going on inside our database system.

375
00:20:32,720 --> 00:20:38,879
So if we just blindly take requests and go take them out, whatever, we're no better than

376
00:20:38,879 --> 00:20:42,159
the OS, right?

377
00:20:42,160 --> 00:20:48,480
So some of these techniques we'll talk about, these optimizations, again, it's going

378
00:20:48,480 --> 00:20:51,759
to motivate why we need to write our own buffer pool manager, why we don't want to do

379
00:20:51,759 --> 00:20:53,320
the OS.

380
00:20:53,320 --> 00:20:58,279
And it'll be a combination of policies that will affect all queries and running at the

381
00:20:58,279 --> 00:21:00,960
same time.

382
00:21:00,960 --> 00:21:06,240
Or it could be things that are going to just help a single query by itself, maybe not necessarily

383
00:21:06,240 --> 00:21:10,759
worrying about other queries that are running at the same time, but we can isolate the

384
00:21:10,759 --> 00:21:17,359
decisions we make for that query so they don't try not to affect others.

385
00:21:17,359 --> 00:21:21,480
And I'm not going to say one of these approaches can be better than another, but you will see

386
00:21:21,480 --> 00:21:26,519
that as we go along, all the major data systems are going to use some combination of all of

387
00:21:26,519 --> 00:21:30,720
them, or some of them are most of them.

388
00:21:30,720 --> 00:21:36,519
So I can't say which one is most important when it implement first, but we'll see how

389
00:21:36,519 --> 00:21:39,680
it's going to go along.

390
00:21:39,680 --> 00:21:42,480
So the things I talked about is using multiple buffer pools.

391
00:21:42,480 --> 00:21:44,960
Actually, that's probably the first one I'm going to take back what I said.

392
00:21:44,960 --> 00:21:46,960
Multiple, multiple pools is obviously going to do first.

393
00:21:46,960 --> 00:21:48,519
We'll see what that looks like.

394
00:21:48,519 --> 00:21:54,160
Prefetchings are about complicated, scan sharing, and then buffer pool bypass.

395
00:21:54,160 --> 00:22:00,039
So my toy exam I showed you beginning, we said there was one page table, one set of frames,

396
00:22:00,039 --> 00:22:03,000
and that was it for the entire system.

397
00:22:03,000 --> 00:22:07,279
But then again, because there's multiple workers running at the same time, we have to use

398
00:22:07,279 --> 00:22:09,799
these latches to protect the data structure.

399
00:22:09,799 --> 00:22:14,639
And for a large number CPU course, a large number of workers running at the same time, those

400
00:22:14,639 --> 00:22:18,079
latches are going to become a bottleneck.

401
00:22:18,079 --> 00:22:22,920
We can, because it's fixed, assuming the page table is fixed size, we don't have to have

402
00:22:22,920 --> 00:22:24,319
a latch for the entire page table.

403
00:22:24,319 --> 00:22:29,720
We're going to have latch for individual pages or locations in the hash table.

404
00:22:29,720 --> 00:22:34,759
But even then, if everybody's trying to go get the same small number of pages, then those

405
00:22:34,759 --> 00:22:38,720
latches are going to be a bottleneck.

406
00:22:38,720 --> 00:22:46,400
So an easy way to alleviate this contention point is just to have multiple buffer pools.

407
00:22:46,400 --> 00:22:48,279
So I still allocate the same amount of memory.

408
00:22:48,279 --> 00:22:52,160
So I have to tell the data system when I boot up, I want 10 gigs of memory for my buffer

409
00:22:52,160 --> 00:22:53,160
pool.

410
00:22:53,160 --> 00:22:59,759
But I'm going to take those 10 gigs and I can divide it into equal size chunks, and then

411
00:22:59,759 --> 00:23:04,640
now have a separate page table for each of them.

412
00:23:04,640 --> 00:23:13,080
It also ensures that for certain access patterns on certain objects in the database, I can

413
00:23:13,080 --> 00:23:17,080
have different policies that can affect one buffer pool versus another based on how I

414
00:23:17,080 --> 00:23:19,800
know that object is going to be used.

415
00:23:19,800 --> 00:23:25,240
So for example, in DB2, DB2 probably has the most sophisticated buffer pool management

416
00:23:25,240 --> 00:23:32,280
configurations, where you can actually define a table space, I think like a name space,

417
00:23:32,279 --> 00:23:36,099
that is backed by a given buffer pool, you can set what the page size should be for that

418
00:23:36,099 --> 00:23:40,000
buffer pool, and then you can tell which tables we manage or indexes we manage by that

419
00:23:40,000 --> 00:23:41,680
buffer pool.

420
00:23:41,680 --> 00:23:47,440
So let's say you have like one table that is primarily used for random access, you can

421
00:23:47,440 --> 00:23:52,839
have some policies, the fiction policies based on that's optimal for random access.

422
00:23:52,839 --> 00:23:55,680
Then you have another buffer pool that's for these other tables, you reduce the

423
00:23:55,680 --> 00:23:58,440
sequential scans, you have a different buffer policies for that sequential scan, maybe

424
00:23:58,440 --> 00:24:01,960
you use larger pages to page sizes.

425
00:24:01,960 --> 00:24:05,200
As far as I know DB2 is the only one that lets you do this, I haven't seen, it's post-cursing

426
00:24:05,200 --> 00:24:08,200
my SQL statement can't do this.

427
00:24:08,200 --> 00:24:12,759
I don't know about the other enterprise ones, but the DB2 one is very sophisticated.

428
00:24:12,759 --> 00:24:18,480
Again, this allows you to customize the buffer pool management for exactly how that object

429
00:24:18,480 --> 00:24:20,600
is going to be used.

430
00:24:20,600 --> 00:24:27,440
Now the question is how do you find and runtime what buffer pool management should use.

431
00:24:28,279 --> 00:24:31,799
So let's say I have two buffer pools.

432
00:24:31,799 --> 00:24:36,920
The first thing I could do is, as I said in the DB2 case, I can assign a buffer pool to

433
00:24:36,920 --> 00:24:40,519
back a given object based on this identifier.

434
00:24:40,519 --> 00:24:45,400
Table 1, 2, 3, table whatever, that's buffer pool 1, and all other tables are buffer pool

435
00:24:45,400 --> 00:24:46,400
2.

436
00:24:46,400 --> 00:24:51,759
So not runtime when I have an op of request, it's obviously not SQL, but somehow I got

437
00:24:51,759 --> 00:24:56,440
a through an index lookup, I figured out that I want to look at record 1, 2, 3, and we

438
00:24:56,440 --> 00:25:01,080
saw before how we can break the record ID into its individual components, usually like

439
00:25:01,080 --> 00:25:07,160
a page ID or slot number, but in case the SQL server, it also had a file number or an object

440
00:25:07,160 --> 00:25:08,160
ID.

441
00:25:08,160 --> 00:25:15,680
So if we can use this from the record ID, then do a lookup and say, okay, object 456, that's

442
00:25:15,680 --> 00:25:19,759
managed by buffer pool 1, and then send the request to that buffer pool.

443
00:25:19,759 --> 00:25:23,000
And all the requests for other objects may end to another buffer pool.

444
00:25:23,000 --> 00:25:29,319
And I've isolated them so that there's less last contention between the two of them.

445
00:25:29,319 --> 00:25:34,519
The simplest approach then is to do what my SQL does is you just take the record ID, hash

446
00:25:34,519 --> 00:25:38,960
it, modify the number of buffer pool managers you have, and that tells you which one you

447
00:25:38,960 --> 00:25:39,960
go to.

448
00:25:39,960 --> 00:25:40,960
Right?

449
00:25:40,960 --> 00:25:41,960
Pretty simple.

450
00:25:41,960 --> 00:25:42,960
Yes.

451
00:25:42,960 --> 00:25:47,160
Do you have to statically allocate how much memory is being able to get in your buffer pool?

452
00:25:47,160 --> 00:25:51,039
Yes, so he said that the question is, do you have to statically allocate how much memory

453
00:25:51,039 --> 00:25:53,759
each buffer pool has?

454
00:25:53,759 --> 00:25:54,759
Yes.

455
00:25:54,759 --> 00:25:59,960
And so in PostGuest MI SQL, in most systems, you cannot change the size of the buffer pool,

456
00:25:59,960 --> 00:26:02,119
but actually having a restarting entire system.

457
00:26:02,119 --> 00:26:06,359
Actually, in the middle of that, all of the open source systems have that limitation.

458
00:26:06,359 --> 00:26:11,319
The commercial systems can be kind of clever and like, I think in Oracle, you can increase

459
00:26:11,319 --> 00:26:16,559
the buffer pool size and allocate the memory and then slowly increment and migrate pages

460
00:26:16,559 --> 00:26:18,799
over from the page to the next.

461
00:26:18,799 --> 00:26:19,799
Right?

462
00:26:19,799 --> 00:26:22,799
So, what is the most tricky thing when most systems are statically allocated?

463
00:26:22,799 --> 00:26:28,200
So, one that potentially weights a lot of memory is your partitioning by database or

464
00:26:28,200 --> 00:26:30,159
table, because I imagine different.

465
00:26:30,159 --> 00:26:31,159
Yes.

466
00:26:31,159 --> 00:26:35,680
So, this question is, could this mean that you are potentially wasting memory if you partition

467
00:26:35,680 --> 00:26:37,720
and say by table?

468
00:26:37,720 --> 00:26:43,079
So for example, if I say, I make a new buffer pool, I put, I say it has 10 gigs and I say,

469
00:26:43,079 --> 00:26:46,599
you're going to manage this table, but I don't put any data in that table.

470
00:26:46,599 --> 00:26:47,919
Is that wasting space?

471
00:26:47,919 --> 00:26:48,919
Yes.

472
00:26:49,400 --> 00:26:52,480
But like, the Davis system did exactly what you wanted to do.

473
00:26:52,480 --> 00:26:53,480
The human was stupid.

474
00:26:53,480 --> 00:26:54,480
Right?

475
00:26:54,480 --> 00:26:59,259
There's, I mean, there's not, yeah, because the problem is, it doesn't know, you think,

476
00:26:59,259 --> 00:27:02,320
okay, well, only allocated one to man as needed.

477
00:27:02,320 --> 00:27:09,080
But like, the amount of engineering effort to sort of accommodate stupid people is probably

478
00:27:09,080 --> 00:27:10,840
not worth it in that case.

479
00:27:10,840 --> 00:27:14,560
If you're using DB2 in theories, you should know, like if you're calling create buffer pool

480
00:27:14,560 --> 00:27:17,360
to command to do it, you kind of should be known what you're doing.

481
00:27:17,360 --> 00:27:18,360
Right?

482
00:27:18,359 --> 00:27:27,439
So again, in my opinion, I think that what I said, this is actually the first optimization

483
00:27:27,439 --> 00:27:32,000
should do to scale your buffer manager, because it's not that much, much work.

484
00:27:32,000 --> 00:27:34,319
The hashing ones probably the easiest one to do as well.

485
00:27:34,319 --> 00:27:35,319
Right?

486
00:27:35,319 --> 00:27:39,639
Because there's no central data structure to say, okay, for this object, you go to this

487
00:27:39,639 --> 00:27:44,519
one, or you just hash it and you're done.

488
00:27:44,519 --> 00:27:46,799
The next optimization we can do is prefetching.

489
00:27:47,639 --> 00:27:53,279
Again, the OS to do prefetching, we'll see, for like simple cases, like when you're doing

490
00:27:53,279 --> 00:27:56,759
sequential scans, we'll see one case where it can't do it.

491
00:27:56,759 --> 00:28:04,039
So the basic idea is that if you run a query and it has to start accessing data in your

492
00:28:04,039 --> 00:28:10,200
table, it's going to open up a cursor that just starts scanning through the pages one by

493
00:28:10,200 --> 00:28:11,200
one.

494
00:28:11,200 --> 00:28:12,200
Right?

495
00:28:12,200 --> 00:28:15,919
And so assuming this in our example here, our buffer pool starts off as empty.

496
00:28:15,920 --> 00:28:21,160
So at the very first page that it sees, page zero, it's not in the buffer pool.

497
00:28:21,160 --> 00:28:23,680
So we just go again, we just go copy and put that in.

498
00:28:23,680 --> 00:28:26,560
Then we scan along, we need page one, page one's not there.

499
00:28:26,560 --> 00:28:29,039
So it goes ahead and copy that.

500
00:28:29,039 --> 00:28:33,080
But now the data system could be smart and say, okay, well, you've read page zero, you've

501
00:28:33,080 --> 00:28:38,680
read page one, it's very likely you're going to read page two, three, and so forth.

502
00:28:38,680 --> 00:28:45,200
So let me go ahead and prefetch those guys while the data system is processing page one,

503
00:28:45,200 --> 00:28:46,200
all right?

504
00:28:46,200 --> 00:28:49,680
Go get page two, page three, put it in.

505
00:28:49,680 --> 00:28:55,360
So by the time you're finished processing page one, and you come to page two, lo and behold,

506
00:28:55,360 --> 00:28:59,440
the page you're looking, the next page you need is already there.

507
00:28:59,440 --> 00:29:02,840
All right, and just do this all the way down the line.

508
00:29:02,840 --> 00:29:05,880
All right, we haven't talked about how we execute queries just yet, but the typically

509
00:29:05,880 --> 00:29:11,240
the way it works is that you request a page, it's going to have a bunch of tuples in it.

510
00:29:11,240 --> 00:29:15,799
Do some kind of computation inside the data that's in those tuples, and then when you're

511
00:29:15,799 --> 00:29:18,799
done, go get the next page, right?

512
00:29:18,799 --> 00:29:22,720
As you're doing your scan along the leaf nodes in the query plan tree.

513
00:29:22,720 --> 00:29:27,160
And so it's not like we're just blindly ripping through the pages and say, you know, get,

514
00:29:27,160 --> 00:29:32,279
get, get, get, get, get, it's get some think time, do some compute, then go get the next

515
00:29:32,279 --> 00:29:33,279
page.

516
00:29:33,279 --> 00:29:35,960
And so that think time is where the data system can say, I have some time to go ahead and

517
00:29:35,960 --> 00:29:38,039
prefetch the things that you're needing.

518
00:29:38,039 --> 00:29:39,279
Right?

519
00:29:40,240 --> 00:29:44,879
And I don't have the diagram here, but this is why we have that, that, that pinning mechanism

520
00:29:44,879 --> 00:29:45,879
before, right?

521
00:29:45,879 --> 00:29:50,079
I don't obviously don't want to fetch in page one, then the data system says in, in

522
00:29:50,079 --> 00:29:54,079
a background thread, okay, let me go prefetch page three and two, and it goes and swaps

523
00:29:54,079 --> 00:29:56,359
out page one while you're still accessing it, right?

524
00:29:56,359 --> 00:30:00,119
The pin will, the pinning mechanism will, will prevent that.

525
00:30:00,119 --> 00:30:01,119
Yes?

526
00:30:01,119 --> 00:30:07,319
Is it, is it possible that different bubbles for, for the standpoint?

527
00:30:07,319 --> 00:30:09,319
Question, ah, it's good point, thank you.

528
00:30:09,319 --> 00:30:13,439
This question is, is it possible for different buffer pool, buffer pool is to hold the same

529
00:30:13,439 --> 00:30:14,439
page?

530
00:30:14,439 --> 00:30:15,439
No.

531
00:30:15,439 --> 00:30:16,519
It's always being one more corresponded.

532
00:30:16,519 --> 00:30:18,240
Every page can only exist in one buffer pool.

533
00:30:18,240 --> 00:30:22,599
Because still that, what would happen if you allowed that, right?

534
00:30:22,599 --> 00:30:29,359
Well, you could have the page, you will point to the same, point to the same page, different

535
00:30:29,359 --> 00:30:32,599
page, you will point to the same page, but then depending where the metadata is, like

536
00:30:32,599 --> 00:30:38,079
the reference counter, the pin marker and things like that, like if that's separate, separate

537
00:30:38,079 --> 00:30:42,480
pages, then one buffer pool may say, okay, no one's accessing it, then let me go swap it

538
00:30:42,480 --> 00:30:46,719
out, but the other page table has a pin, and you would miss that.

539
00:30:46,719 --> 00:30:50,480
So yeah, for that reason, it's a one-way correspondent.

540
00:30:50,480 --> 00:30:53,879
That's good point, thank you.

541
00:30:53,879 --> 00:30:54,879
Yes?

542
00:30:54,879 --> 00:30:58,919
What's the relationship between the pin, counter and the latch?

543
00:30:58,920 --> 00:31:01,279
The question is, what's the relationship between a pin counter and a latch?

544
00:31:01,279 --> 00:31:12,920
A pin counter just says that there's some, some, so the pin counter says there's some worker

545
00:31:12,920 --> 00:31:17,279
that's accessing this page, but I'm not in the page table right now when I'm doing it,

546
00:31:17,279 --> 00:31:18,279
right?

547
00:31:18,279 --> 00:31:22,560
I get the pointer to the page, I'm going back to my original diagram.

548
00:31:22,560 --> 00:31:26,480
Right, go back here.

549
00:31:26,480 --> 00:31:32,240
So it's on the page table, but like this guy says, give me page two, I take a latch in

550
00:31:32,240 --> 00:31:37,160
the page table to go get the pointer to that, to that, to that page.

551
00:31:37,160 --> 00:31:43,120
And then before I get the pointer back to my execution, and I pin it, because now I'm

552
00:31:43,120 --> 00:31:46,279
outside the page table, because the pin is protected, the latch is protecting the data

553
00:31:46,279 --> 00:31:50,400
structure, but I'm out of the data structure, but I have a reference to the page, so I have

554
00:31:50,400 --> 00:31:53,480
the pin just to say someone is actually reading this page.

555
00:31:53,480 --> 00:31:57,960
And then when I'm done with it, I can then, you know, decrement that reference counter,

556
00:31:57,960 --> 00:32:03,200
which potentially unlocked or released the pin, and then now the buffer manager can

557
00:32:03,200 --> 00:32:07,880
side, okay, I know for this page two, nobody has a pin, there's nobody referencing it,

558
00:32:07,880 --> 00:32:10,400
so I'm free to evict it.

559
00:32:10,400 --> 00:32:13,799
So again, the latch protects the data structure, the pin protects the page.

560
00:32:13,799 --> 00:32:14,799
Yes.

561
00:32:14,799 --> 00:32:19,480
So the page table is not like inside the buffer pool, like sort of separately, and it can

562
00:32:19,480 --> 00:32:20,879
actually relate it.

563
00:32:20,879 --> 00:32:26,279
All right, question is the, the, the, the, the, the, the, the, the, the, the, the, the,

564
00:32:26,279 --> 00:32:27,279
it is the buffer pool.

565
00:32:27,279 --> 00:32:29,279
I guess like, it's not in this.

566
00:32:29,279 --> 00:32:35,159
Yeah, this is like a hologram, but like, it's in this box, how about that?

567
00:32:35,159 --> 00:32:36,159
Right?

568
00:32:36,159 --> 00:32:40,319
Like, these are the, like, these are the frames, there's some other page table data structure

569
00:32:40,319 --> 00:32:41,319
here, yes.

570
00:32:42,319 --> 00:32:43,319
Yes.

571
00:32:43,319 --> 00:32:58,599
So this question is, does the buffer manager have access to the query plan?

572
00:32:58,599 --> 00:32:59,839
No, right?

573
00:32:59,839 --> 00:33:04,079
The, because we have these layers, but you can send it hints.

574
00:33:04,079 --> 00:33:07,079
Boss tub doesn't support those hints, but you can send it hints like, I'm accessing this

575
00:33:07,079 --> 00:33:12,480
page, and here's likely the next page is I'm going to access as well.

576
00:33:12,480 --> 00:33:13,480
Yes.

577
00:33:13,480 --> 00:33:14,480
Then you said something about static.

578
00:33:14,480 --> 00:33:19,159
You're not, there's, there's, well, where the buffer pool manager did, where the

579
00:33:19,159 --> 00:33:22,119
topic we know, which thing does it still get?

580
00:33:22,119 --> 00:33:23,119
What do you might statically know?

581
00:33:23,119 --> 00:33:31,119
Like, if I do, if I send a, a comment, yes, yes, what the buffer pool manager did

582
00:33:31,119 --> 00:33:34,519
you know, what the purpose of the bulk in that?

583
00:33:34,519 --> 00:33:35,519
Okay.

584
00:33:35,519 --> 00:33:41,599
So the question is, if I have a select query, who is, who is figuring out what the pre-fetch?

585
00:33:41,599 --> 00:33:42,599
Maybe that's what you're really asking.

586
00:33:42,599 --> 00:33:43,599
Right?

587
00:33:43,599 --> 00:33:45,599
This has to come up with the execution engine.

588
00:33:45,599 --> 00:33:46,599
Right?

589
00:33:46,599 --> 00:33:53,439
The buffer manager doesn't know about, it's just, you know, it's sort of division responsibilities.

590
00:33:53,439 --> 00:33:58,119
Like, the profile manager shouldn't have to infer, like, for this query plan on this

591
00:33:58,119 --> 00:33:59,839
table, what page I'm going to read.

592
00:33:59,839 --> 00:34:02,079
That all comes down below because you think about it too.

593
00:34:02,079 --> 00:34:04,879
That's where the execution has to ask, I know what page it needs to read anyway because

594
00:34:04,879 --> 00:34:06,399
that's a read them.

595
00:34:06,399 --> 00:34:10,079
So all that logic is, is, is in that part of the system, which we'll cover in two weeks.

596
00:34:10,079 --> 00:34:13,599
So like, I said, I think it's an end, and then should we have the call, the call, the

597
00:34:13,599 --> 00:34:14,599
technical security.

598
00:34:14,599 --> 00:34:15,599
Yeah.

599
00:34:15,599 --> 00:34:19,799
So the, the execution engine should say, I'm fetching page one now, but by the way, I'm

600
00:34:19,799 --> 00:34:21,279
also going to fetch page two and three.

601
00:34:21,279 --> 00:34:22,279
Right?

602
00:34:22,279 --> 00:34:27,079
Because think about it, like, you could have a, you could have like, in your select call,

603
00:34:27,079 --> 00:34:30,719
in the query here, you have a limit 10, right?

604
00:34:30,719 --> 00:34:32,159
With no wear clause.

605
00:34:32,159 --> 00:34:37,440
So in the first page, I got five tuples and the second page, or the page here, I got

606
00:34:37,440 --> 00:34:40,639
five tuples and page one, I got two tuples.

607
00:34:40,639 --> 00:34:43,639
Therefore, I know I'm going to have to read more pages so you can send that hit, you know,

608
00:34:43,639 --> 00:34:45,440
ahead of time or something like that.

609
00:34:45,440 --> 00:34:46,440
Right?

610
00:34:46,440 --> 00:34:50,319
And then that's the beauty of having a declarative language like SQL, where you know what

611
00:34:50,319 --> 00:34:52,519
you're going to do ahead of time.

612
00:34:52,519 --> 00:34:53,519
Right?

613
00:34:53,519 --> 00:34:55,119
At least at a, at a high level.

614
00:34:55,119 --> 00:34:59,920
So with enough, you have enough information where you could make these kind of decisions.

615
00:34:59,920 --> 00:35:00,920
Yes?

616
00:35:00,920 --> 00:35:08,279
So you never prefetch pages in the database that you know is the use for simulation, like,

617
00:35:08,279 --> 00:35:09,679
I'm sure.

618
00:35:09,679 --> 00:35:15,880
So, this question is, do you never prefetch pages unless you're absolutely certain you're

619
00:35:15,880 --> 00:35:16,880
going to need them?

620
00:35:16,880 --> 00:35:18,759
Maybe that's what you're asking?

621
00:35:18,759 --> 00:35:22,920
Actually, I don't, I don't know how aggressive they are.

622
00:35:22,920 --> 00:35:27,480
Yeah.

623
00:35:27,480 --> 00:35:32,119
The commercial systems do this much better and they're obviously closed source.

624
00:35:32,119 --> 00:35:36,119
So, virtual scan is pretty easy.

625
00:35:36,119 --> 00:35:43,680
For index scans, next slide, this one can be kind of tricky as well because you may not

626
00:35:43,680 --> 00:35:49,960
be able to prefetch without falling along the pages as much.

627
00:35:49,960 --> 00:35:51,920
You may not be able to jump ahead.

628
00:35:51,920 --> 00:35:54,920
So I think in index scans, the pre-venson is a bit more conservative.

629
00:35:54,920 --> 00:36:00,280
Squatual scan, you could just, you know, you can jump a lot farther.

630
00:36:00,280 --> 00:36:03,519
And we haven't been talking to like, there's all a bunch of other factors too.

631
00:36:03,519 --> 00:36:10,360
As always, in the case in databases, like, there's like multi-versionings or who knows whether

632
00:36:10,360 --> 00:36:14,360
like, you know, yes, the next page I'm going to read has 10 tuples, but like, only three

633
00:36:14,360 --> 00:36:15,559
may be visible.

634
00:36:15,559 --> 00:36:16,960
It's complicated.

635
00:36:17,960 --> 00:36:18,960
All right.

636
00:36:18,960 --> 00:36:23,079
So, going back here, the threshold scan, the OS can kind of do this, right?

637
00:36:23,079 --> 00:36:27,920
Assuming your pages are contiguous, the OS read ahead can kind of figure this out.

638
00:36:27,920 --> 00:36:35,360
What it can't do is infer the logical data structure that the pages physically represent and

639
00:36:35,360 --> 00:36:36,760
prefetch according to that.

640
00:36:36,760 --> 00:36:40,800
Because again, it doesn't know what a B plus 3 is, doesn't know what a hash table is,

641
00:36:40,800 --> 00:36:42,760
at least at the page level.

642
00:36:42,760 --> 00:36:47,240
So we do because we're the ones actually running it.

643
00:36:47,240 --> 00:36:53,400
So let's say I have a query like the select start from A where the value between 150 and

644
00:36:53,400 --> 00:36:58,400
I can do this by reading, by doing an index scan.

645
00:36:58,400 --> 00:37:01,240
We haven't talked about B plus 3's yet, but again, it's a tree data structure that shouldn't

646
00:37:01,240 --> 00:37:06,360
be farmed at anyone in this class and then assume that along the leaf nodes, the values

647
00:37:06,360 --> 00:37:10,040
are sorted based on the key, right?

648
00:37:10,039 --> 00:37:15,279
So to do this, run this particular query, I have to start with the root node, that's page

649
00:37:15,279 --> 00:37:19,360
0, I go get that, put that in my data, put that in my buffer pool.

650
00:37:19,360 --> 00:37:24,840
Then I traverse down to this side of the tree, I get page 1, put that in my buffer pool,

651
00:37:24,840 --> 00:37:29,199
but then now I'm going to jump down to this leaf node here and because I have this, this

652
00:37:29,199 --> 00:37:35,279
wear clause that is going to be reading so many, so many records, I know I need to at least

653
00:37:35,279 --> 00:37:39,199
go to over to the page 5, right?

654
00:37:39,199 --> 00:37:46,399
So I'll go get page 3, but I also can potentially pre-fetch page 5.

655
00:37:46,399 --> 00:37:51,879
But again, the operating system can't do this, can't know this because page 3 and page 5

656
00:37:51,879 --> 00:37:52,960
are not contiguous.

657
00:37:52,960 --> 00:37:58,759
The data system knows because it knows that if I at least get the index page 3, I have

658
00:37:58,759 --> 00:38:04,639
a sibling pointer, a page ID, so I know what the next page I'm going to read over along

659
00:38:04,639 --> 00:38:08,759
the leaf nodes as I scan across, so therefore I can go ahead and pre-fetch that.

660
00:38:09,560 --> 00:38:12,960
Even though it's not in sequential order of the other pages.

661
00:38:12,960 --> 00:38:18,600
Again, there's just another example why we want to do this in the side of the system,

662
00:38:18,600 --> 00:38:20,840
not the OS.

663
00:38:20,840 --> 00:38:27,200
All right, the next optimization we can do is called scan sharing.

664
00:38:27,200 --> 00:38:30,520
Sometimes call it synchronized scans in like some of the order systems.

665
00:38:30,520 --> 00:38:33,960
The basic idea here is a bunch of queries show up.

666
00:38:33,960 --> 00:38:36,480
They want access to the same table.

667
00:38:36,480 --> 00:38:40,240
When I'm getting started, I start scanning through the pages.

668
00:38:40,240 --> 00:38:46,519
We can recognize that they need the same data, so we just pick you back off of them.

669
00:38:46,519 --> 00:38:51,760
Our cursor attaches there, cursor, and we read the same pages at the same time.

670
00:38:51,760 --> 00:38:55,760
So we remove the redundant IO.

671
00:38:55,760 --> 00:38:59,719
This is different than result caching, which we want to talk about the semester of result

672
00:38:59,719 --> 00:39:06,440
caching basically, query shows up, I compute some answer, I save that result in a cache,

673
00:39:06,440 --> 00:39:10,559
query shows up again, potentially slightly different, which is harder to do,

674
00:39:10,559 --> 00:39:15,400
but the same query shows up again, I don't have to rerun the query, I just send the result back.

675
00:39:15,400 --> 00:39:18,800
The scan sharing is really at the low physical levels of the access method,

676
00:39:18,800 --> 00:39:22,360
how we're actually scanning the pages, we can recognize that they two queries

677
00:39:22,360 --> 00:39:30,320
need to read the same thing, and therefore we can reuse any pages as we go along and fetch them.

678
00:39:30,320 --> 00:39:34,440
So we don't have conflicting cursors trying to read the same pages at the same time.

679
00:39:36,440 --> 00:39:39,320
So this is repeating what I just said.

680
00:39:39,320 --> 00:39:48,599
For the DB2 SQL Server, Teradata, and Postgres, they actually support the full scan sharing

681
00:39:48,599 --> 00:39:54,599
for queries that aren't exactly the same, but at least they're reading the same tables.

682
00:39:54,599 --> 00:40:00,320
In Oracle, the only support cursor sharing for queries that show up that look exactly the same.

683
00:40:00,320 --> 00:40:04,360
I mean, literally exactly the same, because they're basically hashing the string

684
00:40:04,360 --> 00:40:06,599
to see whether there's a match.

685
00:40:06,599 --> 00:40:11,079
If you go look at the documentation, if you have queries like Select Star from employees,

686
00:40:11,079 --> 00:40:15,480
Select Star from employees of the capital E, or an extra space before the front calls,

687
00:40:15,480 --> 00:40:19,160
these won't match because when you hash the strings, they're not the same.

688
00:40:19,160 --> 00:40:22,120
So it has to literally be the exact same query running the exact same time,

689
00:40:22,120 --> 00:40:25,559
and then they can share it.

690
00:40:25,559 --> 00:40:26,800
So conceptually looks like this.

691
00:40:26,800 --> 00:40:34,320
So say I have query one, once there's a summation on the value column from table A,

692
00:40:34,320 --> 00:40:39,280
so it attaches a cursor to the pages, starts reading them, fetching them into the buffer pool.

693
00:40:39,280 --> 00:40:40,280
Right?

694
00:40:40,280 --> 00:40:42,280
First thing gets page 0, that's not there.

695
00:40:42,280 --> 00:40:45,519
So it puts that in memory, goes down to page 2, and so forth.

696
00:40:45,519 --> 00:40:46,519
Right?

697
00:40:46,519 --> 00:40:51,160
So now we get to page 3, and we haven't talked about the eviction policy, but page 0 is the

698
00:40:51,160 --> 00:40:52,559
last one that was used.

699
00:40:52,559 --> 00:40:56,600
So we go ahead and evict page 0 and put in page 3.

700
00:40:56,600 --> 00:41:01,800
But now Q2 shows up, once the computer and average instead of summation, on the same table

701
00:41:01,800 --> 00:41:03,000
though.

702
00:41:03,000 --> 00:41:07,719
So the naive thing to do is to have it start at the beginning, just like the first cursor,

703
00:41:07,719 --> 00:41:10,599
and just scan down and read the pages at the same time.

704
00:41:10,599 --> 00:41:11,760
Right?

705
00:41:11,760 --> 00:41:17,440
But obviously this is stupid because in this scenario here, we just, the Q2 needs to read

706
00:41:17,440 --> 00:41:21,639
page 0, but Q1 just got that evict from the buffer pool.

707
00:41:21,639 --> 00:41:22,639
Right?

708
00:41:22,639 --> 00:41:26,960
So the very first thing we would do here is evict, if you want to Q2, we had our evict page

709
00:41:26,960 --> 00:41:28,960
2 to put page 0 in.

710
00:41:28,960 --> 00:41:31,800
But again, we just got rid of page 0.

711
00:41:31,800 --> 00:41:34,720
So the better thing to do is you attach Q2 to Q1.

712
00:41:34,720 --> 00:41:42,680
Again, at the lowest level as you scan the table and let Q2 ride along Q1, see all the pages

713
00:41:42,680 --> 00:41:45,280
that it sees and processes them accordingly.

714
00:41:45,280 --> 00:41:49,880
And then Q1 goes away, but then Q2 recognizes, oh, there's a bunch of pages at the top of

715
00:41:49,880 --> 00:41:51,480
the table that I missed.

716
00:41:51,480 --> 00:41:57,280
Let me go back and get all those, and then compete my query.

717
00:41:57,280 --> 00:41:58,120
Right?

718
00:41:59,119 --> 00:42:02,119
What's the potential problem with this?

719
00:42:02,119 --> 00:42:03,119
Yes.

720
00:42:03,119 --> 00:42:07,119
So it's hard to implement.

721
00:42:07,119 --> 00:42:10,119
Not that hard.

722
00:42:10,119 --> 00:42:22,119
If the aggregate function depends on the ordering of the data, which again, relational model,

723
00:42:22,119 --> 00:42:25,119
it doesn't, we know functions, yes.

724
00:42:26,119 --> 00:42:29,119
You're close to it, but basically it saved you had a limit clause.

725
00:42:29,119 --> 00:42:30,119
Right?

726
00:42:30,119 --> 00:42:31,119
Right?

727
00:42:31,119 --> 00:42:35,119
I want to get the first 100 tuples and compete the average on that.

728
00:42:35,119 --> 00:42:37,119
Again, relational model is unordered.

729
00:42:37,119 --> 00:42:42,119
So technically it's correct if Q2 starts at page 0 versus page 3.

730
00:42:42,119 --> 00:42:45,119
And again, it gets 100 tuples and it competes the average.

731
00:42:45,119 --> 00:42:47,119
Both answers are technically correct.

732
00:42:47,119 --> 00:42:50,119
From the application perspective, this looks fucked up because now you've got queries that

733
00:42:50,119 --> 00:42:54,119
stay in query at two different times show up with different results.

734
00:42:54,119 --> 00:42:57,119
So I was being glib and I said, oh, yeah, it's not that hard to implement.

735
00:42:57,119 --> 00:42:58,119
You can attach the cursor.

736
00:42:58,119 --> 00:43:03,119
Again, if there's no, if there's no ordering constraints, it's easy.

737
00:43:03,119 --> 00:43:11,119
But if you need to make sure that your queries produce the same results over and over again, then this can be a bit tricky.

738
00:43:11,119 --> 00:43:14,119
Now, this is part of the reason, again, I'm not trying to bash Oracle.

739
00:43:14,119 --> 00:43:18,119
Like the easiest thing to do is if it's the exact same query, then I'll do a cursor sharing.

740
00:43:18,119 --> 00:43:23,119
The tricky thing is to figure out, again, to understand the semantics of what the query is actually wants to do,

741
00:43:23,119 --> 00:43:28,119
and then identify when is it safe to attach the one cursor to another,

742
00:43:28,119 --> 00:43:37,119
and then how to maybe go back and get more results as needed.

743
00:43:37,119 --> 00:43:47,119
So this is a, going sort of extreme scan sharing is this idea called continuous scan sharing.

744
00:43:47,119 --> 00:43:51,119
I'll say up front, too, that no real system does this.

745
00:43:51,119 --> 00:43:54,119
I just like it because it's a different way to think about how to build a system.

746
00:43:54,119 --> 00:44:01,119
So again, going back to his point, he said it'd be hard to implement the scan sharing, potentially, yes.

747
00:44:01,119 --> 00:44:10,119
But what if he just did the dumbest thing and just had everything's due scan sharing because the cursor is just running all the time?

748
00:44:10,119 --> 00:44:17,119
So it literally reads one page after another, brings that into your buffer pool, and then when you're done,

749
00:44:17,119 --> 00:44:20,119
it just loops back around and does it all over again.

750
00:44:20,119 --> 00:44:32,119
So now, like when a query shows up, you just kind of pop along whenever it's going and then get what you need, and then you go away and you're done.

751
00:44:32,119 --> 00:44:35,119
Good idea or bad idea?

752
00:44:35,119 --> 00:44:36,119
What's that?

753
00:44:36,119 --> 00:44:38,119
Or bad? Why?

754
00:44:38,119 --> 00:44:42,119
You're probably doing a lot of extra reads that you may not need.

755
00:44:42,119 --> 00:44:44,119
I guess it depends on if...

756
00:44:44,119 --> 00:44:50,119
So assuming every query is doing a full table scan or almost a full table scan.

757
00:44:50,119 --> 00:44:54,119
Where do we have some tables sitting around and we're just staying through it?

758
00:44:54,119 --> 00:44:55,119
What if you have a table?

759
00:44:55,119 --> 00:45:00,119
So you could maybe say the cursor doesn't get fired until the query shows up and touches the table.

760
00:45:00,119 --> 00:45:02,119
You're back.

761
00:45:02,119 --> 00:45:06,119
You can both of our pretty things into memory and then you can read them right after.

762
00:45:06,119 --> 00:45:09,119
Yeah, but...

763
00:45:10,119 --> 00:45:16,119
Well, say you have to do that anyway for the full table scan because the table doesn't fit in memory anyway.

764
00:45:16,119 --> 00:45:22,119
So, yes.

765
00:45:22,119 --> 00:45:24,119
Is it hard to skip around?

766
00:45:24,119 --> 00:45:26,119
It's hard to skip around.

767
00:45:26,119 --> 00:45:28,119
Well, in some ways, it's also too.

768
00:45:28,119 --> 00:45:31,119
It makes your runtime kind of deterministic, right?

769
00:45:31,119 --> 00:45:35,119
Because you know it's going to be at least n, where it ends in a number of pages.

770
00:45:36,119 --> 00:45:37,119
That's just for scans.

771
00:45:37,119 --> 00:45:39,119
It joins our whole number of beats we haven't talked about yet.

772
00:45:39,119 --> 00:45:44,119
So, if it's on prem, you've already paid for the hardware ignoring energy costs.

773
00:45:44,119 --> 00:45:46,119
This is kind of okay.

774
00:45:46,119 --> 00:45:48,119
But if it's running in the cloud, would you actually pay in per eye-op?

775
00:45:48,119 --> 00:45:50,119
Then this is actually terrible, right?

776
00:45:50,119 --> 00:45:53,119
Because you end up reading more data than you eventually actually need.

777
00:45:53,119 --> 00:46:00,119
So I was saying there's only one prototype I know that did this out of ECH Zurich, Corker Skendo.

778
00:46:01,119 --> 00:46:07,119
And they built it specifically for a sort of telecom business where they needed deterministic runtimes or queries.

779
00:46:07,119 --> 00:46:09,119
But this was a few years ago.

780
00:46:09,119 --> 00:46:12,119
And it was another prototype I had the same kind of thing.

781
00:46:12,119 --> 00:46:13,119
So it's an interesting idea.

782
00:46:13,119 --> 00:46:16,119
It's a different way to think about a system that's unorthodox.

783
00:46:16,119 --> 00:46:19,119
I just like to present it as a different way to think about things.

784
00:46:19,119 --> 00:46:20,119
There's wide-loved databases.

785
00:46:20,119 --> 00:46:23,119
There's so many different ways to solve the same problem over and over again.

786
00:46:23,119 --> 00:46:27,119
And we can debate what's good and what's bad.

787
00:46:27,119 --> 00:46:30,119
So last optimization I talk about is called buffer pool bypass.

788
00:46:30,119 --> 00:46:37,119
And the idea here is that if we have a query that's running a Squancho scan,

789
00:46:37,119 --> 00:46:40,119
we have to bring things off a disk in the memory.

790
00:46:40,119 --> 00:46:47,119
But maybe we don't want to put it into our buffer pool because one needs to pay for the maintenance cost of the buffer pool.

791
00:46:47,119 --> 00:46:51,119
In the patient, we'll think of latch, update things, and VIX stuff and so forth.

792
00:46:51,119 --> 00:46:54,119
And then also, too, if we're doing a Squancho scan,

793
00:46:54,119 --> 00:46:57,119
the data we just read may not actually be useful,

794
00:46:57,119 --> 00:47:00,119
well, it's not going to be useful for our query because we're getting Squancho scan,

795
00:47:00,119 --> 00:47:05,119
we're only going to meet the table once, usually.

796
00:47:05,119 --> 00:47:11,119
And so rather than having all these different workers running the same time,

797
00:47:11,119 --> 00:47:14,119
doing Squancho scans and polluting the page table,

798
00:47:14,119 --> 00:47:20,119
what if we just give every worker its own little piece of memory,

799
00:47:20,119 --> 00:47:25,119
like a working memory, and then any page we read,

800
00:47:25,119 --> 00:47:27,119
we put it into that, that worker's memory.

801
00:47:27,119 --> 00:47:29,119
Yes, you could have two duplicates.

802
00:47:29,119 --> 00:47:32,119
It only works if it's read only, you can't do rights.

803
00:47:32,119 --> 00:47:34,119
And then that way it's just like a circular buffer,

804
00:47:34,119 --> 00:47:37,119
we just keep filling up and wrap around.

805
00:47:37,119 --> 00:47:39,119
So a bunch of systems support this,

806
00:47:39,119 --> 00:47:41,119
Oracle SQL Server Postgres and Informix.

807
00:47:41,119 --> 00:47:46,119
I think this originated in Formix, and they call it light scans, light meaning because again,

808
00:47:46,119 --> 00:47:49,119
you don't touch the heavyweight page table in the buffer pool.

809
00:47:49,119 --> 00:47:55,119
And the idea here is that I can potentially not pollute my page table because the data I need,

810
00:47:55,119 --> 00:47:59,119
sorry, but the data I need is sort of local to me.

811
00:47:59,119 --> 00:48:04,119
Of course, the downside of this is that you lose the sharing capability of two workers

812
00:48:04,119 --> 00:48:08,119
need the same pages at the same time, or one soon after another,

813
00:48:08,119 --> 00:48:12,119
then you lose that reuse possibility.

814
00:48:12,119 --> 00:48:18,119
Again, it's another optimization that we can do because we control exactly what the queries are actually,

815
00:48:18,119 --> 00:48:21,119
what they're actually executing in touch.

816
00:48:21,119 --> 00:48:25,119
All right.

817
00:48:25,119 --> 00:48:29,119
So we sort of dance around this idea of a dicting data,

818
00:48:29,119 --> 00:48:31,119
our dicting page is more profitable.

819
00:48:31,119 --> 00:48:34,119
So now we got to talk about how we're actually going to do it.

820
00:48:34,119 --> 00:48:37,119
You need to know this because it's project one.

821
00:48:37,119 --> 00:48:43,119
So when the execution just says, okay, I'm bringing a page into memory,

822
00:48:43,119 --> 00:48:46,119
I'm going to put it into a frame.

823
00:48:46,119 --> 00:48:50,119
But if there's no free frames, it has to decide what to evict.

824
00:48:50,119 --> 00:48:52,119
It's a cache.

825
00:48:52,119 --> 00:48:55,119
It should not be ground-break.

826
00:48:55,119 --> 00:49:02,119
So there's a bunch of different metrics and objectives we have to consider in our

827
00:49:02,119 --> 00:49:07,119
eviction policy that's going to depend on various factors of our implementation of our database system.

828
00:49:07,119 --> 00:49:10,119
Obviously, one, our eviction policy to be correct.

829
00:49:10,119 --> 00:49:13,119
We don't want an evict to page, then immediately that page is the most,

830
00:49:13,119 --> 00:49:17,119
you know, the most used thing and therefore we keep reading and writing it from disk open over again.

831
00:49:17,119 --> 00:49:19,119
Could that be bad?

832
00:49:19,119 --> 00:49:22,119
We want to, our eviction policy to be fast.

833
00:49:22,119 --> 00:49:25,119
We don't want to, you know, if it's a, if it's a,

834
00:49:25,119 --> 00:49:28,119
we're using an NP-complete algorithm or exponential algorithm.

835
00:49:28,119 --> 00:49:31,119
We don't want to take three seconds to decide what page we have to evict because

836
00:49:31,119 --> 00:49:35,119
monitors go read from disk because that would, that would have been a lot faster in the first place.

837
00:49:35,119 --> 00:49:40,119
And related to this, we also don't want to have a big cost of maintaining the metadata we need

838
00:49:40,119 --> 00:49:47,119
to keep track of how pages are being accessed so that we can make a decision what to evict.

839
00:49:47,119 --> 00:49:50,119
So again, this is the oldest problem in computer science.

840
00:49:50,119 --> 00:49:54,119
I have the other oldest problem in computer science is naming something or naming a system.

841
00:49:54,119 --> 00:49:57,119
We can talk about how bus type got named but,

842
00:49:57,119 --> 00:49:59,119
everyone has a caching paper.

843
00:49:59,119 --> 00:50:03,119
I think I have two.

844
00:50:03,119 --> 00:50:08,119
So the most obvious easy thing to do is do LRU, at least recently used.

845
00:50:08,119 --> 00:50:12,119
The basic idea here is just maintaining the timestamp or keep track of a link,

846
00:50:12,119 --> 00:50:17,119
a link list of when a, when pages were last touched.

847
00:50:17,119 --> 00:50:22,119
And then when it comes, comes time to evict the page, we just go to the tail end of the link list and pop,

848
00:50:22,119 --> 00:50:26,119
pop whatever is there and that's, you know, that one has been accessing a while.

849
00:50:26,119 --> 00:50:28,119
We go ahead and evict it.

850
00:50:28,119 --> 00:50:32,119
Right? So say I should have a hero, q1, wants to touch page one.

851
00:50:32,119 --> 00:50:37,119
Page one is already in our link list so we just move it to the front, right, to the head.

852
00:50:37,119 --> 00:50:41,119
And then now say another query wants to come and touch, you know, access page five,

853
00:50:41,119 --> 00:50:42,119
but page five is not in memory.

854
00:50:42,119 --> 00:50:50,119
So we go ahead and page two because it's at the end, right?

855
00:50:50,119 --> 00:50:54,119
This should not be news to anyone.

856
00:50:54,119 --> 00:51:00,119
So another way to do it, achieve the same thing without tracking the actual timestamps in this link list.

857
00:51:00,119 --> 00:51:03,119
So use an approximation algorithm called clock.

858
00:51:03,119 --> 00:51:06,119
Who here has heard a clock before?

859
00:51:06,119 --> 00:51:09,119
Less than five. Okay.

860
00:51:09,119 --> 00:51:12,119
And so clock is that used in a bunch of other systems as well.

861
00:51:12,119 --> 00:51:16,119
I think Linux uses this for its page cache or page table.

862
00:51:16,119 --> 00:51:20,119
They use a multi-hand clock, which we don't even know about that.

863
00:51:20,119 --> 00:51:27,119
But the basic idea here is that instead of keeping track of like the exact ordering of pages in LRU,

864
00:51:27,119 --> 00:51:34,119
instead we just give a user symbol reference bit for every page that we use to keep track of

865
00:51:34,119 --> 00:51:40,119
whenever we set the one, whenever it's access, whenever it's written to a red.

866
00:51:40,119 --> 00:51:45,119
And the idea here is that we'll have this this clock and sweep around and look at all our pages.

867
00:51:45,119 --> 00:51:51,119
And if the bit set to one set it to zero, if it is set to zero, then we go ahead and evict it.

868
00:51:51,119 --> 00:51:56,119
Right. So say we have four pages, we just give a reference bit, reference bit, we set the zero.

869
00:51:56,119 --> 00:52:02,119
Right. And then say page one gets access by a query, we go ahead and set the reference bit to one.

870
00:52:02,119 --> 00:52:06,119
And then now let's say another query once a page that's not in a buffer pool.

871
00:52:06,119 --> 00:52:09,119
So we got a side which of these four we want to evict.

872
00:52:09,119 --> 00:52:13,119
So we go ahead and the clock starts at some starting point.

873
00:52:13,119 --> 00:52:16,119
It looks at the reference bit, if it's set to one, we set it to zero.

874
00:52:16,119 --> 00:52:18,119
And then move on to the next one.

875
00:52:18,119 --> 00:52:20,119
And here page two, the reference bit is set to zero.

876
00:52:20,119 --> 00:52:26,119
So therefore we know the last time the clock swept around and looked for pages to evict, it wasn't touched.

877
00:52:26,119 --> 00:52:29,119
So therefore this is safe to evict.

878
00:52:30,119 --> 00:52:32,119
We go ahead and replace it with another one.

879
00:52:32,119 --> 00:52:35,119
And then say the other page three and four, they've been accessed.

880
00:52:35,119 --> 00:52:38,119
Clock sees around, sets their bit to zero.

881
00:52:38,119 --> 00:52:40,119
And then so forth, come to here.

882
00:52:40,119 --> 00:52:42,119
And then now we're back to page one.

883
00:52:42,119 --> 00:52:44,119
It wasn't accessed since the last time we saw it.

884
00:52:44,119 --> 00:52:48,119
So we go ahead and evict this one.

885
00:52:48,119 --> 00:52:52,119
This is a good idea or a bad idea?

886
00:52:52,119 --> 00:52:53,119
Yes.

887
00:52:53,119 --> 00:52:59,119
It seems good in that it allows us to have a lot of storage between one bit per.

888
00:52:59,119 --> 00:53:02,119
But the downside seems like it's going to evict you,

889
00:53:02,119 --> 00:53:07,119
and if you don't use the room, we might have fewer hits.

890
00:53:07,119 --> 00:53:08,119
So you got the first part right.

891
00:53:08,119 --> 00:53:12,119
The first part is said this is nice because the medited overhead is low,

892
00:53:12,119 --> 00:53:14,119
because it's just a bit per page.

893
00:53:14,119 --> 00:53:16,119
You store that as a continuous bit vector.

894
00:53:16,119 --> 00:53:18,119
That's easy to do.

895
00:53:18,119 --> 00:53:21,119
But then the second you said like you might evict things that you shouldn't, you shouldn't actually need to beck.

896
00:53:21,119 --> 00:53:23,119
The clock only runs.

897
00:53:23,119 --> 00:53:26,119
You only start the sweep whenever you need to evict.

898
00:53:26,119 --> 00:53:28,119
So it's not just running the background all the time.

899
00:53:28,119 --> 00:53:29,119
Yeah.

900
00:53:29,119 --> 00:53:32,119
You don't want to do that.

901
00:53:32,119 --> 00:53:34,119
Yes.

902
00:53:34,119 --> 00:53:39,119
You're not taking into account the frequency of the pages that I've been asking.

903
00:53:39,119 --> 00:53:40,119
Yes.

904
00:53:40,119 --> 00:53:45,119
So he said, and he's correct, that in case of clock, and actually LRU,

905
00:53:45,119 --> 00:53:46,119
it would have this problem.

906
00:53:46,119 --> 00:53:52,119
We're not keeping track of the frequency in pages in which pages are accessed.

907
00:53:52,119 --> 00:53:55,119
And so this makes them susceptible to two problems.

908
00:53:55,119 --> 00:53:57,119
So the first one is what he said down here.

909
00:53:57,119 --> 00:54:00,119
Like in both clock and LRU, we're only tracking when they're accessed,

910
00:54:00,119 --> 00:54:03,119
now not how often they were accessed.

911
00:54:03,119 --> 00:54:09,119
And in both cases also too, they're susceptible to a problem called sequential flooding.

912
00:54:09,119 --> 00:54:14,119
And this is the problem where if a running sequential scans,

913
00:54:14,119 --> 00:54:17,119
we got to go fetch pages from disk, put into our buffer pool.

914
00:54:17,119 --> 00:54:20,119
But if we're tracking the last time they were used,

915
00:54:20,119 --> 00:54:25,119
the last page I just got got from my sequential scan is the most one that's recently used.

916
00:54:25,119 --> 00:54:28,119
But for that sequential scan, it's actually the most useful one,

917
00:54:28,119 --> 00:54:31,119
at least useful one, because just the page that is read,

918
00:54:31,119 --> 00:54:34,119
I'm not going to go back and read it again.

919
00:54:34,119 --> 00:54:35,119
Right?

920
00:54:35,119 --> 00:54:37,119
And in some cases for at least all that workloads,

921
00:54:37,119 --> 00:54:40,119
it's kind of like you went the most recently used one.

922
00:54:40,119 --> 00:54:45,119
It's a gross approximation, but it's another way to think about it.

923
00:54:45,119 --> 00:54:46,119
Right?

924
00:54:46,119 --> 00:54:50,119
Again, so like this, I have a query once it was select star from a table,

925
00:54:50,119 --> 00:54:52,119
but we're only going to get one ID or one record.

926
00:54:52,119 --> 00:54:54,119
And say that's in page 0.

927
00:54:54,119 --> 00:54:58,119
So we go put page 0 in our buffer pool.

928
00:54:58,119 --> 00:55:01,119
Then we have our O-Lat query that's going to scan the entire table

929
00:55:01,119 --> 00:55:04,119
and go fetch all the pages that entire table.

930
00:55:04,119 --> 00:55:07,119
But when it gets to page 3, we're not in space.

931
00:55:07,119 --> 00:55:14,119
So for these pages here, the least recently used page is page 0.

932
00:55:14,119 --> 00:55:18,119
So I'm going to go ahead and evict that and put in page 3.

933
00:55:18,119 --> 00:55:22,119
But for another query comes along, it does the same thing the first guy did

934
00:55:22,119 --> 00:55:25,119
and goes gets page 1 or once, once record 1 in page 1.

935
00:55:25,119 --> 00:55:30,119
That's actually the page I wanted, I need, but I just evicted it.

936
00:55:30,119 --> 00:55:34,119
So this is the worst thing you could do.

937
00:55:34,119 --> 00:55:36,119
Right?

938
00:55:37,119 --> 00:55:40,119
So the sequential flooding is a problem because again,

939
00:55:40,119 --> 00:55:44,119
if we do merge these point queries and then all of a sudden a switch query shows up,

940
00:55:44,119 --> 00:55:52,119
it's going to blow away any useful information we've collected in our LU or clock metadata.

941
00:55:52,119 --> 00:55:56,119
So the solution to this is called LUK.

942
00:55:56,119 --> 00:56:01,119
The idea here is you just keep track of the last K times a page was accessed.

943
00:56:01,119 --> 00:56:05,119
And then when it comes time to decide what to evict,

944
00:56:05,119 --> 00:56:12,119
you compute the interval between the last time, the K times it was accessed.

945
00:56:12,119 --> 00:56:17,119
And whatever one has the largest interval, meaning the time fund it was sort of accessed,

946
00:56:17,119 --> 00:56:25,119
K minus 1, or K minus 2 times 4, if that interval is the largest, then you know that it's likely to not be used in the future.

947
00:56:25,119 --> 00:56:29,119
And therefore you can go ahead and remove it.

948
00:56:30,119 --> 00:56:35,119
You think LU is basically LRUK, where K equals 1.

949
00:56:35,119 --> 00:56:39,119
And with 2 or 3 or whatever, merge systems used to, if you're going to use this,

950
00:56:39,119 --> 00:56:41,119
they just keep track of the last two times.

951
00:56:41,119 --> 00:56:43,119
And I'd say what's the time between the two of those?

952
00:56:43,119 --> 00:56:46,119
And I take the one that is the largest.

953
00:56:46,119 --> 00:56:55,119
And so of course this is susceptible to another problem where I fetch a page in,

954
00:56:55,119 --> 00:57:02,119
and I haven't access it twice yet, so the interval is essentially affinity.

955
00:57:02,119 --> 00:57:04,119
And then it goes ahead and gets immediately evicted.

956
00:57:04,119 --> 00:57:07,119
But say that actually is the hot page, and I want to keep that in memory.

957
00:57:07,119 --> 00:57:11,119
But because I keep evicting it, I lose that, I don't have any history of it.

958
00:57:11,119 --> 00:57:16,119
So the way to solve this is that you maintain a in-mary hash table,

959
00:57:16,119 --> 00:57:21,119
that keeps track of, here's the last couple pages that I've evicted on disk,

960
00:57:21,119 --> 00:57:24,119
and here's the last, here's when they were accessed, the time stamp.

961
00:57:24,119 --> 00:57:27,119
So that when I fetch a page back in, after it was just removed,

962
00:57:27,119 --> 00:57:32,119
I at least now have a history for it, and not assume that it's infinity.

963
00:57:32,119 --> 00:57:38,119
And that means that over time, you'll be able to get information you need to compute the correct interval for pages

964
00:57:38,119 --> 00:57:41,119
when they first, you know, when the first time they brought in a memory.

965
00:57:41,119 --> 00:57:44,119
And again, it's self-correcting, because again, if I bring something to memory

966
00:57:44,119 --> 00:57:49,119
but they never go fetch it again, it'll get removed from my informal cache.

967
00:57:49,119 --> 00:57:56,119
And whenever I need it again, I won't have the history, and I'll know I should have evicted.

968
00:57:56,119 --> 00:57:58,119
It's a simple solution, there's simple problems.

969
00:57:58,119 --> 00:58:01,119
Surprisingly, this was not in the minutes of the 90s.

970
00:58:01,119 --> 00:58:06,119
And as far as I can tell, only Postgres and SQL Server actually do this.

971
00:58:06,119 --> 00:58:10,119
And again, this is why I like open source things, because there's actually the mailing list

972
00:58:10,119 --> 00:58:13,119
post for the Postgres people, and then in 2002, they say,

973
00:58:13,119 --> 00:58:16,119
hey, this LRUK seems like a good idea, we should do it.

974
00:58:16,119 --> 00:58:19,119
And they implement it.

975
00:58:19,119 --> 00:58:24,119
So my SQL doesn't do exactly LRUK as I define, but they use a sort of Proximate1.

976
00:58:24,119 --> 00:58:34,119
And the way they do this is that they sort of logically divide up the link list for the LRU page list.

977
00:58:34,119 --> 00:58:37,119
And they have two different sections, or regions.

978
00:58:37,119 --> 00:58:40,119
They have like the young region, the old region.

979
00:58:41,119 --> 00:58:45,119
And for each of these two different regions, you'll have a different head pointer,

980
00:58:45,119 --> 00:58:48,119
where you would insert new entries.

981
00:58:48,119 --> 00:58:52,119
So let's say that I have a query I want to touch page 1, it's not in memory,

982
00:58:52,119 --> 00:58:55,119
so I have to put it into my buffer pool.

983
00:58:55,119 --> 00:59:00,119
And when I want to add it to my link list here, because page 1 is not already in the link list,

984
00:59:00,119 --> 00:59:05,119
I'm going to add it to the old region, and I'll insert it where the head pointer is.

985
00:59:06,119 --> 00:59:10,119
So I'll pick page 8 and put page 1 there.

986
00:59:10,119 --> 00:59:18,119
Now, if page 1 is never accessed again, it'll slowly make its way to the end of the link list,

987
00:59:18,119 --> 00:59:21,119
and then get evicted.

988
00:59:21,119 --> 00:59:24,119
But if say Q2 comes along and it touches page 1 again,

989
00:59:24,119 --> 00:59:29,119
we would identify that it already exists in my link list,

990
00:59:29,119 --> 00:59:34,119
and it's in the old region, so therefore I'll put it at the head of the young list.

991
00:59:35,119 --> 00:59:37,119
And then slide everyone out.

992
00:59:37,119 --> 00:59:41,119
So again, it's approximately L-A-U-K, because I'm not really keeping track of the intervals

993
00:59:41,119 --> 00:59:47,119
between when it was accessed before, but just knowing that within this boundary of the young versus old,

994
00:59:47,119 --> 00:59:52,119
then I know it's like, it was most likely accessed more recently.

995
00:59:52,119 --> 00:59:55,119
Whereas over here, you haven't seen it before.

996
00:59:55,119 --> 00:59:59,119
You haven't seen it before, it was added to the list.

997
01:00:05,119 --> 01:00:10,119
So I would say also, going back to the L-A-U-K,

998
01:00:10,119 --> 01:00:15,119
there's a bunch of other optimizations you can do that SQL server does,

999
01:00:15,119 --> 01:00:21,119
but I don't think Postgres does, where you can keep track of how,

1000
01:00:21,119 --> 01:00:25,119
of who is accessing or referencing a page.

1001
01:00:25,119 --> 01:00:31,119
And then that can determine whether you would say an access counts for a distinct reference,

1002
01:00:31,119 --> 01:00:33,119
and therefore should update the interval.

1003
01:00:33,119 --> 01:00:40,119
An example would be if I have within the same transaction two separate queries access the same page,

1004
01:00:40,119 --> 01:00:46,119
well that's in the same transaction, so therefore should they be considered distinct or not.

1005
01:00:46,119 --> 01:00:50,119
If there are two separate transactions, then it's very likely this page is hot,

1006
01:00:50,119 --> 01:00:52,119
because a bunch of transactions are accessing it.

1007
01:00:52,119 --> 01:00:59,119
I think of like if you log into Amazon and you go update your account information

1008
01:00:59,119 --> 01:01:03,119
and say for whatever reason that transaction updates your record twice,

1009
01:01:03,119 --> 01:01:07,119
well that's done in the same transaction, is that considered two accesses or one,

1010
01:01:07,119 --> 01:01:10,119
and then SQL server they'll consider that one,

1011
01:01:10,119 --> 01:01:14,119
and then in Postgres they consider that two.

1012
01:01:14,119 --> 01:01:24,119
Again, you knew a bunch of fancy things because you know how the data system is accessing pages.

1013
01:01:24,119 --> 01:01:32,119
There's a bunch of policies you can do for deciding, you know, sort of related this,

1014
01:01:32,119 --> 01:01:37,119
like for a given query what page should I evict.

1015
01:01:37,119 --> 01:01:45,119
Like if this is sort of related to the private cache, like in some systems you can say,

1016
01:01:45,119 --> 01:01:51,119
all right, here's a some subset of the pages that I'm accessing.

1017
01:01:51,119 --> 01:01:56,119
They're being backed by the buffer pool, right, but I'm keeping track of which ones I'm accessing.

1018
01:01:56,119 --> 01:02:00,119
So then I can give a hint to the buffer manager to say, if you don't have any more space,

1019
01:02:00,119 --> 01:02:04,119
here's the pages where I know I'm accessing therefore, and I don't need them again,

1020
01:02:04,119 --> 01:02:06,119
therefore you can go ahead and evict them.

1021
01:02:06,119 --> 01:02:14,119
Now whether or not the data system decides, you should evict them or not, depends on the implementation.

1022
01:02:14,119 --> 01:02:22,119
You can also maintain priority hints about what the type of page or what the object that a page represents.

1023
01:02:22,119 --> 01:02:28,119
And then this provided to the data system to say, you know, the buffer manager whether something should be evict or not.

1024
01:02:28,119 --> 01:02:33,119
So the classic example would be if I have an index of a bunch of pages,

1025
01:02:33,119 --> 01:02:41,119
and a very clear is that it always means certain new records that are just increasing the size of the data I'm starting,

1026
01:02:41,119 --> 01:02:48,119
the value that the index is based on, then I know I'm going to be always hitting the right side of the tree,

1027
01:02:48,119 --> 01:02:54,119
and therefore maybe I want to keep those pages in memory, and I don't care if page 2 over here gets evicted

1028
01:02:54,119 --> 01:02:59,119
because I'm mostly going to be updating things on page 6.

1029
01:02:59,119 --> 01:03:04,119
Otherwise if I do much select queries, I have to use the index,

1030
01:03:04,119 --> 01:03:09,119
and I know the very first thing they're always going to access is the root page index,

1031
01:03:09,119 --> 01:03:13,119
so therefore that should be given a high priority than the other ones.

1032
01:03:13,119 --> 01:03:18,119
Now these seem kind of like band-aids over like LRUK or other mechanisms,

1033
01:03:18,119 --> 01:03:20,119
and they kind of are, right?

1034
01:03:20,119 --> 01:03:24,119
And so it's like, think of it as like a light pin to tell the system,

1035
01:03:24,119 --> 01:03:30,119
hey you really probably shouldn't evict this, but if you have to, yes you can, but please don't.

1036
01:03:31,119 --> 01:03:38,119
But again, there's additional things beyond the LRU tracking that the systems are doing.

1037
01:03:38,119 --> 01:03:42,119
The big challenge though when it comes to evicting pages is whether they're dirty or not.

1038
01:03:42,119 --> 01:03:50,119
So the easiest thing to do is if all my pages are clean to evict them from the bufferable is to do nothing,

1039
01:03:50,119 --> 01:03:55,119
you just drop the page reference in the page table, you need to overwrite whatever's there before, right?

1040
01:03:55,119 --> 01:03:58,119
Because you don't need to flush it back to disk.

1041
01:03:59,119 --> 01:04:05,119
If though, if all the pages are dirty or the one you want to evict is dirty, then you got to write that to disk,

1042
01:04:05,119 --> 01:04:11,119
make sure it's durable and safe, which covered later in the semester, before you can go ahead and say,

1043
01:04:11,119 --> 01:04:16,119
okay this frame is now free, you can go ahead and reuse it.

1044
01:04:16,119 --> 01:04:21,119
It's actually more complicated than that because you actually have to write and show the log record as flushed at disk first,

1045
01:04:21,119 --> 01:04:25,119
before you can flush the dirty page that the log reference talks about.

1046
01:04:25,119 --> 01:04:27,119
Okay, we will cover that later.

1047
01:04:27,119 --> 01:04:31,119
So the reason why this is tricky to do is because it may be the case given all the things we just talked about,

1048
01:04:31,119 --> 01:04:35,119
these priority hints, these different policies, the LRU case stuff,

1049
01:04:35,119 --> 01:04:40,119
it may be the case that the page you want to evict is dirty,

1050
01:04:40,119 --> 01:04:48,119
and so that is going to require a disk flush, but maybe the second page you could evict is clean.

1051
01:04:48,119 --> 01:04:51,119
So should you violate the LRU case in that case?

1052
01:04:51,119 --> 01:04:55,119
Again, just trying to get things out as fast as possible.

1053
01:04:55,119 --> 01:05:01,119
Different systems do different things, and this is why the enterprise systems are much better than the open source systems,

1054
01:05:01,119 --> 01:05:05,119
because they have all this metadata, they have all this sophisticated algorithms to make these decisions.

1055
01:05:05,119 --> 01:05:08,119
Because it actually depends on the speed of the hardware.

1056
01:05:08,119 --> 01:05:13,119
If your disk is super fast, then yeah, I'll write it out the disk, I'll write out a dirty page out right away,

1057
01:05:13,119 --> 01:05:16,119
because that's going to be fairly inexpensive operation.

1058
01:05:16,119 --> 01:05:20,119
If my disk is really slow, or I've got to write it with a network to some slow device,

1059
01:05:20,119 --> 01:05:30,119
then maybe I want to minimize the amount of disk rights I have to do when I have to evict something the exact moment I need a space.

1060
01:05:30,119 --> 01:05:33,119
So there's no easy answer to this.

1061
01:05:33,119 --> 01:05:39,119
If all your pages are dirty and need a vick one, you have to write it.

1062
01:05:39,119 --> 01:05:43,119
But what's one way to avoid this problem?

1063
01:05:43,119 --> 01:05:47,119
To not have the right deal on the critical path when you want access to page.

1064
01:05:52,119 --> 01:05:55,119
Backarm writing, right?

1065
01:05:55,119 --> 01:06:05,119
You could have a separate thread in the background, just walk through a page table, figure out what's dirty, make sure the log is on disk.

1066
01:06:05,119 --> 01:06:09,119
Again, we'll cover that later, but soon you have to court a log first.

1067
01:06:09,119 --> 01:06:14,119
Find pages that are dirty, and go ahead and write them out.

1068
01:06:14,119 --> 01:06:17,119
Then you just flip the bit to say this page is now clean.

1069
01:06:17,119 --> 01:06:24,119
So that when the eviction algorithm runs, and it says, okay, I have to evict something, now has a bunch of options of pages that are clean that could write out.

1070
01:06:24,119 --> 01:06:29,119
Or sorry, just drop, you don't have to write anything.

1071
01:06:29,119 --> 01:06:34,119
But now there's this trade-off routine, like should I have my system be aggressively writing out dirty pages,

1072
01:06:34,119 --> 01:06:39,119
maybe interfering with queries and transactions that are trying to run things with a happy application?

1073
01:06:39,119 --> 01:06:42,119
Or should I delay that?

1074
01:06:42,119 --> 01:06:48,119
But then now the problem is that some point I need to get free space at all my pages are dirty.

1075
01:06:48,119 --> 01:06:54,119
It's a super hard problem, and there's no easy answer.

1076
01:06:54,119 --> 01:07:00,119
But all the systems are going to have some mechanism to do this background writing.

1077
01:07:00,119 --> 01:07:04,119
Then checkpoints are a whole another beast. Checkpoints you flush everything out.

1078
01:07:04,119 --> 01:07:14,119
But that happens every minute, every minute, every second.

1079
01:07:14,119 --> 01:07:18,119
Okay?

1080
01:07:18,119 --> 01:07:23,119
So now we start talking a little bit about what we do these disk writes and disk reads.

1081
01:07:23,119 --> 01:07:27,119
We have to talk about how we actually going to do that.

1082
01:07:27,119 --> 01:07:34,119
And for this one, when we make reading write calls to the file system,

1083
01:07:34,119 --> 01:07:38,119
assuming we're running on the file, not raw partitions,

1084
01:07:38,119 --> 01:07:44,119
there's a bunch of layers below us in our daily system, like the OS and the file system and the hardware,

1085
01:07:44,119 --> 01:07:53,119
that's going to be clever and try to obviously maximize the amount of bandwidth that can achieve by reordering and batching our IO requests.

1086
01:07:53,119 --> 01:07:58,119
Part of the reason why these modern disk drives or SSDs and the MED drives are so fast,

1087
01:07:58,119 --> 01:08:02,119
because they have these long cues and do parallel requests.

1088
01:08:02,119 --> 01:08:06,119
So if you just started to do one read at a time, that's going to be super slow, but you can batch things up,

1089
01:08:06,119 --> 01:08:10,119
and make sure that you're reading a bunch of contiguous data, things will be really fast.

1090
01:08:10,119 --> 01:08:19,119
But the challenge is though, these different layers of the system below the daily system don't know what the requests actually correspond to.

1091
01:08:19,119 --> 01:08:22,119
They just see reads and writes and pages at some location or some address.

1092
01:08:22,119 --> 01:08:27,119
They don't know, oh, this is from the background writer, or this is for an index, or this is for the query,

1093
01:08:27,119 --> 01:08:30,119
then they need to run right now.

1094
01:08:30,119 --> 01:08:37,119
So you can play some games with in Linux setting the IO priority, but that's basically a sledgehammer.

1095
01:08:37,119 --> 01:08:43,119
The only thing you can really do is change the IO priority on a per process level.

1096
01:08:43,119 --> 01:08:48,119
You can't do it on a per single request, which is what we really want.

1097
01:08:48,119 --> 01:08:54,119
So if you read the documentation on a bunch of data systems, they tell you to all get off the default Linux scheduler,

1098
01:08:54,119 --> 01:09:00,119
which is the fair scheduler, and either use deadline or the simple 5.0Q and no op,

1099
01:09:00,119 --> 01:09:10,119
because they don't want the operating system to do a bunch of stuff that the data system can't control.

1100
01:09:10,119 --> 01:09:18,119
So this is why in most data systems, they're going to have their own little shim layer right above the OS

1101
01:09:18,119 --> 01:09:26,119
that can be responsible for keeping track of what requests are outstanding for reads and writes from the Bupfable Manager,

1102
01:09:26,119 --> 01:09:31,119
and decide how to put things together to optimize performance.

1103
01:09:31,119 --> 01:09:38,119
And basically, you think of determining the priorities for the different IO requests based on multitude of different factors,

1104
01:09:38,119 --> 01:09:43,119
because again, we know what the queries are trying to do, we know what pages are in a Bupfable,

1105
01:09:43,119 --> 01:09:48,119
we know what's dirty and not dirty, we know what are the outstanding requests.

1106
01:09:48,119 --> 01:09:52,119
And so to try to keep track of things like what's Swential IO versus Random IO,

1107
01:09:52,119 --> 01:09:56,119
is the request based on the critical path of like a query needs this right now,

1108
01:09:56,119 --> 01:10:03,119
or is this like a background job, the background writer, and therefore, you know, could have a lower priority.

1109
01:10:03,119 --> 01:10:10,119
Is the data accessing for a table and index log records, again, the logs you want to flush as fast as possible,

1110
01:10:10,119 --> 01:10:17,119
is it for a femoral data? Like if it's a Swential scan for a table, that's going to have a lower priority,

1111
01:10:17,119 --> 01:10:21,119
and then maybe you ran them access for index, because as you traverse your B-plustry,

1112
01:10:21,119 --> 01:10:26,119
you're holding latches as you go down, and therefore, that's going to prevent other threads from running at the same time.

1113
01:10:26,119 --> 01:10:29,119
But if you're query, your scan query is a little bit slower,

1114
01:10:29,119 --> 01:10:33,119
you're technically potentially not interfering with other queries running at the same time.

1115
01:10:33,119 --> 01:10:38,119
Well, that's not true, because you can take locks during when you run queries and transactions.

1116
01:10:38,119 --> 01:10:43,119
So there's no easy answer for when one should be faster than another.

1117
01:10:43,119 --> 01:10:46,119
You can't just because it's indexed, because it's a table.

1118
01:10:46,119 --> 01:10:49,119
Again, the OS doesn't know that it's an indexed page versus a table page.

1119
01:10:49,119 --> 01:10:54,119
There's also sometimes in some systems, you can have SLA service level agreements,

1120
01:10:54,119 --> 01:10:59,119
or service level objectives. My queries have to run within the certain deadline as a certain latency.

1121
01:10:59,119 --> 01:11:02,119
And the typically way you do this is you have different user accounts,

1122
01:11:02,119 --> 01:11:05,119
and give one user count higher priority than another user count.

1123
01:11:05,119 --> 01:11:10,119
The front-end application has a higher priority than nightly reporting jobs,

1124
01:11:10,119 --> 01:11:13,119
and you do that based on user roles.

1125
01:11:16,119 --> 01:11:20,119
So the way we can get better performance,

1126
01:11:20,119 --> 01:11:25,119
and try to avoid some of this interference from the OS,

1127
01:11:25,119 --> 01:11:28,119
is to use what's called direct IO.

1128
01:11:28,119 --> 01:11:31,119
So all your disk operations, for the most part,

1129
01:11:31,119 --> 01:11:38,119
are going to have to go through the OS API, unless you're doing direct kernel bypass to the hardware device,

1130
01:11:38,119 --> 01:11:41,119
but most systems don't do that.

1131
01:11:41,119 --> 01:11:45,119
And the idea here is that we don't want the OS to maintain its own cache,

1132
01:11:45,119 --> 01:11:50,119
because we don't want the OS to buffer our reads and writes,

1133
01:11:50,119 --> 01:11:53,119
we want to do that all entirely ourselves.

1134
01:11:53,119 --> 01:11:57,119
Because again, we want to have full control of the hardware.

1135
01:11:57,119 --> 01:12:02,119
So the idea is like this. If I do a read against the file system,

1136
01:12:02,119 --> 01:12:08,119
well, the OS is going to say, let me go maintain my own little buffer pool in the OS,

1137
01:12:08,119 --> 01:12:11,119
across all the processes running.

1138
01:12:11,119 --> 01:12:13,119
It's a global for the OS.

1139
01:12:13,119 --> 01:12:17,119
And then I will store the data I need, you're asking for, my page cache.

1140
01:12:17,119 --> 01:12:21,119
And the next time I do a read, I'll get it from my page cache.

1141
01:12:21,119 --> 01:12:24,119
So instead, we want to bypass this and go around it,

1142
01:12:24,119 --> 01:12:30,119
and store it directly to the hardware, and not have the OS buffer anything.

1143
01:12:30,119 --> 01:12:35,119
So most systems use direct IO. Most systems will use direct IO by default.

1144
01:12:35,119 --> 01:12:38,119
There's only one system that does not.

1145
01:12:38,119 --> 01:12:41,119
I'm going to take a guess what it is.

1146
01:12:41,119 --> 01:12:44,119
Say a single store. No.

1147
01:12:44,119 --> 01:12:47,119
Nice to go. No. It's Postgres.

1148
01:12:47,119 --> 01:12:50,119
Postgres, because there's a relic from the 80s,

1149
01:12:50,119 --> 01:12:53,119
they rely heavily on the OS page cache.

1150
01:12:53,119 --> 01:12:57,119
And so when you allocate a buffer pool in Postgres,

1151
01:12:57,119 --> 01:13:00,119
you set it to like 30% of the amount of memory that's on the box.

1152
01:13:00,119 --> 01:13:04,119
My SQL and every other database system tell you it's 80% of the memory that's available on the box.

1153
01:13:04,119 --> 01:13:08,119
Because in Postgres, they want some of the memory to be for the page cache in the OS,

1154
01:13:08,119 --> 01:13:10,119
some of the memory for the database system.

1155
01:13:11,119 --> 01:13:12,119
So what's the problem with this?

1156
01:13:12,119 --> 01:13:14,119
Well, now I've got redundant copies of my pages.

1157
01:13:14,119 --> 01:13:18,119
The OS is going to have a copy of my page, and my data system is going to have a copy of my page.

1158
01:13:18,119 --> 01:13:20,119
Same things for reads and writes.

1159
01:13:20,119 --> 01:13:27,119
And then the data system is going to have its own policy of how to decide what pages do a bit.

1160
01:13:27,119 --> 01:13:30,119
But then just because I evicted from my buffer pool in my database system,

1161
01:13:30,119 --> 01:13:33,119
the OS can decide to evict it any way it wants to.

1162
01:13:33,119 --> 01:13:37,119
Again, it doesn't know what the pages actually represent.

1163
01:13:37,119 --> 01:13:42,119
And then you also lose control when things actually get flushed out the disk unless you're careful.

1164
01:13:44,119 --> 01:13:49,119
So let's see how far you guys can get with these answers here, even if you haven't taken the OS class.

1165
01:13:49,119 --> 01:13:51,119
So if you call F-Rite, what happens?

1166
01:13:53,119 --> 01:13:55,119
I open a file, right?

1167
01:13:55,119 --> 01:13:57,119
Say, say, say, I open my database file.

1168
01:13:57,119 --> 01:14:02,119
I have my buffer pool manager bring a page in, another query updates it.

1169
01:14:02,119 --> 01:14:03,119
The page is dirty.

1170
01:14:03,119 --> 01:14:05,119
I call F-Rite to write that page out the disk.

1171
01:14:05,119 --> 01:14:06,119
What happens?

1172
01:14:08,119 --> 01:14:10,119
The assume we were not using direct I-O.

1173
01:14:12,119 --> 01:14:14,119
The page lands in the OS page cache.

1174
01:14:14,119 --> 01:14:18,119
Because the operating system is trying to be clever and trying to make things fast for you, right?

1175
01:14:19,119 --> 01:14:21,119
So is it on disk yet?

1176
01:14:21,119 --> 01:14:25,119
When F-Rite returns, is my data safe?

1177
01:14:25,119 --> 01:14:26,119
No.

1178
01:14:26,119 --> 01:14:28,119
Because it's in the OS page cache.

1179
01:14:28,119 --> 01:14:30,119
When is it going to be flushed out the disk?

1180
01:14:32,119 --> 01:14:33,119
When the OS decides to do it.

1181
01:14:34,119 --> 01:14:35,119
Right?

1182
01:14:36,119 --> 01:14:37,119
But, f*** that.

1183
01:14:37,119 --> 01:14:38,119
We want to make sure our things are on disk.

1184
01:14:38,119 --> 01:14:39,119
So what do we call F-Sync?

1185
01:14:41,119 --> 01:14:42,119
What does F-Sync do?

1186
01:14:44,119 --> 01:14:45,119
Flush?

1187
01:14:45,119 --> 01:14:46,119
Flush what?

1188
01:14:47,119 --> 01:14:48,119
Dirty pages out the disk.

1189
01:14:48,119 --> 01:14:50,119
You can kind of do ranges, right?

1190
01:14:50,119 --> 01:14:51,119
It doesn't always work.

1191
01:14:52,119 --> 01:14:59,119
And then the call to F-Sync will block until the harbor complex says your data is persisted.

1192
01:15:00,119 --> 01:15:01,119
Now the harbor complex is playing games too.

1193
01:15:01,119 --> 01:15:03,119
Sometimes the harbor has little battery down there.

1194
01:15:03,119 --> 01:15:06,119
So you'll get your rights that still setting volatile memory.

1195
01:15:06,119 --> 01:15:10,119
But if it is a power, and you can send a response to it, I got your right.

1196
01:15:10,119 --> 01:15:12,119
But then there's a power loss.

1197
01:15:12,119 --> 01:15:15,119
There's just enough battery juice to make sure it gets written to disk.

1198
01:15:15,119 --> 01:15:16,119
Right?

1199
01:15:16,119 --> 01:15:20,119
So it's not always me on the name flash at that point.

1200
01:15:20,119 --> 01:15:22,119
But typically that's good enough.

1201
01:15:23,119 --> 01:15:25,119
But what happens if F-Sync calls a failure?

1202
01:15:25,119 --> 01:15:27,119
What F-Sync says I can't do it for you.

1203
01:15:28,119 --> 01:15:30,119
What does that mean?

1204
01:15:33,119 --> 01:15:38,119
Say the OS doesn't crash, F-Sync just returns an error.

1205
01:15:41,119 --> 01:15:45,119
In Linux, it's going to mark the dirty pages of clean.

1206
01:15:45,119 --> 01:15:46,119
It's not a kernel pack.

1207
01:15:46,119 --> 01:15:50,119
It says, these pages are clean, even though F-Sync failed.

1208
01:15:50,119 --> 01:15:52,119
And then you need to call F-Sync again.

1209
01:15:52,119 --> 01:15:55,119
It's going to stick on those pages that you want to write that down.

1210
01:15:55,119 --> 01:15:58,119
It's going to stick on those pages that you want to write to disk for your F-Rite.

1211
01:15:58,119 --> 01:16:00,119
It's going to come back and say, yep, I got them.

1212
01:16:00,119 --> 01:16:01,119
They're clean.

1213
01:16:01,119 --> 01:16:02,119
It's on disk.

1214
01:16:02,119 --> 01:16:04,119
But it's aligned to you.

1215
01:16:04,119 --> 01:16:05,119
Right?

1216
01:16:05,119 --> 01:16:07,119
Why is it aligned to you?

1217
01:16:10,119 --> 01:16:14,119
Because the kernel developer is worried about someone pulling out a USB stick.

1218
01:16:14,119 --> 01:16:15,119
Right?

1219
01:16:15,119 --> 01:16:22,119
And then the F-Sync failing and having a bunch of pages marked dirty in its page table

1220
01:16:22,119 --> 01:16:26,119
that are never going to get come back again because you're never going to put the USB stick back in again.

1221
01:16:26,119 --> 01:16:27,119
Right?

1222
01:16:27,119 --> 01:16:29,119
Is that the right thing for databases?

1223
01:16:29,119 --> 01:16:30,119
F***ing hell.

1224
01:16:30,119 --> 01:16:31,119
Right?

1225
01:16:31,119 --> 01:16:33,119
We're not running a USB stick.

1226
01:16:33,119 --> 01:16:38,119
So again, we need full control of everything to make sure we're doing things right.

1227
01:16:38,119 --> 01:16:45,119
Well, it turns out that people didn't know that F-Sync was broken in this way for 20 years.

1228
01:16:45,119 --> 01:16:51,119
So in 2018, there was a scandal called F-Sync gate where someone on the Post-GestMate

1229
01:16:51,119 --> 01:16:56,119
on the Post-GestMailing list reported, hey, Post-Gest lost some of my data.

1230
01:16:56,119 --> 01:17:00,119
But I never got a kernel panic and I never had a failure.

1231
01:17:00,119 --> 01:17:05,119
And it turns out because F-Sync, on all these database systems, they would call F-Sync,

1232
01:17:05,119 --> 01:17:09,119
F-Sync would turn to error and they just put it in a wild loop and call it again.

1233
01:17:09,119 --> 01:17:15,119
And then F-Sync came back and said, yep, I got it because they marked out your pages that were dirty to clean even though they were never written.

1234
01:17:15,119 --> 01:17:17,119
It's not just Post-Gest had this problem.

1235
01:17:17,119 --> 01:17:21,119
MongoDB had this problem and wiretiger, a bunch of other systems.

1236
01:17:21,119 --> 01:17:28,119
So now what do they do? Well, F-Sync fails, then the system crashes and then you've got to go figure out what's going on.

1237
01:17:28,119 --> 01:17:33,119
But for 20 years, people didn't know this was an issue.

1238
01:17:33,119 --> 01:17:44,119
Right? And so this is not an issue of the page cache, but this is an example where we need to make sure we have full control of what is getting red and written from disk into memory

1239
01:17:44,119 --> 01:17:49,119
and that when we write things out the disk, we want to make sure that it's actually safe and correct.

1240
01:17:49,119 --> 01:17:58,119
And that the OS can lie to us because the OS doesn't care about databases because the OS is worried about somebody with the USB stick or whatever,

1241
01:17:58,119 --> 01:18:01,119
because it's trying to be a general purpose system.

1242
01:18:01,119 --> 01:18:07,119
And so we need to make sure that we, as a data system, developers, put the mechanisms in place to make sure that we don't get screwed.

1243
01:18:07,119 --> 01:18:11,119
Okay?

1244
01:18:12,119 --> 01:18:16,119
All right, so there's multiple polls other than what we've talked about.

1245
01:18:16,119 --> 01:18:20,119
Just think of a formal cache is for joins and things like that. We'll see that later.

1246
01:18:20,119 --> 01:18:27,119
Okay, so we're always be better than the operating system despite what the Linux people say, the OS people say,

1247
01:18:27,119 --> 01:18:35,119
and because we know what the query plans are wanted to do, we know how queries run access data and we can always do better things.

1248
01:18:35,119 --> 01:18:39,119
So hash tables next class, but let me talk about Project One very quickly.

1249
01:18:39,119 --> 01:18:43,119
So you'll be implementing this in bus tobb obviously, and so you have three parts.

1250
01:18:43,119 --> 01:18:50,119
You're LRUK, replace some policy, a disk schedule, a very primitive one, and then the actual bubble manager instance itself.

1251
01:18:50,119 --> 01:18:53,119
Oh, shit.

1252
01:18:53,119 --> 01:18:59,119
So for the first one, there'll be a separate class, the implement, the basically keep track of all the pages,

1253
01:18:59,119 --> 01:19:04,119
and then there'll be an API implement that says, I need, give me a page to a bit.

1254
01:19:04,119 --> 01:19:11,119
So you sort of implement that first, and then there's, there's, there's, you know, there's tests to make sure this is actually working correctly.

1255
01:19:11,119 --> 01:19:16,119
If none of the pages have been touched on the same you check, then it's always a turn the lowest page ID.

1256
01:19:16,119 --> 01:19:19,119
Because again, you got to fix something.

1257
01:19:19,119 --> 01:19:22,119
And so you decide what to fix based on that.

1258
01:19:22,119 --> 01:19:26,119
The next thing you implement is a disk schedule. Basically, you can take a bunch of different requests,

1259
01:19:26,119 --> 01:19:35,119
or even threads running at the same time, and then have a single queue to decide in which order you should apply those, the, the, the reason rights.

1260
01:19:35,119 --> 01:19:42,119
And the way you implement this to the API, you, there'll be a callback mechanism through the C++ Promise API, Promise Strux.

1261
01:19:42,119 --> 01:19:50,119
And this is a function you invoke once the page, once the data that you, that the, that the, that the, the requester is waiting for is available.

1262
01:19:50,119 --> 01:19:52,119
And you call that back.

1263
01:19:52,119 --> 01:20:02,119
It's not going to be true asynchronous IO, but because, because basically the thread that makes the request will block it from this callback, but this would be the building block to do more sophisticated things.

1264
01:20:02,119 --> 01:20:06,119
So make sure that what you build here is, is thread safe.

1265
01:20:06,119 --> 01:20:12,119
The last thing is the buffer manager itself, and this will be built on top of the LUK implementation and the, and your disk scheduler.

1266
01:20:12,119 --> 01:20:19,119
And you maintain the internal data structure to read and write data, usually disk scheduler, and keep track of what pages are free and when they're accessed.

1267
01:20:19,119 --> 01:20:26,119
And the thing that always trips up people every year is make sure you get the order and correct read when you're pinning an unpainting.

1268
01:20:26,119 --> 01:20:35,119
So, six might not be the right number, but don't change any other file, because everything will get over, overwritten when you load it up into grade scope.

1269
01:20:35,119 --> 01:20:42,119
The project is cumulative. We won't be writing solutions and then post everything on, on Piazza as you go along.

1270
01:20:42,119 --> 01:20:48,119
Like in project zero, you have to make sure you have good code quality, so make sure you run, make format, and then check, clang tidy.

1271
01:20:48,119 --> 01:20:54,119
Because if you don't do this and you upload the grade scope, you'll get a zero.

1272
01:20:54,119 --> 01:20:58,119
We are having a, for this project and all the other projects, there will be extra credit.

1273
01:20:58,119 --> 01:21:06,119
There's a leaderboard with the additional tasks that go beyond the core requirements, and then you'll get ranked when you submit your, submit your thing on grade scope.

1274
01:21:06,119 --> 01:21:13,119
And then the top 20 students will get bonus points for this project, and this will be available for all four projects.

1275
01:21:13,119 --> 01:21:21,119
And then the student at the end of the semester has the highest score, bonus points, and all the students will get a limited edition bus tub hoodie.

1276
01:21:21,119 --> 01:21:29,119
You have to fill out a tax form because their limited edition, they cost $5,000, and again, CME will handle that, the paper will work for that.

1277
01:21:29,119 --> 01:21:35,119
But it's highly desirable. Don't f*** around, don't plagiarize, because again, the grade scope has the automatic,

1278
01:21:35,119 --> 01:21:42,119
plasier-room checker, and we go find the randos of GitHub. We put them as fake students, and if you copy them, you'll get flag.

1279
01:21:42,119 --> 01:21:44,119
Okay? Hit it.

1280
01:22:35,119 --> 01:22:42,119
I'm going to get the rand to every state. When I'm acting, how I'm living, I tell them I'm living great.

