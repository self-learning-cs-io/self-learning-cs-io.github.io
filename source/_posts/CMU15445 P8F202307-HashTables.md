---
title: CMU15445 P8F202307 HashTables
---

1
00:00:00,000 --> 00:00:28,080
Come here, Gordon!

2
00:00:28,080 --> 00:00:29,080
Videpage 1

3
00:00:29,079 --> 00:00:34,079
So real quick, sorry.

4
00:00:34,079 --> 00:00:36,079
Some girl in my office hours looking for you today.

5
00:00:36,079 --> 00:00:37,079
What?

6
00:00:37,079 --> 00:00:38,079
Yeah.

7
00:00:38,079 --> 00:00:45,079
I just, she was trying to find where you were.

8
00:00:45,079 --> 00:00:46,079
That's weird.

9
00:00:46,079 --> 00:00:47,079
I should find a better hide.

10
00:00:47,079 --> 00:00:53,079
So real quick, actually, the facility to be

11
00:00:53,079 --> 00:00:55,079
what actually just came to be, and they said that the

12
00:00:55,079 --> 00:00:57,079
governor is here.

13
00:00:57,079 --> 00:01:01,079
Shapiro, I vote for him because he's not a Trump supporter.

14
00:01:01,079 --> 00:01:04,079
But the main interest is blocked off.

15
00:01:04,079 --> 00:01:06,079
So when the class is over, you can't go through that entrance.

16
00:01:06,079 --> 00:01:08,079
You got to take the elevator and go upstairs.

17
00:01:08,079 --> 00:01:09,079
Okay?

18
00:01:09,079 --> 00:01:11,079
That's why all the federalities and the cops are out there.

19
00:01:11,079 --> 00:01:14,079
All right.

20
00:01:14,079 --> 00:01:15,079
All right.

21
00:01:15,079 --> 00:01:18,079
So again, for the class, again, we had the

22
00:01:18,079 --> 00:01:21,079
the recitation on Monday that's been posted on piata as a

23
00:01:21,079 --> 00:01:23,079
video.

24
00:01:23,079 --> 00:01:26,079
Project one is still due on October 7th, or sorry, October 2nd.

25
00:01:26,079 --> 00:01:29,079
And then we will have, again, the special office hours on

26
00:01:29,079 --> 00:01:31,079
Saturday on the first, and then homework two has been

27
00:01:31,079 --> 00:01:33,079
bumped to be due on October 4th.

28
00:01:33,079 --> 00:01:35,079
And that's a Wednesday and out of Sunday.

29
00:01:35,079 --> 00:01:36,079
Okay?

30
00:01:36,079 --> 00:01:40,079
Any questions about the homework of the projects?

31
00:01:40,079 --> 00:01:41,079
Yes.

32
00:01:41,079 --> 00:01:44,079
The Sunday is October 1st, yeah.

33
00:01:44,079 --> 00:01:45,079
All right.

34
00:01:45,079 --> 00:01:50,079
So it's due on the Sunday, and then the office hours are

35
00:01:50,079 --> 00:01:51,079
on the Saturday.

36
00:01:51,079 --> 00:01:53,079
So whatever that really, what are those real dates are?

37
00:01:53,079 --> 00:01:56,079
Yes.

38
00:01:56,079 --> 00:01:59,079
The website and grade school should be correct.

39
00:01:59,079 --> 00:02:00,079
I'm not.

40
00:02:00,079 --> 00:02:03,079
Other questions?

41
00:02:03,079 --> 00:02:04,079
All right.

42
00:02:04,079 --> 00:02:05,079
Cool.

43
00:02:05,079 --> 00:02:08,079
So then two sort of sort of, or one sort of fun thing to bring up.

44
00:02:08,079 --> 00:02:11,079
Someone said, hey, what about internships on these companies hiring?

45
00:02:11,079 --> 00:02:12,079
The answer is yes.

46
00:02:12,079 --> 00:02:18,079
And actually, somebody posted out on Twitter that, you know,

47
00:02:18,080 --> 00:02:21,080
if you take in my class, they're hiring.

48
00:02:21,080 --> 00:02:26,080
And so space time BP is a, I think it's another time series database system.

49
00:02:26,080 --> 00:02:27,080
I think it's out of Europe.

50
00:02:27,080 --> 00:02:28,080
All right.

51
00:02:28,080 --> 00:02:29,080
I don't know this dude.

52
00:02:29,080 --> 00:02:31,080
You can contact him if you want.

53
00:02:31,080 --> 00:02:35,080
But we'll post on Piotta how we can, you know, how you can,

54
00:02:35,080 --> 00:02:37,080
how you can get me your CV.

55
00:02:37,080 --> 00:02:40,080
And then we can send it to the various database companies that we know.

56
00:02:40,080 --> 00:02:41,080
And that are friends with us.

57
00:02:41,080 --> 00:02:44,080
And again, if you haven't yet, please apply to single store.

58
00:02:44,080 --> 00:02:46,080
And there's that special email address that's just for senior students.

59
00:02:46,080 --> 00:02:49,080
And that'll go directly to the hiring people and not the recruiters.

60
00:02:49,080 --> 00:02:55,080
I'm sorry, if you don't want to do an internship, another way to make money through databases is that

61
00:02:55,080 --> 00:02:57,080
somebody actually posted on an upwork.

62
00:02:57,080 --> 00:03:06,080
And this is real that they're looking for someone to basically design database projects that are basically bus tub and the class projects.

63
00:03:06,080 --> 00:03:14,080
So if you like this stuff you're doing, you can get paid $100 by this guy to go, go, implement that.

64
00:03:15,080 --> 00:03:21,080
And the way we found this was somebody actually emailed Chi the TA and he was like, hey, I can do this job for you.

65
00:03:21,080 --> 00:03:22,080
And he's like, what are you talking about?

66
00:03:22,080 --> 00:03:24,080
Because he thought we posted this.

67
00:03:24,080 --> 00:03:25,080
This is not us.

68
00:03:25,080 --> 00:03:26,080
This is some rando.

69
00:03:26,080 --> 00:03:28,080
$100 is definitely not enough.

70
00:03:28,080 --> 00:03:34,080
Like again, like you should be making $100 an hour in databases, if not more.

71
00:03:34,080 --> 00:03:35,080
Okay.

72
00:03:35,080 --> 00:03:37,080
All right.

73
00:03:37,080 --> 00:03:40,080
So, what, so where we at in the class?

74
00:03:41,080 --> 00:03:51,080
Right. We spent the last week or so talking again about the storage layer and then putting the buffer pool on top of it to actually manage memory as we get pages in and out of of disk.

75
00:03:51,080 --> 00:04:04,080
And so now we're continuing up the stack and are now going to talk about different parts of the system that can operate and execute and process those pages that we brought into our buffer pool that we were retrieving disk.

76
00:04:05,080 --> 00:04:07,080
And so we're sort of in this middle layer here in the access methods.

77
00:04:07,080 --> 00:04:14,080
And now we're going to start talking about how do we construct the execution engine that's going to be responsible for exiting these queries.

78
00:04:14,080 --> 00:04:19,080
And so the access method is going to be the mechanisms for actually accessing the data.

79
00:04:19,080 --> 00:04:25,080
And it can either be through an index or through the tables themselves and potentially other mechanisms.

80
00:04:25,080 --> 00:04:32,080
So to do that, we need to talk about what kind of data structures we would have at these honor, these sort of this part of the system.

81
00:04:33,079 --> 00:04:38,079
And so this class will be on hash tables, which is an unordered data structure.

82
00:04:38,079 --> 00:04:44,079
And then we'll spend all of next week talking about tree data structures, which will give you ordering data.

83
00:04:44,079 --> 00:04:46,079
We'll give you ordering on keys.

84
00:04:46,079 --> 00:04:53,079
Right. And so we're just slowly building up making our way to the top to actually produce results for our queries.

85
00:04:53,079 --> 00:04:59,079
So, I mean, it goes without saying I'm assuming everyone here is taking a data structure class or algorithms class.

86
00:04:59,079 --> 00:05:02,079
Data structures are going to use all throughout the system.

87
00:05:02,079 --> 00:05:08,079
And we've already covered in some ways and some parts of the system so far where we're going to use these things.

88
00:05:08,079 --> 00:05:18,079
But there'll be other parts that we need to have high performance, safe and correct data structures to represent state of the system or the data of the system.

89
00:05:18,079 --> 00:05:21,079
So we've already seen how we can use this for internal metadata.

90
00:05:21,079 --> 00:05:24,079
Right. We talked about the page directory or the page table.

91
00:05:24,079 --> 00:05:32,079
Right. That's more or less a hash table being used to map page IDs to some location on disk or some location in memory.

92
00:05:32,079 --> 00:05:36,079
We could use the data structures for the core storage of the tables themselves.

93
00:05:36,079 --> 00:05:44,079
Remember we talked about the index organized tables where the actual tuples themselves would be in the leaf nodes of the B plus tree.

94
00:05:44,079 --> 00:05:51,079
So you could have your tables actually just be represented directly in a data structure rather than unordered heat files.

95
00:05:51,079 --> 00:06:01,079
We could also use these data structures for query execution to generate a femoral or temporary collections of data that allows execute queries more efficiently.

96
00:06:01,079 --> 00:06:06,079
This is basically how we're going to implement hash joins very fast or how implement joins very quickly using hash joins.

97
00:06:06,079 --> 00:06:15,079
So we'll build a hash table on the fly, populate it with the data from the tables we're scanning, do the join and then throw the hash table away.

98
00:06:15,079 --> 00:06:19,079
Right. So just because we're building hash up it doesn't mean it's going to stick around for a long time.

99
00:06:19,079 --> 00:06:24,079
And then probably the one you're most familiar with is using these data structures for table indexes.

100
00:06:24,079 --> 00:06:35,079
Like when you call create index that's essentially going to create one of these data structures and populate it with the keys and map them to the tuples so you do faster lookups like a glossary in a textbook.

101
00:06:35,079 --> 00:06:45,079
Right. So again, we'll see we'll see these data structures being used throughout the rest of the semester in different scenarios that are covered in this list here.

102
00:06:45,079 --> 00:06:56,079
So now what do we care about when we design our data structures like what are the things we need to be cognizant of to make sure that we have an efficient database system that actually is also correct, which is very important.

103
00:06:56,079 --> 00:07:09,079
Right. So the first thing we got to worry about is how we're actually going to organize the data structure itself in either in memory or pages that will be a memory but backed by by disk and the buffer pool.

104
00:07:09,079 --> 00:07:15,079
And remember I said in the beginning we want to make design choices in how we implement our system.

105
00:07:15,079 --> 00:07:21,079
If we know it's going to be backed by pages on disk that we maximize the amount of sequential IO.

106
00:07:21,079 --> 00:07:31,079
So maybe we'll lay out the pages in such a way the data structure in such a way that we have long strides of data that we can read right through multiple pages instead of doing random IO.

107
00:07:32,079 --> 00:07:35,079
And then we'll talk about how do we actually make our data structures thread safe.

108
00:07:35,079 --> 00:07:42,079
And so for this class we won't really worry about it but we'll spend a whole lecture next week on Thursday or Wednesday next week.

109
00:07:42,079 --> 00:07:55,079
Talk about how do we make sure that the data structure is correct and sound if we have multiple worker threads or processes coming in and reading, writing or modifying the data structure at the same time.

110
00:07:56,079 --> 00:08:03,079
And this last one is going to be tricky because we're going to care sort of two kinds of correctness in our data structures.

111
00:08:03,079 --> 00:08:12,079
If you want to make them multi threaded there's obviously the physical correctness of making sure we don't have a pointer that goes nowhere or a page ID that doesn't exist.

112
00:08:12,079 --> 00:08:24,079
If we have one thread accessing a page another thread is updating it and that the accessing thread reads something that the guy wrote but it hasn't been, you know, it's not safely committed yet or it's not safe correctness.

113
00:08:25,079 --> 00:08:30,079
Then we may end up, you know, falling a pointer to nowhere and the system would crash so we have to avoid that.

114
00:08:31,079 --> 00:08:46,079
But then there's another kind of correctness that we'll get to after the midterm at sort of the logical level to make sure that if we make changes to our data structures that our own thread can see those changes or that it looks correct to it.

115
00:08:47,080 --> 00:08:55,080
Meaning like if my thread deletes a key from an index, if I then go back in that same thread and read, try to read that key in that index, I shouldn't still see it.

116
00:08:56,080 --> 00:09:02,080
The bits may still physically be there, right, because maybe we haven't run garbage collection and maybe there's a little flag that says this thing's been deleted.

117
00:09:03,080 --> 00:09:07,080
So physically it's still there but logically it's not. We need to make sure that we don't see things we shouldn't be seeing.

118
00:09:08,080 --> 00:09:24,080
Again, so we won't focus too much on currency in this week but we'll cover this in more detail next week and this will be a big issue also too when we talk about the critical at the logical level having transactions and making sure we provide asset guarantees.

119
00:09:25,080 --> 00:09:26,080
That'll be after the bitter.

120
00:09:27,080 --> 00:09:37,080
So today's class we're focusing on hash tables. Again, this is a low level building blocks that we can reuse throughout the rest of the system.

121
00:09:38,080 --> 00:09:45,080
And again, this shouldn't be news to anyone here. A hash table is just going to be an associated array that can map keys to values.

122
00:09:46,080 --> 00:09:50,080
You guys okay? Are you good? What's that?

123
00:09:51,080 --> 00:09:52,080
I'm just going to have a delay on fly.

124
00:09:53,080 --> 00:09:56,080
Oh yeah, a delay on fly. Okay, sorry. I think you started the fire. I was like, okay, that's even worse.

125
00:09:59,080 --> 00:10:00,080
Second time.

126
00:10:01,080 --> 00:10:03,080
Oh, today, wow. It was a fire element engaged. Yeah.

127
00:10:04,080 --> 00:10:06,080
Which I didn't cause. I called one last year, not this year.

128
00:10:07,080 --> 00:10:19,080
All right, so with the last DJ. All right, so the way the hash table is going to work is that there's going to be, it's going to be this mapping from keys to values and we're going to use a hash function that's going to allow us to

129
00:10:20,080 --> 00:10:35,080
actually compute some offset within an array. And then it's basically reducing down the, the, an arbitrary key to this integer domain that we can then jump to some location in our hash table to find out things that we're looking for.

130
00:10:36,080 --> 00:10:43,080
Right. And that the hash function has to take any possible key because again, take any column type you can define in your database system.

131
00:10:44,080 --> 00:10:51,080
Also, any internal metadata we'd have in the system itself, we need to about take that, you know, hash function needs to reduce that down to an integer.

132
00:10:52,080 --> 00:11:03,080
So in hash able to space complexity is going to be roughly big n or big o n because we're going to have to store a slot for for every possible key we want to have.

133
00:11:04,080 --> 00:11:16,080
Right. The time complexity is nice because on average we're going to get oh one book ups meaning we hash a key jump to some location in this hash table array and then ideally there's the thing that we're still there.

134
00:11:17,080 --> 00:11:24,080
Here just kill it. All right. So that's why two kills for the semester. That's not bad.

135
00:11:25,080 --> 00:11:35,080
All right. So again, no average is going to be one because it's going to be like, again, hash, take your key, hash some location and then you land the thing that exactly what you're looking for.

136
00:11:36,080 --> 00:11:47,080
Worst case will will be big o n because because what we'll have to deal with collisions. It may be the case that we hash our key land in some location and then the thing we're looking for is not there.

137
00:11:48,080 --> 00:12:01,080
And we've got to scan along on our hash table till we find the thing we're looking for and in maybe the case that all the slots in our hash table are full and we have to wrap around it's basically the one above the one where we land or the hash function, but we had to loop around to find it.

138
00:12:02,080 --> 00:12:08,080
And so the way you sort of handle this and we'll see as we go along is you size the hash table to be roughly two n the number of keys you expect.

139
00:12:09,080 --> 00:12:20,080
Now you may say, okay, Andy, how do you know how do you know what n is? Well, this this is what we'll get through that semester. Like the daily system is going to try to make a decision or try to predict how many keys you're actually going to have and size it accordingly.

140
00:12:21,080 --> 00:12:28,080
So oh one sounds great. And if you're going to take an algorithm class, this is the whole the grow you want this right you want to know when because it's constant time.

141
00:12:29,080 --> 00:12:48,080
But in actuality again in a real system the the constants actually matter a lot. So even though it's oh one you could have one hash function that maybe takes 10 milliseconds to compute another hash function takes one millisecond of you and obviously the one millisecond ones meet a lot faster if you think in large scale tables like billions of keys.

142
00:12:49,080 --> 00:12:58,080
So again, just because the algorithm of the complexity is is ideal on averages one we have to still care about the implementation and make sure we're as efficient as possible.

143
00:13:00,080 --> 00:13:12,080
So let's look at a sort of sort of the sort of toy example what a hash table looks like and we'll see all the problems that you can have with it and then we'll build up this look at more sophisticated schemes that actually used in real world database systems.

144
00:13:13,080 --> 00:13:24,080
So the easiest hash table to build is a static hash table where you just call Malak generate a giant array where you have one slot in your in your array for every key I could possibly have.

145
00:13:25,080 --> 00:13:33,080
And then to find an entry for a given key you just take the you mod the key by the number elements you have and you land some offset in the array.

146
00:13:34,080 --> 00:13:48,080
So here's my offsets and then any key shows up I know exactly where to go find it right and you don't start the keys in this array it's essentially just a pointer to some of the location that's going to have the key and the value together.

147
00:13:49,080 --> 00:14:04,080
Right. The reason why you need to store the original keys because since the hash may you could have collisions with the second like you need to check whether the key you're looking that you land on through your hash table is actually the key you're trying to find.

148
00:14:05,080 --> 00:14:13,080
And the value here could be a pointer to the two pull to get like a record ID or could actually be some some additional values for our purpose today we don't actually care.

149
00:14:14,080 --> 00:14:20,080
So what are some problems with this approach. Yes.

150
00:14:21,080 --> 00:14:25,080
What happens if you have the only key that's open and how do you have?

151
00:14:26,080 --> 00:14:28,080
What do you have what do you have?

152
00:14:29,080 --> 00:14:38,080
Well I'm assuming it's static but so what if you have n plus one keys how do you resize this thing right you have basically in this scenario you have to rehash everything so that that sucks.

153
00:14:39,080 --> 00:14:40,080
What's the other problems yes.

154
00:14:40,080 --> 00:14:43,080
Does it handle collisions what is collision.

155
00:14:44,080 --> 00:15:00,080
Yes that's correct yes you're two keys that have the same value they're going to land the same location in the in in in our array even though they're not the same but I'm assuming that you know that everyone has to be unique and you can't have collisions in this example doesn't handle that.

156
00:15:01,080 --> 00:15:03,080
There's one more problem.

157
00:15:03,080 --> 00:15:20,080
I'm assuming that the keys are unique right I can have key value equals one and key value equals to like same key but different values in my in my my sort of toy example here doesn't handle this.

158
00:15:21,080 --> 00:15:26,080
Right so this is unrealistic again for these three assumptions so the first one is like you have to know all the keys ahead of time.

159
00:15:26,080 --> 00:15:44,080
In some cases you do other cases you don't in the case of the buffer pool and then we talked about last class that if you assume that the you know the size of your buffer pool is fixed you're going to have a fixed number of frames in your buffer pool therefore you know the exact number of slots or you need in your hash table.

160
00:15:45,080 --> 00:15:52,080
But if I'm if I build a hash table index and I keep inserting tuples now my number keys is growing as I insert new tuples.

161
00:15:52,080 --> 00:16:02,080
Every key is unique in this scenario here again how do you need a way to handle keys that you know you can have duplicate keys we have different values got to handle that.

162
00:16:03,080 --> 00:16:12,080
And then the thing the he brought up is that we're assuming here we have what it's called a perfect hash function that guarantees no collisions which does not exist in the real world.

163
00:16:13,080 --> 00:16:20,080
Well it isn't a real world but it's basically toy invitations no database system can actually do this because again you need to know the key domain ahead of time.

164
00:16:20,080 --> 00:16:28,080
Right there's no magical hash function that guarantees for any given key you can generate a unique unique hash value.

165
00:16:29,080 --> 00:16:32,080
Right the way to actually implement implement one of those is through a hash table.

166
00:16:33,080 --> 00:16:42,080
You basically need a hash table for your hash table to do this which for some systems do do that but not for a perfect hash function.

167
00:16:43,080 --> 00:16:49,080
Right so we got to be smarter and we got to make sure that we deal with the environment that we're operating in with databases.

168
00:16:50,080 --> 00:16:56,080
All right so there's two decisions we have to make when we have people in a better hash table so someone says they basically have a hash table it's sort of two parts.

169
00:16:57,080 --> 00:17:06,079
There is the hash function itself again had a map a large key space down to a finite smaller domain right based on the number of slots I'm going to have in my array.

170
00:17:07,079 --> 00:17:15,079
And there will be this trade-off between how fast we want our hash function to be versus the how likely it is that two keys two distinct keys will collide.

171
00:17:16,079 --> 00:17:20,079
Right what's the hash is what's the fastest hash function I could I could build.

172
00:17:22,079 --> 00:17:23,079
What's that?

173
00:17:24,079 --> 00:17:29,079
So Danny you can go faster than that so he says for a given key you spit out the same key but if you have a string key and I got to make it an integer how do I do that?

174
00:17:30,079 --> 00:17:33,079
So you take the first bit.

175
00:17:34,079 --> 00:17:36,079
Yeah that would be pretty fast too.

176
00:17:38,079 --> 00:17:46,079
I was saying you just can turn one right that'll sit in the stack and read it will be super fast.

177
00:17:47,079 --> 00:17:52,079
Now it's the worst hash function in terms of collision because everything is going to map to one but it'll be fast.

178
00:17:53,079 --> 00:18:02,079
So it's this trade-off trying to figure out and you sort of think of the perfect hash function is the other other end the collision rate is zero but it's super slow because you have to do this extra look on.

179
00:18:03,079 --> 00:18:06,079
So you want something in the middle that's going to be fast and have a low collision rate.

180
00:18:07,079 --> 00:18:13,079
And then the hashing scheme is going to be the mechanism we're going to use to handle collisions after we don't our hashing.

181
00:18:14,079 --> 00:18:37,079
And the way the trade-off here is going to be again sort of the classic storage versus compute in computer science like I could I could allocate a two terabyte hash table and I'm pretty unlikely going to have I'm not likely going to have collisions for my key set is super small but I allocated this massive hash table or going to have a smaller one but I have a lot of collisions and therefore I had to spend more compute to handle those collisions.

182
00:18:37,079 --> 00:18:46,079
So again it's trying to figure out how to do the right get the right trade-off between not over allocating but then also not waiting wasting a lot of instructions to deal with with collisions.

183
00:18:47,079 --> 00:18:56,079
All right so today's talk we'll talk a little bit about hash functions just to sort of show you what the state of there it is. I'm not going to say how they work just tell you that they exist.

184
00:18:56,079 --> 00:19:00,079
Again we're data people we're not in the business writing hash functions will let other people do that for us.

185
00:19:01,079 --> 00:19:13,079
And then we'll talk about the sort of the classic static hashing schemes where you know the number of keys ahead of time and then we'll talk about dynamic hashing schemes where you the hash table can actually grow and shrink based on the number of keys.

186
00:19:13,079 --> 00:19:25,079
All right so again we're not in the business of writing hash functions other people that are smarter than us in this space have done it for us so we're just going to rely on them.

187
00:19:25,079 --> 00:19:34,079
Again the basic idea of a hash function is that we have some input key any arbitrary number of bytes of any type and we need to return a integer that represents that key.

188
00:19:34,079 --> 00:19:42,079
Typically 64 bits there are 120 bit hash functions but I don't think the database is used those there are 32 bit hash functions as well.

189
00:19:43,079 --> 00:19:58,079
So it's a one return to integer so in this scheme or in this in a database system we don't care about any sort of protection privacy mechanisms for a hash function meaning we're not going to use anything that has cryptographic guarantees.

190
00:19:59,079 --> 00:20:05,079
So we're not using shot 256 or whatever like we don't we don't care about those things because we're running on the inside of the system.

191
00:20:05,079 --> 00:20:15,079
It's it's we're not worried about leaking anything while we can build a hash table to do a joint because no one on the outside of the system can see that data structure.

192
00:20:15,079 --> 00:20:21,079
So we don't care about any of those things and as a result we can actually run a lot faster.

193
00:20:22,079 --> 00:20:27,079
Right shot 256 will be really slow versus something like like you know murmur hash or XX hash.

194
00:20:28,079 --> 00:20:32,079
As I already said before we want something that's fast and it has a low collision rate.

195
00:20:32,079 --> 00:20:40,079
So this is just a quick quick overview of what other hash or cash function systems are using.

196
00:20:41,079 --> 00:20:52,079
Some senses like postgres roll the own hash function but a lot of the more modern systems they're going to use something off the shelf like XX hash or murmur hash or the spooky hash.

197
00:20:52,079 --> 00:21:01,079
So basically the main takeaway from this is that the state of art one is XX hash from Facebook with the third version XX hash three.

198
00:21:01,079 --> 00:21:08,079
This one is shown to have the some of the fastest performance and also the lowest collision rate.

199
00:21:09,079 --> 00:21:20,079
There are some systems that use CRC 32 or 64 for passion link integers because they actually see few instructions in x86 to do that in a few number of cycles.

200
00:21:20,079 --> 00:21:26,079
So that's there's some systems that do that but in terms of like random strings you typically want to use this.

201
00:21:26,079 --> 00:21:30,079
So remember hashes is because remember hashes written by this like random dude on the internet.

202
00:21:31,079 --> 00:21:41,079
He had a good fast general purpose hash function. Google took that and made city hash by forking it and then they have a newer version called farm hash that has even better collision rates.

203
00:21:41,079 --> 00:21:47,079
There's a bunch of different sort of hash functions out there but XX hash three is what you want to use.

204
00:21:47,079 --> 00:21:59,079
And so there's a bunch of these repositories on GitHub or people have written basically torture chambers or benchmarks to run all possible hash functions that are out there and see what the collision rate is, see what the performance is.

205
00:22:00,079 --> 00:22:11,079
So this is this M hasher SM hasher. There's another one written by the member hash guy and there's another one that's a fork of this that's only for the original of cryptography stuff.

206
00:22:11,079 --> 00:22:18,079
But for this repository here they had this like nice summary here that says these are the ones that work the best and have good good collision rates.

207
00:22:18,079 --> 00:22:23,079
And then the the top one here is XX hash three the Facebook one.

208
00:22:24,079 --> 00:22:32,079
Right. So again we don't care. It's a hash function keys in integer out what is used whatever they have.

209
00:22:32,079 --> 00:22:38,079
And then there's the full list of all the hash functions. Some are tailored to arm some until turn to X86 or whatever.

210
00:22:38,079 --> 00:22:46,079
Like you can you can get you know more low level details based on the environment but XX hash three is going to be a good default choice.

211
00:22:47,079 --> 00:22:50,079
Okay. So now.

212
00:22:51,079 --> 00:22:58,079
So we're you know see where you're running XX hash three we want to talk about what the hash table is going to look like and how to be handled collisions.

213
00:22:58,079 --> 00:23:07,079
So for this lecture I'm going to focus on the public the two most common ones but number one is actually going to be the most common one of all the systems linear pro passion.

214
00:23:07,079 --> 00:23:17,079
It's the simplest and it seems kind of brain dead in some ways but because it's so simple it is actually the fastest.

215
00:23:17,079 --> 00:23:22,079
Right. And then cook or hashing is a variant of this that basically does multiple has functions.

216
00:23:22,079 --> 00:23:27,079
So there's a bunch of other techniques Robinhood hashing hops got hashing Swiss tables from Google.

217
00:23:27,079 --> 00:23:31,079
We will cover that in this semester but if you take me a mass class we will cover those things.

218
00:23:32,079 --> 00:23:44,079
And I would say that the current research basically shows that the linear probing stuff and the Swiss tables are the fastest ones all these sort of extra fancy versions are.

219
00:23:45,079 --> 00:24:01,079
They're trying to be they try to be more performance because they avoid having to spend longer time looking for for keys by moving things around when we when you insert but all that work moving things around is a performance penalty and you're better off just kind of doing the the naive thing in your hashing.

220
00:24:01,079 --> 00:24:04,079
Yes.

221
00:24:04,079 --> 00:24:09,079
There's a reason why we're not so much chain hashing because that's dynamic that'll be next.

222
00:24:09,079 --> 00:24:10,079
Yeah.

223
00:24:10,079 --> 00:24:17,079
Because chain hashings can grow this is fixed size that will cover that later this month or later this class.

224
00:24:17,079 --> 00:24:21,079
Right. These are all static hashing schemes.

225
00:24:21,079 --> 00:24:28,079
There's variations of linear probing the new quadrat quadratic probing we get another that final let's keep a simple.

226
00:24:29,079 --> 00:24:42,079
All right. So linear prob hashing is is is really simple to giant array of of slots and we're going to hash into it.

227
00:24:42,079 --> 00:24:57,079
You know we want to answer we hash into it if the slot is free we insert the thing we're looking for if the slot is not free we just look at the next slot and and and certain there if we can or we keep looking until we have a free slot potentially wrapping around.

228
00:24:57,079 --> 00:25:15,079
Until we find a free location and then if we you know loop background realize we're at the slot where we started that we know the hash tables full and we have a board and you know board it double the size and rehash everything right it's a simple way to grow it.

229
00:25:15,079 --> 00:25:21,079
So the state implementation for this or one is here in the station is this avisal thing from from Google.

230
00:25:21,079 --> 00:25:33,079
And they have the flash hash flash hash map type for data structure and they have pretty good documentation describe how actually how it works and some of the organizations they do will cover.

231
00:25:33,079 --> 00:25:50,079
So this is sometimes called open addressing open addressing hashing because the ideas that it's no guarantee that for a given key it's going to always be in the same address or same location in the slot depending on what what got inserted before it and make it make it moved around.

232
00:25:50,079 --> 00:25:56,079
Right. If you get a dictionary and Python this is essentially what you're getting as well.

233
00:25:56,079 --> 00:26:25,079
Right. So let's see how it works. So say we want to insert key a right so we hash it mod it by the number of slots we have and then we hit land on this location here so we insert our key along with the value together right again the reason why we need the key because if we go to a look up again for looking for a we need you know what has to the same location but now we got to do an equality check to see whether the key that we're looking for is the key in a given slot.

234
00:26:25,079 --> 00:26:32,079
Same thing so we want to hash be same thing hashed here mod by the number of slots we end up here and we sort of the top.

235
00:26:32,079 --> 00:26:47,079
So now we want to start insert C so we hash see it lands at the same location where a is but with that slot is occupied so we can't insert it there so we just follow down to the next slot and insert our key there.

236
00:26:47,079 --> 00:27:06,079
Right. Same thing with D. D. Once ago where C is we can't because that slot is occupied so just move down the next one and start it there right and we just keep going down for all the other keys we want to store right and in this case here if it's say if F1 and then this space is occupied.

237
00:27:06,079 --> 00:27:13,079
Yeah F kind of wrapped around start at the beginning and the start the top right think of it as a giant circular buffer.

238
00:27:13,079 --> 00:27:18,079
Pretty simple right.

239
00:27:18,079 --> 00:27:21,079
What are some potential problems with this.

240
00:27:21,079 --> 00:27:26,079
He says, the least stocks. He loses the whole chain. What do you mean?

241
00:27:26,079 --> 00:27:33,079
Like if you delete C and then you look at something that got like accidentally pushed down like.

242
00:27:33,079 --> 00:27:39,079
And you know like we've heard somewhere see it so you don't like things at the same.

243
00:27:39,079 --> 00:27:50,079
Right so he says. I don't think so slides ahead of time but I'm sorry what happens if you delete C right so delete C we hash it we land where a is.

244
00:27:50,079 --> 00:28:04,079
Right now we do that quality check to see this is a equals C no so we know that's not the key we're looking for and then we keep going until we find empty slot or the key we're looking for so in this case here after jumping down we find C and now we need to delete it.

245
00:28:04,079 --> 00:28:10,079
But now we have an empty space as he said so if I try to go do a look up on something like D.

246
00:28:10,079 --> 00:28:24,079
D is going to hash to this empty spot and it's going to say oh well nothing's here right but it really is the you know it's the next slot down but because the way the protocol works the scheme works if I see an empty slot then I know I'm done.

247
00:28:24,079 --> 00:28:28,079
Right so let's only handle this.

248
00:28:28,079 --> 00:28:31,079
Gravestone.

249
00:28:31,079 --> 00:28:37,079
Gravestone that's one approach yes we'll get we'll get to there which that is the correct answer.

250
00:28:37,079 --> 00:28:42,079
So you could do this you could just like rehash or move it right off right.

251
00:28:42,079 --> 00:28:49,079
Is that a good idea or bad idea well clear it's a bad idea because that said no one does this right but why is it a bad idea.

252
00:28:49,079 --> 00:29:00,079
It's a move everything again thank you so I have a billion keys I can go rehash everyone that'd be terrible right so that so it's super expensive and no one does this.

253
00:29:00,079 --> 00:29:23,079
Let's give you this right so yeah this is not the sense you don't want to do this the correct solution is what he was saying is what it's called a tombstone and the idea here is that I delete C but instead again set it setting it as empty I'm going to put a little marker here to say this slot there was a key here and now it's been deleted.

254
00:29:23,079 --> 00:29:41,079
So that way if anybody comes along like doing a look up and D it sees the tombstone says well it's not empty something was here but there's nothing here that I'm looking for so let me let me then look down and keep scanning along until I find nothing I'm looking for right.

255
00:29:41,079 --> 00:30:07,079
So you can reuse these these you can reuse the slot with the market of tombstone for new keys because you just insert over top of it and that doesn't break the flow or break break anything else in the hash table right now maybe the case you want to peer out of anyone garbage question because you can start accumulating much of these tombstones and it's just wasted space if you're not using them but for purposes we you know we can ignore that.

256
00:30:07,079 --> 00:30:30,079
So I want to put say G G can go right here and that's fine now I'm not going to discuss this too much details but like there is a challenge though how you actually want to represent these tombstones and also represent something that's that that it's empty and potentially also represent that I have a null key which you can do in a database system right.

257
00:30:30,079 --> 00:30:31,079
So we can do the trick we talked about before with slot pages where we could have a bit map in front of at the top of every header of every page in our hash table like I'm not showing the division here between pages but think of like for simplicity every page is to these slots so in the header that page I can keep track of like okay here's the slots that are empty here's the slots that are null or here's the slots that are that are marked with the tombstone right so I need some additional metadata to keep track of these things and you obviously don't want to do it on the

258
00:31:00,079 --> 00:31:29,079
perky basis because that that can mess up with the alignment of things and waste space. So the other thing we got to deal with now is is not unique keys right so there's two approaches to do this one is that instead of storing the value in our our giant hash array or it or array along with the keys instead the value would just be a pointer like I said I'm going to do that.

259
00:31:30,079 --> 00:31:59,079
So I'm going to do a page ID to some other location that will store my list of keys or sorry yeah list of values right so for the key xyz there's a pointer to some some basically a link list that houses all the possible values and then for the other key the same thing right what's nice about this is as I insert new keys or it's insert good the keys I'm ever again I'm not really changing the main hash table I'm sort of pending to this sort of link list.

260
00:32:00,079 --> 00:32:29,079
It's like the chain hash table that I talked about before or he asked about before we'll get in a second. The more common approach is to just store the time keys together right and again this doesn't break the open addressing of the linear profession scheme is that I I always hash you know hash on the key I landed some location and I find a you know I just find a free slot and I started thinking looking for this does make a little bit more tricky when you would do look up like give me all the give me all the keys of xyz key.

261
00:32:30,079 --> 00:32:59,079
So I'm going to do a value pairs because now I got I know like I got to keep I got to keep scanning to I find an empty location empty slot to know that I'm not going to see xyz ever again whereas in the first the first scenario I find xyz in my hash table then I and I landed the you know I follow the pointer to the list of values and I know that's all the possible values I could have for that given key right but for simplicity reasons instead of having maintained the sort of separate link list for non-unique keys and the non-unique key.

262
00:32:59,079 --> 00:33:13,079
And then the non the in the in line version for for unique keys most systems to store the use the redundant key approach because you don't you don't have to you don't have to have multiple invitations.

263
00:33:13,079 --> 00:33:24,079
Yes. How would you differentiate between and how would you differentiate between.

264
00:33:24,079 --> 00:33:39,079
So it's questions how how would you differentiate an update of a value versus a insert of a value.

265
00:33:39,079 --> 00:33:45,079
Yeah, for hash tables you really don't do updates or be a delete followed by an insert right.

266
00:33:45,079 --> 00:34:01,079
And of course now the tricky thing is like if I want to delete key xyz with value to like I can't look sorry if I just say delete if I don't want to delete one of these like I have to know what the value is and to make sure I only do that one.

267
00:34:01,079 --> 00:34:06,079
I you know I'm only all xyz which may not be what I want yes.

268
00:34:06,079 --> 00:34:13,079
This way like if you have multiple thing like sorry I keep with multiple values you just ask the entire like people.

269
00:34:13,079 --> 00:34:20,079
The question if you have so if you have a key with multiple values you just hashed entire table what do you mean by that.

270
00:34:20,079 --> 00:34:30,079
Because at this point like you're you're searching for specific key and value so consider both of them as if you have it at a set.

271
00:34:30,079 --> 00:34:50,079
Yes so like yes so David is like if I'm looking if I'm looking for exact match in that case like I don't need to hash table because I have right but like if I'm trying to remove it from the data structure like it's exact like key value pair right then you basically like how does this you.

272
00:34:50,079 --> 00:35:04,079
You you need to find that exact pair you need to have it at the time so it's just maintaining the data structure.

273
00:35:04,079 --> 00:35:16,079
Question what why do this so like if I'm doing a join the relationship between the two joint tables is that one side might not be unique right.

274
00:35:16,079 --> 00:35:31,079
So I need to have this and so I want to get all of the like you have basically an iterator says give me all the values or key equals xyz and it starts spinning this thing out as I'm as I'm traversing the hash table because I'm doing the join.

275
00:35:31,079 --> 00:35:41,079
So question is would you be hash when it's completely full or 80% full so the different systems have different.

276
00:35:41,079 --> 00:35:53,079
There's like a threshold and say if I go above this I know I'm going to overflow or run out of space so we go ahead and trigger a rehash.

277
00:35:53,079 --> 00:36:16,079
So at this point yes like the you get closer to that worst case scenario where like if it gets it starts to get full so rather than waiting to it's like 100% full maybe go to 80% because that it's better off to pay the penalty to resize the hash table which is doubling you to resize hash table double size of it go through all your keys and rehash them put them into the new hash table and then throw away the old one that's not that's expensive.

278
00:36:16,079 --> 00:36:28,079
So there's a trade off like okay if I'm at 80% full I rather pay that penalty to double the size rather than all the additional operations that I need to do spend a long time searching through.

279
00:36:28,079 --> 00:36:31,079
There's no there's no one answer.

280
00:36:31,079 --> 00:36:45,079
That's why there's usually a tunable threshold whether or not they expose that to you as like a user of the system it depends on the implementation but there usually a threshold say when do you want to go ahead and resize.

281
00:36:45,079 --> 00:37:05,079
So some other conversations we can do that one is you could have different hash table implementations that have these different mechanisms are like you know the decisions about when to split how to store things and whatnot based on the data type you're storing.

282
00:37:05,079 --> 00:37:31,079
So an obvious thing would be like if I have I want to be on hash tables that support string keys if my strings are very small like 64 bits or 64 bits or 64 bytes or 64 bits or bytes less the magnets are that in line my hash table but it's a really large string I don't want to store that in my hash table maybe I just want to have a pointer to the actual string itself so now I could have a 64 bit pointer.

283
00:37:31,079 --> 00:37:45,079
But now it's going to be expensive to do that look up to see whether I have a match or maybe I actually want to store the hash of that string as part of the key in my hash table so void having to do that that look up right.

284
00:37:45,079 --> 00:38:02,079
We talked about storing the metadata like it's something a tombstone or something a no value or a or an in a slot you throw that in the page header because now you have a bunch of packed bits you actually throw that in an entire hash table itself right.

285
00:38:02,079 --> 00:38:18,079
So the Google hash map does this where they have a separate hash table just for the metadata that's much smaller and compact you do a look up on that to tell you what is the thing you're about to go look up in the real hash table is that thing you know no or or or empty or not.

286
00:38:19,079 --> 00:38:26,079
And then this one is interesting this is from this one comes from click house the OLAP system that came out of the index in Russia.

287
00:38:26,079 --> 00:38:41,079
So they talk about how they want to be wrote since it's so expensive to allocate the memory for a hash table you don't want to just you know allocate a bunch of memory use it once and then throw it away which you actually want to just reuse that memory over and over again but you need a fast way to clear it out.

288
00:38:41,079 --> 00:39:10,079
So instead of going through and marking all the slots as deleted you just maintain a virgin counter a virgin ID and whenever you say I want to delete the contents of this table you just increment that that virgin counter on the table and then now any look up you do inside of a slot inside that table if the virgin IDs don't match then you if you're this the slot version number is less than the table version number you know it's been deleted and you ignore everything in there that clears it out and then.

289
00:39:10,079 --> 00:39:13,079
And you can implement the version ID.

290
00:39:13,079 --> 00:39:27,079
So the bunch of different tricks you can do in different scenarios to make these things one more efficiently and the very systems do different things click house in my opinion there's a that link there with taking the blog article they claim they have 30 different implementations of hash tables.

291
00:39:27,079 --> 00:39:48,079
A lot of it's templatized based in C++ based on the data type and you do a bunch of compiler tricks to remove code you don't need if you know like something cannot be known or is this string of certain size they probably in my opinion of all the other systems I looked at they're probably most sophisticated one that I have the most sophisticated hash tables.

292
00:39:48,079 --> 00:40:17,079
So one variant of linear hash linear prep hashing is a technique called kuku hashing and the idea here is that instead of having a single patch function to do a look up to one location in my in my hash table what if I had multiple hash functions and I hash up multiple locations and I find whatever one has a free look free slot and I use that one instead of having to scan through now until I find a free slot.

293
00:40:17,079 --> 00:40:27,079
For my key. So this is going to guarantee that all my look ups and deletions will be oh one because.

294
00:40:27,079 --> 00:40:30,079
No matter how many hash functions I have.

295
00:40:30,079 --> 00:40:39,079
You know I don't have to scan through I'm going to land some location in my in my hash map or my hash table that has the data that I'm looking for or doesn't exist.

296
00:40:39,079 --> 00:40:45,079
Insert to be more expensive because we'll see in a second you may start moving things around and reorganizing stuff.

297
00:40:45,079 --> 00:40:59,079
So there's only one system I know that does kuku hashing that the public talks about it and that's this O lap accelerator from IBM called blue BLU and in their paper they talk about how they make heavy use of kuku hashing.

298
00:40:59,079 --> 00:41:06,079
And as far as you know the best open source invitation of a kuku hash table is actually from Dave Anderson from from the CEO.

299
00:41:06,079 --> 00:41:10,079
I think Google you said Dave claims Google uses a lot of it.

300
00:41:10,079 --> 00:41:15,079
So the name has to do with the kuku has to do with like awesome.

301
00:41:15,079 --> 00:41:17,079
Okay let's head Google.

302
00:41:17,079 --> 00:41:20,079
Yeah.

303
00:41:20,079 --> 00:41:23,079
So where was it?

304
00:41:23,079 --> 00:41:30,079
So the name kuku has to do with this bird where they lay their eggs they can lay their eggs on another bird's nest.

305
00:41:30,079 --> 00:41:38,079
And so the idea is again my key may end up make end up stealing somebody else's slot in my hash table if I try to go there and they're using it.

306
00:41:38,079 --> 00:41:45,079
So let's see examples have say we have a game we have a same same hash table but now when it and tell me we do an operation when I have two hash functions.

307
00:41:45,079 --> 00:41:57,079
So it's going to be the same hash function implementation that we talked about for like XX hash murmur hash spooky hash it doesn't matter but we'll just give it a different seed to the hash function that guarantees for giving key.

308
00:41:57,079 --> 00:42:04,079
It doesn't guarantee but it's very likely that for a given key it's going to it's going to produce two different hash values.

309
00:42:04,079 --> 00:42:09,079
So I hash a and I have these two locations here and send the very beginning my hash tables empty.

310
00:42:09,079 --> 00:42:17,079
So I can either flip a coin or pick the first one it doesn't matter and so I'll slide that for inserting a goes in this the first slot here.

311
00:42:17,079 --> 00:42:20,079
Now I want to put I want to put be in.

312
00:42:20,079 --> 00:42:32,079
And so the first hash function hash is to where a is the second hash function goes to an empty slot so because the other one is occupied I'm going to always choose the empty one and I'll put be at the top like that.

313
00:42:32,079 --> 00:42:43,079
Now where things get tricky is that we have multiple or two hash functions or multiple hash functions hashed to two locations that both have better both are being occupied.

314
00:42:43,079 --> 00:42:50,079
So in this case here for whatever you know whatever protocol whatever scheme you want to use say we can flip a coin we decide we want to a big B.

315
00:42:50,079 --> 00:42:59,079
So we'll go ahead and bash B on the head take its location put C in there but now we got it now we got to put B back in.

316
00:42:59,079 --> 00:43:10,079
So because B landed on this using the second hash function after we after we take it out and put it back in we need to use the first hash function.

317
00:43:10,079 --> 00:43:21,079
But then that takes us to the location where a is located so B is allowed to steal from a so B goes there a comes out we hash a with the second hash function and then we land to another location.

318
00:43:21,079 --> 00:43:39,079
And again just like before in linear per passing you need to keep track of if you're stuck in a loop right so you just got to keep track is this is the key I'm putting in they same key I try to first put it in the very beginning and I just loop back around and I'm stuck in an infinite loop and therefore I need a board double the size of the hash table and rehash everything.

319
00:43:40,079 --> 00:43:53,079
So now when I want to do look up on B right I take B hash it twice and I get two different locations and now you might check to see is the key stored in this slot the key I'm looking for if yes and I had the thing I'm looking for.

320
00:43:53,079 --> 00:44:05,079
Again now and I don't need to do that linear probe scanning of looking for an empty slot or key I'm looking for because I'm guaranteed either the keys going to be there after hashing or does not exist in the table.

321
00:44:09,079 --> 00:44:16,079
This is a good idea bad idea.

322
00:44:20,079 --> 00:44:29,079
It seems like there be more collisions.

323
00:44:29,079 --> 00:44:34,079
Well no right because if.

324
00:44:34,079 --> 00:44:48,079
But so like there's trade-offs right so like yes could be more collisions but like these in linear per passing you're guaranteed always put something in there right it may be the worst slot maybe the slot right above the one you try to go into and you'll

325
00:44:48,079 --> 00:44:55,079
get a lot of time to go back around but at least there's a free slot you'll get it.

326
00:44:55,079 --> 00:44:58,079
Yes.

327
00:44:58,079 --> 00:45:02,079
This does random I.O.

328
00:45:02,079 --> 00:45:07,079
Yeah absolutely right so this is doing random I.O.s because I'm jumping around well the hash table is doing essentially random I.O.s but once I land somewhere doing a random look up then it's a

329
00:45:07,079 --> 00:45:14,079
good idea. This is always random right yes.

330
00:45:14,079 --> 00:45:20,079
I think your friend she said to please the repot.

331
00:45:20,079 --> 00:45:23,079
Oh.

332
00:45:23,079 --> 00:45:30,079
Yeah.

333
00:45:30,079 --> 00:45:33,079
What?

334
00:45:33,079 --> 00:45:36,079
Yeah I mean that's this is what I'm saying.

335
00:45:36,079 --> 00:45:41,079
Do you have a girlfriend?

336
00:45:41,079 --> 00:45:48,079
This is the this is the conversation I've been explaining to her is the most I mean.

337
00:45:48,079 --> 00:45:51,079
That's the life.

338
00:45:51,079 --> 00:45:54,079
It's impressive.

339
00:45:54,079 --> 00:45:57,079
Yeah.

340
00:45:57,079 --> 00:46:03,079
Okay I mean you're a good DJ and you do database and so it makes sense.

341
00:46:03,079 --> 00:46:06,079
Yeah.

342
00:46:06,079 --> 00:46:11,079
Congrats.

343
00:46:11,079 --> 00:46:20,079
All right so your question was.

344
00:46:20,079 --> 00:46:27,079
Data this is yeah.

345
00:46:27,079 --> 00:46:33,079
Yeah so this question is is it possible to paralyze the access to different locations so.

346
00:46:33,079 --> 00:46:36,079
Yes you could do that.

347
00:46:36,079 --> 00:46:39,079
Like there's two different ways of paralyze what we'll mention get to that.

348
00:46:39,079 --> 00:46:47,079
Yeah like you have multiple threads or single threads but do vectorized instructions, simd instructions like.

349
00:46:47,079 --> 00:46:55,079
And for simd this won't you could do this but it requires you moving data around a bit much because you have to make sure things are aligned.

350
00:46:55,079 --> 00:47:00,079
But so you may introduce it with a single thread with vectorized instructions.

351
00:47:00,079 --> 00:47:02,079
I know there's techniques exist.

352
00:47:02,079 --> 00:47:05,079
I don't know about kuku hashing though.

353
00:47:05,079 --> 00:47:11,079
But for make this multi threaded it'd be so much work or just too much work to.

354
00:47:11,079 --> 00:47:17,079
Tell to threads okay we're looking this key you have to this way I'll have to this way and then.

355
00:47:17,079 --> 00:47:23,079
To then synchronize on who produces back result that is just not worth it.

356
00:47:23,079 --> 00:47:30,079
Yes.

357
00:47:30,079 --> 00:47:37,079
Yes.

358
00:47:37,079 --> 00:47:46,079
So your question is do you have the guarantee that the hash functions always go differently.

359
00:47:46,079 --> 00:47:53,079
So for example if a is all like the output of the hatches.

360
00:47:53,079 --> 00:47:58,079
Yes.

361
00:47:58,079 --> 00:48:03,079
So do you have to guarantee your hash function can't do that.

362
00:48:03,079 --> 00:48:15,079
You can't right that's why again that's why I'm saying like you want to choose a hash function that has a low collision rate so the like you can't guarantee that won't happen but the likelihood that that that will happen is low.

363
00:48:15,079 --> 00:48:21,079
The only thing you can get is a perfect hash function.

364
00:48:21,079 --> 00:48:28,079
This question does it fall to the leaner program after that what do you mean.

365
00:48:28,079 --> 00:48:29,079
Yes.

366
00:48:29,079 --> 00:48:30,079
Yes.

367
00:48:30,079 --> 00:48:37,079
Yes.

368
00:48:37,079 --> 00:48:50,079
If I run out of locations for this either because all the slots are full or I get a wrap around when I try to do this the cook with the thing I'm taking right you double the size of it.

369
00:48:50,079 --> 00:48:51,079
Yes.

370
00:48:51,079 --> 00:48:57,079
So does it increase the cost of getting like the general cases of crazy because we have a check multiple slots.

371
00:48:57,079 --> 00:49:04,079
Or is there like a defined order specifically where like to check this one and that only if there was a collision with the component.

372
00:49:04,079 --> 00:49:15,079
So the question is is there a defined order such that like you can maybe just always check the first hash like I'm showing two lines coming out of it but in a in a assume it's not parallel it is.

373
00:49:15,079 --> 00:49:16,079
Excuse me.

374
00:49:16,079 --> 00:49:18,079
Like could I.

375
00:49:18,079 --> 00:49:21,079
Is there some protocol say check this.

376
00:49:21,079 --> 00:49:26,079
And then only fetch the page for the second one if I know it's not going to be there.

377
00:49:26,079 --> 00:49:29,079
I mean you can do much of things you can pre-fetch the second page.

378
00:49:29,079 --> 00:49:30,079
Right.

379
00:49:30,079 --> 00:49:33,079
And then the thing is actually cheap it's the look up is expensive right.

380
00:49:33,079 --> 00:49:40,079
So maybe I I could choose one that I have two page IDs I went into a look up on.

381
00:49:40,079 --> 00:49:45,079
So if I have a way to go peak with one actually exists first maybe go check that one pre-fetched the other one.

382
00:49:45,079 --> 00:49:47,079
Again it depends on the implementation.

383
00:49:47,079 --> 00:49:53,079
Maybe it's like I think it's like you always try to get catch one first and then only that fails.

384
00:49:53,079 --> 00:49:57,079
And then you always get to look at that one first.

385
00:49:57,079 --> 00:50:02,079
But depending on what got inserted and how things are moving around like.

386
00:50:02,079 --> 00:50:03,079
Right.

387
00:50:03,079 --> 00:50:07,079
But the fact of coming up sort of so many.

388
00:50:07,079 --> 00:50:12,079
Is there another woman.

389
00:50:12,079 --> 00:50:14,079
Is it going to bathroom?

390
00:50:14,079 --> 00:50:15,079
What's he doing?

391
00:50:15,079 --> 00:50:18,079
All right.

392
00:50:18,079 --> 00:50:23,079
Anyway, so the fact of coming up with different ways to do this that's sure how complicated it is where is like Lena

393
00:50:23,079 --> 00:50:26,079
prepashing you just kind of rip through it.

394
00:50:26,079 --> 00:50:27,079
All right.

395
00:50:27,079 --> 00:50:28,079
Quick question.

396
00:50:28,079 --> 00:50:31,079
I do the general production.

397
00:50:31,079 --> 00:50:33,079
It's always like through how many.

398
00:50:33,079 --> 00:50:34,079
It's questions in a general system.

399
00:50:34,079 --> 00:50:39,079
What is the default actually I don't know like I don't we can go look up on Dave's code and the default it might be three.

400
00:50:39,079 --> 00:50:40,079
I have no idea.

401
00:50:40,079 --> 00:50:41,079
Yeah.

402
00:50:41,079 --> 00:50:43,079
All right.

403
00:50:43,079 --> 00:50:50,079
I want to get through the chain hashing and hashing and external hashing because you know we'll need this for one of the projects.

404
00:50:50,079 --> 00:50:55,079
So again, all of these protocols that showed so far these are all static hashing schemes.

405
00:50:55,079 --> 00:51:03,079
Again, if we run out of space or we we loop back around then we we need to double the size of the hash table and repopulate it in this expensive.

406
00:51:03,079 --> 00:51:10,079
So we want to talk about different techniques to incrementally resize the hash table without having to rebuild the entire thing.

407
00:51:10,079 --> 00:51:16,079
So the most common one is going to be chain hashing and again this is what most people think of when you think about hash table sometimes.

408
00:51:16,079 --> 00:51:24,079
And then but they will look at two more advanced techniques that actually are used in real systems.

409
00:51:24,079 --> 00:51:32,079
So chain hashing the basic idea is that instead of having this giant array of all the slots where we actually insert keys.

410
00:51:32,079 --> 00:51:47,079
Our array is just going to be pointers to essentially link lists or chains or buckets where all the keys that map to that slot in our hash table will be found in that in that link list.

411
00:51:47,079 --> 00:51:48,079
Right.

412
00:51:48,079 --> 00:51:52,079
If you allocate a hash map in Java, this is essentially what you get.

413
00:51:52,079 --> 00:51:59,079
So the link list part can essentially grow infinitely because again, in worst case scenario, all my keys hash are the same slot.

414
00:51:59,079 --> 00:52:05,079
I'm just depending to this giant list and I'm falling down or basically end up with a sequential scan.

415
00:52:05,079 --> 00:52:12,079
But again, ideally if I have a hash function that's good, I won't I'll have a good distribution of keys.

416
00:52:12,079 --> 00:52:21,079
The way to think about this is that we're essentially partitioning our giant hash table we have before into to smaller hash tables themselves or smaller tables.

417
00:52:21,079 --> 00:52:24,079
We can get unique keys to the same tricks we did before.

418
00:52:24,079 --> 00:52:27,079
Right. Just keep appending the redundant keys to this giant list.

419
00:52:27,079 --> 00:52:34,079
Right. We can still use tombstones, but oftentimes compaction is just faster in this case.

420
00:52:34,079 --> 00:52:37,079
All right. So now again, we have here we have our bucket pointers.

421
00:52:37,079 --> 00:52:40,079
And this is where hash functions are going to hash into.

422
00:52:40,079 --> 00:52:44,079
And then these are just be pointers to the different buckets that exist.

423
00:52:44,079 --> 00:52:49,079
Right. So if we want to put a in, we hash it mod in by the number of bucket pointers we have.

424
00:52:49,079 --> 00:52:54,079
And then we land on that bucket. We find the first free slot and we just insert it.

425
00:52:54,079 --> 00:52:59,079
All right. Say thing we're going to put b, b goes the top here just just as before.

426
00:52:59,079 --> 00:53:05,079
And then now in case of c, c, hash it to the same bucket where a is located.

427
00:53:05,079 --> 00:53:12,079
We just scan through squentially till we find the first free slot.

428
00:53:12,079 --> 00:53:15,079
You got to call.

429
00:53:15,079 --> 00:53:19,079
All right. You put d in.

430
00:53:19,079 --> 00:53:23,079
D goes where a is. It's scans through all the slots empty.

431
00:53:23,079 --> 00:53:26,079
You can put something in the page editor and say I have no more free slots.

432
00:53:26,079 --> 00:53:29,079
Therefore, always expand me when you get to me. It doesn't matter.

433
00:53:29,079 --> 00:53:35,079
And then basically the this page here would then point to another page where you can find d.

434
00:53:35,079 --> 00:53:39,079
And then we want to put e and follow through to find find e here.

435
00:53:39,079 --> 00:53:43,079
All right. And f can go here.

436
00:53:43,079 --> 00:53:54,079
So again, the nice thing about this is that I can grow the key list within a bucket without affecting other parts of the table.

437
00:53:54,079 --> 00:54:01,079
But you can have a sort of like two level, two level hash tables were like this is a hash table that takes you into these buckets.

438
00:54:01,079 --> 00:54:06,079
Another hash table or simplicity we're just showing it as a link list like this.

439
00:54:06,079 --> 00:54:12,079
But doing that. Yes, question.

440
00:54:12,079 --> 00:54:17,079
Question is when you create a new bucket, how do you determine the size of it?

441
00:54:17,079 --> 00:54:24,079
So we're not talking about whether for this lecture, we haven't talked about whether something is like backed by pages on on disk or in memory.

442
00:54:24,079 --> 00:54:29,079
But like if it is a assume it's backed by pages that are on disk and are buffer pool.

443
00:54:29,079 --> 00:54:34,079
So if whatever the page size in the database, that'll be the page size of a bucket.

444
00:54:34,079 --> 00:54:40,079
So in post-curses 8 kilobytes, my SQL is 16 kilobytes.

445
00:54:40,079 --> 00:54:46,079
Yeah, I mean, I'm showing within one page two keys because it's powerpoint.

446
00:54:46,079 --> 00:54:53,079
So again, if I have a lot of keys hash in the same location, this linear scan here can be expensive.

447
00:54:53,079 --> 00:55:04,079
So actually a really simple optimization you can do is in your bucket pointer list, you also store a bloom filter that just tells you whether a key exists in my link list.

448
00:55:04,079 --> 00:55:09,079
So if I want to look up now in G, I first check the bloom filter. I ask it whether it exists or not.

449
00:55:09,079 --> 00:55:14,079
If yes, then I'll keep following the pointer and go then scan along to I find the thing I'm looking for.

450
00:55:14,079 --> 00:55:18,079
If not, if it says no, then I don't do that scan.

451
00:55:18,079 --> 00:55:22,079
So that avoids that having to do that extra traversal.

452
00:55:23,079 --> 00:55:27,079
So if I know what a bloom filter is, no, okay.

453
00:55:27,079 --> 00:55:29,079
So I ask, go it up.

454
00:55:29,079 --> 00:55:35,079
Bloom filters are awesome and there'll be useful for a bunch of things.

455
00:55:35,079 --> 00:55:37,079
All right, quickly.

456
00:55:37,079 --> 00:55:42,079
A bloom filter is a probabilistic data structure that can tell you that can answer set membership queries.

457
00:55:42,079 --> 00:55:45,079
So a filter is different than an index.

458
00:55:45,079 --> 00:55:48,079
An index tells you for a given key, where is it?

459
00:55:49,079 --> 00:55:54,079
In this record idea or in this page, a filter can only say does the key exist yes or no?

460
00:55:54,079 --> 00:55:58,079
Can't tell you where it is, it just tells you where there exists.

461
00:55:58,079 --> 00:56:05,079
So a bloom filter, the guy was named Bloom, I think from the 70s.

462
00:56:05,079 --> 00:56:15,079
So the bloom filter is a probabilistic data structure, meaning like it can tell you with absolute 100% correctness that a key does not exist.

463
00:56:16,079 --> 00:56:20,079
But if you say it can tell you that a key does exist and it might actually be wrong.

464
00:56:20,079 --> 00:56:23,079
So it can give you false positives.

465
00:56:23,079 --> 00:56:26,079
And you can only do two operations on the basic bloom filter.

466
00:56:26,079 --> 00:56:28,079
You can do an insert and you can do a look up.

467
00:56:28,079 --> 00:56:30,079
You can't do the leads.

468
00:56:30,079 --> 00:56:32,079
We'll see what.

469
00:56:32,079 --> 00:56:36,079
So it's basically just like most of them, it's just a bit map.

470
00:56:36,079 --> 00:56:42,079
And a bit will reset based on the keys that get inserted.

471
00:56:42,079 --> 00:56:45,079
I started inserting members of the Wu-Tang Clam.

472
00:56:45,079 --> 00:56:46,079
So I insert Rizm.

473
00:56:46,079 --> 00:56:50,079
And so I'll have the hash functions.

474
00:56:50,079 --> 00:56:51,079
I'll hash it again.

475
00:56:51,079 --> 00:56:53,079
Same hash implementation, just a different seed.

476
00:56:53,079 --> 00:56:55,079
I get some hash value out.

477
00:56:55,079 --> 00:57:00,079
And then I mod it by the number of bits that I have in my bloom filter.

478
00:57:00,079 --> 00:57:04,079
And then whatever that number is, I set those bits to one.

479
00:57:04,079 --> 00:57:06,079
Flip it from zero to one.

480
00:57:06,079 --> 00:57:08,079
I insert just the same thing.

481
00:57:08,079 --> 00:57:12,079
Hash it, mod it by the number of bits, and set those bits to one.

482
00:57:12,079 --> 00:57:16,079
Now if I want to do a look up, like on Rizm, same thing.

483
00:57:16,079 --> 00:57:21,079
I just do a hash the key I'm looking for, mod it by the number.

484
00:57:21,079 --> 00:57:24,079
And then I go check to see whether all the bits,

485
00:57:24,079 --> 00:57:28,079
bit locations that I've hashed to, if they're set to one,

486
00:57:28,079 --> 00:57:32,079
if they're set to one, then I know that this was set.

487
00:57:32,079 --> 00:57:34,079
I know that, sorry.

488
00:57:34,079 --> 00:57:38,079
If it's set to one, then I think it could exist.

489
00:57:38,079 --> 00:57:42,079
But I could be wrong because something else might have set those bits.

490
00:57:42,079 --> 00:57:44,079
So I'll get back true for this.

491
00:57:44,079 --> 00:57:48,079
If I do a look up on Rekwon, the chef, again, when I do a look up,

492
00:57:48,079 --> 00:57:50,079
one of the bits is set to zero.

493
00:57:50,079 --> 00:57:53,079
So I know that cannot have been inserted because otherwise there's one of those bits,

494
00:57:53,079 --> 00:57:55,079
all those bits would have been set.

495
00:57:55,079 --> 00:57:56,079
So I get false.

496
00:57:56,079 --> 00:57:58,079
But I look up ODB, rest in peace.

497
00:57:58,079 --> 00:58:01,079
Again, now I can get a false positive because I never inserted it.

498
00:58:01,079 --> 00:58:03,079
His bits were set to one.

499
00:58:03,079 --> 00:58:08,079
So therefore it's true, but it's actually wrong.

500
00:58:08,079 --> 00:58:12,079
So you can put that Bloomfotter in front of your bucket chain.

501
00:58:12,079 --> 00:58:17,079
And it'll be popular with the bits set for the keys that were actually inserted into it.

502
00:58:17,079 --> 00:58:20,079
And I can maintain it incrementally because every time I add a new,

503
00:58:20,079 --> 00:58:24,079
insert a new key into that bucket list, I update my Bloomfotter.

504
00:58:24,079 --> 00:58:27,079
There's different variations of Bloomfotters.

505
00:58:27,079 --> 00:58:28,079
You can have different levels of them.

506
00:58:28,079 --> 00:58:30,079
You have decaying ones.

507
00:58:30,079 --> 00:58:32,079
The size of the Bloomfotter can vary their hash functions.

508
00:58:32,079 --> 00:58:33,079
There's a whole bunch of different things.

509
00:58:33,079 --> 00:58:36,079
But this data structure is super useful as we use all throughout the system.

510
00:58:36,079 --> 00:58:37,079
Yes.

511
00:58:37,079 --> 00:58:40,079
How does the rate of false positive change in the case of the hash function?

512
00:58:40,079 --> 00:58:42,079
Its question is how does the rate of false positive change at G-Sundale?

513
00:58:42,079 --> 00:58:46,079
There's some formula that says like if you want a 1% false positive rate,

514
00:58:46,079 --> 00:58:50,079
you need to have a Bloomfotter this size and with this number of hash functions.

515
00:58:50,079 --> 00:58:52,079
Is it really exponential?

516
00:58:52,079 --> 00:58:54,079
Is the expression is an exponential linear?

517
00:58:54,079 --> 00:58:55,079
I don't know.

518
00:58:55,079 --> 00:58:56,079
I remember.

519
00:58:57,079 --> 00:58:59,079
This website here, the Bloomfotter calculator,

520
00:58:59,079 --> 00:59:02,079
you say what false positive rate you want, how many keys you have,

521
00:59:02,079 --> 00:59:04,079
and it'll tell you the size of the Bloomfotter you want,

522
00:59:04,079 --> 00:59:07,079
and then the number of hash functions.

523
00:59:07,079 --> 00:59:08,079
Yes.

524
00:59:08,079 --> 00:59:10,079
How does Bloomfotter tend to deletion?

525
00:59:10,079 --> 00:59:13,079
The question is how does Bloomfotter tend to deletion?

526
00:59:13,079 --> 00:59:14,079
They don't.

527
00:59:14,079 --> 00:59:16,079
Right?

528
00:59:16,079 --> 00:59:20,079
There are variations of them that with multiple levels you can do it.

529
00:59:20,079 --> 00:59:23,079
For the basic one, they don't.

530
00:59:23,079 --> 00:59:25,079
And Bloomfotters are super useful.

531
00:59:25,079 --> 00:59:27,079
We'll use this throughout the system in a bunch of ways.

532
00:59:27,079 --> 00:59:29,079
We'll use it for hash joins.

533
00:59:29,079 --> 00:59:33,079
Because again, it's a lot cheaper to go look up to say is it in my Bloomfotter

534
00:59:33,079 --> 00:59:36,079
than go look up, you know, actually follow a page and look on this

535
00:59:36,079 --> 00:59:40,079
because you weather something exists or not.

536
00:59:40,079 --> 00:59:41,079
Okay.

537
00:59:44,079 --> 00:59:47,079
So a most sophisticated scheme is called extendable hashing.

538
00:59:47,079 --> 00:59:49,079
And this is going to be like chain hashing,

539
00:59:49,079 --> 00:59:53,079
but we're going to allow the,

540
00:59:53,079 --> 00:59:57,079
we're going to split the buckets to avoid these infinitely long bucket lists.

541
00:59:57,079 --> 01:00:00,079
And we're going to split it in such a way that we only,

542
01:00:00,079 --> 01:00:06,079
we only need to do it incrementally in a small part of the hash table rather than having to rehash everything.

543
01:00:06,079 --> 01:00:11,079
And the key idea of this is going to work is that we're going to expand the number of bits we have to look at

544
01:00:11,079 --> 01:00:17,079
when we do lookups in our bucket list or bucket hash table to go find the bucket chain that we're looking for.

545
01:00:17,079 --> 01:00:25,079
And we can vary this per sort of per per value or per key type, not key type.

546
01:00:25,079 --> 01:00:30,079
We can vary this based on what bucket list we're looking at.

547
01:00:30,079 --> 01:00:35,079
So it may be the case that two different locations or multiple locations in our bucket array

548
01:00:35,079 --> 01:00:41,079
will point to the same bucket list, but then that can expand and break up as we need it as we go along.

549
01:00:41,079 --> 01:00:44,079
So I didn't actually think this is, this is bit complicated.

550
01:00:44,079 --> 01:00:48,079
And I didn't think actually any system actually uses it, but it turns out GDBM,

551
01:00:48,079 --> 01:00:52,079
which is a new database manager, think of like, you know,

552
01:00:52,079 --> 01:00:56,079
a key value store, like sort of like rocks DB or SQLite,

553
01:00:56,079 --> 01:00:58,079
you can run this embedded in your system.

554
01:00:58,079 --> 01:01:01,079
That's based on you on a set of hash tables.

555
01:01:01,079 --> 01:01:05,079
And then Asterix DB is a big data project at a UC Irvine,

556
01:01:05,079 --> 01:01:09,079
and they're using a set of my hashing in their implementation.

557
01:01:09,079 --> 01:01:12,079
Let's see how this works.

558
01:01:12,079 --> 01:01:15,079
Right, so the first thing we're going to have is that we have our, again, our slot array

559
01:01:15,079 --> 01:01:17,079
and it's going to point to our bucket list.

560
01:01:17,079 --> 01:01:22,079
And then we're going to have this global identifier that tells us how many bits we need to look at

561
01:01:22,079 --> 01:01:28,079
for our hash values to determine how we do our lookups in our bucket array.

562
01:01:28,079 --> 01:01:32,079
And then for sort of bookkeeping reasons, every bucket list as well will also have

563
01:01:32,079 --> 01:01:36,079
with our local bit, bit sizes, number of bits we need to look at.

564
01:01:36,079 --> 01:01:40,079
So you see in the case here, these first two slots here,

565
01:01:40,079 --> 01:01:43,079
they're both going to be pointing to the same bucket list,

566
01:01:43,079 --> 01:01:46,079
whereas these two ones at the bottom, they're going to be pointing to different locations.

567
01:01:46,079 --> 01:01:52,079
Right, and this is because we need to look at globally,

568
01:01:52,079 --> 01:01:57,079
we're going to look at two bits, but for the first two entries,

569
01:01:57,079 --> 01:02:00,079
when the, when the certificate bit is zero,

570
01:02:00,079 --> 01:02:06,079
they're going to reuse the same bucket list identified by the local identifier appear.

571
01:02:06,079 --> 01:02:09,079
Right, so let's say now I want to do a lookup

572
01:02:09,079 --> 01:02:14,079
on this key here, I hash it, I then look at the top two bits,

573
01:02:14,079 --> 01:02:18,079
because that's what set my global identifier, a global counter.

574
01:02:18,079 --> 01:02:21,079
And then I hash it this location, I just follow the pointer,

575
01:02:21,079 --> 01:02:27,079
and I land in that bucket, and I can just do the linear search to find the thing I'm looking for.

576
01:02:27,079 --> 01:02:30,079
Say now I want to put B,

577
01:02:30,079 --> 01:02:33,079
B again, globally, I know I need to look at the top two bits.

578
01:02:33,079 --> 01:02:38,079
I do a lookup in my bucket list based on those two bits,

579
01:02:38,079 --> 01:02:42,079
then I land to this location here, and I go ahead and insert it.

580
01:02:42,079 --> 01:02:44,079
But now I want to put C in,

581
01:02:44,079 --> 01:02:47,079
and if I look at the last two bits,

582
01:02:47,079 --> 01:02:49,079
it lands in the same location when I insert a B,

583
01:02:49,079 --> 01:02:53,079
but now this bucket is full, I can't put any more entries in,

584
01:02:53,079 --> 01:02:59,079
so I need to expand the number of bits I'm looking at to now expand the number of options that I have.

585
01:02:59,079 --> 01:03:02,079
So I'm going to increment the global counter from two to three.

586
01:03:02,079 --> 01:03:07,079
I'm going to double the size of the number of pointers I have in my bucket array,

587
01:03:07,079 --> 01:03:11,079
my bucket array, but then the, and create the new entry,

588
01:03:11,079 --> 01:03:15,079
but then when the bit is set to zero,

589
01:03:15,079 --> 01:03:18,079
they're all still going to point to the first bucket here,

590
01:03:18,079 --> 01:03:20,079
because I haven't that one yet,

591
01:03:20,079 --> 01:03:22,079
so I only need to look at one bit for that.

592
01:03:22,079 --> 01:03:25,079
For the next, when the bits are 1,1,

593
01:03:25,079 --> 01:03:28,079
that points to this other bucket down here,

594
01:03:28,079 --> 01:03:31,079
and the same thing with these other ones.

595
01:03:31,079 --> 01:03:34,079
So now when I'm going to do a lookup to put C in,

596
01:03:34,079 --> 01:03:36,079
I need to look at three bits,

597
01:03:36,079 --> 01:03:38,079
and I need to look at all the pointers here,

598
01:03:38,079 --> 01:03:41,079
and that then takes me to this bucket location.

599
01:03:41,079 --> 01:03:45,079
So going back here, when I did my split, I had a resize.

600
01:03:45,079 --> 01:03:48,079
These guys just slid down,

601
01:03:48,079 --> 01:03:51,079
and I only had to insert one new bucket,

602
01:03:51,079 --> 01:03:54,079
but I took what was here, because this one was full,

603
01:03:54,079 --> 01:03:57,079
and I just split that one and created a new bucket for it.

604
01:03:57,079 --> 01:03:59,079
I didn't have to touch the one at the bottom,

605
01:03:59,079 --> 01:04:02,079
and they didn't have to touch the one at the top.

606
01:04:02,079 --> 01:04:06,079
But like that's, you have to take a latch on it,

607
01:04:06,079 --> 01:04:08,079
when you do it, because you have to make a copy,

608
01:04:08,079 --> 01:04:11,079
and resize it, but it's not that big of a deal.

609
01:04:11,079 --> 01:04:14,079
You can do that pretty quickly.

610
01:04:18,079 --> 01:04:20,079
Any questions about this?

611
01:04:20,079 --> 01:04:21,079
Yes?

612
01:04:21,079 --> 01:04:23,079
Is this good?

613
01:04:23,079 --> 01:04:29,079
So, resizing this lot of rays relatively cheap,

614
01:04:29,079 --> 01:04:32,079
it's clever, it's a good idea.

615
01:04:32,079 --> 01:04:36,079
Sorry, it's a clever idea, whether or not it's good or not.

616
01:04:36,079 --> 01:04:41,079
It, engineering-wise, it's a bit tricky to keep track of all the metadata,

617
01:04:41,079 --> 01:04:44,079
where like, you know, what, you know,

618
01:04:44,079 --> 01:04:48,079
what bits I need to be looking at as a hash into it.

619
01:04:48,079 --> 01:04:50,079
But it's basically just chain-hashing.

620
01:04:50,079 --> 01:04:53,079
So all the benefits I get from chain-hashing are applicable here.

621
01:04:53,079 --> 01:04:56,079
I have an extra mechanism now to split things up,

622
01:04:56,079 --> 01:05:02,079
so I don't have this infinitely growing, you know, linked list.

623
01:05:02,079 --> 01:05:05,079
So it's just a way to handle resizing,

624
01:05:05,079 --> 01:05:10,079
in a way you couldn't do in regular chain-hashing.

625
01:05:10,079 --> 01:05:11,079
Yes?

626
01:05:11,079 --> 01:05:14,079
Why don't you use it to start the process?

627
01:05:14,079 --> 01:05:18,079
I think, yeah, again, linear probing is probably the easiest thing to do,

628
01:05:18,079 --> 01:05:25,079
and the lock the whole tail and double the size of it is sometimes good enough.

629
01:05:26,079 --> 01:05:28,079
Right.

630
01:05:28,079 --> 01:05:31,079
So the last one is linear-hashing.

631
01:05:31,079 --> 01:05:34,079
And this is actually what Postgres does,

632
01:05:34,079 --> 01:05:36,079
or something very close to this.

633
01:05:36,079 --> 01:05:38,079
And the reason why Postgres,

634
01:05:38,079 --> 01:05:42,079
well, the notes that's not Berkeley-DB that also does this,

635
01:05:42,079 --> 01:05:46,079
the company that built Berkeley-DB was company called Sleepycat software.

636
01:05:46,079 --> 01:05:49,079
So the people that build wire-tiger,

637
01:05:49,079 --> 01:05:52,079
they originally started Sleepycat,

638
01:05:52,079 --> 01:05:55,079
that got sold to Oracle, so Oracle owns Berkeley-DB,

639
01:05:55,079 --> 01:05:57,079
and then they went out and started the new company,

640
01:05:57,079 --> 01:05:58,079
and instead of calling it Sleepycat,

641
01:05:58,079 --> 01:06:01,079
they called it a wire-tiger,

642
01:06:01,079 --> 01:06:03,079
like a tie-iron cocaine or whatever.

643
01:06:03,079 --> 01:06:05,079
It was trying to be the opposite.

644
01:06:05,079 --> 01:06:09,079
But the woman that wrote the linear-hashing implementation in Postgres

645
01:06:09,079 --> 01:06:12,079
in the early 90s was the founder of Berkeley-DB.

646
01:06:12,079 --> 01:06:16,079
So she wrote it for Postgres and then wrote it for Berkeley-DB.

647
01:06:16,079 --> 01:06:20,079
And she was one of Stonebreakers PG students at Berkeley.

648
01:06:20,079 --> 01:06:24,079
So the linear-hashing will be more complicated than

649
01:06:24,079 --> 01:06:26,079
the XML-hashing potentially,

650
01:06:26,079 --> 01:06:32,079
but the basic idea is that we're going to keep track of

651
01:06:32,079 --> 01:06:37,079
the next bucket list we want to split,

652
01:06:37,079 --> 01:06:41,079
and that when any time there's an overflow in our bucket list chain,

653
01:06:41,079 --> 01:06:43,079
and anywhere in our hash table,

654
01:06:43,079 --> 01:06:45,079
whatever we're pointing at with our split pointer,

655
01:06:45,079 --> 01:06:47,079
that's the one we're going to split.

656
01:06:48,079 --> 01:06:51,079
And the idea here is that again, we want to do this incrementally

657
01:06:51,079 --> 01:06:53,079
and not have to lock the whole table while we resize,

658
01:06:53,079 --> 01:06:56,079
so we can make small changes as we go along.

659
01:06:56,079 --> 01:06:59,079
And the idea here is again, you're amortizing the cost of resizing,

660
01:06:59,079 --> 01:07:01,079
so it's sort of shared across multiple workers.

661
01:07:01,079 --> 01:07:04,079
So there's not like one worker who's the unlucky one that shows up,

662
01:07:04,079 --> 01:07:06,079
tries to insert something,

663
01:07:06,079 --> 01:07:10,079
and then they draw the short straw and they're responsible for resizing the whole thing.

664
01:07:10,079 --> 01:07:12,079
And incrementally, as you go along,

665
01:07:12,079 --> 01:07:14,079
and that sort of smooths out performance.

666
01:07:14,079 --> 01:07:22,079
So again, the idea here is that we're going to split whatever the next one

667
01:07:22,079 --> 01:07:23,079
we need to split,

668
01:07:23,079 --> 01:07:25,079
which may not be the one that overflowed,

669
01:07:25,079 --> 01:07:28,079
it's just be whatever the next one is in our incremental order,

670
01:07:28,079 --> 01:07:33,079
and then we'll have maintain multiple hash functions that are going to help us determine

671
01:07:33,079 --> 01:07:39,079
which location within our bucket list we should be looking at.

672
01:07:39,079 --> 01:07:42,079
Let me show the diagram, and this makes more sense.

673
01:07:43,079 --> 01:07:46,079
So again, just look at what we have our bucket list here,

674
01:07:46,079 --> 01:07:49,079
and that's going to map to bucket chains.

675
01:07:49,079 --> 01:07:52,079
And then we're going to have a split pointer that's going to say,

676
01:07:52,079 --> 01:07:54,079
here's the next thing we want to split any time,

677
01:07:54,079 --> 01:07:56,079
anything overflows in our hash table.

678
01:07:56,079 --> 01:07:58,079
And then we have, at the very beginning,

679
01:07:58,079 --> 01:08:01,079
we assume we have one hash function that's just the key,

680
01:08:01,079 --> 01:08:06,079
say it's the, you know, the identity, the key mod by the end,

681
01:08:06,079 --> 01:08:07,079
for simplicity reasons.

682
01:08:07,079 --> 01:08:10,079
But again, assuming like it's taking any arbitrary string,

683
01:08:10,079 --> 01:08:13,079
or arbitrary byte sequence, and spitting out integer.

684
01:08:13,079 --> 01:08:16,079
So say I want to get six,

685
01:08:16,079 --> 01:08:20,079
I do my lookup, and at two, and I follow along,

686
01:08:20,079 --> 01:08:22,079
and I find the key I'm looking for.

687
01:08:22,079 --> 01:08:24,079
That looks just like before, nothing special.

688
01:08:24,079 --> 01:08:26,079
But now I want to put 17,

689
01:08:26,079 --> 01:08:29,079
and it should go into this bucket here,

690
01:08:29,079 --> 01:08:31,079
but that thing's full.

691
01:08:31,079 --> 01:08:33,079
So we're just going to do an overflow,

692
01:08:33,079 --> 01:08:36,079
just like chain hashing, we're going to extend it with another bucket,

693
01:08:36,079 --> 01:08:42,079
and insert it into the, insert it into that new page.

694
01:08:42,079 --> 01:08:45,079
But now, because we overflowed,

695
01:08:45,079 --> 01:08:48,079
we need to split whatever the split pointer was pointing at.

696
01:08:48,079 --> 01:08:52,079
So in this case here, it's pointing to bucket zero,

697
01:08:52,079 --> 01:08:53,079
a bucket list zero.

698
01:08:53,079 --> 01:08:55,079
Even though that didn't overflow.

699
01:08:55,079 --> 01:08:59,079
So what we need to do now is look at all the entries inside this bucket list,

700
01:08:59,079 --> 01:09:03,079
and we're going to rehash them based on the,

701
01:09:03,079 --> 01:09:05,079
based now on two n,

702
01:09:05,079 --> 01:09:10,079
because we're going to incrementally grow the size of the bucket list by one each time.

703
01:09:10,079 --> 01:09:12,079
So we had four entries,

704
01:09:12,079 --> 01:09:15,079
now after we got to split, now we'll have five.

705
01:09:15,079 --> 01:09:18,079
So we go through, and this points there,

706
01:09:18,079 --> 01:09:21,079
for every single key, we're going to rehash it based on,

707
01:09:21,079 --> 01:09:23,079
instead of mod n, but mod two n.

708
01:09:23,079 --> 01:09:25,079
So eight, mod eight is zero,

709
01:09:25,079 --> 01:09:26,079
so that stays where it was.

710
01:09:26,079 --> 01:09:28,079
20, mod eight is now four,

711
01:09:28,079 --> 01:09:32,079
so that's going to get moved down to this new page down here.

712
01:09:32,079 --> 01:09:36,079
And then now the split pointer just moves down by one,

713
01:09:36,079 --> 01:09:39,079
and we continue doing whatever, you know,

714
01:09:39,079 --> 01:09:41,079
continue operating on the hash table.

715
01:09:41,079 --> 01:09:43,079
Right?

716
01:09:43,079 --> 01:09:45,079
So now I do a get 20.

717
01:09:45,079 --> 01:09:49,079
When I first hash it, I would get zero,

718
01:09:49,079 --> 01:09:51,079
but then I know that the,

719
01:09:51,079 --> 01:09:57,079
that location in my bucket list here is above where the split pointer is pointing at.

720
01:09:57,079 --> 01:10:00,079
So I've known I've already split everything up above it.

721
01:10:00,079 --> 01:10:02,079
So after I mod it by four,

722
01:10:02,079 --> 01:10:05,079
I got to mod it by eight now to figure out where it really is.

723
01:10:05,079 --> 01:10:10,079
And then that's how I can find it down here at the bottom.

724
01:10:10,079 --> 01:10:11,079
So I want to get nine.

725
01:10:11,079 --> 01:10:14,079
In this case here, it's pointing to exactly where the bucket,

726
01:10:14,079 --> 01:10:15,079
the split pointer is pointing at.

727
01:10:15,079 --> 01:10:17,079
So I know I haven't split it yet,

728
01:10:17,079 --> 01:10:19,079
so I can just only hash it once,

729
01:10:19,079 --> 01:10:25,079
and I scan along the link list until I find the thing I'm looking for.

730
01:10:25,079 --> 01:10:27,079
Right?

731
01:10:27,079 --> 01:10:30,079
And at some point, the split pointer will get to the bottom,

732
01:10:30,079 --> 01:10:32,079
and I'll have eight slots,

733
01:10:32,079 --> 01:10:37,079
and I just loop back around and start all over again.

734
01:10:37,079 --> 01:10:39,079
So this seems kind of counter-tuitive,

735
01:10:39,079 --> 01:10:41,079
that like I'm not splitting the thing that overflowed,

736
01:10:41,079 --> 01:10:43,079
I'm splitting to whatever the split pointer points at.

737
01:10:43,079 --> 01:10:46,079
But the idea is again that like if, you know,

738
01:10:46,079 --> 01:10:49,079
say this location, or this, you know, slot one,

739
01:10:49,079 --> 01:10:50,079
this thing is super hot,

740
01:10:50,079 --> 01:10:52,079
and I keep overflowing and overflowing,

741
01:10:52,079 --> 01:10:55,079
I'm eventually going to split it.

742
01:10:55,079 --> 01:10:56,079
Right?

743
01:10:56,079 --> 01:10:59,079
So eventually everything gets split out

744
01:10:59,079 --> 01:11:01,079
and sort of re-size correctly.

745
01:11:04,079 --> 01:11:05,079
Yes?

746
01:11:05,079 --> 01:11:07,079
One thing I'm confused about is,

747
01:11:07,079 --> 01:11:09,079
it seems like every time we overscore it by one,

748
01:11:09,079 --> 01:11:11,079
the split pointer moves down by one,

749
01:11:11,079 --> 01:11:13,079
and then we add one to take the when,

750
01:11:13,079 --> 01:11:15,079
which is the split pointer after the round.

751
01:11:15,079 --> 01:11:18,079
Because the move down by one is the same time you had it.

752
01:11:18,079 --> 01:11:21,079
So the question is when would it actually wrap around?

753
01:11:21,079 --> 01:11:23,079
Because you each have one.

754
01:11:23,079 --> 01:11:25,079
So you would get to the point where like,

755
01:11:25,079 --> 01:11:27,079
so we'd be like five, six, seven,

756
01:11:27,079 --> 01:11:29,079
and then you'd be seven,

757
01:11:29,079 --> 01:11:30,079
and then you'd have to,

758
01:11:30,079 --> 01:11:32,079
you'd loop back around to zero,

759
01:11:32,079 --> 01:11:33,079
because you'd know that like,

760
01:11:33,079 --> 01:11:36,079
from when it's here,

761
01:11:36,079 --> 01:11:38,079
when it's only from zero to three,

762
01:11:38,079 --> 01:11:42,079
you'd want to get past seven,

763
01:11:42,079 --> 01:11:44,079
you'd know where you start at the starting point

764
01:11:44,079 --> 01:11:46,079
that that's two n from where you started at.

765
01:11:46,079 --> 01:11:48,079
So then you'd loop back around.

766
01:11:48,079 --> 01:11:53,079
But don't we add a new page every time we do that?

767
01:11:53,079 --> 01:11:55,079
You add a new page, but like,

768
01:11:55,079 --> 01:11:58,079
I know that I should wrap around when I go to eight,

769
01:11:58,079 --> 01:12:00,079
position eight,

770
01:12:00,079 --> 01:12:01,079
because when I started,

771
01:12:01,079 --> 01:12:04,079
I had four, so two times four is eight,

772
01:12:04,079 --> 01:12:05,079
so when I get past eight,

773
01:12:05,079 --> 01:12:06,079
I'd loop back around.

774
01:12:06,079 --> 01:12:08,079
Then you do that until you get sixteen,

775
01:12:08,079 --> 01:12:09,079
and then loop back around.

776
01:12:15,079 --> 01:12:17,079
Good idea or bad idea?

777
01:12:23,079 --> 01:12:24,079
It's clever, right?

778
01:12:24,079 --> 01:12:28,079
Again, it's a nice technique to do this incrementally,

779
01:12:28,079 --> 01:12:30,079
but again, there's a lot more bookkeeping,

780
01:12:30,079 --> 01:12:31,079
a lot more machinery,

781
01:12:31,079 --> 01:12:33,079
in order to actually implement those.

782
01:12:33,079 --> 01:12:34,079
Yes?

783
01:12:34,079 --> 01:12:36,079
When you do a look up to zero,

784
01:12:36,079 --> 01:12:38,079
do you do the top and the top?

785
01:12:38,079 --> 01:12:40,079
And this question is,

786
01:12:40,079 --> 01:12:41,079
if you only do a look up,

787
01:12:41,079 --> 01:12:42,079
you only have a hash at most twice,

788
01:12:42,079 --> 01:12:44,079
in this scenario, yes.

789
01:12:45,079 --> 01:12:47,079
Like if this thing is massive,

790
01:12:47,079 --> 01:12:48,079
I could have like,

791
01:12:48,079 --> 01:12:50,079
yes, so actually what happens is,

792
01:12:50,079 --> 01:12:52,079
once I get to, say I got to eight,

793
01:12:52,079 --> 01:12:53,079
and I wrap back around,

794
01:12:53,079 --> 01:12:55,079
I can drop the first hash function.

795
01:12:55,079 --> 01:12:58,079
In this case, you're even at most two, yes.

796
01:13:02,079 --> 01:13:03,079
Yes?

797
01:13:03,079 --> 01:13:06,079
Do you look in the next slide?

798
01:13:06,079 --> 01:13:08,079
About deletes?

799
01:13:08,079 --> 01:13:11,079
I guess you start showing there only five reasons,

800
01:13:11,079 --> 01:13:12,079
but you're on what,

801
01:13:12,079 --> 01:13:14,079
how do you make the next slide,

802
01:13:14,079 --> 01:13:17,079
five, two, and three?

803
01:13:17,079 --> 01:13:19,079
So what do you get, seven?

804
01:13:20,079 --> 01:13:21,079
Yes, question is,

805
01:13:22,079 --> 01:13:25,079
and here, like I'm trying to mod by eight,

806
01:13:25,079 --> 01:13:28,079
but what if I get into seven and I don't have it?

807
01:13:28,079 --> 01:13:30,079
But again, you wouldn't be able to get seven

808
01:13:30,079 --> 01:13:35,079
because you'd be below the split pointer,

809
01:13:35,079 --> 01:13:38,079
and you'd only hash by four, not eight.

810
01:13:39,079 --> 01:13:41,079
So this demarcation line says,

811
01:13:41,079 --> 01:13:43,079
I've split everything above,

812
01:13:43,079 --> 01:13:45,079
and nothing below avoids that problem.

813
01:13:45,079 --> 01:13:47,079
Like you don't land here,

814
01:13:47,079 --> 01:13:50,079
and you really, like, you don't hash first and land here,

815
01:13:50,079 --> 01:13:53,079
but if you hash by two and you land something here,

816
01:13:53,079 --> 01:13:55,079
then you have a split cap.

817
01:13:55,079 --> 01:13:58,079
The split pointer waterline avoids that problem.

818
01:14:04,079 --> 01:14:06,079
All right, so,

819
01:14:06,079 --> 01:14:08,079
splitting bucket space on the flip pointer

820
01:14:08,079 --> 01:14:11,079
eventually gets you all a little bit buckets.

821
01:14:11,079 --> 01:14:13,079
Again, when everybody's had this,

822
01:14:13,079 --> 01:14:14,079
when you reach the bottom,

823
01:14:14,079 --> 01:14:16,079
you just drop the first hash function and loop back around.

824
01:14:17,079 --> 01:14:21,079
In this technique, also allows you to do contraction

825
01:14:23,079 --> 01:14:25,079
or coalescing as well,

826
01:14:25,079 --> 01:14:30,079
because you could identify that a bucket list is empty,

827
01:14:30,079 --> 01:14:32,079
and you could do the reverse.

828
01:14:32,079 --> 01:14:34,079
You could throw it away,

829
01:14:34,079 --> 01:14:37,079
consolidate the, well, one bucket's empty,

830
01:14:37,079 --> 01:14:39,079
so you could throw it away,

831
01:14:39,079 --> 01:14:41,079
and you move the split pointer back up,

832
01:14:41,079 --> 01:14:44,079
and that allows you actually shrink the size of the hash table.

833
01:14:45,079 --> 01:14:48,079
So going back here, say I delete 20,

834
01:14:48,079 --> 01:14:50,079
I mod it by four,

835
01:14:50,079 --> 01:14:53,079
but then I realize that's below the split pointer,

836
01:14:53,079 --> 01:14:55,079
and I gotta get down to the bottom here,

837
01:14:55,079 --> 01:14:56,079
and I go ahead and delete it,

838
01:14:56,079 --> 01:14:58,079
but now this page is empty,

839
01:14:58,079 --> 01:14:59,079
so if I wanted to,

840
01:14:59,079 --> 01:15:01,079
I could just move the split pointer back up,

841
01:15:01,079 --> 01:15:05,079
and then drop that last entry and drop the last hash table.

842
01:15:05,079 --> 01:15:06,079
Right?

843
01:15:06,079 --> 01:15:08,079
And obviously, you need to be clever,

844
01:15:08,079 --> 01:15:09,079
and make sure that, like,

845
01:15:09,079 --> 01:15:11,079
I don't oscillate, like, insert 20, delete 20, insert 20,

846
01:15:11,079 --> 01:15:13,079
and like, I keep splitting it and coalescing.

847
01:15:13,079 --> 01:15:15,079
That would be bad,

848
01:15:15,079 --> 01:15:19,079
but you could contract the data structure based on this.

849
01:15:19,079 --> 01:15:20,079
Right?

850
01:15:20,079 --> 01:15:21,079
You don't want to insert 21,

851
01:15:21,079 --> 01:15:23,079
then overflow, and split all over again.

852
01:15:23,079 --> 01:15:24,079
Right?

853
01:15:26,079 --> 01:15:31,079
I don't think Postgres supports shrinking the size of the hash table.

854
01:15:31,079 --> 01:15:32,079
As far as I know,

855
01:15:32,079 --> 01:15:34,079
without this re-building the whole thing.

856
01:15:36,079 --> 01:15:37,079
Okay?

857
01:15:38,079 --> 01:15:40,079
All right, so, hash tables.

858
01:15:40,079 --> 01:15:41,079
Again, super useful.

859
01:15:41,079 --> 01:15:44,079
Most systems are going to just implement the linear-proven hashing,

860
01:15:44,079 --> 01:15:46,079
but again, you can still specialize in based on the data type

861
01:15:46,079 --> 01:15:49,079
and other aspects of how it's going to be used,

862
01:15:49,079 --> 01:15:52,079
and click houses, probably the best example of this.

863
01:15:52,079 --> 01:15:54,079
For a lot of commercial systems,

864
01:15:54,079 --> 01:15:56,079
it's very hard to know what hatchable they're actually using,

865
01:15:56,079 --> 01:15:58,079
unless there's a paper talking about it,

866
01:15:58,079 --> 01:16:01,079
or we know people that work there that can tell us.

867
01:16:01,079 --> 01:16:03,079
You know, this is not something you,

868
01:16:03,079 --> 01:16:04,079
as, like, you know,

869
01:16:04,079 --> 01:16:06,079
someone using SQL and application developer,

870
01:16:06,079 --> 01:16:08,079
you should know or care.

871
01:16:08,079 --> 01:16:09,079
But it's nice to know what,

872
01:16:09,079 --> 01:16:12,079
sometimes, what, how these systems are implemented.

873
01:16:12,079 --> 01:16:14,079
Nice thing about hatch functions, again,

874
01:16:14,079 --> 01:16:15,079
it'll be fast,

875
01:16:15,079 --> 01:16:17,079
but it's for O1 lookups in the best case scenario.

876
01:16:17,079 --> 01:16:20,079
But again, we need to be able to make sure that we can,

877
01:16:20,079 --> 01:16:25,079
you know, we may need to grow efficiently if we estimate the size

878
01:16:25,079 --> 01:16:27,079
incorrectly,

879
01:16:27,079 --> 01:16:30,079
and we'll see how we do those estimations later on.

880
01:16:30,079 --> 01:16:35,079
So, some systems will give you hash tables when you call Create Index.

881
01:16:35,079 --> 01:16:36,079
Postgres will let you do this.

882
01:16:36,079 --> 01:16:37,079
Postgres will be called Create Index.

883
01:16:37,079 --> 01:16:39,079
You can say using hash,

884
01:16:39,079 --> 01:16:40,079
and you'll get a hash table.

885
01:16:40,079 --> 01:16:43,079
You'll get their linear hash table implementation.

886
01:16:43,079 --> 01:16:46,079
But this is not the default.

887
01:16:46,079 --> 01:16:50,079
For almost all systems when you call Create Index.

888
01:16:50,079 --> 01:16:52,079
There may be no Y.

889
01:16:52,079 --> 01:16:56,079
No range against, yes.

890
01:16:56,079 --> 01:16:58,079
The only thing you can do with the hash table

891
01:16:58,079 --> 01:17:00,079
is a quality lookups,

892
01:17:00,079 --> 01:17:02,079
and you need to have the entire key.

893
01:17:02,079 --> 01:17:04,079
If my key is on,

894
01:17:05,079 --> 01:17:06,079
you can see,

895
01:17:06,079 --> 01:17:07,079
column A and column B,

896
01:17:07,079 --> 01:17:08,079
I can do composite keys.

897
01:17:08,079 --> 01:17:10,079
If I don't have A,

898
01:17:10,079 --> 01:17:11,079
or I don't have B,

899
01:17:11,079 --> 01:17:13,079
I can't do a lookup.

900
01:17:13,079 --> 01:17:14,079
In a B plus tree,

901
01:17:14,079 --> 01:17:16,079
which we'll discuss next class,

902
01:17:16,079 --> 01:17:18,079
you can do these prefix lookups.

903
01:17:18,079 --> 01:17:22,079
And it is the best data structure of all time for databases.

904
01:17:22,079 --> 01:17:24,079
Tries are actually pretty good too,

905
01:17:24,079 --> 01:17:27,079
but you can put tries in your B plus trees.

906
01:17:27,079 --> 01:17:29,079
Do you have a bunch of things like that?

907
01:17:29,079 --> 01:17:32,079
So, so the default choice of most of these systems

908
01:17:32,079 --> 01:17:34,079
are going to be a B plus tree,

909
01:17:34,079 --> 01:17:36,079
and that's what we'll discuss next week.

910
01:17:36,079 --> 01:17:39,079
But again, we'll assume it's single threaded

911
01:17:39,079 --> 01:17:40,079
on Monday,

912
01:17:40,079 --> 01:17:41,079
and then on Wednesday,

913
01:17:41,079 --> 01:17:43,079
we'll see how to make it multi threaded.

914
01:17:43,079 --> 01:17:44,079
Okay?

915
01:17:44,079 --> 01:17:46,079
All right, hit it.

916
01:17:47,079 --> 01:17:49,079
I'm gonna go to the next class.

917
01:17:49,079 --> 01:17:51,079
I'm gonna go to the next class.

918
01:17:51,079 --> 01:17:53,079
I'm gonna go to the next class.

919
01:17:53,079 --> 01:17:55,079
I'm gonna go to the next class.

920
01:17:55,079 --> 01:17:57,079
I'm gonna go to the next class.

921
01:17:57,079 --> 01:17:59,079
I'm gonna go to the next class.

922
01:17:59,079 --> 01:18:01,079
I'm gonna go to the next class.

923
01:18:01,079 --> 01:18:03,079
I'm gonna go to the next class.

924
01:18:03,079 --> 01:18:05,079
I'm gonna go to the next class.

925
01:18:05,079 --> 01:18:07,079
I'm gonna go to the next class.

926
01:18:07,079 --> 01:18:09,079
I'm gonna go to the next class.

927
01:18:09,079 --> 01:18:11,079
I'm gonna go to the next class.

928
01:18:11,079 --> 01:18:12,079
I'm gonna go to the next class.

929
01:18:12,079 --> 01:18:13,079
I'm gonna go to the next class.

930
01:18:13,079 --> 01:18:14,079
I'm gonna go to the next class.

931
01:18:14,079 --> 01:18:15,079
I'm gonna go to the next class.

932
01:18:15,079 --> 01:18:17,079
I'm gonna go to the next class.

933
01:18:17,079 --> 01:18:18,079
I'm gonna go to the next class.

934
01:18:18,079 --> 01:18:19,079
I'm gonna go to the next class.

935
01:18:19,079 --> 01:18:20,079
I'm gonna go to the next class.

936
01:18:20,079 --> 01:18:21,079
I'm gonna go to the next class.

937
01:18:21,079 --> 01:18:22,079
I'm gonna go to the next class.

938
01:18:22,079 --> 01:18:23,079
I'm gonna go to the next class.

939
01:18:23,079 --> 01:18:24,079
I'm gonna go to the next class.

940
01:18:24,079 --> 01:18:25,079
I'm gonna go to the next class.

941
01:18:25,079 --> 01:18:26,079
I'm gonna go to the next class.

942
01:18:26,079 --> 01:18:27,079
I'm gonna go to the next class.

943
01:18:27,079 --> 01:18:28,079
I'm gonna go to the next class.

944
01:18:28,079 --> 01:18:29,079
I'm gonna go to the next class.

945
01:18:29,079 --> 01:18:30,079
I'm gonna go to the next class.

946
01:18:30,079 --> 01:18:31,079
I'm gonna go to the next class.

947
01:18:31,079 --> 01:18:32,079
I'm gonna go to the next class.

948
01:18:32,079 --> 01:18:33,079
I'm gonna go to the next class.

949
01:18:33,079 --> 01:18:34,079
I'm gonna go to the next class.

950
01:18:34,079 --> 01:18:35,079
I'm gonna go to the next class.

951
01:18:35,079 --> 01:18:36,079
I'm gonna go to the next class.

952
01:18:36,079 --> 01:18:37,079
I'm gonna go to the next class.

953
01:18:37,079 --> 01:18:38,079
I'm gonna go to the next class.

954
01:18:38,079 --> 01:18:39,079
I'm gonna go to the next class.

955
01:18:39,079 --> 01:18:40,079
I'm gonna go to the next class.

956
01:18:40,079 --> 01:18:41,079
I'm gonna go to the next class.

957
01:18:41,079 --> 01:18:42,079
I'm gonna go to the next class.

958
01:18:42,079 --> 01:18:43,079
I'm gonna go to the next class.

959
01:18:43,079 --> 01:18:44,079
I'm gonna go to the next class.

960
01:18:44,079 --> 01:18:45,079
I'm gonna go to the next class.

961
01:18:45,079 --> 01:18:46,079
I'm gonna go to the next class.

962
01:18:46,079 --> 01:18:47,079
I'm gonna go to the next class.

963
01:18:47,079 --> 01:18:48,079
I'm gonna go to the next class.

964
01:18:48,079 --> 01:18:49,079
I'm gonna go to the next class.

965
01:18:49,079 --> 01:18:50,079
I'm gonna go to the next class.

966
01:18:50,079 --> 01:18:51,079
I'm gonna go to the next class.

967
01:18:51,079 --> 01:18:52,079
I'm gonna go to the next class.

968
01:18:52,079 --> 01:18:53,079
I'm gonna go to the next class.

969
01:18:53,079 --> 01:18:54,079
I'm gonna go to the next class.

970
01:18:54,079 --> 01:18:55,079
I'm gonna go to the next class.

971
01:18:55,079 --> 01:18:56,079
I'm gonna go to the next class.

