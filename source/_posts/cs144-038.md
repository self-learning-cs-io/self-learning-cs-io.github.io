---
title: 计算机网络 038 Principles Packet Switching How a packet switch works(2)
date: 2025-10-19 10:00:37
---


发言人   00:00
This video is a continuation of our first video on how packet switches work. In the first video, we saw that there are two basic operations to a packet switch. First, packet addresses have to be looked up into a forwarding table, and then the packet has to be switched or transferred to the correct output port so that it can be sent under the correct outgoing link. In the last video, we saw how addresses are looked up in tables for Ethernet switches and Internet routers. And in this video I'm going to explain how packets are switched to the correct egress port. I'm going to at a number of different techniques, output queueing, input queueing, and virtual output queues. And we'll see and get a sense for how these packet switches are actually built. 
本视频是我们关于数据包交换机工作原理的第一个视频的延续。在第一个视频中，我们看到数据包交换有两种基本操作。首先，必须在转发表中查找数据包的地址，然后必须将数据包交换或转移到正确的输出端口，以便可以在正确的出站链路下发送。在上一个视频中，我们看到了如何在表中查找以太网交换机和互联网路由器的地址。在这个视频中，我将解释数据包如何切换到正确的出口端口。我将介绍许多不同的技术，输出队列、输入队列和虚拟输出队列。我们将看到并获得这些分组交换机实际构建方式的感知。

发言人   00:42
I'm going to start with the sort of the basic vanilla switch, which is the 1 I showed you before. We have the address lookup on the left over here. And then on the, this is the forwarding table where we look up the addresses, and then we have the packet cueing and then the buffer memory where the packets are held during times of congestion. 
我将从基本的香草开关开始，也就是我之前向您展示的1。我们在这里的左边有地址查找。然后在上面，这是转发表，我们在那里查找地址，然后我们有数据包提示，然后有在拥塞时保存数据包的缓冲内存。


发言人   01:05
When packets arrive, here are three packets arriving with different egress ports indicated by the color of the header of the packet. So the red one at the top is going to the red port over here, the one in the middle. So when these packets traverse the backplane, we see that the blue 1 is able to go to it's output. One of the red ones can be delivered immediately and the other one is held in the output queue waiting for its turn. So as soon as the first two have left, this one can then the in FIFO order, We often refer to a switch like this as an output cube switch because the cues are at the output. 
当数据包到达时，这里有三个数据包到达，它们使用不同的出站端口，由数据包标头的颜色指示。顶部的红色将会到达这里的红色端口，中间的那个。因此，当这些数据包穿过背板时，我们看到蓝色的1能够到达它的输出。其中一个红色的可以立即交付，而另一个则被保留在输出队列中等待轮到它。因此，一旦前两个开关离开，这个开关就可以按FIFO顺序排列，我们经常将这样的开关称为输出立方体开关，因为提示在输出处。

发言人   01:46
And, this is a certain ramification for the for the performance of the switch. Let's take a look at that. When we have packets arriving, it's possible in the worst case that all the packets coming in at the same time from the outside will be wanting to go to the same output queue, let's say this one here. So if we have n ports each running at rate r, and there are, let's say, there are n of them, then in the worst case, we could actually have a writing rate of n times r into this output que similarly, and we always have a reading rate from this query rate r, so, so we say in the output queue switch that this memory must run an aggregate, a total rate of up to n plus one times r? The somewhat annoying thing or frustrating thing about this is that long term, it can't possibly be the case that we're writing into this Q at rate n times r, the system could not sustain that. This only really works if some mechanism is at play, like congestion control, to hold the average rate of writing into this queue at no more than one hour. So it feels as though the maximum rate that we should need is two times r, that was what we would stra for. Unfortunately, we're paying this penalty of n and n could be a larger number, it could be hundreds or even thousands, so this memory has to run much faster outs output CS, which is said to be limited by this problem that they have to have memories that run very, very fast. 
并且，这是交换机性能的一个特定分支。让我们来看看这个。当数据包到达时，在最坏的情况下，所有从外部同时进来的数据包都可能想要去同一个输出队列，比如说这个。因此，如果我们有n个端口以速率r运行，并且有n个端口，那么在最坏的情况下，我们实际上可以将n倍的写入速率写入到这个输出quque中，并且我们总是从这个查询速率r中获得读取速率，因此，我们在输出队列开关中说，这个内存必须运行一个聚合，总速率高达n加上一次r？有点烦人或令人沮丧的是，从长远来看，我们以n倍r的速率写入这个Q是不可能的，系统无法维持这种情况。只有在某些机制 (如拥塞控制) 起作用时，这才会真正起作用，这些机制可以将写入此队列的平均速率保持在不超过一个小时的水平。所以感觉好像我们应该需要的最大速率是r的两倍，这就是我们要寻找的。不幸的是，我们正在支付n的罚款，而n可能是一个更大的数字，可能是数百甚至数千，因此这个内存必须运行得更快，输出CS，据说这是受到这个问题的限制，他们必须拥有运行非常非常快的内存。

发言人   03:23
And it becomes quite a challenge when building scalable output cube switches to find or use memories or create a memory hierarchy that will run fast enough. One obvious way to solve this problem is to move the cues from the output over to the input. Let's take a look at what happens when we do this. For obvious reason, we call this input cubed packet switch. 
当构建可扩展的输出立方体开关以查找或使用内存或创建运行速度足够快的内存层次结构时，这成为了一个相当大的挑战。解决这个问题的一个明显方法是将输出中的提示转移到输入中。让我们来看看当我们这样做时会发生什么。出于显而易见的原因，我们将此称为输入立方数据包交换机。


发言人   03:47
Now, the queues where packets will be held are at the input side of the switch. The advantage of this will perhaps be obvious in a moment if we consider packets arriving to the switch same pattern as before two reds, one blue. In this case, what we would do is all of the packets would come through the switch. Only one of them needs to be held, the one down here waiting for its turn to go across the switch, and that's because its output line is busy and there's no cue at the output to hold it, so we hold it back at the input, and then later when its turn comes, it can depart just like it would from an output cube. So show on the face of it. The good news is that things look like they work the same, and the better news is that the buffer memory here is now when he has to accept one packet at most one packet from the ingross at a time, and has to only send one packet into the switch in a packet time. So its speed has been reduced from n plus one times r just down to our minimum, and our goal, which was two times r, so a factor of almost n reduction. So this makes a huge difference. 
现在，将保存数据包的队列位于交换机的输入端。如果我们考虑到到达交换机的数据包与之前相同的模式，那么这样做的优势可能很快就会很明显了。在这种情况下，我们要做的是所有的数据包都要通过交换机。只有其中一个需要保持，这里的一个等待轮到它穿过交换机，这是因为它的输出线繁忙，输出没有提示可以保持它，所以我们在输入时保持它。然后当轮到它的时候，它可以像从输出多维数据集一样离开。所以表面上表现出来。好消息是，看起来它们的工作方式是一样的，而更好的消息是，这里的缓冲区内存现在是他必须一次最多接受一个来自入口的一个数据包，并且在一个数据包时间内只发送一个数据包到交换机的时候。因此，它的速度已经从n加1倍r降低到我们的最小值，我们的目标是r的两倍，因此几乎减少了n倍。这就产生了巨大的差异。

发言人   04:59
And for this reason, people often say that input CD switches are much more scalable. And indeed, quite a few big switches are made this way, but with a caveat. 
由于这个原因，人们经常说输入CD开关更具可扩展性。确实，有相当多的重大开关是这样做的，但有一个警告。

发言人   05:08
And there is a problem that we're going to have a take a look at right now in an input c-ute switch, the problem is something called head of line blocking. And this problem is something that you'll see in many contexts, So I want to explain it here so you'll recognize it when you see it in other environments. Let me go through an example. These are three inputs representing the inputs of the switch, so these are the input buffers I've taken away of everything else on the switch, just to make it a little bit clearer, and we're going to see packets arrive to these. Here they are, they red ones going to the red output, black ones to the black output, green ones to the green output. And imagine that you have the task of deciding which packets to go, and you look at the packets at the head of line of this and see that they're all red problem is that you could only send one of them at a time. And so in this particular instance, we'd only be able to send the red one, even though there are green and black packets in the system that could go to these unused outputs, because we've arranged everything as a single queue, we get this head of line blocking effect, natural solution to this, which is pretty widely used, is something called virtual output queues, where each input maintains a separate cue for each output. 
在输入c-ute开关中，有一个问题我们现在要看一看，这个问题叫做线头阻塞。这个问题你会在很多情况下看到，所以我想在这里解释一下，这样你在其他环境中看到它时就会认出它。让我举个例子。这些是代表交换机输入的三个输入，所以这些是输入缓冲区，我已经拿走了交换机上的所有其他内容，只是为了更清楚一些，我们将看到数据包到达这些输入缓冲区。这里是红色的输出到红色的输出，黑色的输出到黑色的输出，绿色的输出到绿色的输出。并想象一下，你有一个决定要发送哪些数据包的任务，你看着这个头部的数据包，发现它们都是红色问题，你一次只能发送其中一个。因此，在这个特定情况下，即使系统中有绿色和黑色的数据包可以发送到这些未使用的输出，我们也只能发送红色的数据包，因为我们将所有内容安排为单个队列，我们会得到这种头线阻塞效应。这个问题的自然解决方案，被广泛使用的是虚拟输出队列，其中每个输入为每个输出维护一个单独的提示。



发言人   06:31
So in this case, we have a 3 by 3 switch. So this q here is a FIFO q of packets waiting to go to output 1, the red output for output 2 and for output 3. So when packets arrive here are the same set of packets arriving as before, but now they get preg class-size and place into a queue corresponding to the output they're going to. That's why we call them virtual output. 
所以在这种情况下，我们有一个3乘3的开关。所以这里的q是一个FIFO q的数据包，等待发送到输出1，红色输出用于输出2和输出3。因此，当数据包到达这里时，到达的数据包与以前相同，但是现在它们获得了preg类大小并放入与它们将要输出相对应的队列中。这就是为什么我们称它们为虚拟输出。

发言人   06:55
Heres it's a queue of packets going to all going to the same output. The good news now is that because each queue holds packets going to the same output, no packet can be held up by a packet ahead of it going to a different output. So it can't be held up because it's head of line is blocked by someone who is stuck. So now the we can look at this and say, aha, we have visibility into all of the head of line packets and we can deliver all three in one go and therefore get a higher instantaneous throughput. It's an obvious solution. It can be a little tricky to implement in practice, but the nice, the nice thing is that it overcomes this head of line blocking entirely. So the good news overall is we've reduced the speed of the cues to two times a speed of the memories because remember, we can only have one packet come in at a time and only one packet to power at a time. And we're able to sustain the same throughput performance as before. 
这里是一个数据包队列，所有数据包都将发送到相同的输出。现在好消息是，因为每个队列都包含前往相同输出的数据包，所以数据包在前往不同输出之前不能被数据包所阻碍。所以它不能被卡住，因为它的头被卡住的人挡住了。所以现在我们可以看看这个并说，啊哈，我们可以看到所有的线头数据包，我们可以一次性交付所有三个数据包，从而获得更高的瞬时吞吐量。这是一个显而易见的解决方案。在实践中实现它可能有点棘手，但好处是它完全克服了这种线路阻塞。所以总的来说，好消息是我们已经将提示的速度降低到了记忆速度的两倍，因为请记住，我们一次只能有一个数据包进入，一次只能有一个数据包上电。我们能够保持与以前相同的吞吐量性能。

发言人   07:57
Just to look at this on a graph, we often see graphs that look like this. This is a, this is a plot of the delay or the average delay that a packet would experience as a function of the load. This is basically how busy the ingross lines are. The best that any queueing system can achieve is this line here. And this corresponds to a system in which as the load approaches 100%, the delay increases or the average delay increases and is asymptotic to 100%. In fact, this is what we will see with an output cube switch. An output cube switch is perfect in the sense that you can't achieve a higher throughput or you can't achieve a lower average delay. 
只是为了在图表上看这个，我们经常看到看起来像这样的图表。这是延迟或数据包作为负载的函数所经历的平均延迟的绘图。这基本上就是入口线有多忙。任何排队系统可以实现的最好结果就是这条线。这对应于一个系统，当负载接近100% 时，延迟增加或平均延迟增加并且渐近于100%。事实上，这就是我们将看到的输出立方体开关。感知无法实现更高的吞吐量或更低的平均延迟，输出立方体开关是完美的。


发言人   08:41
Let's take a look at the main properties of output CE switches. First, we say they are work conserving. Work conserving means that an output line is never idle when there is a packet in the system waiting to go to it. That means there's no blocking internally preventing a packet getting to that line. Whenever that line is is idle. There is no packet in the system waiting for it. As a consequence, throughput is maximized because you cannot have a higher throughput than keeping all the lines busy whenever there's a packet available for them, and the expected delay is minimized because we're always doing useful work delivering packets under the outgoing line. 
让我们来看看输出欧洲合格认证开关的主要属性。首先，我们说他们在节约工作。节约工作意味着当系统中有数据包等待传输时，输出线永远不会空闲。这意味着内部没有阻止阻止数据包到达该线路。每当那条线路空闲时。系统中没有等待的数据包。因此，吞吐量会最大化，因为当有数据包可用时，您不能拥有比保持所有线路繁忙更高的吞吐量，并且预期的延迟会最小化，因为我们总是在做有用的工作，在传出线路下传送数据包。



发言人   09:21
Just to recap the performance that we suffer from with head of line blocking, this was our perfect output switch, output Ced switch here on the right. This nice performance here with head of line blocking. 
只是为了回顾一下我们因线头阻塞而遭受的性能，这是我们完美的输出开关，右侧的输出Ced开关。这里带有线头阻塞的出色性能。

发言人   09:32
It's a well known result that the throughput can be reduced. In other words, this asymptote when things fall apart, gets reduced to 2 -2 minus square root of 2, or approximately 58%. So we lose almost half the performance of the system as a consequence of this head of line blocking. The actual number will vary depending on the particular arrival pattern, but in general, it's pretty bad news. But if we use virtual output queues, this 58% gets pushed back up again to the full 100% of the system. It doesn't entirely match the output CD switch. The asymptote will still be 100% over here. Actually, with virtual output cues, the delay will be slightly higher, but the asymptote is to 100%. 
一个众所周知的结果是吞吐量可以降低。换句话说，当物体分崩离析时，这个渐近线会减小到2-2减去2的平方根，即大约58%。因此，由于这种线路阻塞，我们失去了几乎一半的系统性能。实际数量将根据特定的到达模式而有所不同，但总的来说，这是一个相当糟糕的消息。但是如果我们使用虚拟输出队列，这58% 会再次被推送回系统的全部100%。它不完全匹配输出CD开关。这里的渐近线仍然是100%。实际上，使用虚拟输出提示时，延迟会稍高一些，但渐近线为100%。


发言人   10:20
I'd like to say a few last words about virtual output queues. 
我想就虚拟输出队列说几句最后的话。


发言人   10:23
Virtual output cues are actually used very widely, and you may even have noticed them when driving on the street. So in the us where we drive on the right, it's very common to have a left hand turn lane like the one that's shown here. This is to hold cars that are arriving and that are blocked because of cars coming the other way. So these ones are blocked and can't turn left until there's nothing coming the other way. However, cars in this lane here can keep going straight on or can turn right. They are not held up or blocked because of a packet ahead of it going to an output that in this over here, which is temporarily unavailable. So in cars in countries where you drive the on the left hand side, then a right hand hand lane is quite common as well. So next time you're driving and you see one of these, just remember this is virtual output queued. 
虚拟输出提示实际上被广泛使用，你甚至可能在街上开车时注意到它们。所以在我们靠右边行驶的美国，左转车道是很常见的，就像这里展示的那样。这是为了容纳那些正在到达的车辆，并且由于汽车从另一边驶来而受阻的车辆。所以这些被封锁了，不能向左转，直到另一边没有任何东西。然而，这条车道上的车辆可以继续直行或右转。它们不会因为数据包到达此处暂时不可用的输出而被阻塞。所以在你驾驶左侧的国家的汽车中，右手车道也很常见。所以下次你开车时看到其中一个，请记住这是虚拟输出排队。

发言人   11:14
So in summary, we've seen that packet switches perform two basic operations. They look up addresses in a forwarding table. We saw examples of that in the last video for Ethernet switches and for Internet routers. And they also, once they're decided where a packet is going, they have to switch it, they have to deliver it to the correct egress port so it can go under the correct output link. 
总之，我们已经看到分组交换机执行两个基本操作。他们在转发表中查找地址。在上一个视频中，我们看到了以太网交换机和互联网路由器的例子。而且，一旦他们决定了数据包的去向，他们就必须将其切换，必须将其传送到正确的出口端口，以便它可以在正确的输出链路下传输。

发言人   11:35
The simplest and slowest switches use output queueing because this maximizes the throughput and minimizes the expected delay of packets, whereas more scalable switches often use input queues with virtual output queues to maximize the throughput. That's the end of this video. 
最简单和最慢的交换机使用输出队列，因为这可以最大化吞吐量并最小化数据包的预期延迟，而更具可扩展性的交换机通常使用具有虚拟输出队列的输入队列来最大化吞吐量。这就是这个视频的结尾。